{
  "id": "arxiv_2502.21313v1",
  "text": "Unsupervised Parameter Efficient Source-free Post-pretraining\nAbhishek Jha1*\nTinne Tuytelaars1\nYuki M. Asano2\n1ESAT-PSI, KU Leuven\n2Fundamental AI Lab, University of Technology Nuremberg\nAbstract\nFollowing the success in NLP, the best vision models are\nnow in the billion parameter ranges. Adapting these large\nmodels to a target distribution has become computation-\nally and economically prohibitive. Addressing this chal-\nlenge, we introduce UpStep, an Unsupervised Parameter-\nefficient Source-free post-pretraining approach, designed to\nefficiently adapt a base model from a source domain to\na target domain: i) we design a self-supervised training\nscheme to adapt a pretrained model on an unlabeled target\ndomain in a setting where source domain data is unavail-\nable. Such source-free setting comes with the risk of catas-\ntrophic forgetting, hence, ii) we propose center vector regu-\nlarization (CVR), a set of auxiliary operations that minimize\ncatastrophic forgetting and additionally reduces the com-\nputational cost by skipping backpropagation in 50% of the\ntraining iterations. Finally iii) we perform this adaptation\nprocess in a parameter-efficient way by adapting the pre-\ntrained model through low-rank adaptation methods, result-\ning in a fraction of parameters to optimize. We utilize var-\nious general backbone architectures, both supervised and\nunsupervised, trained on Imagenet as our base model and\nadapt them to a diverse set of eight target domains demon-\nstrating the adaptability and generalizability of our pro-\nposed approach.\n1. Introduction\nMany recent advancements in computer vision like genera-\ntive models [1, 47], multi-modal models [35, 45], and learn-\ning generalized representations [11, 17], can be attributed to\nlarge models trained on large diverse datasets [17, 54, 55].\nThese datasets typically contain natural images, collected\nby scraping the open-web, and hence models trained on\nsuch corpora tend to capture a generalized representation\nof the natural scenes. The motivation behind training these\ngeneralized representations is to transfer them to niche do-\nmains by adapting the parameters of these models for the\nrespective target tasks. However, the size of recent visual\nmodels, reaching up to 22B parameters [11, 50] makes them\n*email:abhishek.jha@esat.kuleuven.be\ncomputationally and economically expensive to finetune,\nwhile being prohibitive to (re)train, for the larger part of\nthe research community. Recent work in parameter-efficient\ntraining of large language models and large vision models\nlike low-rank adaptation (LoRA) [25], and prompt learning\n[37], have shown effectiveness in adapting large language\nmodels by modifying a small subset of model parameters.\nWhile there has been some prior work on utilizing these\nadapters in domain adaptation [66], they are often limited\nto similar target domains.\nAnother challenge in adapting these models to a new tar-\nget task is to generate enough samples from the target do-\nmain along with their respective task specific labels, as such\nlabeling tasks can be expensive and prone to human error\nat scale [42, 56]. Self-supervision as the training task has\nshown to learn a generalized representation of the input dis-\ntribution, that is easily transferred to a task in that domain.\nHowever, the use of such objectives for unsupervised target\ndomain-adaptation is under-explored [58].\nAdapting a pretrained model to a new data distribution\nleads to forgetting of previous knowledge, also known as\ncatastrophic forgetting [10]. For large foundation models,\nthe source domain is usually big. While including the entire\nsource domain data is computationally prohibitive, includ-\ning a subset requires coming up with sampling strategies, as\nin replay buffer methods [10]. In this work, we consider the\nsource-free setting, where the model doesn’t have access to\nthe source domain data.\nWe approach these challenges by proposing an Unsu-\npervised Parameter-efficient Source-free post-preTraining,\nwhich we call UPST(ep), or UpStep. Specifically, in this\npaper:\ni) we adapt a general model pretrained on a large source\ndomain dataset, Imagenet [32] or WIT-400M [45] to a di-\nverse set of target domains, using a self-supervised objec-\ntive. We train a projector on top of the base model to project\nit into a lower dimensional space, where we perform online\nclustering.\nii) We propose a set of regularizers, collectively called\nCenter vector regularization (CVR), based on variance max-\nimization of the representation. We enforce this by mini-\nmizing the expected representation of the batch, called the\n1\narXiv:2502.21313v1  [cs.CV]  28 Feb 2025\n\n\ncenter vector [26]. CVR contains CV term as an auxiliary\nloss to the main self-supervised objective; a learning rate\nmultiplier to promote the parameter updates when the vari-\nance is high and reduce its effect when the variance is low;\nand CV conditional gate, which skips the training when the\nvariance decreases.\niii) We use low-rank adaptation technique to adapt our\ngeneral source domain models to the target domain.\nFor evaluation, we employ a diverse set of datasets,\nshowcasing the generalization capacity of our proposed ap-\nproach, while building on different general pretrained mod-\nels showcasing the adaptability of our model.\n2. Method\nOur proposed approach, UpStep, adapts a pretrained visual\nmodel to a target domain efficiently, without requiring ac-\ncess to source domain and without requiring target labels.\nThe aim of our method is to extend the utility of generalized\npretrained representations by tailoring them to a target dis-\ntribution instead of a specific target task. Since the primary\nobjective is domain adaptation rather than task-specific fine-\ntuning, we refer to this process as “post-pretraining,” in that\nthe model’s representation is the key output of our method,\nwhich can then be further refined for a given downstream\ntask. The following sections provide a detailed description\nof our approach.\n2.1. Self-supervised Objective\nWe begin with a base model, pretrained on a general\nsource domain, such as ImageNet [32]. Models pretrained\nwith self-supervised learning (SSL) objectives have shown\nstrong generalization capabilities; thus, we employ pre-\ntrained SSL encoders as our base model.\nTo further train this model on a target dataset, we utilize\na two-stream (online-offline) self-supervised clustering-\nbased approach, similar to SwAV [4]. To maintain simplic-\nity and efficiency, we employ fixed cluster centers, which\nare uniformly sampled on the unit hypersphere [4], thereby\nreducing the training cost and minimizing the risk of model\ncollapse.\nLet x represent a sample from the target domain. We\ngenerate two augmented views, xs and xt. These views\nare processed by the online and offline streams respectively,\nwhich share identical architectures, each consisting of an\nencoder initialized with the base model, followed by a ran-\ndomly initialized projector and a fixed set of cluster centers\nshared by both streams, as shown in Fig. 1\nThe encoded representations of xs and xt, denoted as\nf(xs) and f(xt), are further projected to obtain z(xs) and\nz(xt), which are then assigned to clusters based on sim-\nilarity to the fixed prototypes. The offline stream’s clus-\nter assignments pt are regularized using Sinkhorn-Knopp\nequipartitioning to enforce uniform distribution across clus-\nters:\npt = SinkhornKnopp(z(xt) · C)\n(1)\nwhere C is the matrix of fixed cluster prototypes.\nFinally, we compute a cross-entropy loss between the on-\nline and offline assignments:\nLCE = −\nX\nk\np(k)\nt\nlog p(k)\ns ,\n(2)\nwhere ps represents the online cluster assignments of\nz(xs).\nThis loss is backpropagated through the online\nstream to update the encoder and projector weights, while\nthe offline stream is updated by the weights of the online\nstream after each iteration.\n2.2. Loss Regularization via Center vector\nFurther training a pretrained model on a target domain can\nresult in unstable training loss, potentially degrading the\npretrained knowledge. This is particularly an issue for small\ntarget distributions lacking sufficient diversity. To address\nthis, we regularize the learning rate for the online network\nusing batch statistics of the training samples.\nFollowing [26, 63], we define the center vector (CV), sk,\nas the average feature vector after normalization across the\nbatch,\nsk = 1\nN\nN\nX\nt=1\nf(xs)\n∥f(xs)∥,\n(3)\nwhere N is the batch size, and k indexes the training\niteration. Training stability is inversely related to the mag-\nnitude of ∥sk∥, which measures the batch’s diversity, when\nthe representations are lying on the unit hypersphere. Here,\nwe define our auxiliary CV loss:\nLCV =\n\r\r∥sk∥−sϕ\n\r\r\n(4)\nwhere sϕ is a constant hyperparameter ∈[0, 1], which con-\ntrols the variance of the features on the unit sphere.\nFurthermore, the learning rate η is adjusted for each\nbatch iteration by CV learning rate regularization as fol-\nlows:\nη = η0 × (1 −∥sk∥),\n(5)\nwhere η0 is the base learning rate. This learning rate\nregularization strategy, inspired by Catastrophic Forgetting\nMeasurement (CFM) introduced in PADCLIP [34], helps\n2\n\n\nLoRA\nLoRA\nLoRA\nLoRA\nLoRA\nLoRA\nx\nOffline\nOnline\nP\nP\nQ\nQ\nSwapped \nPrediction\nFixed \nprototypes\ncodes\nprojection\nc\nCenter vector \nconditional gate\nViT Block LoRA tuning\nAdd & Norm\nAttention\nQ      K      V\nAdd & Norm\nFeed \nForward\nInput Embedding\nLoRA\nLoRA\nViT blocks\nCenter Vector Regularization\nCV loss\nFigure 1. Proposed UpStep architecture: During training, we train the pass the augmented view of the input images through the online\nand offline streams. These online and offline are identical in architecture consisting of LoRA adapted pretrained Base models. The encoded\nrepresentations are projected to the prototype space where an online clustering loss is applied. We apply an auxiliary loss, learning rate\nregularization and a gating mechanism to skip training for certain iterations conditioned upon the magnitude of center vector, as shown by\nthe shaded region, Center vector regularization. For each layer in ViT model, we adapt the QKV matrices and the projection layers. During\nevaluation, we only use the LoRA adapted target domain base network in ensemble with the source domain base model.\nstabilize training, especially for less diverse datasets. How-\never, unlike CFM, which requires the access to source\ndata, Center vector learning rate regularization only re-\nquires samples from target domain.\nFinally, we introduce a gating mechanism that allows\ngradients to backpropagate only if the center vector magni-\ntude minimization condition is met. Specifically, the gradi-\nent is propagated when the current iteration’s center vector\nmagnitude ∥s∥is lower than that of the previous iteration.\nHence, our UpStep objective is given by:\nLUpStep =\n(\u0000LCE + LCV\n\u0001\n,\nif ∥sk∥< ∥sk−1∥\n0,\notherwise\n(6)\nThis encourages the network to update weights only when\nthe variance of the samples in the feature space is high.\n2.3. Low-Rank Adaptation\nTo perform resource-efficient post-pretraining, we adapt the\nencoder using Low-Rank Adaptation (LoRA) layers applied\nto the Query-Key-Value (QKV) projection matrices in the\nself-attention and projection modules of each transformer\nlayer. This structured adaptation allows for flexible and ef-\nficient fine-tuning, as only essential components are mod-\nified, making it feasible to adapt large models like VFMs\nwithout updating the full set of parameters.\n2.4. Efficient Model Ensemble\nFinally, during evaluation, we use the ensemble of the base\nencoder model and the UpStep encoder model by concate-\nnating the features from both models. This approach re-\nduces the problem of catastrophic forgetting, which is es-\npecially critical when the target domain differs significantly\nfrom the source. Separately, we show that enforcing center-\nvector loss minimization during training helps reduce for-\ngetting of the source domain knowledge, resulting in lesser\ndependence on the source domain model within the ensem-\nble.\nLet fbase(x) and fUpStep(x) denote the feature represen-\ntations from the base and UpStep models, respectively. The\nfinal feature vector used for evaluation is given by:\nfensemble(x) = [fbase(x); fUpStep(x)]\n(7)\nThis concatenated representation leverages both models’\nknowledge, balancing adaptation to the target domain and\nretaining essential information from the source domain.\n3. Experiments\n3.1. Datasets\nFor the source domain, we use the ImageNet-1k dataset\n[32], a widely recognized benchmark for pretraining vi-\n3\n\n\nMethod\n% Train Params CIFAR-10 CIFAR-100 DTD EuroSAT Flowers102 Oxford Pets SUN397 UCF101 Avg\nPretrained model: ViT-B/32CLIP\nBase Model (k-NN)\n0%\n0.909\n0.694\n0.666\n0.858\n0.818\n0.768\n0.687\n0.753\n0.769\nBase Model (lin)\n0%\n0.946\n0.795\n0.723\n0.950\n0.907\n0.887\n0.755\n0.828\n0.849\nMask (lin) [58]\n100%\n0.971\n0.834\n0.738\n0.978\n0.973\n0.891\n0.668\n0.815\n0.858\nUpStep (k-NN)\n4.5%\n0.960\n0.801\n0.714\n0.965\n0.885\n0.826\n0.712\n0.790\n0.832\nUpStep (lin)\n4.5%\n0.966\n0.843\n0.749\n0.972\n0.930\n0.901\n0.761\n0.845\n0.871\nTable 1. Comparison of Top-1 Accuracy and Training Parameters (Train Params) for Downstream Task. We report the percentage of\ntraining parameters required for each method and the top-1 accuracy across eight image classification benchmarks, with k-NN classification\nand linear probing. The final column shows the average accuracy across all datasets.\nsual models, consisting of 1.2 million training samples of\nnatural images. For evaluating the target domain adapta-\ntion, we employ a diverse set of datasets. The CIFAR-10\nand CIFAR-100 datasets [31] provide 50,000 training sam-\nples each, containing 10 and 100 classes respectively, and\nrepresent general object categories. EuroSAT [23], which\nis focused on satellite imagery, includes 27,000 training\nsamples across 10 classes, while SUN397 [60], covering a\nbroad range of scene types, has 19,850 training samples and\n397 classes. Flowers102 [41], representing various flower\nspecies, contains 1,020 training samples spanning 102 cat-\negories. UCF101 [52], focused on action recognition, has\n13,000 samples with 101 classes. DTD [7], designed for\ntexture analysis, comprises 1,880 training samples across 47\nclasses, and Oxford Pets, which includes images of differ-\nent pet breeds, has 200 training samples across 37 classes.\nThis diverse selection of datasets allows us to comprehen-\nsively assess the generalization capability of our approach\nacross different visual domains, testing its robustness and\nadaptability to new and varied distributions.\n3.2. Implementation Details\nWe initialize our model with ImageNet-pretrained models\nas the base. For the projector network, we employ a ran-\ndomly initilized two-layer MLP with a hidden dimension of\n2048 and an output dimension of 128, using ReLU as the\nactivation function between layers. A total of 3000 clus-\nter prototypes are utilized, which are uniformly sampled on\nthe unit hypersphere to ensure balanced cluster distribution.\nThe training is conducted with a batch size of 160, and un-\nlike SwAV, we do not use a queue mechanism. For the cen-\nter vector loss, we use sϕ = 0.5, and s0 is initilized as 1.\nIn applying the Sinkhorn-Knopp [4] algorithm, we set\nϵ to 0.3 and perform 3 iterations to achieve equipartition-\ning of the clusters. Both the encoder and projector modules\nare trained with a learning rate of 0.03, optimized using the\nAdam optimizer for stable convergence. For evaluation, we\nemploy k-nearest neighbors (k-NN) classification, setting\nk = 20 to determine the neighborhood size. We use the\nHugging Face implementation of LoRA with the following\nparameters: the rank of the LoRA matrix is set to 16, while\nthe scaling factor for the low-rank matrices is set to 1. Ad-\nditionally, no dropout is applied to the LoRA matrix.\n3.3. Comparison against Baselines\nTo assess the quality of the target representation, we evalu-\nate it on the downstream task of classification. Since our ap-\nproach builds upon the base model, the primary baseline is\nthe ImageNet-1k pretrained model. Additionally, we com-\npare our method against the approach proposed by Warmer-\ndam et al. [58], which trains a unsupervised masking net-\nwork on top of the base model to adapt it to the unlabeled\ntarget domain, providing a strong baseline for the compari-\nson.\nAs shown in Tab. 1, our method achieves competitive\nperformance across all datasets compared to both the base-\nlines, while offering the additional advantages of much re-\nduced training parameters when compared against Warmer-\ndam et al. [58].\n3.4. Evaluating Different Base Models\nWe evaluate our method on a range of base models to as-\nsess its generalization capability across diverse pretrained\nrepresentations, while using k-NN classification evaluation.\nSpecifically, we use Imagenet-1k Supservised ViT-Base16\n[12], DINO-pretrained models [5]: ViT-Small16 and ViT-\nBase16, CLIP ViT-Base32 [45], and DINOv2R [9] as our\nbase models.\nAs shown in Tab. 2, our method demon-\nstrates strong performance across different datasets and pre-\ntrained models, often outperforming the baselines. These\nresults demonstrate that UpStep can be effectively applied\nto different based models pretrained with supervised, self-\nsupervised or multimodal objectives.\n3.5. Ablation Study\nWe perform an ablation study to understand the importance\nof different components in our method. Specifically, we ab-\nlate the center vector regularization (CV Loss, learning rate\nregularization, and CV conditioned gating), and the ensem-\nble of the base model and the UpStep model. As shown in\n4\n\n\nMethod\n% Training Parameters CIFAR-10 CIFAR-100 DTD EuroSAT Flowers102 Oxford Pets SUN397 UCF101\nPretrained model: ViT-BaseSupervised\nBase Model\n0%\n93.77\n77.26\n66.54\n91.30\n95.10\n90.59\n64.10\n73.96\nUpStep (our)\n5.1%\n97.81\n89.56\n72.60\n97.0\n98.66\n92.12\n67.64\n79.56\nPretrained model: DINO ViT-Small\nBase Model\n0%\n95.03\n79.72\n71.27\n95.39\n80.19\n90.81\n59.26\n73.16\nUpStep (our)\n7.5%\n96.96\n85.11\n70.26\n97.80\n89.26\n91.52\n59.74\n75.23\nPretrained model: DINO ViT-Base\nBase Model\n0%\n96.43\n82.87\n71.12\n95.19\n82.69\n89.64\n61.09\n75.41\nUpStep (our)\n5.1%\n97.97\n87.51\n71.01\n97.61\n89.59\n90.40\n61.55\n77.05\nPretrained model: DINOv2R\nBase Model\n0%\n97.39\n86.09\n75.05\n92.0\n99.41\n93.24\n70.2\n79.3\nUpStep (our)\n10.6%\n98.36\n89.94\n76.64\n97.06\n99.51\n93.45\n71.59\n81.25\nTable 2. Performance Comparison of Base Model and UpStep on Various Pretrained Models. We report the top-1 accuracy for eight\nimage classification benchmarks using different pretrained base models: Supervised ViT-Base/16 [12], DINO ViT-Small/16, DINO ViT-\nBase/16 [5], and DINOv2R [9]. UpStep (our) demonstrates improved performance across all datasets.\nTab. 3, each design choice contributes significantly to the\nperformance of our method. It should be noted that while\nadding CV conditioned gating reduces the performance of\nBase only naive adaptation from 85.02% to 84.9%, it re-\nduces the total number of training iterations by 50%, see\nSec. 4.4, resulting in a more efficient training scheme. Most\nperformance gain can be attributed to CV Loss and en-\nsembel, both of these operations minimize the catastrophic-\nforgetting, as discussed in detail in Sec. 4.2 and Sec. 2.4 re-\nspectively. Learning rate regularization also promote min-\nimization of center vector magnitude, hence results in an\nimprovement over naive Base only model. Overall, we ob-\nserve that the center vector regularization and ensemble is\ncrucial for faster convergence and minimization of catas-\ntrophic forgetting of the source domain knowledge.\nConfig\nBase\nCV\nEnsemb Acc %\nLR Reg Cond Loss\nBase Only\n✓\n85.02\nBase + CV Cond\n✓\n✓\n84.90\nBase + CV LR Reg\n✓\n✓\n85.28\nBase + CV Loss\n✓\n✓\n87.51\nBase + CV (All)\n✓\n✓\n✓\n✓\n87.75\nUpStep\n✓\n✓\n✓\n✓\n✓\n89.26\nTable 3. Ablation Study of UpStep Components. We evaluate\nthe impact of each component, including the LoRA base model\n(Base), CV learning rate regularization (LR Reg), CV conditioned\ngating (Cond), CV loss (Loss), and ensemble (Ensemb), on Flow-\ners102 dataset. Checkmarks indicate the components present in\neach configuration, with accuracy (Acc) reported for each combi-\nnation.\n3.6. Can we make the number of trainable param-\neters even lower?\nOur method uses Low-Rank Adaptation (LoRA) to effi-\nciently adapt weight matrices by training low-rank matri-\nces A and B with an intermediate rank, reducing trainable\nparameters compared to full fine-tuning. Vector-based Ran-\ndom Matrix Adaptation (VeRA) [30] builds on LoRA by\nfreezing a single pair of low-rank matrices, shared across\nall layers, and introducing layer-specific trainable scaling\nvectors. These scaling vectors enable each layer to be ad-\njusted independently while sharing the same low-rank basis,\ndrastically lowering trainable parameters.\nLike LoRA, VeRA’s trained scaling vectors can be\nmerged into the original weights after training, adding no\ninference latency.\nWhen employed our method as the\nadapter, VeRA can further reduce parameter requirements\nfrom 7.9% to 3.1% for ViT-Base/16 model), thereby en-\nhancing efficiency for large models or resource-limited set-\ntings with comparative performance. We compare the per-\nformances of UpStep-LoRA and UpStep-Vera versions of\nour approach in Tab. 4.\n4. Analysis and Discussion\n4.1. Correlation between performance and Center\nvector magnitude\nOne of the key components of our method is the center vec-\ntor regularization. Hence it is important to understand how\nthe center vector behaves during post-pretraining on the tar-\nget domain. To this end we analyze the training time center\nvector magnitude and the k-NN classification performance\nof the model without the center vector loss and ensemble on\nthe target domain test set. We observe that, in the absence\nof center vector loss, the center vector magnitude increases\n5\n\n\nFigure 2. Impact of Center Vector Magnitude on Model Performance. Higher center vector magnitudes correlate with reduced k-NN\naccuracy for the majority of the datasets, underscoring the stabilizing role of center vector regularization.\nDataset\nUpStep-LoRA\nUpStep-VeRA\nTraining Parameters\n7.9%\n3.1%\nParameters to store\n5.5%\n0.03%\nCIFAR-10\n96.96\n96.51\nCIFAR-100\n85.11\n83.07\nDTD\n70.26\n70.10\nEuroSAT\n97.80\n97.07\nFlowers102\n89.26\n88.06\nOxford Pets\n91.52\n91.82\nSUN397\n59.74\n59.27\nUCF101\n75.23\n73.98\nAvg\n83.24\n82.49\nTable 4.\nPerformance Comparison of UpStep-LoRA and\nUpStep-VeRA Across Datasets. We evaluate the accuracy of Up-\nStep with LoRA and VeRA on various datasets, highlighting the\neffectiveness of each approach in adapting to different domains\nwith efficient parameter usage. Avg shows average performance\nover all datasets.\nduring training which is in line with similar findings on the\nself-supervised pretaining task by Jha et al. [26]. They at-\ntribute stability of the self-supervised pretraining to center\nvector magnitude minimization, which explains our obser-\nvation for the under-performance of the model on the tar-\nget domain in the absence of center vector loss. To further\nvalidate this, we also analyze the correlation between the\ncenter vector magnitude and the k-NN classification perfor-\nmance of the model on the target domain test set. From\nFig. 2 we observe a negative correlation between center\nvector magnitude and k-NN classification performance in\nDifference in k-NN accuracy of the model with and without Ensemble\nFigure 3. Impact of Center Vector Regularization on Catas-\ntrophic Forgetting. Bar plots shows the difference in accuracy\nbetween the ensembled model, and un-ensembled model. Red bars\nare corresponding to UpStep model with center vector (CV) reg-\nularization, while the Blue bars represent the ablated version of\nUpStep without the CV regularization.\nfive out of eight datasets, suggesting that lower center vec-\ntor magnitude often aligns with improved training stability\nduring post-pretraining. However, for the remaining three\ndatasets, a positive correlation is observed, indicating that\nthe relationship may vary based on specific target domain\ncharacteristics.\n4.2. Center Vector and Catastrophic Forgetting\nTo minimize the effect of the catastrophic forgetting of\nsource domain knowledge, we employ an ensemble of\nthe post-pretrained model with the original source-domain\nmodel. By retaining the source model parameters, we pro-\n6\n\n\nFigure 4. Effect of Number of Prototypes on Model Perfor-\nmance. Performance of the model with varying numbers of proto-\ntypes in the non-ensemble setting, on Flowers102 dataset [41].\ntect against the negative impact of adapting to smaller or\nless diverse target domains. Interestingly, we observe that\nminimizing center vector magnitude also reduces catas-\ntrophic forgetting, thereby decreasing the ensemble’s re-\nliance on the source domain model.\nTo examine this, we compare the classification perfor-\nmance of models trained with and without center vector reg-\nularization and analyze their performance in both ensem-\nble and non-ensemble setups. Our results, Fig. 3 indicate\nthat, without center vector regularization, the performance\ngap between the ensembled and non-ensembled models is\nlarger, highlighting the role of center vector regularization\nin preserving source domain knowledge. For baseline per-\nformances that are saturated, i.e. ( 97%), in the case of CI-\nFAR 10 and EuroSAT, we do not observe this phenomenon.\n4.3. Effect of the Number of Prototypes\nIn our experiments, we use 3000 prototypes following the\ndesign choice in SwAV [4]. Here, we analyze the effect of\nvarying the number of prototypes on the UpStep model’s\nperformance in the non-ensemble setting, as this model par-\nticipates in the loss minimization and is directly affected by\nthe prototype design choice. As shown in Fig. 4, we observe\nthat a very low number of prototypes results in a lower per-\nformance, likely because fewer prototypes cannot capture\nthe diversity of the target domain, causing samples of dif-\nferent concepts to compete for the same prototypes. Con-\nversely, an excessive number of prototypes creates a high-\ndimensional space that can easily overfit the target domain.\nOptimal performance is achieved with a moderate number\nof prototypes, balancing diversity representation with over-\nfitting prevention.\n4.4. Reduction in Training Time\nOur method incorporates center vector conditional batch\ntraining, where training occurs only if the current batch’s\ncenter vector is lower than that of the previous batch. This\nstrategy reduces training time by skipping non-informative\nupdates.\nOn average, training time is reduced by 50%\nacross datasets, as shown in Fig. 5 (a), with comparative\nperformance, as shown in Fig. 5 (b). Combined with LoRA,\nthis approach not only reduces trainable parameters but also\nimproves computational efficiency for post-pretraining.\n5. Related Work\nThe rapid expansion of vision model scales has acceler-\nated research into parameter-efficient adaptation methods,\naimed at overcoming the computational and economic bar-\nriers in adapting large models to novel domains. This sec-\ntion explores significant advancements in unsupervised and\nsource-free domain adaptation, frozen network adaptation,\nfoundational model adaptation, adversarial learning for fea-\nture alignment, self-supervised learning, and masking tech-\nniques. These methodologies contribute to the development\nof our UpStep approach.\nUnsupervised and Source-Free Domain Adaptation.\nUnsupervised domain adaptation (UDA) methods seek to\ntransfer knowledge from a labeled source domain to an un-\nlabeled target domain by reducing domain shift through\ntechniques like instance re-weighting and feature adapta-\ntion [15, 16, 28, 53, 57, 61]. Source-free domain adapta-\ntion (SFDA) extends UDA by addressing scenarios where\nsource data is inaccessible, e.g. due to privacy constraints.\nSFDA methods adapt a pretrained source model to the target\ndomain without access to source samples, utilizing strate-\ngies like self-supervised pseudo-labeling and knowledge\nextraction from the source model [29, 36, 44]. Our approach\nfollows this source-free paradigm, adapting a pretrained\nmodel to a target domain without requiring labeled target\ndata, thus addressing source-data availability constraints.\nMoreover, unlike SFDA approaches, we consider larger do-\nmain difference through a diverse set of target datasets.\nFrozen Network Adaptation.\nRecent advancements\nin parameter-efficient adaptation techniques, that preserve\ncore network weights, have shown promising results for\ndomain adaptation without excessive computational costs.\nLow-rank adaptation (LoRA), for example, integrates train-\nable low-rank layers into pretrained models, enabling effi-\ncient adaptation with minimal parameter modification [25].\nOther notable methods, including feature adapters [14, 65],\nbias tuning [3, 62], and visual prompting [2, 27], achieve\ndomain specialization by modifying only selected layers.\nOur work builds on these advancements by employing\nLoRA for parameter-efficient adaptation of the pretrained\nmodels to the target domain.\nVision Foundation Models (VFMs). Vision Founda-\ntion Models, such as DinoV2 [43] and MAE [22], exhibit\nexceptional generalization across diverse tasks, largely due\nto their self-supervised pretraining on large-scale datasets\nlike ImageNet [6, 18, 21, 43]. Such models, pretrained on\n7\n\n\n(a)\nNumber of training iteration used in UpStep\n(b) Performance of UpStep with center vector gating (cv cond) against \nUpStep without it (no CV cond)\nUpStep (with CV cond)\n is better\nUpStep (no CV cond)\nis better\nFigure 5. Training Time Reduction with Center Vector Conditional Training. (a) Average reduction in training time across datasets. (b)\nPerformance comparison between UpStep with and without center vector (CV cond) conditioned gating. With a comparable performance\nover the dataset, Upstep with CV conditioned gating reduces the number of training iterations (backpropagation) to 50%.\nextensive data distributions, serve as robust starting points\nfor domain-specific adaptation [8, 40, 48, 64]. Our method\nleverages VFMs pretrained on ImageNet [32], LVD-142M\n[43], and WIT-400M [45], while adapting it to target do-\nmains through a LoRA-enhanced self-supervised approach\nto optimize the domain-specific performance.\nSelf-supervised Learning on Restricted Domains.\nSelf-supervised learning (SSL) techniques have shown no-\ntable success in extracting robust representations from un-\nlabeled data [4, 5, 13, 20]. In our UpStep approach, SSL\nplays a critical role in capturing the semantics of the target\ndomain ensuring label-free domain adaptation.\nSSL-based knowledge distillation has demonstrated en-\nhanced model performance on downstream tasks in vision\nand natural language processing by continuing training on\nunlabeled target datasets [19, 24, 49]. However, for these\nmodels the source and target distributions typically lie in\nsimilar domains, which is not a constraint for our approach.\nMoreover, these models train all the parameters of the pre-\ntrained model on the target dataset, making them resource-\nexpensive, while our approach employs parameter efficient\nadaptation.\nSparsity and Low-Rank. Masking and sparsity meth-\nods enable efficient domain adaptation by selectively acti-\nvating key subnetworks or by pruning unnecessary param-\neters. For instance, the pass-through trick activates specific\nsubnetworks within pretrained models, enhancing adap-\ntation efficiency without complete retraining [38, 39, 46,\n59]. Pruning techniques for convolutional neural networks\n(CNNs) and vision transformers further support adaptation\nwith minimal overhead [33, 51]. Although UpStep does not\ndirectly incorporate masking, these techniques highlight the\nadvantages of creating a sparse, efficient low-rank adapta-\ntion strategy.\nIn conclusion, we present a parameter-efficient solu-\ntion for domain adaptation by integrating LoRA with self-\nsupervised objective, offering a novel, label-free and cost-\neffective approach for adapting large vision models to\ndomain-specific data.\n6. Conclusion\nIn this work, we introduced UpStep, an efficient, unsu-\npervised, source-free post-pretraining approach for adapt-\ning large pretrained vision models to new target domains\nwithout requiring source domain data or target labels.\nOur approach leverages three main contributions: first, a\nself-supervised training scheme to enable effective domain\nadaptation; second, center vector regularization (CVR) to\nmitigate catastrophic forgetting and to reduce training time\nby skipping backprpagation for 50% of the training itera-\ntions; and third, in the unsupervised context, a parameter-\nefficient fine-tuning strategy through Low-Rank Adaptation\n(LoRA) that significantly minimizes the number of train-\nable parameters.\nExtensive evaluations across diverse datasets and pre-\ntrained models demonstrate that UpStep achieves competi-\ntive or superior performance, often outperforming baselines\nwhile maintaining efficiency. We observed that center vec-\ntor regularization helps retain discriminability of features,\nas shown in the analysis of correlation between center vec-\ntor magnitude and model performance. Additionally, the re-\nsults indicate that UpStep’s flexibility allows it to be applied\neffectively across various architectures and target domains,\nfurther establishing its adaptability. While our method gen-\nerally performs well, we observed that the correlation be-\ntween center vector magnitude and classification perfor-\nmance varied across certain datasets, indicating an area for\nfurther investigation into the relationship between feature\nvariance and domain-specific adaptation needs.\n8\n\n\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\nKatherine Millican, Malcolm Reynolds, et al. Flamingo: a\nvisual language model for few-shot learning. Advances in\nNeural Information Processing Systems, 35:23716–23736,\n2022. 1\n[2] Hyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, and\nPhillip Isola. Exploring visual prompts for adapting large-\nscale models. arXiv preprint arXiv:2203.17274, 2022. 7\n[3] Han Cai, Chuang Gan, Ligeng Zhu, and Song Han. Tinytl:\nReduce activations, not trainable parameters for efficient on-\ndevice learning. arXiv preprint arXiv:2007.11622, 2020. 7\n[4] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi-\notr Bojanowski, and Armand Joulin. Unsupervised learning\nof visual features by contrasting cluster assignments. Ad-\nvances in neural information processing systems, 33:9912–\n9924, 2020. 2, 4, 7, 8\n[5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv´e J´egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In Pro-\nceedings of the IEEE/CVF international conference on com-\nputer vision, pages 9650–9660, 2021. 4, 5, 8\n[6] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.\nImproved baselines with momentum contrastive learning.\narXiv preprint arXiv:2003.04297, 2020. 7\n[7] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A.\nVedaldi. Describing textures in the wild. In Proceedings of\nthe IEEE Conf. on Computer Vision and Pattern Recognition\n(CVPR), 2014. 4\n[8] Yezhen Cong, Samar Khanna, Chenlin Meng, Patrick Liu,\nErik Rozi, Yutong He, Marshall Burke, David Lobell, and\nStefano Ermon. Satmae: Pre-training transformers for tem-\nporal and multi-spectral satellite imagery. Advances in Neu-\nral Information Processing Systems, 35:197–211, 2022. 8\n[9] Timoth´ee Darcet, Maxime Oquab, Julien Mairal, and Piotr\nBojanowski. Vision transformers need registers, 2023. 4, 5\n[10] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah\nParisot, Xu Jia, Aleˇs Leonardis, Gregory Slabaugh, and\nTinne Tuytelaars. A continual learning survey: Defying for-\ngetting in classification tasks. IEEE transactions on pattern\nanalysis and machine intelligence, 44(7):3366–3385, 2021.\n1\n[11] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr\nPadlewski, Jonathan Heek, Justin Gilmer, Andreas Peter\nSteiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdul-\nmohsin, et al. Scaling vision transformers to 22 billion pa-\nrameters. In International Conference on Machine Learning,\npages 7480–7512. PMLR, 2023. 1\n[12] Alexey Dosovitskiy.\nAn image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 4, 5\n[13] Zhiyuan Fang, Jianfeng Wang, Lijuan Wang, Lei Zhang,\nYezhou Yang, and Zicheng Liu.\nSeed:\nSelf-supervised\ndistillation for visual representation.\narXiv preprint\narXiv:2101.04731, 2021. 8\n[14] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao\nFang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao.\nClip-adapter: Better vision-language models with feature\nadapters. International Journal of Computer Vision, 132(2):\n581–595, 2024. 7\n[15] Boqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman.\nGeodesic flow kernel for unsupervised domain adaptation.\nIn CVPR, pages 2066–2073, 2012. 7\n[16] Raghuraman Gopalan, Ruonan Li, and Rama Chellappa. Do-\nmain adaptation for object recognition: An unsupervised ap-\nproach. In ICCV, pages 999–1006. IEEE, 2011. 7\n[17] Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu,\nPengchao Wang, Vivek Pai, Mannat Singh, Vitaliy Liptchin-\nsky, Ishan Misra, Armand Joulin, et al.\nSelf-supervised\npretraining of visual features in the wild.\narXiv preprint\narXiv:2103.01988, 2021. 1\n[18] Jean-Bastien Grill, Florian Strub, Florent Altch´e, Corentin\nTallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch,\nBernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh-\nlaghi Azar, et al. Bootstrap your own latent-a new approach\nto self-supervised learning. Advances in neural information\nprocessing systems, 33:21271–21284, 2020. 7\n[19] Suchin Gururangan, Ana Marasovi´c, Swabha Swayamdipta,\nKyle Lo, Iz Beltagy, Doug Downey, and Noah A Smith.\nDon’t stop pretraining: Adapt language models to domains\nand tasks. arXiv preprint arXiv:2004.10964, 2020. 8\n[20] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\nGirshick. Momentum contrast for unsupervised visual rep-\nresentation learning. arxiv e-prints, art.\narXiv preprint\narXiv:1911.05722, 2019. 8\n[21] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll´ar, and Ross Girshick. Mae: Masked autoencoders are\nscalable vision learners. arXiv preprint arXiv:2111.06377,\n2021. 7\n[22] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll´ar, and Ross Girshick. Masked 790 autoencoders are\nscalable vision learners. arXiv preprint arXiv:2111.06377,\n391, 2021. 7\n[23] Patrick Helber, Benjamin Bischke, Andreas Dengel, and\nDamian Borth.\nIntroducing eurosat: A novel dataset and\ndeep learning benchmark for land use and land cover clas-\nsification. In IGARSS 2018-2018 IEEE International Geo-\nscience and Remote Sensing Symposium, pages 204–207.\nIEEE, 2018. 4\n[24] Jeremy Howard and Sebastian Ruder. Universal language\nmodel fine-tuning for text classification.\narXiv preprint\narXiv:1801.06146, 2018. 8\n[25] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 1, 7\n[26] Abhishek Jha, Matthew B Blaschko, Yuki M Asano, and\nTinne Tuytelaars. The common stability mechanism behind\nmost self-supervised learning approaches.\narXiv preprint\narXiv:2402.14957, 2024. 2, 6\n[27] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie,\nSerge Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi-\n9\n\n\nsual prompt tuning. In European Conference on Computer\nVision, pages 709–727. Springer, 2022. 7\n[28] Jing Jiang and ChengXiang Zhai. Instance weighting for do-\nmain adaptation in nlp. ACL, 2007. 7\n[29] Youngeun Kim, Donghyeon Cho, Priyadarshini Panda, and\nSungeun Hong.\nProgressive domain adaptation from a\nsource pre-trained model. arXiv preprint arXiv:2007.01524,\n2020. 7\n[30] Dawid Jan Kopiczko, Tijmen Blankevoort, and Yuki M\nAsano. Vera: Vector-based random matrix adaptation. In\nThe Twelfth International Conference on Learning Represen-\ntations, 2024. 5\n[31] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple\nlayers of features from tiny images. 2009. 4\n[32] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classification with deep convolutional neural net-\nworks. In Advances in neural information processing sys-\ntems, pages 1097–1105, 2012. 1, 2, 3, 8\n[33] Franc¸ois Lagunas, Ella Charlaix, Victor Sanh, and Alexan-\nder M Rush. Block pruning for faster transformers. arXiv\npreprint arXiv:2109.04838, 2021. 8\n[34] Zhengfeng Lai, Noranart Vesdapunt, Ning Zhou, Jun Wu,\nCong Phuoc Huynh, Xuelu Li, Kah Kuen Fu, and Chen-Nee\nChuah.\nPadclip: Pseudo-labeling with adaptive debiasing\nin clip for unsupervised domain adaptation. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 16155–16165, 2023. 2\n[35] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for uni-\nfied vision-language understanding and generation. In In-\nternational Conference on Machine Learning, pages 12888–\n12900. PMLR, 2022. 1\n[36] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need\nto access the source data? source hypothesis transfer for un-\nsupervised domain adaptation. In International conference\non machine learning, pages 6028–6039. PMLR, 2020. 7\n[37] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hi-\nroaki Hayashi, and Graham Neubig. Pre-train, prompt, and\npredict: A systematic survey of prompting methods in nat-\nural language processing. ACM Computing Surveys, 55(9):\n1–35, 2023. 1\n[38] A Mallya and S Lazebnik. Packnet: Adding multiple tasks\nto a single network by iterative pruning. corr abs/1711.05769\n(2017). arXiv preprint arXiv:1711.05769, 2017. 8\n[39] Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggy-\nback: Adapting a single network to multiple tasks by learn-\ning to mask weights. In Proceedings of the European con-\nference on computer vision (ECCV), pages 67–82, 2018. 8\n[40] Th´eo Moutakanni, Piotr Bojanowski, Guillaume Chas-\nsagnon, C´eline Hudelot, Armand Joulin, Yann LeCun,\nMatthew Muckley, Maxime Oquab, Marie-Pierre Revel, and\nMaria Vakalopoulou.\nAdvancing human-centric ai for ro-\nbust x-ray analysis through holistic self-supervised learning.\narXiv preprint arXiv:2405.01469, 2024. 8\n[41] Maria-Elena Nilsback and Andrew Zisserman. Automated\nflower classification over a large number of classes. In 2008\nSixth Indian conference on computer vision, graphics & im-\nage processing, pages 722–729. IEEE, 2008. 4, 7\n[42] Curtis G. Northcutt, Anish Athalye, and Jonas Mueller. Per-\nvasive label errors in test sets destabilize machine learning\nbenchmarks, 2021. 1\n[43] Maxime Oquab, Timoth´ee Darcet, Theo Moutakanni, Huy V.\nVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Rus-\nsell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-\nWen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nico-\nlas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou,\nJulien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bo-\njanowski. Dinov2: Learning robust visual features without\nsupervision, 2023. 7, 8\n[44] Zhen Qiu, Yifan Zhang, Hongbin Lin, Shuaicheng Niu,\nYanxia Liu, Qing Du, and Mingkui Tan.\nSource-free do-\nmain adaptation via avatar prototype generation and adapta-\ntion. arXiv preprint arXiv:2106.15326, 2021. 7\n[45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748–8763. PMLR, 2021. 1, 4, 8\n[46] Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kemb-\nhavi, Ali Farhadi, and Mohammad Rastegari. What’s hid-\nden in a randomly weighted neural network? In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 11893–11902, 2020. 8\n[47] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In International Confer-\nence on Machine Learning, pages 8821–8831. PMLR, 2021.\n1\n[48] C Reed, Ritwik Gupta, Shufan Li, Sarah Brockman, Christo-\npher Funk, Brian Clipp, S Candido, M UyttenDAele, and T\nDarrell. Scale-mae: A scale-aware masked autoencoder for\nmultiscale geospatial representation learning. 2023 ieee. In\nCVF International Conference on Computer Vision (ICCV),\npages 4065–4076, 2022. 8\n[49] Colorado J Reed, Xiangyu Yue, Ani Nrusimha, Sayna\nEbrahimi, Vivek Vijaykumar, Richard Mao, Bo Li, Shang-\nhang Zhang, Devin Guillory, Sean Metzger, et al.\nSelf-\nsupervised pretraining improves self-supervised pretraining.\nIn Proceedings of the IEEE/CVF Winter Conference on Ap-\nplications of Computer Vision, pages 2584–2594, 2022. 8\n[50] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim\nNeumann, Rodolphe Jenatton, Andr´e Susano Pinto, Daniel\nKeysers, and Neil Houlsby. Scaling vision with sparse mix-\nture of experts. Advances in Neural Information Processing\nSystems, 34:8583–8595, 2021. 1\n[51] Victor Sanh, Thomas Wolf, and Alexander Rush. Movement\npruning: Adaptive sparsity by fine-tuning. Advances in neu-\nral information processing systems, 33:20378–20389, 2020.\n8\n[52] K Soomro. Ucf101: A dataset of 101 human actions classes\nfrom videos in the wild. arXiv preprint arXiv:1212.0402,\n2012. 4\n[53] Baochen Sun and Kate Saenko. Subspace distribution align-\n10\n\n\nment for unsupervised domain adaptation. In BMVC, pages\n24–1, 2015. 7\n[54] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhi-\nnav Gupta. Revisiting unreasonable effectiveness of data in\ndeep learning era. In Proceedings of the IEEE international\nconference on computer vision, pages 843–852, 2017. 1\n[55] Bart Thomee, David A Shamma, Gerald Friedland, Ben-\njamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and\nLi-Jia Li. Yfcc100m: The new data in multimedia research.\nCommunications of the ACM, 59(2):64–73, 2016. 1\n[56] Vijay Vasudevan, Benjamin Caine, Raphael Gontijo Lopes,\nSara Fridovich-Keil, and Rebecca Roelofs.\nWhen does\ndough become a bagel? analyzing the remaining mistakes\non imagenet. Advances in Neural Information Processing\nSystems, 35:6720–6734, 2022. 1\n[57] Rui Wang, Masao Utiyama, Lemao Liu, Kehai Chen, and Ei-\nichiro Sumita. Instance weighting for neural machine trans-\nlation domain adaptation. In Proceedings of the 2017 Con-\nference on Empirical Methods in Natural Language Process-\ning, pages 1482–1488, 2017. 7\n[58] Alfonso Taboada Warmerdam, Mathilde Caron, and Yuki M\nAsano. Self-masking networks for unsupervised adaptation.\narXiv preprint arXiv:2409.07577, 2024. 1, 4\n[59] Mitchell Wortsman,\nVivek Ramanujan,\nRosanne Liu,\nAniruddha Kembhavi, Mohammad Rastegari, Jason Yosin-\nski, and Ali Farhadi. Supermasks in superposition. Advances\nin Neural Information Processing Systems, 33:15173–15184,\n2020. 8\n[60] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva,\nand Antonio Torralba.\nSun database: Large-scale scene\nrecognition from abbey to zoo. In 2010 IEEE computer so-\nciety conference on computer vision and pattern recognition,\npages 3485–3492. IEEE, 2010. 4\n[61] Yi Yao and Gianfranco Doretto. Boosting for transfer learn-\ning with multiple sources. In Computer vision and pattern\nrecognition (CVPR), 2010 IEEE conference on, pages 1855–\n1862. IEEE, 2010. 7\n[62] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit:\nSimple parameter-efficient fine-tuning for transformer-based\nmasked language-models. arXiv preprint arXiv:2106.10199,\n2021. 7\n[63] Chaoning Zhang, Kang Zhang, Chenshuang Zhang, Trung X\nPham, Chang D Yoo, and In So Kweon.\nHow does sim-\nsiam avoid collapse without negative samples? a unified un-\nderstanding with self-supervised contrastive learning. arXiv\npreprint arXiv:2203.16262, 2022. 2\n[64] Jielu Zhang, Zhongliang Zhou, Gengchen Mai, Lan Mu,\nMengxuan Hu, and Sheng Li. Text2seg: Remote sensing im-\nage semantic segmentation via text-guided visual foundation\nmodels. arXiv preprint arXiv:2304.10597, 2023. 8\n[65] Renrui Zhang, Wei Zhang, Rongyao Fang, Peng Gao, Kun-\nchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li.\nTip-\nadapter: Training-free adaption of clip for few-shot classi-\nfication. In European conference on computer vision, pages\n493–510. Springer, 2022. 7\n[66] Yitao Zhu, Zhenrong Shen, Zihao Zhao, Sheng Wang, Xin\nWang, Xiangyu Zhao, Dinggang Shen, and Qian Wang.\nMelo: Low-rank adaptation is better than fine-tuning for\nmedical image diagnosis. In 2024 IEEE International Sym-\nposium on Biomedical Imaging (ISBI), pages 1–5. IEEE,\n2024. 1\n11\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21313v1.pdf",
    "total_pages": 11,
    "title": "Unsupervised Parameter Efficient Source-free Post-pretraining",
    "authors": [
      "Abhishek Jha",
      "Tinne Tuytelaars",
      "Yuki M. Asano"
    ],
    "abstract": "Following the success in NLP, the best vision models are now in the billion\nparameter ranges. Adapting these large models to a target distribution has\nbecome computationally and economically prohibitive. Addressing this challenge,\nwe introduce UpStep, an Unsupervised Parameter-efficient Source-free\npost-pretraining approach, designed to efficiently adapt a base model from a\nsource domain to a target domain: i) we design a self-supervised training\nscheme to adapt a pretrained model on an unlabeled target domain in a setting\nwhere source domain data is unavailable. Such source-free setting comes with\nthe risk of catastrophic forgetting, hence, ii) we propose center vector\nregularization (CVR), a set of auxiliary operations that minimize catastrophic\nforgetting and additionally reduces the computational cost by skipping\nbackpropagation in 50\\% of the training iterations. Finally iii) we perform\nthis adaptation process in a parameter-efficient way by adapting the pretrained\nmodel through low-rank adaptation methods, resulting in a fraction of\nparameters to optimize. We utilize various general backbone architectures, both\nsupervised and unsupervised, trained on Imagenet as our base model and adapt\nthem to a diverse set of eight target domains demonstrating the adaptability\nand generalizability of our proposed approach.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}