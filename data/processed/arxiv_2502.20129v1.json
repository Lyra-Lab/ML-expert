{
  "id": "arxiv_2502.20129v1",
  "text": "Finite State Automata Inside Transformers with Chain-of-Thought:\nA Mechanistic Study on State Tracking\nYifan Zhang1,2*, Wenyu Du3, Dongming Jin1,2, Jie Fu4†, Zhi Jin1,2†\n1Key Laboratory of High Confidence Software Technology (PKU), MOE, China\n2School of Computer Science, Peking University, China\n3The University of Hong Kong, 4Shanghai AI Lab\nyifanzhang@stu.pku.edu.cn, fujie@pjlab.org.cn, zhijin@pku.edu.cn\nAbstract\nChain-of-Thought (CoT) significantly en-\nhances the performance of large language mod-\nels (LLMs) across a wide range of tasks, and\nprior research shows that CoT can theoretically\nincrease expressiveness.\nHowever, there is\nlimited mechanistic understanding of the algo-\nrithms that Transformer+CoT can learn. In this\nwork, we (1) evaluate the state tracking capabil-\nities of Transformer+CoT and its variants, con-\nfirming the effectiveness of CoT. (2) Next, we\nidentify the circuit—a subset of model compo-\nnents—responsible for tracking the world state,\nfinding that late-layer MLP neurons play a key\nrole. We propose two metrics, compression\nand distinction, and show that the neuron sets\nfor each state achieve nearly 100% accuracy,\nproviding evidence of an implicit finite state\nautomaton (FSA) embedded within the model.\n(3) Additionally, we explore three realistic set-\ntings: skipping intermediate steps, introducing\ndata noise, and testing length generalization.\nOur results demonstrate that Transformer+CoT\nlearns robust algorithms (FSA), highlighting its\nresilience in challenging scenarios.\n1\nIntroduction\nTransformer-based large language models (LLMs)\nTouvron et al., 2023; OpenAI, 2023 revolution-\nize natural language processing (NLP) by demon-\nstrating significant progress across various tasks.\nHowever, they still face challenges with basic cal-\nculations (Zhou et al., 2023), complex reasoning\n(Valmeekam et al., 2024; Han et al., 2024), and\nregular languages (Bhattamishra et al., 2020). Ap-\nproaches such as Chain-of-Thought (CoT) prompt-\ning (Wei et al., 2023) and scratchpads (Nye et al.,\n2021) address these limitations by generating inter-\nmediate reasoning steps. To understand the success\nof CoT, prior work has analyzed its expressive-\nness through the lenses of formal language theory\n*Work done during internship at Shanghai AI Lab.\n†Corresponding author.\n. . .\n. . .\n. . .\n. . .\n. . .\nAttention\nMLP\nRecover\n𝑞1\n𝑞0\n𝑚1\n𝑚1\n𝑚0\n𝑚0\n𝑞0, 𝑚0 →\n𝑞0, 𝑚1 →\n𝑞1, 𝑚0 →\n𝑞1, 𝑚1 →\nTrain\nFSA\nTransition Rules\n𝑞0\n𝑞1\n𝑞1\n𝑞0\nEquivalent\nInput Sequence\nOutput State\n𝑚\n...𝑞\n𝑞\nෝ𝑚\nො𝑞\nഥ𝑚\nFigure 1: An illustration of one of the simplest state\ntracking problems, Z2. After training on data generated\nby transition rules of Z2, Transformer+CoT successfully\nrecovers an implicit FSA by differentiating two states\n(q0 and q1) of the Z2 using two distinct sets of neurons\nin late-layer MLPs.\nand circuit complexity. Theoretical work (Zhang\net al., 2024; Qiu et al., 2024; Li et al., 2024) demon-\nstrates that incorporating a linear number of inter-\nmediate steps increases the expressive power of\nlog-precision transformers1 (Merrill, William and\nSabharwal, Ashish, 2024), enabling them to repre-\nsent all Finite State Automata (FSA), which are a\nfoundational class of automata.\nHowever, theoretical expressiveness indicates\nonly upper and lower bounds on what an architec-\nture can express; it does not guarantee successful\nlearning during training. For instance, while re-\ncurrent neural networks (RNNs) are theoretically\nmore expressive than transformers in Chomsky’s\ncomputational hierarchy—where RNNs can handle\nregular languages and transformers are positioned\nlower (Delétang et al., 2023)—RNNs often fail to\noutperform transformers in practice. Consequently,\nsome studies investigate the expressiveness of these\narchitectures through the lens of learnability by\nmeasuring performance in language modeling. For\n1This paper refers to log-precision transformers; for sim-\nplicity, it uses “transformers” interchangeably.\narXiv:2502.20129v1  [cs.CL]  27 Feb 2025\n\n\nexample, Liu et al. (2023a) demonstrated that train-\ning transformers with recency-biased scratchpads\nimproves sequential accuracy. However, even near-\nperfect next-token prediction accuracy does not im-\nply that generative models reconstruct a true world\nmodel (Vafa et al., 2024). This raises a critical\nquestion: Does CoT help Transformer recover\na world model in the form of FSA, or do they\nmerely learn shortcuts?\nTo address this question, we extend the study of\nlearnability beyond accuracy improvements, per-\nforming an internal mechanistic analysis of CoT’s\nsuccess. Specifically, we focus on state tracking, a\nstandard task for evaluating expressiveness (Mer-\nrill et al., 2024). In state tracking, a sequence of\nupdates modifies the world state, which is repre-\nsented as an FSA. The goal is to determine the final\nstate after applying all updates sequentially. State\ntracking is a core capability of generative models\nand underpins many downstream tasks—such as\nentity tracking (Kim and Schuster, 2023), chess\n(Toshniwal et al., 2022), and map navigation (Liu\net al., 2023b). Figure 1 illustrates Z22, one of the\nsimplest state tracking problems, along with its\ncorresponding FSA and transition rules.\nWe begin by comprehensively evaluating the\nstate tracking capabilities of Transformer+CoT,\ncomparing it with other models (RNNs), trans-\nformer variants (e.g., those with recurrence (Fan\net al., 2021; Yang et al., 2022)), and CoT variants\n(e.g., implicit CoT (Goyal et al., 2024)). Empiri-\ncally, we show that Transformer+CoT is the only\nmodel capable of efficiently learning state track-\ning for sequences of arbitrary lengths across three\ngroups: Z60, A4 × Z5, and A5, in both in-domain\nand out-of-distribution settings.\nNext, to provide a mechanistic explanation\nfor this success, we apply interpretability tech-\nniques to analyze the algorithms learned by Trans-\nformer+CoT. Using activation patching (Vig et al.,\n2020), we identify the circuits (specific model com-\nponents) responsible for state tracking and observe\nthat Transformer+CoT relies heavily on late-layer\nMLP neurons. These neurons can be effectively\ngrouped into states based on transition rules. To\nquantify this, we introduce two metrics: compres-\nsion and distinction. Compression measures the\nsimilarity of representations for the same state un-\nder different input prompts, while distinction quan-\n2The state tracking problem Z2 is equivalent to parity, a\nformal language describing binary sequences with specific\nevenness or oddness properties.\ntifies the separation between different states, even\nwhen their inputs are similar. We find nearly 100%\naccuracy on both metrics at every intermediate step,\nproviding strong evidence that the model recon-\nstructs the world model (i.e., FSA). For instance,\nin Figure 1, Transformer+CoT compresses inputs\ncorresponding to two states (i.e. q0 and q1) by ac-\ntivating two distinct sets of neurons. Additionally,\nwe observe shifts in attention patterns as the num-\nber of intermediate steps increases.\nTo evaluate robustness in real-world scenarios,\nwe test Transformer+CoT under three challenging\nsettings: skip-step reasoning, noisy scratchpads,\nand length generalization. Our results show that\nTransformer+CoT learns robust algorithms even\nin noisy environments, indicating that the under-\nlying FSA is resilient. This robustness supports\nthe implementation of state tracking in more com-\nplex tasks, providing a theoretical foundation for\ndownstream applications.\nIn summary, this work is the first to extend the\nstudy of learnability and expressiveness through\nmechanistic interpretation, uncovering the underly-\ning algorithms of Transformer+CoT. Our contribu-\ntions are as follows:\n1. We conduct a comprehensive evaluation\nof the state tracking capabilities of Trans-\nformer+CoT, demonstrating its unique ability\nto track states of arbitrary lengths across mul-\ntiple groups (Z60, A4 × Z5, and A5) in both\nin-domain and out-of-distribution settings.\n2. Using interpretability techniques, including\nactivation patching, we analyze the learned\nalgorithms in Transformer+CoT. We identify\nthe activation of late-layer MLP neurons and\nclassify them into states based on transition\nrules, achieving nearly 100% accuracy in met-\nrics of compression and distinction, which\nconfirms the model’s reconstruction of the\nworld model (FSA).\n3. We explore Transformer+CoT in three chal-\nlenging settings and find that it learns resilient\nalgorithms capable of effective state tracking\nin noisy conditions.\n2\nRelated Work\n2.1\nFSA and State Tracking\nWe adopt the conventional definition of a finite state\nautomaton (FSA) as a tuple A = (Σ, Q, q0, δ),\nwhere Σ is the alphabet, Q is a set of states, q0\n\n\nis the initial state, and δ is the transition func-\ntion(Hopcroft and Ullman, 1979). State tracking\ncan be framed as solving a word problem over a\nfinite monoid (M, ·), where the objective is to com-\npute the product m1 · m2 · . . . · mn ∈M(Merrill\net al., 2024). This computation can be performed\nby a finite state automaton. As generative mod-\nels, transformers augmented with chain-of-thought\ngenerate state sequences q1 . . . qn ∈M∗based on\ninput sequences m1 . . . mn ∈M∗. Our work fo-\ncuses on word problems over groups, specifically\nthe cyclic group Zm and the symmetric group Sm.\n2.2\nMechanistic Interpretability\nThe use of mechanistic interpretability (Rai et al.,\n2024; Ferrando et al., 2024) to explain LLMs is\nan emerging research direction. This approach em-\nploys various techniques, including logit lens(Geva\net al., 2021), probing(Gurnee et al., 2023), causal\nmediation analysis(Wang et al., 2022), sparse au-\ntoencoders(Cunningham et al., 2023), and visual-\nization(Cooney and Nanda, 2023), to identify and\nanalyze the features and circuits of LLMs.\n3\nEvaluating State Tracking Capability\nAcross Architectures\nBesides standard transformer and CoT, there are\nvarious theoretical works (Zhang et al., 2024; Fan\net al., 2024; Yang et al., 2022; Fan et al., 2021)\nattempting to inject recurrence into transformers,\nwhile another line of work (Goyal et al., 2024; Hao\net al., 2024) proposing modifications of chain-of-\nthought (implicit chain-of-thought in contrast to ex-\nplicit chain-of-thought). In this section, we will ex-\nplore the state tracking capability with an empirical\nlens: can standard transformer with/without CoT\nand these variants successfully learn state tracking?\nDataset: A5, A4×Z5, Z60.\nFollowing the formal\ndefinition of state tracking in (Merrill et al., 2024;\nGrazzi et al., 2024), we model state tracking as\nword problems, and consider three kinds of groups\nwith increasing difficulty: Z60, an abelian group\nencoding mod-60 addition, A4 × Z5, a non-abelian\nbut solvable group, which is a direct product group\nof one alternating group A4 (a subgroup of the\nsymmetric group S4 containing only even permuta-\ntions) and one cyclic group Z5, and A5, the alternat-\ning group on five elements, which is the smallest\nnon-solvable subgroup. With the same number\nof elements 60, the three groups belong to TC0,\nTC0, and NC1-complete respectively, with vary-\ning difficulty mainly deriving from the complexity\nof learning the group multiplication operation.\nArchitectures.\nWe choose a GPT2-like (Radford\net al., 2019) transformer with chain-of-thought\n(denoted as Transformer+CoT), choose the real-\nistic setting with a bounded number of layers, and\nlog-precision. To compare Transformer+CoT with\nother models, we also consider recurrent neural net-\nworks (RNNs Jain and Medsker, 1999 and LSTMs\nHochreiter and Schmidhuber, 1997), S4 (Gu et al.,\n2022), Mamba (Gu and Dao, 2024), implicit chain-\nof-thought: transformers with pause (denoted as\nPause) (Goyal et al., 2024) and other variants of\ntransformers: standard recurrent transformer (de-\nnoted as Recurrent) (Yang et al., 2022), looped\ntransformer (denoted as Looped) (Fan et al., 2024).\nIn order to disentangle recursion introduced by CoT\nfrom other variants, we classify different models\ninto encoder and decoder and adopt the Token-\nTagging task (TT) and Language Modeling task\n(LM), respectively. Only Transformer+CoT and im-\nplicit CoT (Pause) are designed as decoders, while\nother models or variants encoder. The reason we\ndesign some models as encoders rather than de-\ncoders without CoT is that, we use the same labels\nwhether the model is an encoder or decoder, in\norder to eliminate the influence of labels on su-\npervised training, like the hint setting in (Li et al.,\n2024). So that we can contribute the performance\nimprovement to recurrence in the structure for vari-\nants without chain-of-thought. Model details refer\nto Appendix B.\nPerformance (In-Domain).\nWe hold a compre-\nhensive evaluation of models’ state tracking ca-\npability on word problems, with the same model\ndepth and dimension. We fix the same number of\nlayers and same model dimension for all models\nof log-precision. We use sequence accuracy as a\nmetric, where the generated sequence is true only\nwhen same to ground truth sequence. Figure 2\ngives different models’ performance across differ-\nent input sequence length and different groups. We\ndraw several conclusions:\n1. Consistent with theoretical study (Liu et al.,\n2023a; Merrill et al., 2024), transformers and\nstate-space models can not express arbitrary\nlength A5 word problems, in contrast to RNN\nand LSTM.\n2. O(N) intermediate steps extend the expres-\nsive power of transformers.\nIn particular,\n\n\nTransformer+CoT can empirically converge\nto a nearly perfect generative model on word\nproblems of arbitrary length, which allows\nfor parallel training and a smoother flow of\ngradients like Transformer due to no archi-\ntecture change. However, other variants of\ntransformers (Pause, Looped, and Recurrent)\nfail to converge on longer sequence lengths,\ndespite postponing the accuracy drop to longer\nlengths compared to the standard transformer.\n3. Expressiveness doesn’t equal learnability. Al-\nthough A4 × Z5 belongs to TC0, it can not\nbe learned by models with circuit complexity\nTC0 on sequences of arbitrary length.\n4. Transformer+CoT achieve a dual win in both\nexpressiveness and learnability. The model\nnot only achieves higher expressiveness than\nthe vanilla transformer but also successfully\nconverges to sequences that are 100 times the\nnumber of layers.\nPerformance (Out-of-Distribution).\n(Liu et al.,\n2023a) analyze transformers’ failure in out-of-\ndistribution of parity, and argue that transformers\nlearn a shortcut solution, which compute the parity\nby counting the number of 1s in the input sequences\nand compute mod-2, thus failing to generalize to\nsequences with unseen sum of 1s. (Zhang et al.,\n2024) discuss the role of chain-of-thought in com-\nputability, and point that CoT simulates the recur-\nrent connection by iteratively encode and decode\nback and forth between states and tokens. The\nkey of Transformer+CoT’s expressiveness on state\ntracking is that previous computation can be re-\ntrieved, through concatenating previous step state\nto the end of scratchpad.\nTo investigate whether Transformer+CoT learns\nan algorithm based solely on the input sequences\nm1 . . . mn or combines input and scratchpad as the-\noretical work expects, We divide the elements of\nthe three groups into proper subsets. And we train\nthe model on sequences with m belonging to one\nproper subset, but evaluate on the full set. We stress\nthat, through restricting the input sequences into\nseparate subsets, the possible state sequences re-\nmain the same, for the reason that any subset can ex-\npress the whole group through group operation. If\nthe model learns a shortcut solution, that attends to\nonly input, it can not generalize to sequences with\ngroup elements sampled from the full set, because\nthe model has not seen mixed input sequences in\nthe training set. As outlined in Section 3, both\nLSTM and Transformer+CoT successfully learn\nall word problems in-distribution, and we test their\nperformance in out-of-distribution. Results in Fig-\nure 2 show that Transformer+CoT achieves perfect\ngeneralization on three groups Z60, A4 × Z5, A5\nin contrast to LSTM, implying that the model at-\ntends to not only input but also scratchpad. We\nprovide more experimental settings in Appendix C.\nThe out-of-distribution performance eliminates the\npossibility that the model learns specific shortcuts\nsimilar to those Liu et al. (2023a) found on Z2\ngroup, but this does not rule out the possibility that\nother shortcuts exist, which necessitates an inter-\npretation on the mechanism.\n4\nMechanism Inside Transformer+CoT:\nFSA\nBoth transformers with chain-of-thought and recur-\nrent neural network achieve perfect performance\nwithin distribution, and the former even generalize\nwell in out-of-distribution. Due to the transformer’s\nblack-box nature, the mechanism behind its imple-\nmentation of state tracking remains unknown, and\nwe cannot answer the questions that what algorithm\nthe model has learned to keep track of world state,\nand to what extent the model can generalize. Con-\nsidering that the word problems involves a series\nof state transitions, the model’s state computation\nis dynamic and consecutive while generating inter-\nmediate steps. In this section, we try to analyze the\ncircuit transformers with chain-of-thought use to\nkeep track of world state first, and then hold deep\ncomponent analysis to interpret the mechanism.\n4.1\nCircuit Localization\nTo understand the mechanism of state tracking, we\nwill first localize the circuit (a subset of compo-\nnents) using activation patching (Vig et al., 2020).\nWe format the word problems as prompt pi =\nm1 . . . mn|qi . . . qi−1 and result state token qi at\ni-th step. At each intermediate step, we sample\na prompt pi with its corresponding result state qi,\nand then sample a counterfactual prompt p′\ni, re-\nsulting in a different result state q′\ni. We hold in-\ntervention experiments, by replacing the activation\nof a specific MLP layer or attention head with the\npre-computed for p′\ni, and then assessing how this\nimpacts the probabilities of answer tokens. To ac-\ncess the importance of one component at i-th step,\nwe average the following intervention effect (IE)\nmetric (Nikankin et al., 2024) across all prompts,\n\n\n         \n                    \n       \n  \n   \n \n \n  \n    \n  \n  \n   \n \n \n  \n    \n  \n  \n   \n \n \n  \n    \n  \n  \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n               \n           \n      \n    \n     \n     \n         \n  \n   \n           \n               \nFigure 2: Model accuracy across sequence lengths for Z60, A4 × Z5 and A5.\nwhich is the mean of impacts of qi and q′\ni:\nIE(qi, q′\ni) = 1\n2\n\u0014P∗(q′\ni) −P(q′\ni)\nP(q′\ni)\n+ P(qi) −P∗(qi)\nP∗(qi)\n\u0015\nwhere P and P∗are the pre- and post-intervention\nprobability distributions.\nWe localize the circuit Transformer+CoT use to\nkeep track of world state at each intermediate step\nin A5 word problems. As Figure 3 shows, we find\nthat the circuit across each intermediate step mainly\nconsists of MLPs, with attention heads few impacts.\nAnd across each intermediate step, the circuit has\nhardly changed, that is, the first MLP (MLP0 Mc-\nDougall et al., 2023) in position mi and late-layer\nMLPs in last position play a significant role in\nstate tracking at the i-th step. Above all, given in-\nput sequences m1 . . . mn ∈M∗concatenated with\nscratchpad q1 . . . qi−1 ∈M∗, the MLPs mainly im-\nplement state transition, and the late-layer MLPs in\nthe last position qi−1 promote the correct token qi.\nm1 m2 m3 m4 m5 m6 m7 m8 m9 m10 :\nq1 q2 q3 q4\n0\n2\n4\n6\n8\n10\n0\n2\n4\n6\n8 10\n−10\n0\n10\nPosition\nHead Index\nLayer\nMLPs\nHeads\nFigure 3: Activation patching results for A5 word\nproblems of length 10 at 4-th step, with the prompt\np5 = m1 . . . m10|q1 . . . q4 and ground truth q5.\n4.2\nMLP Neuron Analysis\nHaving identified the circuit, we then hold deeper\ncomponent analysis on how late-layer MLPs im-\nplement state tracking. Geva et al. (2022) point\nout that the MLP contributes additive updates to\nthe residual stream, which can be further decom-\nposed into weighted collections of sub-updates. In\nparticular, given input xl at layer l, MLP can be ex-\npressed using the parameters Kl, V l ∈Rdmlp×dm,\nwhere dmlp is the MLP intermediate dimension\nand dm is the model dimension. Additionally, a\nnon-linear activation function f is applied:\nMLP l(xl) = f(Klxl)V l\nExpanding this further, it can be decomposed as:\nMLP l(xl) =\ndmlp\nX\nj=1\nf(xl · kl\nj)vl\nj =\ndmlp\nX\nj=1\nml\njvl\nj\nwhere kl\nj ∈Rd and vl\nj ∈Rd correspond to the\nj-th row vectors of Kl and V l, respectively. The\nscalar ml\nj = f(xl · kl\nj) represents the activation\ncoefficient associated with the neuron vl\nj. Notably,\nwhen mapped into the vocabulary space, these in-\ndividual sub-updates ml\njvl\nj can be interpreted in a\nhuman-understandable manner (Geva et al., 2022).\nWe first group all possible prompts into prompt\nsubsets according to the resulting world state.\nSpecifically, any prompt m1 . . . mn|qi . . . qi−1 at\ni-th step in one subset should reduce to the same\nstate qi under group operation (for simplicity, de-\nnoted as q prompt subset3). On each q prompt\nsubset, we calculate the contribution of each late-\nlayer MLP to q in the layer representation using\nlogit lens (Geva et al., 2021) and average on all\nintermediate steps. The results in Figure 4 show\nthat, among late-layer MLPs, the last three layers\nof MLP play a significant role and MLP11 mainly\nimplements the state transition, accounting for the\n72% logit increase.\n3We use q to represent qi\n\n\n0\n10\n20\n30\n40\n50\n60\n0\n10\n20\n30\nLogit Increase\nLayer 6\nLayer 7\nLayer 8\nLayer 9\nLayer 10\nLayer 11\nFigure 4: Average state q logit increase in the layer\nrepresentation for different late-layer MLPs.\nTo analyze what the MLP neurons’ activation\npattern is, we divide the prompts into differ-\nent (mi, qi−1) pairs (for simplicity, (m, q) pairs4),\nwhere mi is the i-th input element, and qi−1 is\nthe previous world state. We analyze the activa-\ntion pattern of MLP neurons in layer l across all\n(m, q) pairs. Interestingly, late-layer MLP neu-\nrons get activated only on a specific subset of all\n(m, q) pairs (denoted as predictions). More specifi-\ncally, the top 60 (m, q) pairs are distributed across\neach row or column. Interestingly, according to\nthe transition rules, there are 60 (m, q) pairs deduc-\ning to state q (corresponding to q prompt subset),\ndistributed across each row and column (denoted\nlabels). Based on this, we can ask the following\nquestions:\n• Q1: Does the activation pattern correspond\nto a bag of MLP neurons being activated on\nthe q prompt subset, which deduces to state q\naccording to transition rules?\n• Q2: Can we classify the MLP neurons accord-\ning to the resulting states?\nTo answer the questions, we hold neurons clas-\nsification experiments (with the detailed proce-\ndures provided in Appendix D) to classify all\nMLP neurons at layer l.\nThrough this experi-\nment, we successfully mapped the MLP11, MLP10\nand MLP9 neurons to states with successful ratio\n90.0%, 79.6% and 38.7%. Furthermore, nearly\n15.5% MLP11 neurons even have perfect f1 score\nwith the ground truth subset of (m, q) pairs, mean-\ning the neuron to state mapping N to 1. After clas-\nsifying neurons into states, we can compute the\naccuracy and recall of the activated neurons, refer\nto Appendix F for details.\nFor Q1, most neurons have high f1 score with\nground truth subset of (m, q) pairs of state q, which\nmeans that the neurons get activated on q prompt\nsubset, which deduce to state q according to tran-\nsition rules. For Q2, we can classify all activated\n4We use q to represent qi−1.\nneurons at layer l across intermediate steps, and\neven 15.5% MLP11 neurons satisfy N to 1 mapping.\nWe can conclude that transformers with chain-of-\nthought keep track of world state through a bag\nof late-layer MLP neurons, which get activated in\nspecific subset of (m, q) pairs, and promote cor-\nrect state token q to the residual stream. Moreover,\na correct state transition is achieved through the\ncollective sub-updates of multiple neurons, that be-\nlong to the state q. Beyond the late-layer MLPs, we\nalso analyze another significant component of the\ncircuit, MLP0, which primarily achieves effective\nembedding of mi for subsequent state transitions.\nFurther details can be found in Appendix G.\n4.3\nTransformer+CoT Recovers FSA\nNearly perfect in next-token prediction does not\nnecessarily imply that the generative model has\nreconstructed the world model to the same extent,\nfor example, cumulative Connect-4 in (Vafa et al.,\n2024). In terms of state tracking, the world model\nFSA can compress different sequences as long as\nthey deduce to the same state, and distinguish se-\nquences as long as they deduce to different states,\nno matter how similar the sequences are. As a\ngenerative model, Transformer+CoT has achieved\nnearly perfect sequence accuracy in word prob-\nlems, while the learned structure remains under-\nresearched. Results in Section 4.2 imply that the\nmodel implements state tracking, through activat-\ning a bag of neurons. Based on this, we can mea-\nsure the generative model’s FSA recovery, from\nthe perspective of sequence compression and dis-\ntinction at the granularity of the MLP neurons. For\nactivated neurons on two prompts pi, p′\ni deducing\nto the same state, we design the following compres-\nsion metric:\nCompression =\n|Npi ∩Np′\ni|\n|Npi ∪Np′\ni|\nwhere Npi, Np′\ni represent activated neurons on\npi, p′\ni. And for activated neurons on two prompts\npi, p′\ni deducing to different states, we design the\nfollowing distinction metric:\nDistinction = 1 −\n|Npi ∩Np′\ni|\n|Npi ∪Np′\ni|\nFor each intermediate step, we calculate compres-\nsion and distinction pairwise in all m, q pairs. We\nfind that Transformer+CoT can not only compress\nprompts sampled from the q prompt subset, but also\n\n\ndistinguish prompts from different subsets with av-\nerage metric close to 1, at each intermediate step.\nBeyond A5, we also find the similar FSA existing\non Z60 and A4 × Z5, and large model scales, with\ndetails referring to Appendix H. And we conclude\nthat Transformer+CoT recovers FSA on state track-\ning even at the granularity of MLP neurons.\n4.4\nDynamic Circuit Analysis: Attention\nPatterns\nIn the context of state tracking, the model’s circuit\nis dynamic and consecutive during inferring each\nstate qi in the scratchpad. Previous sections mainly\nanalyze the MLP circuit as static circuit composed\nof late-layer MLPs, in this section we focus on\nthe dynamic attention patterns across intermedi-\nate steps. We first obtain the principal attention\nheads based on the activation patching results in\nSection 4.1 and analyze their activation patterns.\nResults in Figure 5 show that there are two kinds\nof attention patterns:\n1. Across all steps, specific attention heads trans-\nfer information from position mi to position\nqi−1.\n2. As the sequence length increases, the model\nhas developed another attention pattern: the\nmodel passes the information from m1 . . . mn\nbackwards until the delay position “:”, which\nsplits input and scratchpad and marks the be-\nginning of state tracking, so that the model can\nattend to the delay position during inference.\nWe discuss more about the two attention patterns\nin Appendix I.\nBOS\nm1\nm2\nm3\nm4\nm5\nm6\nm7\nm8\nm9\nm10\n:\nq1\nq2\nq3\nq4\nBOS\nm1\nm2\nm3\nm4\nm5\nm6\nm7\nm8\nm9\nm10\n:\nq1\nq2\nq3\nq4\nBOS\nm1\nm2\nm3\nm4\nm5\nm6\nm7\nm8\nm9\nm10\n:\nq1\nq2\nq3\nq4\n−1\n−0.5\n0\n0.5\n1\nSource Position\nSource Position\nDestination Position\nFigure 5: Two attention patterns.\n5\nRobustness and Generalizability of FSA\nSections 3 and 4 have demonstrated positive results\nin FSA recovery. However, the word problems\nconsidered are too idealistic, with sequence length\ncontrolled and noise excluded. And the actual data\ndistribution may differ from this. In this section, we\nwill investigate the robustness and generalizability\nof FSA inside Transformer+CoT, considering inter-\nmediate step jumps, scratchpad noise, and length\ngeneralization. We found that optimizing the dis-\ntribution of training set data can guide the learned\nmodel to adapt to the above scenarios, except for\nlength generalization. And solving the latter may\nnecessitate exploring different perspectives, such\nas modifying the model architecture, improving\ntraining strategies, or other.\n5.1\nAllow Skipping\nIn practical training data, intermediate step jumps\nare very common.\nFor example, the length of\nq1 . . . qn may be less than n, with some intermedi-\nate steps skipped. In our experiment setting, we\nskip each intermediate state qi except qn with skip-\nping probability, and train Transformer+CoT in\nthe dataset. In order to study the extent of the\nmodel’s step jumps, we use a linear classifier to\nprobe the states embedded in the residual stream\nin the last position. Specifically, we probe the\ntoken qi in the last position, given the prompt\nm1 . . . mn|q1 . . . qi−1. From the results in Figure 6,\nwe can find that qi, qi+1, and qi+2 are all embedded\nin the residual stream, suggesting that the model\nhas learned single-step reasoning, two-step reason-\ning (skipping one step), as well as three-step rea-\nsoning (skipping two steps). Moreover, we find that\nthe index of layers with sufficiently high probing\nresults for qi+2 is greater than that for qi+1, which\nis greater than that for qi in turn. Moreover, we pro-\npose possible mechanisms for skipping and design\nexperiments for analysis as in the appendix J. Fi-\nnally, we conclude that adding skipping to the train-\ning set can guide the FSA inside Transformer+CoT\nto adapt to the scenario.\n5.2\nNoise in Scratchpad\nIn this section, we consider injecting noise into\nscratchpad q1 . . . qi1, where some previous state\nis false. We divide the noise into the following\ntwo types: any false state except qi−1 and false\nqi−1. We have tested the accuracy of the model in\npredicting the next token with noise in scratchpad\n(we use token accuracy as metric here). For the\nfirst type noise, the model still achieves almost\nperfect predictions. The reason is obvious: when\nthe model implements state transition in the i-th\n\n\nm1\nm2\nm3\nm4\nm5\nm6\nm7\nm8\nm9 m10\n:\nq1\nq2\nq3\nq4\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n−0.5\n0\n0.5\nPosition\nLayer\nFigure 6: Probing results for state qi at 4-th step. We can\nfind the positive results appearing two positions earlier.\nstep, it mainly focuses on mi and the previous state\nqi−1, while ignoring intermediate states prior to\nqi−1. And for the second type of noise, the model’s\nperformance dramatically decreases to an average\naccuracy of nearly zero (0.017).\nHere, we attempt once again to deal with the sce-\nnario by adjusting the dataset distribution. Specif-\nically, we train the model using the data with\nthe second type of noise in scratchpad and suc-\ncessfully enhanced the robustness of the model,\nwith the second type noise accuracy increased\nfrom 0.017 to 0.896. Furthermore, we probe the\nlayer representation in the last position to investi-\ngate whether the model has retrieved correct state\ntracking. Specifically, given the corrupt prompt\nm1 . . . mn|q1 . . . ˆ\nqi−1 ( ˆ\nqi−1 stands for the error\nstate), we probe the correct next state qi, false pre-\nvious state\nˆ\nqi−1 and true previous state qi−1. We\nfind that the layer representation maintains positive\nprobing results for\nˆ\nqi−1 until the MLP11, which\nexhibits high accuracy for qi , and low for qi−1.\nBased on this, we propose one possible mechanism\nthat the model attends to intermediate representa-\ntion at previous step, thus retrieving the correct\ninput state qi−1 and updating the state to qi suc-\ncessfully. And we have designed experiments to\nanalyze the hypothesis as in Appendix K. We con-\nclude that optimizing dataset distribution can help\nin maintaining the accuracy and reliability of the\nFSA inside Transformer+CoT.\n5.3\nLength Generalization and Error Analysis\nFurthermore, we investigate length generalization\nin out-of-distribution setting, which is a critical\nchallenge in the development of transformer-based\nlanguage models. We replace GPT2’s original abso-\nlute positional embedding with NoPE (without ex-\nplicit positional embedding) for better length gen-\neralization performance, which has been proven\nto resemble T5’s relative positional embedding\n(Kazemnejad et al., 2024). And we choose LSTM\nfor comparison for the reason that it learns any\nword problems on sequences of arbitrary length.\nTable 1 shows that on sequences of lengths that it\nhas not ever seen during training, different mod-\nels exhibit varying levels of length generalization\nability: LSTM perfect, while transformers with\nchain-of-thought weak. We analyze the failure\nfrom the perspective of MLP neurons, and find an\ninteresting “U-turn” phenomenon in activated neu-\nrons precision during the last few steps of inference.\nMore analysis on this phenomenon refers to the Ap-\npendix L. Finally, we conclude that improving the\nlength generalization ability of the model requires\nother methods, perhaps the model structure, loss\nfunction or optimization algorithm.\nModels\n20\n21\n22\n23\n24\n25\n30\nLSTM\n1\n1\n1\n1\n1\n1\n1\nTransformer+CoT\n0.99\n0.983\n0.868\n0.527\n0.24\n0.088\n0\nTable 1: Length generalization performance of LSTM\nand Transformer+CoT. The models are trained on se-\nquences of length up to 20.\n6\nConclusions\nIn this work, we investigate the learnability of\nTransformer+CoT through the lens of mechanistic\ninterpretability in state tracking. We begin by reaf-\nfirming the success of Transformer+CoT in both\nin-domain and out-of-domain settings. Next, we\nidentify the key components of the circuit, specif-\nically the neurons in the last layers of the MLP,\nwhich play a critical role. We find evidence of an\nimplicit FSA within the model, where each state\nis compressed by a distinct set of neurons. Finally,\nwe evaluate the model in three realistic settings and\nshow that the learned FSA is robust to noise and se-\nquence skipping but struggles with generalization\nto significantly longer sequences. This suggests\nthe need for architectural or optimization improve-\nments. Our findings reveal that Transformer+CoT\nachieves near-perfect performance in next-token\nprediction while internalizing an FSA-like struc-\ntured state representation, bridging the gap between\nexpressiveness and learnability. This work provides\ninsights into structured reasoning for sequential\ntasks.\n\n\nLimitations\nIn this paper, we define state tracking as word prob-\nlems involving cyclic and symmetric groups, which\nform the basis of our conclusions. Our study fo-\ncuses solely on GPT2-like models, leaving the ex-\nploration of additional models, such as Llama, for\nfuture research. Furthermore, while we have made\nefforts to address more realistic word problems,\nstate tracking in real-world tasks is intricate, mak-\ning it challenging to separate state tracing abilities\nfrom other skills.\nReferences\nSatwik Bhattamishra, Kabir Ahuja, and Navin Goyal.\n2020. On the Ability and Limitations of Transform-\ners to Recognize Formal Languages. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n7096–7116, Online. Association for Computational\nLinguistics.\nAlan Cooney and Neel Nanda. 2023.\nCircuitsvis.\nhttps://github.com/TransformerLensOrg/\nCircuitsVis.\nHoagy Cunningham, Aidan Ewart, Logan Riggs, Robert\nHuben, and Lee Sharkey. 2023. Sparse autoencoders\nfind highly interpretable features in language models.\nPreprint, arXiv:2309.08600.\nGrégoire Delétang, Anian Ruoss, Jordi Grau-Moya, Tim\nGenewein, Li Kevin Wenliang, Elliot Catt, Chris\nCundy, Marcus Hutter, Shane Legg, Joel Veness, and\nPedro A. Ortega. 2023. Neural networks and the\nchomsky hierarchy. Preprint, arXiv:2207.02098.\nAngela Fan, Thibaut Lavril, Edouard Grave, Armand\nJoulin, and Sainbayar Sukhbaatar. 2021. Address-\ning some limitations of transformers with feedback\nmemory. Preprint, arXiv:2002.09402.\nYing Fan, Yilun Du, Kannan Ramchandran, and Kang-\nwook Lee. 2024. Looped transformers for length\ngeneralization. Preprint, arXiv:2409.15647.\nJavier Ferrando, Gabriele Sarti, Arianna Bisazza, and\nMarta R. Costa-jussà. 2024. A primer on the in-\nner workings of transformer-based language models.\nPreprint, arXiv:2405.00208.\nMor Geva, Avi Caciularu, Kevin Wang, and Yoav Gold-\nberg. 2022. Transformer feed-forward layers build\npredictions by promoting concepts in the vocabulary\nspace. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 30–45, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer\nLevy. 2021. Transformer feed-forward layers are\nkey-value memories. Preprint, arXiv:2012.14913.\nSachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Kr-\nishna Menon, Sanjiv Kumar, and Vaishnavh Na-\ngarajan. 2024.\nThink before you speak: Train-\ning language models with pause tokens. Preprint,\narXiv:2310.02226.\nRiccardo Grazzi, Julien Siems, Jörg K. H. Franke, Arber\nZela, Frank Hutter, and Massimiliano Pontil. 2024.\nUnlocking state-tracking in linear rnns through nega-\ntive eigenvalues. Preprint, arXiv:2411.12537.\nAlbert Gu and Tri Dao. 2024.\nMamba:\nLinear-\ntime sequence modeling with selective state spaces.\nPreprint, arXiv:2312.00752.\nAlbert Gu, Karan Goel, and Christopher Ré. 2022. Effi-\nciently modeling long sequences with structured state\nspaces. Preprint, arXiv:2111.00396.\nWes Gurnee, Neel Nanda, Matthew Pauly, Katherine\nHarvey, Dmitrii Troitskii, and Dimitris Bertsimas.\n2023. Finding neurons in a haystack: Case studies\nwith sparse probing. Preprint, arXiv:2305.01610.\nSimeng Han, Hailey Schoelkopf, Yilun Zhao, Zhent-\ning Qi, Martin Riddell, Wenfei Zhou, James Coady,\nDavid Peng, Yujie Qiao, Luke Benson, Lucy Sun,\nAlex Wardle-Solano, Hannah Szabo, Ekaterina\nZubova, Matthew Burtell, Jonathan Fan, Yixin Liu,\nBrian Wong, Malcolm Sailor, Ansong Ni, Linyong\nNan, Jungo Kasai, Tao Yu, Rui Zhang, Alexander R.\nFabbri, Wojciech Kryscinski, Semih Yavuz, Ye Liu,\nXi Victoria Lin, Shafiq Joty, Yingbo Zhou, Caim-\ning Xiong, Rex Ying, Arman Cohan, and Dragomir\nRadev. 2024. Folio: Natural language reasoning with\nfirst-order logic. Preprint, arXiv:2209.00840.\nShibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li,\nZhiting Hu, Jason Weston, and Yuandong Tian. 2024.\nTraining large language models to reason in a contin-\nuous latent space. Preprint, arXiv:2412.06769.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long\nshort-term memory. Neural Computation, 9(8):1735–\n1780.\nJ. E. Hopcroft and J. D. Ullman. 1979. Introduction\nto Automata Theory, Languages, and Computation.\nAddison-Wesley, Reading, MA.\nL. C. Jain and L. R. Medsker. 1999. Recurrent Neu-\nral Networks: Design and Applications, 1st edition.\nCRC Press, Inc., USA.\nAmirhossein\nKazemnejad,\nInkit\nPadhi,\nKarthikeyan Natesan Ramamurthy,\nPayel Das,\nand Siva Reddy. 2024.\nThe impact of positional\nencoding on length generalization in transformers.\nIn Proceedings of the 37th International Conference\non Neural Information Processing Systems, NIPS\n’23, Red Hook, NY, USA. Curran Associates Inc.\nNajoung Kim and Sebastian Schuster. 2023. Entity\ntracking in language models. In Proceedings of the\n61st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n\n\n3835–3855, Toronto, Canada. Association for Com-\nputational Linguistics.\nZhiyuan Li, Hong Liu, Denny Zhou, and Tengyu\nMa. 2024. Chain of thought empowers transform-\ners to solve inherently serial problems. Preprint,\narXiv:2402.12875.\nBingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay\nKrishnamurthy, and Cyril Zhang. 2023a.\nTrans-\nformers learn shortcuts to automata.\nPreprint,\narXiv:2210.10749.\nXiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xu-\nanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding,\nKaiwen Men, Kejuan Yang, Shudan Zhang, Xiang\nDeng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang,\nSheng Shen, Tianjun Zhang, Yu Su, Huan Sun,\nMinlie Huang, Yuxiao Dong, and Jie Tang. 2023b.\nAgentbench: Evaluating llms as agents. Preprint,\narXiv:2308.03688.\nCallum McDougall, Arthur Conmy, Cody Rushing,\nThomas McGrath, and Neel Nanda. 2023.\nCopy\nsuppression: Comprehensively understanding an at-\ntention head. Preprint, arXiv:2310.04625.\nWilliam Merrill, Jackson Petty, and Ashish Sabharwal.\n2024. The illusion of state in state-space models.\nPreprint, arXiv:2404.08819.\nMerrill, William and Sabharwal, Ashish. 2024. A logic\nfor expressing log-precision transformers. In Pro-\nceedings of the 37th International Conference on\nNeural Information Processing Systems, NIPS ’23,\nRed Hook, NY, USA. Curran Associates Inc.\nYaniv Nikankin, Anja Reusch, Aaron Mueller, and\nYonatan Belinkov. 2024. Arithmetic without algo-\nrithms: Language models solve math with a bag of\nheuristics. Preprint, arXiv:2410.21272.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,\nHenryk Michalewski, Jacob Austin, David Bieber,\nDavid Dohan, Aitor Lewkowycz, Maarten Bosma,\nDavid Luan, Charles Sutton, and Augustus Odena.\n2021. Show your work: Scratchpads for interme-\ndiate computation with language models. Preprint,\narXiv:2112.00114.\nOpenAI. 2023. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774.\nRuizhong Qiu, Zhe Xu, Wenxuan Bao, and Hanghang\nTong. 2024. Ask, and it shall be given: Turing com-\npleteness of prompting. Preprint, arXiv:2411.01992.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nDaking Rai, Yilun Zhou, Shi Feng, Abulhair Saparov,\nand Ziyu Yao. 2024. A practical review of mecha-\nnistic interpretability for transformer-based language\nmodels. Preprint, arXiv:2407.02646.\nShubham Toshniwal, Sam Wiseman, Karen Livescu,\nand Kevin Gimpel. 2022.\nChess as a testbed\nfor language model state tracking.\nPreprint,\narXiv:2102.13249.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. Preprint,\narXiv:2302.13971.\nKeyon Vafa, Justin Y. Chen, Ashesh Rambachan, Jon\nKleinberg, and Sendhil Mullainathan. 2024. Evaluat-\ning the world model implicit in a generative model.\nPreprint, arXiv:2406.03689.\nKarthik Valmeekam, Kaya Stechly, and Subbarao Kamb-\nhampati. 2024. Llms still can’t plan; can lrms? a\npreliminary evaluation of openai’s o1 on planbench.\nPreprint, arXiv:2409.13373.\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov,\nSharon Qian, Daniel Nevo, Yaron Singer, and Stu-\nart Shieber. 2020. Investigating gender bias in lan-\nguage models using causal mediation analysis. In\nProceedings of the 34th International Conference on\nNeural Information Processing Systems, NIPS ’20,\nRed Hook, NY, USA. Curran Associates Inc.\nKevin Wang, Alexandre Variengien, Arthur Conmy,\nBuck Shlegeris, and Jacob Steinhardt. 2022.\nIn-\nterpretability in the wild:\na circuit for indirect\nobject identification in gpt-2 small.\nPreprint,\narXiv:2211.00593.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2023. Chain-of-thought prompting elic-\nits reasoning in large language models. Preprint,\narXiv:2201.11903.\nJiewen Yang, Xingbo Dong, Liujun Liu, Chao Zhang,\nJiajun Shen, and Dahai Yu. 2022. Recurring the\ntransformer for video action recognition. In 2022\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 14043–14053.\nXiang Zhang, Muhammad Abdul-Mageed, and Laks\nV. S. Lakshmanan. 2024. Autoregressive + chain of\nthought = recurrent: Recurrence’s role in language\nmodels’ computability and a revisit of recurrent trans-\nformer. Preprint, arXiv:2409.09239.\nHattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin,\nOmid Saremi, Josh Susskind, Samy Bengio, and Pree-\ntum Nakkiran. 2023. What algorithms can transform-\ners learn? a study in length generalization. Preprint,\narXiv:2310.16028.\nA\nSymbol Description\nIn this section, we briefly introduce the symbols\nused in this article as in Table 2.\n\n\nB\nModel used for Performance\nEvaluation\nIn Section 3, we hold a comprehensive evaluation\non various model vatiants and implicit CoT. The\nmodels and task designs are listed as Table 3.\nC\nSeparating Elements in Groups\nIn Section 3, we separate all m into proper subsets\nto explore the models’ performance. As Table 4\nshows, we divide m in Z60, A4_x_Z5 and A5 ac-\ncording to values, orders and permutation types,\nrespectively. We then prove that the division is rea-\nsonable, for the reason that every proper subset can\ngenerate the whole group.\nTheorem 1 Every sepated proper subset can gen-\nerate the whole group under group operation.\nProof C.1 For Cyclic group, every element is a\ngenerator, so that proper subsets can generate h ∈\nZ60.\nFor alternating group, the 3-cycle and 5-cycle in\nA5 can both generate the group.\nFor direct product group, we continue the proof\nas follows:\nThe elements of order 15 in the direct product\ngroup A4×Z5 are those elements of the form (g, h),\nwhere g ∈A4 is a 3-cycle (order 3) and h ∈Z5\nis a nonzero element (order 5). Although these\nelements of order 15 form a proper subset of the\nentire group, their projections onto the factors A4\nand Z5 separately generate the full groups:\n• The 3-cycle in A4 can generate A4 (since A4\nis generated by its 3-cycles).\n• Any nonzero element in Z5 is a generator, thus\ncan generate the entire Z5.\nBy the generation theorem of direct product groups,\nif a subset has surjective projections onto each\nfactor, then the group it generates must be the entire\ndirect product. Thus, all elements of order 15 can\ngenerate the entire group A4 × Z5.\nWe train the model with sequences sampled\nfrom one subset, and evaluate on sequences with\nm sampled from the group. We emphasize that\nAlthough the model did not encounter mixed se-\nquences during training, the proper subset is able\nto generate the entire group. Therefore, all possible\nstate transitions are included in the data. Trans-\nformer+CoT success in out-of-distribution shows\nthat the learned algorithm does not rely on finding\na certain pattern in the input sequence.\nSurprisingly, LSTM fails to generalize when\nused as an encoder. We have also explored its\nperformance as a decoder, but here, LSTM even\nfails to converge. We hypothesize that the dispar-\nity in out-of-distribution performance stems from\nLSTM being used as an encoder, whereas Trans-\nformer+CoT, as a decoder, can integrate all ele-\nments of the group into the sequence by concate-\nnating each possible state after the input sequence.\nD\nClassification Algorithms\nWe provide the late-layer MLP neurons classifica-\ntion procedures and pseudocode Algorithm 1 as\nfollows:\n1. Utilize the priori transition rules to pre-\ncompute all labels across all possible resulting\nstates.\n2. Measure all activation coefficient ml\nj across\nall (m, q) pairs.\n3. Utilize the logit lens to calculate the logits of\ntokens embedded in vl\nj, and convert to a 2D\npattern, where the cell in index (m, q) is the\nlogit of the result state q = m · q.\n4. Multiply the intermediate results of the pre-\nvious two steps element-wise, resulting in ef-\nfective logit contribution of the neuron to the\ncorresponding state for each (m, q) pair.\n5. Extract the top 60 (m, q) pairs from the acti-\nvation pattern as the prediction.\n6. Compute the f1 scores between prediction and\nall labels. The neuron can be classified to state\nwith f1 score no less than threshold θ = 0.25.\nE\nExpected F1 Score of Randomly\nActivated Neurons\nIn Section 4.2, we empirically set the value of θ as\n0.2 to classify MLP neurons, which is non-trivial\ncompared with random F1 score 0.017. And we\nprovide detailed calculation process of randomly\nactivated neurons’ F1 score as follows.\nLet S be a set containing 3600 elements:\nS = {0, 1, 2, . . . , 3599}, and let L be the la-\nbel set consisting of the first 60 elements: L =\n{0, 1, 2, . . . , 59}.\nWe randomly sample 60 ele-\nments from S to form a subset x: x ⊆S and\n5Here the value of θ is an empirical setting, and the random\nvalue is 0.017, refer to Appendix E for the computation of the\nrandom value.\n\n\nAlgorithm 1 Classify MLP Neurons at Layer l\n1: Input: Prior transition rules T, activation co-\nefficients matrix Ml\nj for all (m, q) pairs, value\nvector vl\nj, threshold θ\n2: Output: Classified neurons\n3: Compute all labels L from T\n4: for each neuron j in layer l do\n5:\nCompute logit matrix Zj for all (m, q)\npairs from vl\nj using logit lens\n6:\nCompute effective logit contribution Cj =\nMl\nj ⊙Zj\n7:\nSelect top 60 (m, q) pairs from Cj as pre-\ndiction Pj\n8:\nCompute F1 score between Pj and L\n9:\nif F1 score ≥θ then\n10:\nClassify neuron j as relevant\n11:\nelse\n12:\nClassify neuron j as irrelevant\n13:\nend if\n14: end for\n15: return Classified neurons\n|x| = 60. Our goal is to compute the expected val-\nues of precision, recall, and F1 score for the subset\nx with respect to the label set L.\nDefine T = |x ∩L| as the number of correctly\nselected elements. Since x is chosen uniformly\nat random from S, and given that L contains 60\nelements, T follows a hypergeometric distribution.\nIts expected value is:\nE[T] = |x| · |L|\n|S|\n= 60 × 60\n3600\n= 1.\nThe precision and recall are defined by\nPrecision = |x ∩L|\n|x|\n= T\n60\nRecall = |x ∩L|\n|L|\n= T\n60\nThus, their expected values are\nE[Precision] = E\n\u0014 T\n60\n\u0015\n= E[T]\n60\n= 1\n60,\nE[Recall] = 1\n60.\nThe F1 score is the harmonic mean of precision\nand recall:\nF1 = 2 · Precision · Recall\nPrecision + Recall .\nSince Precision = Recall =\nT\n60, we substitute to\nobtain\nF1 = 2 · T\n60 · T\n60\nT\n60 + T\n60\n= 2 ·\nT 2\n3600\n2T\n60\n= T\n60.\nHence, the expected F1 score is\nE[F1] = E\n\u0014 T\n60\n\u0015\n= 1\n60 ≈0.01667.\nF\nActivated Neurons Analysis\nHaving classified neurons into states as in Sec-\ntion 4.2, We can compute the accuracy and recall of\nthe activated neurons, averaged over all q prompt\nsubsets, during the single-step state transition of\nthe model. We acquire the top-K activated neurons\naccording to the activation coefficient ml\nj, and com-\npute the precision and recall with classified neurons\nas labels. Results in Figure 7 show that across in-\ntermediate steps, the model successfully activate\nMLP11 neurons with high precision 0.797 but low\nrecall 0.253. We stress that the high precision ex-\nplains the accuracy on word problems. The low\nrecall imply that, the eurons activated at different\nsteps are also distinguished.\n1\n2\n3\n4\n5\n6\n7\n8\n9\nScratchpad Step\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nValue\nPrecision (Layer 9)\nRecall (Layer 9)\nPrecision (Layer 10)\nRecall (Layer 10)\nPrecision (Layer 11)\nRecall (Layer 11)\nFigure 7: Averaged precision and recall across different\nsteps.\nG\nMLP0 in Circuit\nActivation patching results in Section 4.1 show that\nthe circuit mainly consists of MLP0 and late-layer\nMLPs. Given a prompt m1 . . . mn|q1 . . . qi−1, the\nlate-layer MLPs implement state transition in the\npostion qi−1, while the MLP0 mainly achieves ef-\nfective word embedding in position mi. As Figure8\nshows, the MLP0 promotes input mi into the resid-\nual stream, which will then be transferred to the\nlast position qi−1 by attention heads for subsequent\nstate transitions.\n\n\n0\n10\n20\n30\n40\n50\n60\n0\n2\n4\n6\n8\nLogit Increase\nMLP0\nFigure 8: Average input m logit increase in the layer rep-\nresentation for MLP0. The MLP0 mainly implements\neffective word embedding in the circuit.\nH\nAutomta still exists on other groups\nwith large model scale.\nWe have found FSA with GPT2-small on A5 as\nin Section4.3, in this section, we will extrapolate\nthis conclusion to other problems and models. We\nfirst conduct similar experiments on A4 × Z5 and\nZ60. We find that MLP11 neurons can still be clas-\nsified according to transition rules, and the model\ncan still compress and distinguish different input\nsequences. Moreover, we explore whether FSA\nstill exists with model scale larger. We have re-\npeated the experiments, increasing the model scale\nfrom GPT2 small (124M) to GPT2 large (744M),\nand found FSA still exists. Results are shown in\nFigure 9.\nI\nDynamic Attention Pattern\nIn terms of the attention patterns in Section 4.4, a\nnatural question is how the attention heads extract\nmi from the numerous input m1 . . . mn embedded\nin the residual stream at the delay position.\nWe hypothesize that this extraction depends on\nthe first type of attention pattern, which has injected\nthe input information mi into the current position,\nfacilitating the second type attention heads to differ-\nentiate the mixed information embedded in delay\nposition. Furthermore, we validate this hypothesis\nby masking position mi at the qi−1 position and\nprobing the changes of retrieving input mi at the\ncurrent step. When there is no mask, the average\naccuracy of probing mi in the representation of the\nmiddle and later layers, except for the last layer\n(high results for qi), is greater than 0.9. However,\nwhen the mask is applied, it drops to nearly 0. We\nconclude that, the model uses two interrelated dy-\nnamic attention patterns to focus on the correct\ninput mi at i-th step, corresponding to the FSA\naccepting each step of the input.\nJ\nAllow Skipping\nAccording to the probing results in Figure 6, we\npropose two possible mechanisms for skipping in\nTransformer+CoT: one is to use continuous lay-\ners to achieve multi-step state transitions, and the\nother is to use a single layer to achieve multi-step\ntransitions. The key difference is that the former\ndepends on the skipped states, while the latter does\nnot. To explore what algorithms the model uses\nin skipping, we design intervention experiments\nby suppressing skipped tokens. Specifically, given\nthe prompt m1 . . . mn|q1 . . . qi−1, we evaluate the\nprobing changes for qi+1 in the last position, while\nsuppressing token qi. We find that suppressing qi\nhas caused the positive probing results to drop al-\nmost to zero in the last position with skipping prob-\nability ranging between 0 and 1, suggesting that\nTransformer+CoT implements skipping through\ncontinuous layers.\nK\nNoise in Scratchpad\nBased on the probing results in Section 5.2, we\npropose that at the i-th step, the model mitigates\nthe influence of the incorrect input state\nˆ\nqi−1 in the\nfinal layer while focusing on the intermediate rep-\nresentation from the previous step. This allows it to\nrecover the correct input state qi−1 and accurately\nupdate to qi.\nTo verify this, we mask the position of qi−2 in\nthe final layer and analyze the resulting changes\nin probing accuracy for state qi at the last position.\nThe results show that after masking, the MLP11\nprobing score drops from above 0.9 to nearly zero,\nindicating that attending to qi−2 in the last layer is\ncrucial for accurate state transitions at the current\nstep. In contrast, we hold ablation experiments\nwith a model trained without noise, and we observe\nminimal change in the probing accuracy for state\nqi while masking. This suggests that the automa-\nton may develop adaptive attention mechanisms to\ncompensate for noise in the scratchpad.\nL\nGeneralization Performance for Other\nLength Ranges\nThere are many factors contributing to weak length\ngeneralization. Here, we explain the model’s fail-\nure in length generalization from the perspective\nof MLP neurons. We calculate neurons precision\nand recall (Section 4.2) on word problems with one\nlength exceeding the training set and another below\nthe training set. Results in Figure 10 show that neu-\nrons’ precision and recall decrease sharply, with\nthe inference step larger than the maximum length\nseen before. Interestingly, we have found that when\n\n\nthe sequence length exceeds the training data by\na small margin, there is a “U-turn” phenomenon\nin precision during the last few steps of inference,\nwhich will yet disappear with the length increasing.\nTo explore the “U-turn” phenomenon, we conduct\nexperiments with models train on various length\nranges and find that the margin is roughly between\n125% and 150%, beyond which the phenomenon\ndisappears. Additionally, this phenomenon also ex-\nists when the test length is slightly shorter than the\ntraining length. This may indicate that activating\ncorrect neurons is not the determining factor limit-\ning length generalization. Finally, we conclude that\nimproving the length generalization ability of the\nmodel requires other methods, perhaps the model\nstructure, loss function or optimization algorithm.\n\n\nTable 2: Symbols used in this paper\nSymbol\nDescription\nΣ\nFinite set of characters\nQ\nFinite set of states in the automaton\nq0\nStart state, an element of the state set Q\nδ\nState-transition function\n(M, ·)\nA monoid\nM∗\nSet of all possible sequences from M\nmi\nThe ith input of in word problems\ne\nThe starting state\nZm\nCyclic group of order m\nSm\nSymmetric group of order m\nTC0\nA complexity class in computational complexity theory, consisting of problems\nsolvable by uniform constant-depth threshold circuits with a polynomial number\nof gates.\nNC1\nA complexity class containing problems solvable by uniform logarithmic-depth\nBoolean circuits with a polynomial number of gates and bounded fan-in.\nZ60\nan abelian group encoding mod-60 addition\nA5\nthe alternating group on five elements\nA4 × Z5\na non-abelian but solvable group\npi\nPrompt at the ith step\nqi\nResult state at the ith step\nˆqi\nError state\np′\ni\nCounterfactual prompt at the ith step\nq′\ni\nCounterfactual result state at the ith step\nP\nPre-intervention probability distribution\nP∗\nPost-intervention probability distribution\nxl\nInput at layer l\nKl\nThe weight matrix at layer l\nV l\nThe bias vector at layer l\ndmlp\nthe MLP intermediate dimension\ndm\nthe model dimension\nf\nNon-linear activation function\nkl\nj\nthe j-th row vectors of Kl\nvl\nj\nthe j-th row vectors of V l\nml\nj\nActivation coefficient at the jth neuron\nNpi\nSet of activated neurons on prompt pi\nNp′\ni\nSet of activated neurons on prompt p′\ni\nLn−1\nLength of the state sequence q1 . . . qn−1 except last state\nLn\nLength of the state sequence q1 . . . qn\n\n\n0\n1000\n2000\n3000\n0\n1000\n2000\n3000\nPrompt Index\nPrompt Index\n(a)\n0\n1000\n2000\n3000\n0\n1000\n2000\n3000\nPrompt Index\nPrompt Index\n(b)\n0\n1000\n2000\n3000\n0\n1000\n2000\n3000\nPrompt Index\nPrompt Index\n(c)\n0\n1000\n2000\n3000\n0\n1000\n2000\n3000\nPrompt Index\nPrompt Index\n(d)\nFigure 9: (a): Compression and distinction metrics for MLP10 on A5. (b): Compression and distinction metrics for\nMLP11 on A4 × Z5. (c): Compression and distinction metrics for MLP11 on Z60. (d): Compression and distinction\nmetrics for MLP10 on A5 with larger model scale. The entire square’s dark color represents that the metric of any\npairwise combination of prompts is nearly 1.\n\n\nModel\nArchitecture\nTask\nRNN\nEncoder\nTT\nLSTM\nEncoder\nTT\nS4\nEncoder\nTT\nMamba\nEncoder\nTT\nTransformer\nEncoder\nTT\nTransformer+CoT\nDecoder\nLM\nRecurrent\nEncoder\nTT\nLooped\nEncoder\nTT\nPause\nDecoder\nLM\nTable 3: Depth and time complexity of different models.\nTT and LM represent Token-Tagging task and Language\nModeling task, respectively.\n1 2 3 4 5 6 7 8 9 10111213141516171819202122232425\nScratchpad Step\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nValue\nPrecision (Step 8)\nRecall (Step 8)\nPrecision (Step 24)\nRecall (Step 24)\nFigure 10: MLP11 neurons precision and recall across\nintermediate steps on A5 with length 8 and 25. The\nmodel is trained on sequences with length ranging from\n10 to 20.\n\n\nGroup\nProper Subsets\nA5\nIdentity and Double Transpositions:\n0, 3, 8, 11, 12, 13, 14, 27, 30, 33, 41, 43, 47, 53, 55, 59\n3-Cycles:\n1, 2, 4, 5, 6, 7, 9, 10, 15, 19, 22, 24, 28, 29, 37, 39, 40, 49, 51, 52\n5-Cycles:\n16, 17, 18, 20, 21, 23, 25, 26, 31, 32, 34, 35, 36, 38, 42, 44, 45, 46,\n48, 50, 54, 56, 57, 58\nA4_x_Z5\nOrder 15:\n6, 7, 8, 9, 11, 12, 13, 14, 21, 22, 23, 24, 26, 27, 28, 29, 31, 32, 33,\n34, 36, 37, 38, 39, 46, 47, 48, 49, 51, 52, 53, 54\nOther Orders:\n0, 15, 40, 55, 5, 10, 20, 25, 30, 35, 45, 50, 1, 2, 3, 4, 16, 17, 18,\n19, 41, 42, 43, 44, 56, 57, 58, 59\nZ60\n< 30:\n0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29\n≥30:\n30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46,\n47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59\nTable 4: Division of elements for three groups in out-of-distribution.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20129v1.pdf",
    "total_pages": 18,
    "title": "Finite State Automata Inside Transformers with Chain-of-Thought: A Mechanistic Study on State Tracking",
    "authors": [
      "Yifan Zhang",
      "Wenyu Du",
      "Dongming Jin",
      "Jie Fu",
      "Zhi Jin"
    ],
    "abstract": "Chain-of-Thought (CoT) significantly enhances the performance of large\nlanguage models (LLMs) across a wide range of tasks, and prior research shows\nthat CoT can theoretically increase expressiveness. However, there is limited\nmechanistic understanding of the algorithms that Transformer+CoT can learn. In\nthis work, we (1) evaluate the state tracking capabilities of Transformer+CoT\nand its variants, confirming the effectiveness of CoT. (2) Next, we identify\nthe circuit, a subset of model components, responsible for tracking the world\nstate, finding that late-layer MLP neurons play a key role. We propose two\nmetrics, compression and distinction, and show that the neuron sets for each\nstate achieve nearly 100% accuracy, providing evidence of an implicit finite\nstate automaton (FSA) embedded within the model. (3) Additionally, we explore\nthree realistic settings: skipping intermediate steps, introducing data noise,\nand testing length generalization. Our results demonstrate that Transformer+CoT\nlearns robust algorithms (FSA), highlighting its resilience in challenging\nscenarios.",
    "published_date": "2025-02-27",
    "source": "arxiv"
  }
}