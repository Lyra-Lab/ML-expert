{
  "id": "arxiv_2502.21284v1",
  "text": "Controlled Model Debiasing through Minimal and Inter-\npretable Updates\nFederico Di Gennaro1,2\nfederico.digennaro@epfl.ch\nEPFL, Lausanne, Switzerland\nThibault Laugel1\nthibault.laugel@axa.com\nAXA, Paris, France\nTRAIL, LIP6, Sorbonne Université, Paris, France\nVincent Grari\nAXA, Paris, France\nMarcin Detyniecki\nAXA, Paris, France\nTRAIL, LIP6, Sorbonne Université, Paris, France\nPolish Academy of Science, IBS PAN, Warsaw, Poland.\nAbstract\nTraditional approaches to learning fair machine learning models often require rebuilding\nmodels from scratch, generally without accounting for potentially existing previous models.\nIn a context where models need to be retrained frequently, this can lead to inconsistent model\nupdates, as well as redundant and costly validation testing. To address this limitation, we\nintroduce the notion of controlled model debiasing, a novel supervised learning task relying on\ntwo desiderata: that the differences between new fair model and the existing one should be (i)\ninterpretable and (ii) minimal. After providing theoretical guarantees to this new problem,\nwe introduce a novel algorithm for algorithmic fairness, COMMOD, that is both model-\nagnostic and does not require the sensitive attribute at test time. In addition, our algorithm\nis explicitly designed to enforce (i) minimal and (ii) interpretable changes between biased and\ndebiased predictions—a property that, while highly desirable in high-stakes applications, is\nrarely prioritized as an explicit objective in fairness literature. Our approach combines a\nconcept-based architecture and adversarial learning and we demonstrate through empirical\nresults that it achieves comparable performance to state-of-the-art debiasing methods while\nperforming minimal and interpretable prediction changes.\n1\nIntroduction\nThe increasing adoption of machine learning models in high-stakes domains—such as criminal justice (Klein-\nberg et al., 2016) and credit lending (Bruckner, 2018)—has raised significant concerns about the potential\nbiases that these models may reproduce and amplify, particularly against historically marginalized groups.\nRecent public discourse, along with regulatory developments such as the European AI Act (2024/1689), has\nfurther underscored the need for adapting AI systems to ensure fairness and trustworthiness (Bringas Col-\nmenarejo et al., 2022). Consequently, many of the machine learning models deployed by organizations are,\nor may soon be, subject to these emerging regulatory requirements. Yet, such organizations frequently in-\nvest significant resources (e.g. time and money) in validating their models with the assistance of domain\nexperts before deploying them at scale (see, e.g., (Mata et al., 2021) for dam safety monitoring and (Tsopra\net al., 2021) for precision medicine), to ensure their performance and trustworthiness. As new regulatory\n1Equal contribution.\n2Work carried out during an internship at AXA.\n1\narXiv:2502.21284v1  [cs.LG]  28 Feb 2025\n\n\nconstraints emerges for these models, the ability to make these models comply with new constraints while\nminimizing the need for revalidation has thus become critically important.\nThe field of algorithmic fairness has experienced rapid growth in recent years, with numerous bias mitigation\nstrategies proposed (Romei & Ruggieri, 2014; Mehrabi et al., 2021).\nThese approaches can be broadly\ncategorized into three types: pre-processing (e.g.,(Belrose et al., 2024)), in-processing (e.g.,(Zhang et al.,\n2018)), and post-processing (e.g., (Kamiran et al., 2010)), based on the stage of the machine learning pipeline\nat which fairness is enforced. While the two former categories do not account at all for any pre-existing biased\nmodel being available for the task, post-processing approaches aim to impose fairness by directly modifying\nthe predictions of a biased classifier. This thus naturally imposes some kind of consistency between the\nnew, fair model and the old, biased one. However, these methods are generally deemed to achieve lower\nperformance, and the changes performed are generally not traceable.\nMotivated by these considerations, in this paper we consider a new paradigm, proposing to frame algorithmic\nfairness as a model update task. The goal is thus to enforce fairness through small and understandable,\nupdates to a pretrained, presumably biased, model, in order to facilitate model monitoring. For this purpose,\nwe introduce the notion of Controlled Model Debiasing, based on two intuitive desiderata: (i) changes between\nthe new model and the existing one should be minimal, and (ii) these changes should be understandable.\nThis formulation combines assumptions from both post-processing (e.g., (Kamiran et al., 2010)) and in-\nprocessing (e.g., (Zhang et al., 2018)) approaches, proposing to learn a new fair model by leveraging a\npreviously trained biased one. After providing theoretical guarantees on the solution and feasibility of this\nnew proposed problem, we introduce COMMOD (COncept-based Minimal MOdel Debiasing), our method to\naddress it. Leveraging the fields of concept-based interpretability (Alvarez Melis & Jaakkola, 2018; Koh et al.,\n2020b) and fair adversarial learning (Zhang et al., 2018; Grari et al., 2021), COMMOD is a model-agnostic,\ninterpretable debiasing method that aims to improve fairness in an interpretable way, while minimizing\nprediction changes relative to the original model. Through experiments, we demonstrate that our method\nachieves comparable fairness and accuracy performance to existing algorithmic fairness approaches, while\nrequiring fewer prediction changes. Additionally, we show that COMMOD enables more meaningful and\neasier-to-understand prediction changes, enhancing its utility in practice. To summarize, our contributions\nare as follows:\n• We introduce a new notion of Controlled Model Debiasing, which aims to account for a previously\ntrained model by performing minimal and interpretable updates to mitigate bias (Section 3). To the\nbest of our knowledge, we are the first to address interpretability directly in the updating process.\nPreviously, only post-hoc interpretation of the debiasing process (e.g., (Ţifrea et al., 2023)) has been\nexplored.\n• We provide two theoretical guarantees for the proposed problem: the formulation of the Bayes-\nOptimal Classifier in a fairness-aware setting and the feasibility of the problem in the general setting\n(Section 4).\n• We introduce a new method, COMMOD, to address the new optimization problem in a model-\nagnostic, fairness-unaware setting (Section 5). We validate its performance through experiments on\nclassical fairness datasets, showcasing its debiasing efficacy and ability to perform fewer and more\ninterpretable changes (Section 6).\n2\nBackground and notation\nLet X ⊆Rd be an input space consisting of d features and Y an output space. In a traditional algorithmic\nfairness framework in supervised learning, we want an algorithm that outputs ˆY ⊂Y such that ˆY is unbiased\nfrom a sensitive variable S, only available at training time. For the sake of simplicity, we focus in this paper\non a binary classification problem where Y = {0, 1} and for which also the sensitive attribute S takes values\non a binary sensitive space S = {0, 1}.\n2\n\n\n2.1\nGroup fairness\nOver the years, several notions of fairness have been proposed in the literature to define whether or not\na variable ˆY is unbiased from a sensitive variable S ((Dwork et al., 2012; Hardt et al., 2016; Jiang et al.,\n2020)). In this paper, we focus on two of the most used ones: Demographic Parity (Calders et al., 2009) and\nEqualizing Odds (Hardt et al., 2016).\n2.1.1\nDemographic Parity\nA classifier satisfies Demographic Parity (DP) if the prediction ˆY is independent from sensitive attribute S.\nIn the case of a binary classification, this is equivalent to P( ˆY = 1 | S = 0) = P( ˆY = 1 | S = 1). Hence,\nDemographic Parity can be measured using the P-rule:\nP-Rule = min\n \nP( ˆY = 1 | S = 1)\nP( ˆY = 1 | S = 0)\n, P( ˆY = 1 | S = 0)\n( ˆY = 1 | S = 1)\n!\n.\n2.1.2\nEqualizing Odds\nA classifier satisfies Equalizing Odds (EO) if the prediction ˆY is conditionally independent of the sensitive\nattribute S given the actual outcome Y . In the case of a binary classification problem, this is equivalent to\n( ˆY = 1 | Y = y, S = 0) = ( ˆY = 1 | Y = y, S = 1), for all y ∈{0, 1}. Hence, Equalizing Odds can be\nmeasured using the Disparate Mistreatment (DM) (Zafar et al., 2017):\nDT P R = |TPRS=1 −TPRS=0|,\nDF P R = |FPRS=1 −FPRS=0|,\nwhere TPRS=s = ( ˆY = 1|Y = 1, S = s), FPRS=s = ( ˆY = 1|Y = 0, S = s), and s ∈{0, 1}. In the rest of\nthe paper, we use DM = DT P R + DT NR.\n2.2\nMitigation strategies\nBias mitigation algorithms are generally grouped into three different categories, based on when the debiasing\nprocess is carried on in the machine learning pipeline (Romei & Ruggieri, 2014). In particular, pre-processing\nmethods (e.g. (Zemel et al., 2013)) modify the training data such that they are unbiased with respect to\nS. In-processing methods (e.g. (Zhang et al., 2018)) aim to train a new, fair, classifier by integrating some\nfairness constraints in the optimization objective. Finally, post-processing methods approaches aim to achieve\nfairness by adjusting the predictions ˆY f of an already existing model f : X →Y. Hence, post-processing\nmethods learn a function m: Y →Y that maps ˆY f into fairer predictions ˆY . As such, a notion of predictive\nsimilarity between these two sets of predictions often being optimized, either directly (Jiang et al., 2020;\nNguyen et al., 2021; Alghamdi et al., 2022) or indirectly through heuristics (Kamiran et al., 2018). However,\nrather than an assumed end-goal, this objective generally remains a proxy for the true goal of optimizing\naccuracy, as the true labels Y are generally assumed to be unavailable at inference time in the post-processing\nsetting. Furthermore, these approaches generally suffer from several limitations, such as not being model-\nagnostic (Calders & Verwer, 2010; Kamiran et al., 2010; Du et al., 2021), or requiring access to the sensitive\nattribute at test time (Hardt et al., 2016; Pleiss et al., 2017), which greatly hurt their practical use. On\nthis topic, we provide a review of existing post-processing approaches and their limitations in Table 1 in\nAppendix.\n3\nControlled model debiasing\nLet us now consider a context where a model f : X →Y needs to be updated with a model g. In addition\nto maintaining its accuracy, the objective is to make it fairer with respect to a specific fairness definition\ndiscussed in Section 2.1. As discussed in Sec. 1, blindly training g can be harmful for several reasons. First,\nKrco et al. (2023) have shown that some bias mitigation approaches performed needlessly large numbers of\nprediction changes for similar levels of accuracy and fairness. Yet, a large number of inconsistent decisions\nmay negatively impact users’ trust, as shown by Burgeno & Joslyn (2020) in the context of weather forecast.\n3\n\n\nOn top of this, beyond statistical testing, models in production may undergo extensive testing by domain\nexperts Tsopra et al. (2021). In this context, being able to understand the changes from one model to the\nother would be crucial to not being forced to undergo all the testing again, in addition to increasing expert\ntrust.\nDriven by the above-mentioned considerations, we introduce the notion of Controlled Model Update, which\nis based on the following notions of Interpretable and Minimal updates.\n3.1\nInterpretable updates\nTo facilitate the adoption of a new model g, we propose to generate explanations that describe how the\ndebiasing process modified the old model f. Contrary to traditional XAI methods (see, e.g. (Guidotti et al.,\n2018) for a survey), rather than explaining the decisions of the model g, our aim here is to generate insights\nabout the differences between f and g. An intuitive solution to this problem would be to first train a fair\nmodel and then generating explanations for these differences in a post-hoc manner, akin to (Renard et al.,\n2024). However, post hoc interpretability methods, in general, are often criticized for their lack of connection\nwith ground-truth data (Rudin, 2019; Laugel et al., 2019). Therefore, in order to ensure better trust in the\nnew model, we pursue the direction of self-explainable models (Alvarez Melis & Jaakkola, 2018), generating\nglobal explanations for the differences between g and f while learning the predictive task.\n3.2\nMinimal updates\nThe constraint of ensuring that the new model remains as similar as possible to the initial one can be\ndirectly added to the training objective (e.g. the one in (Zhang et al., 2018)). In particular, to the usual\noptimization problem of fairness, we added a constraints that control the probability for a prediction ˆY f to\nchange label after the debiasing process. We emphasize once again that making minimal changes to a biased\nmodel is beneficial in cases where the model has already been validated based on user needs. Significant\nupdates to make the model fairer might risk no longer satisfying those needs, requiring once again a costly\nvalidation.\nWe then frame our problem in a traditional algorithmic-fairness way, in which the self-explainable model g\nis required to be accurate and fair. On top of these constraints, g is required to be similar to f in terms\nof predicted classes. The combination of these desiderata allows us to understand what changes are required\nfrom a biased model to become fairer focusing on minimal adjustments.\nLet fair(·) be the fairness criteria (e.g. Demographic Parity or Equalizing Odds). Further, let us define the\npredictions of the biased model f and edited model g as ˆY f = 1f(X)>0.5 and ˆY = 1g(X)>0.5 respectively. We\ncan then formalize our optimization problem through the following Controlled Model Update objective:\nmin\nAcc( ˆY , Y )\ns.t.\nfair( ˆY ) ≥τ\n( ˆY f ̸= ˆY ) ≤p\n(1)\nwhere τ ∈R+ and p ∈[0, 1]. Observe that setting p = 0, i.e. forbidding any prediction change to happen,\nimposes g = f. On the contrary, the more p increases and the more g is free to differ from f and, ideally, to\nreach higher fairness scores. When p = 1, Eq. 1 becomes a classical algorithmic fairness learning problem.\nNotice also that the interpretability is all carried by the self-explainable model g itself, and hence it will be\nnot explicitly appear in the optimization problem of Equation 1. We will discuss our self-explainable model\ng in Section 5.2.\n4\nTheoretical guarantees on the minimal update problem\nIn this section, we present two theoretical contributions to the problem formalized in Eq. 1. First (Sec. 4.1),\nwe show that it is possible to define the Bayes Optimal Classifier of the problem in the fairness-aware setting.\n4\n\n\nThen, in Section 4.2, we assess the feasibility of the problem in general, giving guarantees on the trade-off\nbetween fairness and number of prediction changes. All proofs can be found in Appendix B.\n4.1\nResult 1: Bayes-optimal classifier in a fairness-aware setting\nLet (X, S, Y, ˆY f) ∼Djnt be a joint distribution on X × {0, 1} × {0, 1} × {0, 1}, where X is the instance,\nY the target feature, S the sensitive variable, and ˆY f the output of the black-box classifier. Let D be\nthe distribution over (X, Y ), ¯D a suitable distribution (DP or EO) over (X, S) and D∗a distribution\nover (X, ˆY f).\nWe want to show that despite the new constraint on the number of changes, our opti-\nmization problem in Eq. 1 still has an analytical solution (i.e. the Bayes-optimal classifier (BOC)) in a\nfairness aware setting knowing the true distributions above mentioned. To do so, we leverage the work\nof (Menon & Williamson, 2018), who defined the BOC in the traditional fairness setting, relying on a\nreformulation of the optimization problem using the notion of cost-sensitive risk1.\nFor what concerns\nthe fairness constraint, they show (Lemma 3) that P-Rule(g) ≥τ is equivalent to CSbal(g, ¯D, ¯c) ≥k,\nwhere CSbal(g, ¯D, ¯c) = (1−¯c)·FNR(g; ¯D)+¯c·FPR(g; ¯D) is the balanced cost-sensitive risk and ¯c = 1/(1+τ).\nFollowing the same idea, we first establish the equivalence between the constraint on the number of changes\nand a cost-sensitive risk:\nLemma 4.1. Pick any random classifier g. Then, for any p ∈[0, 1],\n( ˆY f ̸= ˆY ) ≤p ⇔CSbal(g; D∗, c∗) ≤p,\nwith CSbal(g; D∗, c∗) = (1 −c∗) · FNR(g; D∗) + c∗· FPR(g; D∗)2and c∗= ( ˆY = 1).\nFinally, in a similar fashion of the main result in (Menon & Williamson, 2018) about the Bayes-Optimal\nClassifier, we can define for any cost c, ¯c, c∗∈[0, 1], λfair, λratio ∈R the Lagrangian RCS(g; D, ¯D, D∗) of\nthe optimization problem in Eq. 1 as:\nRCS(g; D, ¯D, D∗) = CS(g; D, c) −λfairCSbal(g; ¯D, ¯c)\n−λratioCSbal(g; D∗, c∗).\n(2)\nProposition 4.2 (Fairness-aware Bayes-optimal classifier). The Bayes optimal classifier of RCS in a fairness\naware setting and knowing the distributions D, ¯D, D∗is given by\ngopt = arg min\ng∈[0,1]X RCS(g; D, ¯D, D∗) = {Hα ◦s∗(x) | α ∈[0, 1]}\nwhere η(x) = (Y = 1 | X = x), ¯η(x) = (S = 1 | X = x), η∗(x) = ( ˆY f = 1 | X = x), s∗(x) =\nη(x) −c −λratio(¯η(x) −¯c) −λfair(η∗(x) −c∗) and Hα(z) = 1{z>0} + α1{z=0} is the modified Heaviside (or\nstep) function for a parameter α ∈[0, 1].\nDespite its theoretical importance, the BOC presented in Proposition 3.2 remains impractical for computa-\ntion, as the joint and marginal distributions of the random variables X, S, Y, ˆY f are typically inaccessible in\nreal-world applications. Furthermore, the above result only holds within a fairness-aware setting, a context\nthat is uncommon in practical scenarios. For this reason, we introduce in Sec. 5 a novel algorithm that\noptimizes a relaxed form of the optimization problem in Eq. 1, which does not require knowledge of the\nsensitive attribute at test time.\n1From (Menon & Williamson, 2018): The risk of a randomized classifier g : X →(0, 1) computed on Y is called cost sensitive\nrisk for a cost parameter c ∈(0, 1) and π = (Y = 1) if it can be written as CS(g, D, c) = π · (1 −c) · FNR(g; D) + (1 −π) · c ·\nFPR(g; D), where FNR(g; D) = EX|Y =1[1 −g(X)] and FPR(g; D) = EX|Y =0[g(X)].\n2Notice that we changed the distribution w.r.t. FPR and FNR are computed. Hence, FNR(g; D∗) = ( ˆY = 0| ˆY f = 1) and\nFPR(g; D∗) = ( ˆY = 1| ˆY f = 0).\n5\n\n\n4.2\nResult 2: fairness level under a maximum change constraint\nWe now aim to theoretically understand what happens experimentally to the trade-off between fairness score\n(DP in this case) and the similarity between f and g defined in Eq. 1. For this purpose, we study the impact\nthat K changes to the predictions of f can have on the P-Rule.\nWe start by defining the following notations, used to divide a set of instances X = {Xi}N\ni=1 based on the\nvalues of their outcomes ˆY f = 1f(X)>0.5 and of their sensitive attribute S:\nˆY f = 1\nˆY f = 0\nS = 1\nγ11\nγ01\nS1\nS = 0\nγ10\nγ00\nS0\nUsing this notation, the P-Rule is experimentally measured as P-Rule = min\n\u0010\nC · γ11\nγ10 ,\n1\nC · γ10\nγ11\n\u0011\n, where\nC =\nS0\nS1 is a fixed and not editable constant that depends on the population’s distribution with respect\nto the binary sensitive attribute. Further, without loss of generality, we now suppose that γ11 < γ10, i.e.\nP-Rule = C · γ11\nγ10 . Hence, in order to increase the P-Rule to a higher value P-Rule′ with K changes on the\npredictions, i.e. by modifying the set { ˆY f(Xi)}N\ni=1 into the set { ˆY (Xi)}N\ni=1, two types of actions on γ11 and\nγ10 could be performed:\n(a) Increasing γ11 (i.e. changing the prediction of individuals belonging to the minority group S = 1\nfrom ˆY f = 0 to ˆY = 1).\n(b) Decreasing γ10 (i.e. changing the prediction of individuals belonging to the majority group from\nˆY f = 1 to ˆY = 0).\nNote that both these two actions could lead to a switch in the minimum of the definition of P-Rule in the\nsense that from P-Rule = C · γ11\nγ10 one could arrive at P-Rule′ = min\n\u0010\nC · γ′\n11\nγ′\n10 ,\n1\nC · γ′\n10\nγ′\n11\n\u0011\n= 1\nC · γ′\n10\nγ′\n11 . This is due\nto the fact that when one instance of the minimum decreases, the other one increases. Let us consider an\nalgorithm A that, given K, only plays a strategy that is either (a), (b) or a combination of the two. For\nsuch an algorithm, we can introduce the following definition.\nDefinition 4.3. (Switching point) The switching point KA\ns\nof an algorithm A of the type above is the\nnumber of changes required to get a P-Rule ≈1.\nInformally, this is the number of changes required to get γ′\n10 ≈Cγ11, where the approximation is needed\nbecause the quantities γij are integer values and C ∈R in general. With this in mind, we can prove the\nfollowing proposition:\nProposition 4.4. Let A an algorithm of the type above.\nThen, given K changes with K ≤KA\ns , the\nmaximum P-Rule we can get is given by an algorithm that always plays action (a) or always action (b).\nThe only thing one should keep in mind is that by doing K changes one needs to have either enough instances\nfalling into γ01 (if we increase the numerator) or into γ10 (if we decrease the denominator).\nWe believe this result represents a significant milestone in the theoretical characterization of the effects of\ndebiasing a model using DP as a fairness measure. Although this result provides valuable insights into the\ndynamics of the debiasing process, Proposition 4.4 does not account for the potential impact of these optimal\nchanges on model accuracy. This highlights the possibility that, in the context of algorithmic fairness—where\nthe goal is to balance fairness and accuracy—debiasing algorithms may prioritize alternative changes that\nminimize adverse effects on accuracy. On top of this, this theoretical result would be algorithmically appli-\ncable only in a fairness aware setting due to the fact that the knowledge of proportion γij, i, j ∈{0, 1} is\nrequired. To this goal, we evaluate the performances of an algorithm A of the type above in the Appendix\nB.2.1.\n6\n\n\n5\nCOMMOD: an algorithm for controlled model debiasing\nAs previously mentioned, while the BOC of Proposition 4.2 is theoretically relevant in the Fairness-Aware\nsetting, it lacks practical utility. In order to solve Problem 1, we thus propose a new algorithm, called COM-\nMOD, circumventing these limitations and integrating the interpretability objective described in Section 3.\nCOMMOD aims to edit the probabilities scores f(X) of the biased model into scores g(X), whose associated\npredictions ˆY = 1g(X)>0.5 are more fair. The update is done through a multiplicative factor r(X) as follows:\ng(X) = σ(r(X)flogit(X)),\n(3)\nwhere σ(·) is the sigmoid function to ensure g(X) ∈[0, 1] and flogit is the logit value of the biased model f.\nBesides being intuitive, this formulation enables a more direct modelling of the differences between f and\ng, as ˆY ̸= ˆY f ⇔r(X) < 0. We then propose to model the ratio rwg as a Neural Network with parameters\nwg and taking as input X. In the subsections below, we describe how both desiderata of similarity and\ninterpretability are integrated in COMMOD.\n5.1\nPenalization term for minimal updates\nWe propose to minimize model changes by adding a penalization term Lratio. Numerous similarity measures\nbetween f and g can be considered, depending on the considered scenario: for instance, existing works in post-\nprocessing fairness generally focus on distances between distributions such as the Wasserstein distance (Jiang\net al., 2020) or the KL-divergence (Ţifrea et al., 2023). Here, we propose to use either a mean squared norm\nfunction Lratio(rwg(X)) = ||rwg(X) −1||2\nM defined by a positive-definite inner product M. This term ensure\nthat the score f(X) is not modified unnecessarily, and thus that the calibration of g remains consistent (cf.\nFigure 9 in App. D.4 for a related discussion). For the sake of simplicity, in the experiments of our paper\nwe set M = I, recovering the classical Euclidean distance.\n5.2\nConcept-based Architecture for Interpretable Changes\nIn our approach, we prioritize interpretability by utilizing a self-explainable model. For classification prob-\nlems, interpretable models par excellence are decision trees. On the other hand, for regression we do not have\nthe same theoretical understanding but a model that is usually considered interpretable is Sparse Linear\nRegression. On top of that, inspired by the growing body of works on concepts (Koh et al., 2020a; Yuksek-\ngonul et al., 2023; Fel et al., 2023; Zarlenga et al., 2023), we propose to learn k concepts C : X →R in an\nunsupervised manner where every concept is a sparse linear combination of the input features. Consistently\nwith previous works, we also posit that these concepts should be both diverse (Alvarez Melis & Jaakkola,\n2018; Fel et al., 2023; Zarlenga et al., 2023) to be meaningful and the least redundant possible. Hence, the\nmodel g is interpretable due to the structure of rwg that is a neural network without activation functions.\nThe final architecture rwg consists of an initial (linear) bottleneck that maps the input features X into k\nconcepts, followed by an output layer that linearly combines these concepts to produce the multiplicative\nratio needed for the update. By using linear combinations for both the features-to-concepts mapping and\nthe concepts-to-output mapping, we can directly examine the learned weights to understand the direction\n(positive or negative) in which each concept—and, by extension, each feature—contributes to the value of\nthe ratio. This level of interpretability was not achievable in previous models like TabCBM (Zarlenga et al.,\n2023). To ensure that the learned concepts are meaningful, we introduce penalization terms for diversity\nand sparsity, similar to those in\n(Zarlenga et al., 2023). Given our linear architecture, these terms are\ndefined as follows: for sparsity, we use a Lasso penalization term, and for diversity, we define the penaliza-\ntion as Ldiversity = P\ni,j≤k ρ(W i, W j), where ρ denotes the cosine distance and W i, W j are the network’s\nweights from input features to concepts i and j respectively. The combined penalization term is then given\nby Lconcepts = Ldiversity + Lsparsity.\nBeyond ensuring interpretability, using a linear transformation rather than a more complex architecture\nstems from our observations that it often lead to comparable results along our metrics of interest (see D.3\nin the Appendix) - an observation also made by (Ţifrea et al., 2023).\n7\n\n\n5.3\nFinal model\nTo ensure fairness, we propose to leverage the technique proposed by (Zhang et al., 2018), which relies\non a dynamic reconstruction of the sensitive attribute from the predictions using an adversarial model\nhwh : Y →S to estimate and mitigate bias.\nThe higher the loss LS(hwh(rwg(X)flogit(X)), S) of this\nadversary is, the more fair the predictions gwg(X) are.\nFurthermore, this allows us to mitigate bias in\na fairness-blind setting (i.e. without access to the sensitive attribute at test time), thus overcoming the\nlimitation of most post-processing fairness methods (cf Table 1). Thus, the relaxed version of Equation 1\nfor DP can be written as:\nmin\nwg\nE[LY (gwg(X), Y )]\n[1]\ns.t\nE[LS(hwh(rwg(X)flogit(X)), S)] ≥ϵ′\n[2]\nand\nE[Lratio(rwg(X))] ≤η′\n[3]\n(4)\nwith ϵ′, η′ > 0. In practice, we relax Problem 4, integrating the interpretability constraints on the concepts:\narg min\nwg\nmax\nwh\n1\nN\nN\nX\ni=1\nLY (gwg(xi), yi)\n−λfairLS(hwh(rwg(xi)flogit(xi)), si)\n+ λratioLratio(rwg(xi))\n+ λconceptsLconcepts(rwg(xi)),\n(5)\nwith λfair, λratio and λconcepts three hyperparameters. Similarly to (Zhang et al., 2018) this loss function can\neasily be adapted to address EO by modifying the adversary hwh to take as both the true labels Y and the\npredictions gwg(X).\n6\nExperiments\nAfter describing our experimental setting, we propose in this section to empirically evaluate the two dimen-\nsions of the controlled model update: the ability to achieve accuracy and fairness with minimal (Section 6.2),\nand interpretable (Section 6.3) prediction changes.\n6.1\nExperimental protocol\n6.1.1\nDatasets\nWe experimentally validate two binary classification datasets, commonly used in the fairness literature (Hort\net al., 2024): Law School (Wightman, 1998) and Compas (Angwin et al., 2016). The sensitive attribute for\nboth datasets is race.\n6.1.2\nCompetitors and baselines\nTo assess the efficacy of COMMOD, we first use an in-processing debiasing method, AdvDebias (Zhang\net al., 2018), described in Section 2.1. We use the implementation provided in the AIF360 library (Bellamy\net al., 2018). Additionally, as discussed in Section 6.2, post-processing debiasing methods also use\na pretrained classifier as input. Yet, most methods are not directly comparable as they are not model-\nagnostic or require the sensitive attribute a test time. Two exceptions are LPP (Xian & Zhao, 2024) and\nFRAPPE (Ţifrea et al., 2023), which we use as competitors. Furthermore, although not directly comparable,\nwe include the fairness-aware algorithms proposed by (Xian & Zhao, 2024) (that we name \"Oracle (LPP)\")\nand (Kamiran et al., 2012) (\"Oracle (ROC)) as baselines. As they directly use the sensitive attribute to\nmake predictions, these two algorithms are expected to outperform all the others.\n6.1.3\nSet-up\nAfter splitting each dataset in Dtrain (70%) and Dtest (30%), we train our pretrained classifier f to optimize\nsolely accuracy. In these experiments, we use a Logistic Regression classifier from the scikit-learn library,\n8\n\n\n0.00\n0.10\n0.20\n0.30\n0.40\n% Changes\nQ1 Fairness\nQ2 Fairness\n0.60\n0.65\n0.70\n0.75\n0.80\nAccuracy\n0.00\n0.10\n0.20\n0.30\n0.40\n% Changes\nQ3 Fairness\n0.60\n0.65\n0.70\n0.75\n0.80\nAccuracy\nQ4 Fairness\nLAW SCHOOL\nCOMMOD\nLPP\nOracle (ROC)\nOracle (LPP)\nZhang\nFRAPPE\n0.00\n0.10\n0.20\n0.30\n% Changes\nQ1 Fairness\nQ2 Fairness\n0.60\n0.62\n0.64\n0.66\nAccuracy\n0.00\n0.10\n0.20\n0.30\n% Changes\nQ3 Fairness\n0.60\n0.62\n0.64\n0.66\nAccuracy\nQ4 Fairness\nCOMPAS\nCOMMOD\nLPP\nOracle (ROC)\nOracle (LPP)\nZhang\nFRAPPE\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n% Changes\nQ1 Fairness\nQ2 Fairness\n0.65\n0.70\n0.75\n0.80\nAccuracy\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n% Changes\nQ3 Fairness\n0.65\n0.70\n0.75\n0.80\nAccuracy\nQ4 Fairness\nLAW SCHOOL\nCOMMOD\nLPP\nOracle (ROC)\nOracle (LPP)\nZhang\nFRAPPE\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n% Changes\nQ1 Fairness\nQ2 Fairness\n0.60\n0.62\n0.64\n0.66\nAccuracy\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n% Changes\nQ3 Fairness\n0.60\n0.62\n0.64\n0.66\nAccuracy\nQ4 Fairness\nCOMPAS\nCOMMOD\nLPP\nOracle (ROC)\nOracle (LPP)\nZhang\nFRAPPE\nFigure 1: Number of prediction class changes (y-axis) for comparable levels of fairness (P-Rule or DM\nquartiles Q1-Q4) and accuracy (x-axis) for DP (top) and EO (bottom) on Law School and Compas.\nbut any other classifier could be used since COMMOD and the proposed competitors are model-agnostic.\nWe then train COMMOD, the competitors and the baselines on Dtrain, varying hyperparameter values for\neach method to achieve a range of fairness and accuracy scores over Dtest. For COMMOD, we set a fixed\nvalue for the number of concepts k: 2 for Law School and 5 for Compas. Further details on implementation\nare available in Section C of the Appendix.\n6.2\nExperiment 1: achieving fairness through minimal changes\nIn this first experiment, we aim to assess the ability of COMMOD to achieve competitive results in terms\nof fairness and accuracy while performing a lower number of changes. For this purpose, we measure the\nproportion P of prediction changes between the black-box model and the fairer model on the test set Dtest.\nAs this value is directly linked to the fairness and accuracy levels of the models, we intend to evaluate P\nfor models with comparable levels of fairness and accuracy. To do so, we discretize the fairness scores into\nfour segments, defined as quartiles of the scores of AdvDebias: Q1 corresponds to the most biased models,\nand Q4 to the most fair ones. The full details of these segments are available in Appendix C.2, along with\na robustness analysis (cf. Appendix D.1) in which, to ensure that our results do not depend on the specific\nsegment definition chosen, we reproduce the same experiment with other segment definitions.\nFor each\n9\n\n\nsegment, we then display the Pareto graph between P (y-axis, lower is better) and Accuracy (x-axis, higher\nis better) in Figure 1: in this representation, the most efficient method will be the closest one to the bottom\nleft corner.\nWe observe that for similar levels of accuracy and fairness, COMMOD consistently achieves lower values of\nnumber of changes than its competitors. For less fair models (segment Q1), the difference is more subtle: as\nmodels put less emphasis on fairness than on accuracy, they tend to adopt a similar behavior, i.e. the one\nof the biased model f. In other segments however, the value of COMMOD is more notable.\n6.3\nExperiment 2: interpretability of the model updates\nIn this second experiment, we aim to evaluate the quality of the explanations generated with COMMOD.\n6.3.1\nIllustrative results\nage\nage_cat_Gr\neater than\n 45\nc_charge_d\nesc_Poss P\nyrrolidino\nvalerophen\n0.00\n0.02\n0.04\n0.06\nConcept 0\nc_charge_d\nesc_Poss P\nyrrolidino\nvalerophen\nage\nc_charge_d\nesc_Delive\nry Of Drug\n Paraphern\n0.0015\n0.0010\n0.0005\n0.0000\n0.0005\nConcept 1\n( ˆY ̸= ˆY f)\nXC0\n1.0\n¯XC0\n0.0\nXC1\n0.0\n¯XC1\n0.08\nFigure 2: Example of explanation for Compas. Left: feature contributions to the concepts. Right: ( ˆY ̸= ˆY f)\nfor the segments described by these features.\nStarting with an example of explanation on the Compas Dataset. After training COMMOD with k = 2\nconcepts, we obtain a final test P-Rule score of 0.78 (up from 0.60 with f) and accuracy of 0.62 (down from\n0.66) with P = 0.08. The first two plots of Figure 2 highlight the diversity between these concepts, as they\ntarget different features. For instance, the concept C0, contributing positively (w0 = 0.41) to class changes,\nis primarily activated by older individuals; on the other hand, C1, contributing negatively (w1 = −0.23),\ntargets a specific crime category. In the third graph, we calculate the probability values ( ˆY ̸= ˆY f) in the\nsets corresponding to the instances activated by the corresponding features. Although preliminary, these\nobservations show the potential of COMMOD to explain the debiasing process.\n6.3.2\nQuantitative results\nWe now aim to generalize the previous observation by verifying that the concepts learned are diverse and\nsparse over the features of X. To assess sparsity, we evaluate the total concept sparsity computed by summing\nacross concepts the sparsity of each matrix after applying a threshold of ϵ = 0.01: P\ni≤k ||W i > ϵ||0. For\ndiversity, we measure the Jaccard Index between the signed non-null coefficients: diverse concepts may still\nuse the same feature of X if said feature contribution is in the opposite direction. As a baseline, we compare\nCOMMOD (with k = 2) with FRAPPE (Ţifrea et al., 2023), when the latter is trained using a linear network\nwith the same architecture as the one used for our ratio rwg for their additive term. Figure 3, highlights\nthat for all levels of number of changes, the concepts learned using COMMOD are indeed more sparse and\ndiverse as expected. Finally, we aim to show that the instances targeted by COMMOD are generally located\nin regions that are easier to interpret. In classification, there exists a theoretical understanding on how a\nclass of models is interpretable based on how it can be translated into a decision tree (Bressan et al., 2024).\nDriven by this work, we propose a new evaluation protocol relying on measuring the accuracy of a decision\ntree of constrained depth trained to predict whether an instance has its prediction changed or not (i.e. on\nlabels 1ˆY̸=ˆYf). The idea behind this evaluation protocol is that a decision tree with a constrained depth will\n10\n\n\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n% changes\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nConcept Redundancy\nLAW SCHOOL\nFRAPPE (Tifrea et al. 2023)\nCOMMOD (Ours)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n% changes\n0\n2\n4\n6\n8\n10\nTotal Concept Sparsity\nLAW SCHOOL\nFRAPPE (Tifrea et al. 2023)\nCOMMOD (Ours)\nFigure 3: Comparison of concept redundancy (left, lower is better) and sparsity (right, higher is better) for\nthe Law School dataset between FRAPPE and COMMOD.\n2\n3\n4\n5\n6\n7\n8\n9\n10\nMax Depth\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nF1 Score\nLAW SCHOOL\nAdvDebias (Zhang et al., 2018)\nFRAPPE (Tifrea et al., 2023)\nCOMMOD (Ours)\n2\n3\n4\n5\n6\n7\n8\n9\n10\nMax Depth\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nF1 Score\nCOMPAS\nAdvDebias (Zhang et al., 2018)\nFRAPPE (Tifrea et al., 2023)\nCOMMOD (Ours)\nFigure 4: Avg. F1 Score with standard deviations computed over 5 runs for each model (with similar levels\nof both fairness and accuracy) of a decision tree trained on labels 1ˆY̸=ˆYf depending on the tree depth.\nmore accurately model prediction changes if these changes can be described with a lower number of features,\nand if these instances are more concentrated in the feature space. We report in Figure 4 the F1 scores of the\ndecision trees trained to predict the class changes of COMMOD, FRAPPE and AdvDebias. We observe that\nfor all values of maximum tree depth considered, thanks to the regularization with Lconcept, the predictions\nchanged by COMMOD are indeed much easier to interpret with a decision tree.\n7\nConclusion\nIn this work, we introduced COMMOD, a novel model update technique that manages to enforce fairness and\naccuracy while making less, and more interpretable, changes to the original biased classifier. Additionally,\nwe also provided theoretical results for this new optimization problem. Future works include researching\nthe Controlled Model Debiasing task in non-linear settings, and further exploring the tradeoffs between the\ndifferent hyperparameters to propose automated selection strategies.\nReferences\nWael Alghamdi, Hsiang Hsu, Haewon Jeong, Hao Wang, P Winston Michalak, Shahab Asoodeh, and Flavio P\nCalmon. Beyond adult and compas: Fairness in multi-class prediction. arXiv preprint arXiv:2206.07801,\n2022.\n11\n\n\nDavid Alvarez Melis and Tommi Jaakkola. Towards robust interpretability with self-explaining neural net-\nworks. Advances in neural information processing systems, 31, 2018.\nJulia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias. ProPublica, May 23, 2016,\n2016.\nRachel K. E. Bellamy, Kuntal Dey, Michael Hind, Samuel C. Hoffman, Stephanie Houde, Kalapriya\nKannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta, Aleksandra Mojsilovic, Seema Nagar,\nKarthikeyan Natesan Ramamurthy, John Richards, Diptikalyan Saha, Prasanna Sattigeri, Moninder Singh,\nKush R. Varshney, and Yunfeng Zhang. AI Fairness 360: An extensible toolkit for detecting, understand-\ning, and mitigating unwanted algorithmic bias, October 2018.\nNora Belrose, David Schneider-Joseph, Shauli Ravfogel, Ryan Cotterell, Edward Raff, and Stella Biderman.\nLeace: Perfect linear concept erasure in closed form. Advances in Neural Information Processing Systems,\n36, 2024.\nMarco Bressan, Nicolò Cesa-Bianchi, Emmanuel Esposito, Yishay Mansour, Shay Moran, and Maximilian\nThiessen. A theory of interpretable approximations. arXiv preprint arXiv:2406.10529, 2024.\nAlejandra Bringas Colmenarejo, Luca Nannini, Alisa Rieger, Kristen M Scott, Xuan Zhao, Gourab K Pa-\ntro, Gjergji Kasneci, and Katharina Kinder-Kurlanda. Fairness in agreement with european values: An\ninterdisciplinary perspective on ai regulation. In Proceedings of the 2022 AAAI/ACM Conference on AI,\nEthics, and Society, pp. 107–118, 2022.\nMatthew Adam Bruckner. The promise and perils of algorithmic lenders’ use of big data. Chi.-Kent L. Rev.,\n93:3, 2018.\nJessica N Burgeno and Susan L Joslyn. The impact of weather forecast inconsistency on user trust. Weather,\nclimate, and society, 12(4):679–694, 2020.\nToon Calders and Sicco Verwer. Three naive bayes approaches for discrimination-free classification. Data\nmining and knowledge discovery, 21:277–292, 2010.\nToon Calders, Faisal Kamiran, and Mykola Pechenizkiy. Building classifiers with independency constraints.\nIn 2009 IEEE International Conference on Data Mining Workshops, pp. 13–18, 2009.\nEvgenii Chzhen, Christophe Denis, Mohamed Hebiri, Luca Oneto, and Massimiliano Pontil. Leveraging\nlabeled and unlabeled data for consistent fair binary classification. Advances in Neural Information Pro-\ncessing Systems, 32, 2019.\nMengnan Du, Subhabrata Mukherjee, Guanchu Wang, Ruixiang Tang, Ahmed Awadallah, and Xia Hu.\nFairness via representation neutralization. Advances in Neural Information Processing Systems, 34:12091–\n12103, 2021.\nCynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel.\nFairness through\nawareness. In Proceedings of the 3rd innovations in theoretical computer science conference, pp. 214–226,\n2012.\nThomas Fel, Agustin Picard, Louis Bethune, Thibaut Boissin, David Vigouroux, Julien Colin, Rémi Cadène,\nand Thomas Serre. Craft: Concept recursive activation factorization for explainability. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2711–2721, 2023.\nVincent Grari, Sylvain Lamprier, and Marcin Detyniecki.\nFairness-aware neural rényi minimization for\ncontinuous features. In Proceedings of the Twenty-Ninth International Conference on International Joint\nConferences on Artificial Intelligence, pp. 2262–2268, 2021.\nRiccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and Dino Pedreschi.\nA survey of methods for explaining black box models. ACM computing surveys (CSUR), 51(5):1–42, 2018.\n12\n\n\nMoritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. Advances in\nneural information processing systems, 29, 2016.\nMax Hort, Zhenpeng Chen, Jie M Zhang, Mark Harman, and Federica Sarro. Bias mitigation for machine\nlearning classifiers: A comprehensive survey. ACM Journal on Responsible Computing, 1(2):1–52, 2024.\nRay Jiang, Aldo Pacchiano, Tom Stepleton, Heinrich Jiang, and Silvia Chiappa. Wasserstein fair classifica-\ntion. In Uncertainty in artificial intelligence, pp. 862–872. PMLR, 2020.\nFaisal Kamiran, Toon Calders, and Mykola Pechenizkiy. Discrimination aware decision tree learning. In\n2010 IEEE international conference on data mining, pp. 869–874. IEEE, 2010.\nFaisal Kamiran, Asim Karim, and Xiangliang Zhang. Decision theory for discrimination-aware classification.\nIn 2012 IEEE 12th International Conference on Data Mining, pp. 924–929, 2012.\nFaisal Kamiran, Sameen Mansha, Asim Karim, and Xiangliang Zhang. Exploiting reject option in classifi-\ncation for social discrimination control. Information Sciences, 425:18–33, 2018.\nMichael P Kim, Amirata Ghorbani, and James Zou. Multiaccuracy: Black-box post-processing for fairness in\nclassification. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pp. 247–254,\n2019.\nJon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair determination\nof risk scores, 2016.\nPang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy\nLiang. Concept bottleneck models. In Hal Daumé III and Aarti Singh (eds.), Proceedings of the 37th\nInternational Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research,\npp. 5338–5348. PMLR, 13–18 Jul 2020a.\nPang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy\nLiang.\nConcept bottleneck models.\nIn International conference on machine learning, pp. 5338–5348.\nPMLR, 2020b.\nNatasa Krco, Thibault Laugel, Jean-Michel Loubes, and Marcin Detyniecki. When mitigating bias is unfair:\nA comprehensive study on the impact of bias mitigation algorithms, 2023.\nThibault Laugel, Marie-Jeanne Lesot, Christophe Marsala, Xavier Renard, and Marcin Detyniecki. The\ndangers of post-hoc interpretability: unjustified counterfactual explanations. In Proceedings of the 28th\nInternational Joint Conference on Artificial Intelligence, pp. 2801–2807, 2019.\nPranay K Lohia, Karthikeyan Natesan Ramamurthy, Manish Bhide, Diptikalyan Saha, Kush R Varshney,\nand Ruchir Puri. Bias mitigation post-processing for individual and group fairness. In Icassp 2019-2019\nieee international conference on acoustics, speech and signal processing (icassp), pp. 2847–2851. IEEE,\n2019.\nJuan Mata, Fernando Salazar, José Barateiro, and António Antunes. Validation of machine learning models\nfor structural dam behaviour interpretation and prediction. Water, 13(19):2717, 2021.\nNinareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey on\nbias and fairness in machine learning. ACM computing surveys (CSUR), 54(6):1–35, 2021.\nAditya Krishna Menon and Robert C Williamson. The cost of fairness in binary classification. In Conference\non Fairness, accountability and transparency, pp. 107–118. PMLR, 2018.\nDang Nguyen, Sunil Gupta, Santu Rana, Alistair Shilton, and Svetha Venkatesh. Fairness improvement for\nblack-box classifiers with gaussian process. Information Sciences, 576:542–556, 2021.\nGeoff Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, and Kilian Q Weinberger. On fairness and cali-\nbration. Advances in neural information processing systems, 30, 2017.\n13\n\n\nXavier Renard, Thibault Laugel, and Marcin Detyniecki. Understanding prediction discrepancies in classi-\nfication. Machine Learning, pp. 1–30, 2024.\nAndrea Romei and Salvatore Ruggieri. A multidisciplinary survey on discrimination analysis. The Knowledge\nEngineering Review, 29(5):582–638, 2014.\nCynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use inter-\npretable models instead. Nature machine intelligence, 1(5):206–215, 2019.\nAlexandru Ţifrea, Preethi Lahoti, Ben Packer, Yoni Halpern, Ahmad Beirami, and Flavien Prost. Frapp\\’e:\nA post-processing framework for group fairness regularization. arXiv preprint arXiv:2312.02592, 2023.\nRosy Tsopra, Xose Fernandez, Claudio Luchinat, Lilia Alberghina, Hans Lehrach, Marco Vanoni, Felix\nDreher, O Ugur Sezerman, Marc Cuggia, Marie de Tayrac, et al.\nA framework for validating ai in\nprecision medicine: considerations from the european itfoc consortium. BMC medical informatics and\ndecision making, 21:1–14, 2021.\nDennis Wei, Karthikeyan Natesan Ramamurthy, and Flavio Calmon. Optimized score transformation for fair\nclassification. In Silvia Chiappa and Roberto Calandra (eds.), Proceedings of the Twenty Third Interna-\ntional Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning\nResearch, pp. 1673–1683. PMLR, 26–28 Aug 2020.\nLinda F. Wightman. Lsac national longitudinal bar passage study. In LSAC Research Report Series., 1998.\nRuicheng Xian and Han Zhao. Optimal group fair classifiers from linear post-processing, 2024.\nMert Yuksekgonul, Maggie Wang, and James Zou. Post-hoc concept bottleneck models. In The Eleventh\nInternational Conference on Learning Representations, 2023.\nMuhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rogriguez, and Krishna P Gummadi. Fairness con-\nstraints: Mechanisms for fair classification. In Artificial intelligence and statistics, pp. 962–970. PMLR,\n2017.\nMateo Espinosa Zarlenga, Zohreh Shams, Michael Edward Nelson, Been Kim, and Mateja Jamnik. Tabcbm:\nConcept-based interpretable neural networks for tabular data. Transactions on Machine Learning Re-\nsearch, 2023.\nRich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations. In\nInternational conference on machine learning, pp. 325–333. PMLR, 2013.\nBrian Hu Zhang, Blake Lemoine, and Margaret Mitchell.\nMitigating unwanted biases with adversarial\nlearning. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pp. 335–340,\n2018.\nA\nAppendix\n14\n\n\nA\nOverview of post-processing approaches\nIn order to better have an overview of existing post-processing approaches, we decided to summarize (part\nof) them into the following table.\nPaper\nModel\nagnostic\nNo\nSensitive\nat test\ntime\nMetric\noptimized\nMinimizes\nchanges\nMNB Calders & Verwer (2010)\n✗(NB)\n✗\nDP\n✗\nLeaf Relabeling Kamiran et al. (2010)\n✗(C4.5)\n✗\nDP\n✗\nROC Kamiran et al. (2012)\n✓\n✗\nDP\n✗\nEO post-processing Hardt et al. (2016)\n✓\n✗\nEO\n✗\nInformation Withholding Pleiss et al. (2017)\n✓\n✗\nEO\n✗\nIGD post-processing Lohia et al. (2019)\n✓\n✗\nDP\n✗\nMultiaccuracyBoost Kim et al. (2019)\n✓\n✗\nother\n✗\nCFBC Chzhen et al. (2019)\n✓\n✗\nEO\n✗\nWass-1 post-processing Jiang et al. (2020)\n✓\n✗\nDP\n✓\nWass-1 Penalized LogReg Jiang et al. (2020)\n✗(LogReg)\n✓\nDP\n✓\nFST Wei et al. (2020)\n✓\n✓\nDP, EO\n✗\nRNF Du et al. (2021)\n✗(NN)\n✓\nDP, EO\n✗\nFCGP Nguyen et al. (2021)\n✓\n✗\nDP\n✓\nFairProjection Alghamdi et al. (2022)\n✓\n✗\nDP, EO\n✓\nFRAPPÉ Ţifrea et al. (2023)\n✓\n✓\nDP, EO\n✓\nLPP (sensitive-unaware) Xian & Zhao (2024)\n✓\n✓\nDP, EO\n✗\nLPP (sensitive-aware) Xian & Zhao (2024)\n✓\n✗\nDP, EO\n✗\nCOMMOD (Ours)\n✓\n✓\nDP, EO\n✓\nTable 1: Comparison between post-processing methods. NB stands for Naive Bayes, and NN for Neural\nNetworks.\nFrom the table above, we can observe that our method is one of the few that is simultaneously model-\nagnostic, does not require the sensitive variable during inference, and explicitly minimizes the difference\nbetween the black-box scores and the edited ones.\n15\n\n\nB\nProofs of results in Section 4\nIn this section, we present the proofs for the propositions and lemmas stated in the paper. Additionally, we\noffer some context for certain lemmas from Menon & Williamson (2018), which remain necessary within our\nnew framework of Equation 1.\nB.1\nBayes-Optimal Classifier (Result 1)\nBuilding on the results proposed in the paper, we first analyze the Bayes-Optimal Classifier of Equation 1.\nTo facilitate this analysis, we draw upon the work of Menon & Williamson (2018), who demonstrated that\nby translating the components of the optimization problem into a cost-sensitive framework, the problem\nbecomes analytically equivalent but more tractable to solve.\nB.1.1\nA cost-sensitive view of fairness Menon & Williamson (2018).\nFor any randomized classifier g: X ×[0, 1] →[0, 1] that post-process the prediction of a classifier f : X →[0, 1]\nwe can write FNR(g; ¯D) = P( ˆY = 0 | S = 1) and FPR(g; ¯D) = P( ˆY = 1 | S = 0), where we recall that\nˆY = 1g(X)>0.5.\nIf we want to study for example Demographic Parity, we can refers and compute the following quantity to\nmeasure fairness:\nRfair(g) = DI(g; ¯D) = FPR(g; ¯D)/(1 −FNR(g; ¯D)).\n(6)\nThen, we can translates this quantity into a cost-sensitive risks.\nLemma B.1 (Lemma 1 in Menon & Williamson (2018)). Pick any random classifier g. Then, for any\nτ ∈(0, ∞), if k =\nτ\n1+τ\nDI(g; ¯D) ≥τ ⇔CSbal(g; ¯D, 1 −k) ≥k,\nwhere CSbal(g; ¯D, c) = (1 −c) · FNR(g; ¯D) + c · FPR(g; ¯D).\nFurther, we can also rewrite the desired “symmetrised” fairness constraint as\nmin(Rfair(g), Rfair(1 −g)) ≥τ ⇔CSbal(g; ¯D, ¯c) ∈[k, 1 −k],\nfor a suitable choice of ¯c.\nB.1.2\nA cost-sensitive view of minimal changes (Ours).\nIn the paper, we propose then to find an equivalence of the extra (compared to the usual optimization problem\nin fairness) constraint we have in our new optimization problem. To achieve this, we present Lemma 4.1,\nwhose proof is provided below.\nLemma 4.1 of Section 4.1. By law of total probability we have:\nP( ˆY g ̸= ˆY ) = P( ˆY g ̸= ˆY | ˆY g = 1)P( ˆY g = 1) + P( ˆY g ̸= ˆY | ˆY g = 0)P( ˆY g = 0)\nSince we are in a binary classification setting, i.e. ˆY g ∈{0, 1}, we can define\nc∗= P( ˆY g = 1),\n1 −c∗= P( ˆY g = 0).\nHence,\nP( ˆY g ̸= ˆY ) ≤p ⇔c∗P( ˆY g ̸= ˆY | ˆY g = 1) + (1 −c∗)P( ˆY g ̸= ˆY | ˆY g = 0)P ≤p\n⇔c∗FNR(g; D∗) + (1 −c∗)FPR(g; D∗) ≤p\n⇔CSbal(g; D∗, c∗) ≤p.\n16\n\n\nB.1.3\nBayes-Optimal-Classifier.\nFinally, we can rewrite in a Lagrangian version the cost-sensitive problem and solve it analytically.\nLemma B.2. Pick any distributions D, ¯D, D∗, fairness measure DI and constraint on number of changes.\nPick any c, τ, p ∈(0, 1). Then, ∃λfair, λratio ∈R, ¯c, c∗∈(0, 1) with\nmin\ng\nCS(g; D, c)\ns.t.\nmin(Rfair(g), Rfair(1 −g)) ≥τ\nP( ˆY g ̸= ˆY ) ≤p\n(7)\nis equivalent to\nmin\ng\nCS(g; D, c) −λfairCSbal(g; ¯D, ¯c) −λratioCSbal(g; D∗, c∗).\nProof (Lemma B.2). By Lemma B.1 and Lemma 4.1 we have that\nmin(Rfair(g), Rfair(1 −g)) ≥τ ⇔CSbal(g; ¯D, ¯c) ∈[k, 1 −k],\nP( ˆY g ̸= ˆY ) ≤p ⇔CSbal(f; D∗, c∗) ≤p.\nConsequently, for λ1, λ2, λ3 ≥0, the corresponding Lagrangian version will be\nmin\ng\nCS(g; D, c) + λ1(CSbal(g; ¯D, ¯c) −(1 −k)) −λ2(CSbal(g; ¯D, ¯c) −k) −λ3(CSbal(g; D∗, c∗) −p).\nLetting λfair = λ1 −λ2 and λratio = λ3 shows the results.\nLemma B.3. Pick any randomised classifier g. Then, for any cost parameter c ∈(0, 1),\nCS(g; c) = (1 −c)π + EX[(c −η(X))g(X)],\nwhere η(x) = P(Y = 1|X = x) and π = P(Y = 1)\nProof (Lemma B.3). For the case of accuracy and fairness constraints the proof is in Lemma 9 of Menon &\nWilliamson (2018). For what concerns the constraint on similarity between the black-box predictions ˆY f\nand the one coming from the post-processed classifier g(X), FPR and FNR refers to ˆY f and no more to S.\nHence, in the same spirit of the above mentioned proofs,\nCS(g; D∗, c∗) = (1 −c∗)π∗EX| ˆY f =1[1 −g(X)] + c∗(1 −π∗)EX| ˆY f =0[g(X)]\n= (1 −c∗)π∗EX[(c∗−η∗(X))g(X)],\nwhere η∗(X) = P( ˆY f = 1|X = x) and π∗= P( ˆY f = 1).\nAfter introduced all these lemma and definitions, we are now able to provide the proof of Lemma 4.2 stated\nin the paper and that provides the definition of the Bayes-Optimal Classifier.\nProof (Lemma 4.2). By Lemma B.3, measures of accuracy, fairness and minimal changes are\nCS = (1 −c)π + EX[(c −η(X))g(X)],\nCSfair\nbal\n= (1 −¯c)¯π + EX[(¯c −¯η(X))g(X)],\nCSratio\nbal\n= (1 −c∗)π∗+ EX[(c∗−η∗(X))g(X)].\nThen, ignoring constants not dependent from g, the overall objective becomes\nmin\ng\nRCS(g; D, ¯D, D∗) = min\ng\nCS(g; D, c) −λfairCSbal(g; ¯D, ¯c) −λratioCSbal(g; D∗, c∗)\n= min\ng\nEX[{η(x) −c −λratio(¯η(x) −¯c) −λfair(η∗(x) −c∗)}g(X)]\n= min\ng\nEX[−s∗(X)g(X)].\nThus, at optimality, when s∗(x) ̸= 0, gopt = [[s∗(x) > 0]]. When s∗(x) = 0, any choice of gopt is admissible.\n17\n\n\nB.2\nFairness Level Under a Maximum Change Constraint (Result 2)\nProposition 4.4. Without loss of generality, let us suppose that P-Rule = C · γ11\nγ10 . Then, by doing K changes\nwith K ≤KA\ns , we get a new value of P-Rule′ that is given by\nP-Rule′(α) = C ·\nγ11 + α\nγ10 −(K −α),\nwhere α is the number of times action (a). Hence, the number of times action (b) is taken is given by K −α.\nFurther, if we compute the derivative with respect to alpha of P-Rule′ we get\nd\ndαP-Rule′(α) = C\nγ11 −K −γ10\n(γ10 −(K −α))2 ,\n(8)\nand we observe that P-Rule′(α) is either everywhere decreasing or increasing. Hence,\nmax\n0≤α≤KP-Rule′(α) =\n(\nP-Rule′(0) if K > γ10 −γ11\nP-Rule′(K) otherwise\n.\nNotice that action (a) corresponds to α = 0 and action (b) corresponds to α = K. Finally, the same proof\napplies to the case where the starting value of the P-Rule is\n1\nC\nγ11\nγ10 .\nB.2.1\nExperimental evidence and discussion.\nLaw School.\nˆY f = 1\nˆY f = 0\nS = 1\nγ11\nγ01\nS1\nS = 0\nγ10\nγ00\nS0\nCompas.\nˆY f = 1\nˆY f = 0\nS = 1\nγ11\nγ01\nS1\nS = 0\nγ10\nγ00\nS0\n18\n\n\nC\nImplementation details\nC.1\nLoss and Hyperparameters\nAs with any deep learning-based method, fine-tuning hyperparameters has been crucial for our COMMOD\nalgorithm. Specifically, we adjusted the weights of the terms in the loss function using the hyperparameters\nλfair, λratio, and λconcepts.\nTo explore the entire Pareto frontier of Fairness vs.\nAccuracy, as presented\nin this paper, we performed a grid search over these hyperparameters. For implementation purposes, we\noccasionally found it easier to decompose the concept loss Lconcepts into the sum of sparsity loss and diversity\nloss, each controlled by separate hyperparameters. The range of values we tested remained consistent across\ndifferent datasets, with λfair ≤10, λratio ≤0.5, and λconcepts ≤1.\nC.2\nQuartiles definition\nIn the paper, we referred several times to the quartiles of fairness and accuracy. We used them in order\nto compare the methods in the region of the Pareto for which they have approximately the same levels of\nfairness and accuracy. These quartiles have been computed on the AdvDebias method and their values are\nshown in the tables below.\nC.3\nLaw School Dataset\nC.3.1\nDemographic Parity.\nWe recall that in order to measure Demographic Parity we used the P-Rule (the higher, the better). The\nvalue of the point (fairness, accuracy) of the black-box model (Logistic Regression) is (0.2764, 0.7970).\nQuartile\nFairness range\nAccuracy range\nQ1\n[0, 0.5587)\n[0, 0.6709)\nQ2\n[0.5587, 0.7212)\n[0.6709, 0.7294)\nQ3\n[0.7212, 0.8719)\n[0.7294, 0.7550)\nQ4\n[0.8719, 1]\n[0.7550, 1]\nTable 2: Fairness (Demographic Parity) and Accuracy Quartiles on Law School dataset.\nC.3.2\nEqualizing Odds.\nWe recall that in order to measure Equalizing Odds we used the Disparate Mistreatment (the lower, the\nbetter).\nThe value of the point (fairness, accuracy) of the black-box model (Logistic Regression) is\n(0.4935, 0.7970).\nQuartile\nFairness range\nAccuracy range\nQ1\n(0.3429, 1]\n[0, 0.7230)\nQ2\n(0.2415, 0.3429]\n[0.7230, 0.7458)\nQ3\n(0.1503, 0.2415]\n[0.7458, 0.7577)\nQ4\n[0, 0.1503]\n[0.7577, 1]\nTable 3: Fairness (Equalizing Odds) and Accuracy Quartiles on Law School dataset.\nC.4\nCOMPAS Dataset\nC.4.1\nDemographic Parity\n. We recall that in order to measure Demographic Parity we used the P-Rule (the higher, the better). The\nvalue of the point (fairness, accuracy) of the black-box model (Logistic Regression) is (0.6310, 0.6580).\n19\n\n\nQuartile\nFairness range\nAccuracy range\nQ1\n[0, 0.7345)\n[0, 0.6242)\nQ2\n[0.7345, 0.7695)\n[0.6242, 0.6391)\nQ3\n[0.7695, 0.8058)\n[0.6391, 0.6537)\nQ4\n[0.8058, 1]\n[0.6537, 1]\nTable 4: Fairness (Demographic Parity) and Accuracy Quartiles on COMPAS dataset.\nC.4.2\nEqualizing Odds.\nWe recall that in order to measure Equalizing Odds we used the Disparate Mistreatment (the lower, the\nbetter).\nThe value of the point (fairness, accuracy) of the black-box model (Logistic Regression) is\n(0.2828, 0.6580).\nQuartile\nFairness range\nAccuracy range\nQ1\n(0.2198, 1]\n[0, 0.6242)\nQ2\n(0.2079, 0.2198]\n[0.6242, 0.6391)\nQ3\n(0.1869, 0.2079]\n[0.6391, 0.6537)\nQ4\n[0, 0.1869]\n[0.6537, 1]\nTable 5: Fairness (Equalizing Odds) and Accuracy Quartiles on COMPAS dataset.\n20\n\n\nD\nAdditional Experiments and Further Results\nD.1\nExperiment 1: robustness over segment definition\nTo ensure that the results of Experiment 1 are robust to the choice of the segment definitions for accuracy\nand fairness, we propose in this section to reproduce the experiment with other segment definitions. To define\nthese segments, we thus propose, to take the quartile values of the results achieved by LPP (Definition 2)\nand ROC (Definition 3), instead of AdvDebias.\nD.1.1\nDefinition 2.\nThe quartiles values obtained through the result of LPP are the following\nQuartile\nFairness range\nAccuracy range\nQ1\n[0, 0.4709]\n[0, 0.7459)\nQ2\n(0.4709, 0.5918]\n[0.7459, 0.7541)\nQ3\n(0.5918, 0.7763]\n[0.7541, 0.7723)\nQ4\n[0.7763, 1.0]\n[0.7723, 1]\nTable 6: Fairness (DP) and Accuracy Quartiles (Definition 2) on Law School dataset.\nUsing this values, we can then plot the analogue of Fig. 1\n0.00\n0.10\n0.20\n0.30\n0.40\n% Changes\nQ1 Fairness\nQ2 Fairness\n0.60\n0.65\n0.70\n0.75\n0.80\nAccuracy\n0.00\n0.10\n0.20\n0.30\n0.40\n% Changes\nQ3 Fairness\n0.60\n0.65\n0.70\n0.75\n0.80\nAccuracy\nQ4 Fairness\nLAW SCHOOL\nCOMMOD (Ours)\nLPP\nOracle (ROC)\nOracle (LPP)\nZhang\nFRAPPE\nFigure 5: Experiment 1 results, with Definition 2 for Accuracy and Fairness segments\nD.1.2\nDefinition 3.\nThe quartiles values obtained through the results of ROC are the following\nUsing this values, we can then plot the analogue of Fig. 1\nD.2\nAccuracy and Fairness Performance\nAs traditionally done for bias mitigation methods, we evaluate the efficacy of COMMOD to preserve accuracy\nwhen enforcing fairness through the so-called Pareto plot. Figure 7 shows the accuracy and fairness scores\n21\n\n\nQuartile\nFairness range\nAccuracy range\nQ1\n[0, 0.3247]\n[0, 0.7033)\nQ2\n(0.3247, 0.4127]\n[0.7033, 0.7559)\nQ3\n(0.4127, 0.5695]\n[0.7559, 0.7793)\nQ4\n[0.5695, 1]\n[0.7793, 1]\nTable 7: Fairness (DP) and Accuracy Quartiles (Definition 3) on Law School dataset.\n0.00\n0.10\n0.20\n0.30\n0.40\n% Changes\nQ1 Fairness\nQ2 Fairness\n0.60\n0.65\n0.70\n0.75\n0.80\nAccuracy\n0.00\n0.10\n0.20\n0.30\n0.40\n% Changes\nQ3 Fairness\n0.60\n0.65\n0.70\n0.75\n0.80\nAccuracy\nQ4 Fairness\nLAW SCHOOL\nCOMMOD (Ours)\nLPP\nOracle (ROC)\nOracle (LPP)\nZhang\nFRAPPE\nFigure 6: Experiment 1 results, with Definition 3 for Accuracy and Fairness segments\nachieved by all methods on Law School and Compas datasets (each dot represents one run).\nIn terms\nof direct competitors, we observe that COMMOD outperforms LPP (resp. FRAPPE) on the Law School\ndataset (resp. Compas) and achieves similar results in the Compas dataset (resp. Law School). Further,\nwe show here that even by adding the Minimal Changes and Interpretability constraints in Equation 4\nCOMMOD achieves almost similar results on all datasets as the in-processing method AdvDebias. Finally,\nas expected, Oracle (and also by ROC in Compas) generally outperform all methods. Hence, COMMOD\nremains a competitive algorithm in terms of fairness and accuracy, regardless of its other benefits, addressed\nin the next sections.\nD.3\nIntuition behind linear self-explainable model.\nOne might question why we chose to introduce explainability into our method through a linear architecture\nthat learns concepts as linear combinations of the input features. Typically, using a linear model instead of\na more complex one can lead to a decrease in model performance. However, the rationale behind our choice\nis clarified through the following experiment, where we evaluate different architectures for the network that\nlearns the multiplicative ratio. From the plot above, we can observe that the linear architecture performs\nsimilarly to more complex models. This observation aligns with the findings in Ţifrea et al. (2023), where a\nsimilar conclusion was drawn regarding their additive term, which is conceptually similar to our multiplicative\nratio. Based on these insights, we decided to design the architecture of the ratio as a one-hidden-layer linear\nnetwork. In this design, the hidden layer acts as a bottleneck that represents the concepts, and the output\nlayer computes the ratio as a linear combination of these concepts.\nAdditionally, maintaining linearity between the concepts and the output ratio allows us to easily determine\n22\n\n\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nP-Rule\n0.60\n0.63\n0.65\n0.68\n0.70\n0.73\n0.75\n0.78\n0.80\nAccuracy\nLAW SCHOOL\nAdvDebias (Zhang et al., 2018)\nCOMMOD (Ours)\nROC (Kamiran et al., 2012)\nLPP (Xian et al., 2024)\nOracle (Xian et al., 2024)\nFRAPPÈ (Tifrea et al., 2024)\n0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00\nP-Rule\n0.59\n0.60\n0.61\n0.62\n0.63\n0.64\n0.65\n0.66\nAccuracy\nCOMPAS\nAdvDebias (Zhang et al., 2018)\nCOMMOD (Ours)\nROC (Kamiran et al., 2012)\nLPP (Xian et al., 2024)\nOracle (Xian et al., 2024)\nFRAPPÈ (Tifrea et al., 2024)\n0.1\n0.2\n0.3\n0.4\n0.5\nDisparate Mistreatment (DM)\n0.64\n0.66\n0.68\n0.70\n0.72\n0.74\n0.76\n0.78\n0.80\nAccuracy\nLAW SCHOOL\nAdvDebias (Zhang et al., 2018)\nCOMMOD (Ours)\nROC (Kamiran et al., 2012)\nLPP (Xian et al., 2024)\nOracle (Xian et al., 2024)\nFRAPPÈ (Tifrea et al., 2024)\n0.1\n0.1\n0.2\n0.2\n0.2\n0.3\nDisparate Mistreatment (DM)\n0.60\n0.61\n0.62\n0.63\n0.64\n0.65\n0.66\nAccuracy\nCOMPAS\nAdvDebias (Zhang et al., 2018)\nCOMMoD (Ours)\nROC (Kamiran et al., 2012)\nLPP (Xian et al., 2024)\nOracle (Xian et al., 2024)\nFRAPPÈ (Tifrea et al., 2024)\nFigure 7: Fairness (x-axis) vs Accuracy (y-axis) trade-offs on Law School and Compas datasets for Demo-\ngraphic Parity (top) and Equalized Odds (bottom).\nthe direction (positive or negative) in which each concept influences the ratio’s value. This design choice\nenhances both the interpretability and transparency of the model.\nD.4\nBenefits of MSE Loss\nIn this section, we propose a visualization in support to the MSE loss we chose to keep as similar as possible\nthe edited scores with the ones outputted by the black-box algorithm. To do so, we used a \"calibration\" plot\non edited scores vs black-box scores for our COMMOD algorithm and for AdvDebias for similar levels of\n(fairness, accuracy). From such a plot, one can observe that the curve of COMMOD is more stick to the\ndashed line, representing when probabilities are not changed at all by the editing. On the other hand, we\nobserve that in AdvDebias even if we do not change a prediction label, the probabilities are more scattered\nin the area of the dashed line. This behavior is beneficial for our motivation. When you want to edit a\nmodel for fairness, you usually want to do changes that flip labels (and hence that modifies the values of\nyour fairness definition) and keep the other instances at the same confidence they were before.\n23\n\n\n0.30\n0.40\n0.50\n0.60\n0.70\n0.80\n0.90\n1.00\nP-Rule\n0.60\n0.65\n0.70\n0.75\n0.80\nAccuracy\nLAW SCHOOL\nLinear Ratio\n2 hidden layers\n3 hidden layers\nFigure 8: Fairness vs Accuracy trade-off on Law School for different ratio architectures. Each dot corresponds\nto one model run.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBiased probabilities\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEdited probabilities\nLAW SCHOOL\nSensitive Group = 1\nSensitive Group = 0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nScores Black-Box (Logistic Regression)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nScores AdvDebias\nLAW SCHOOL\ny=x\nFigure 9: Calibration plot (predicted probabilities vs black-box probabilities) on Law School dataset for\nDemographic Parity.\nD.5\nPosthoc vs self explainable\nAs a final experiment, we aim to demonstrate the need for an approach specifically designed for Controlled\nModel Debiasing such as COMMOD. For this purpose, we build on the previous experiment and show\nthat the combination of FRAPPE (which minimizes the number of changes) and a post-hoc interpretability\ntool leads to less efficient models in terms of debiasing; in other words, that adding interpretability in a\npost-hoc way comes at a higher cost for fairness. We train FRAPPE and COMMOD on the Law School\ndataset such that their scores in terms of accuracy, fairness and number of changes are comparable. We then\ntrain a decision tree to model FRAPPE’s additive output, and show in Figure 10 the final P-Rule obtained\nby COMMOD and this built competitor. We observe that, regardless of the tree’s depth, controlling the\ndebiasing by introducing interpretability in a post-hoc way heavily degrades fairness, contrary to COMMOD.\n24\n\n\n2\n3\n4\n5\n6\n7\n8\n9\nDepth\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nFairness\nmax_depth=None\nFRAPPE + CART\nCOMMOD\nFigure 10: Comparison of fairness levels of FRAPPE+CART (trained on the additive term) with the one of\na self-explainable model for different levels of depth\n25\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21284v1.pdf",
    "total_pages": 25,
    "title": "Controlled Model Debiasing through Minimal and Interpretable Updates",
    "authors": [
      "Federico Di Gennaro",
      "Thibault Laugel",
      "Vincent Grari",
      "Marcin Detyniecki"
    ],
    "abstract": "Traditional approaches to learning fair machine learning models often require\nrebuilding models from scratch, generally without accounting for potentially\nexisting previous models. In a context where models need to be retrained\nfrequently, this can lead to inconsistent model updates, as well as redundant\nand costly validation testing. To address this limitation, we introduce the\nnotion of controlled model debiasing, a novel supervised learning task relying\non two desiderata: that the differences between new fair model and the existing\none should be (i) interpretable and (ii) minimal. After providing theoretical\nguarantees to this new problem, we introduce a novel algorithm for algorithmic\nfairness, COMMOD, that is both model-agnostic and does not require the\nsensitive attribute at test time. In addition, our algorithm is explicitly\ndesigned to enforce minimal and interpretable changes between biased and\ndebiased predictions -a property that, while highly desirable in high-stakes\napplications, is rarely prioritized as an explicit objective in fairness\nliterature. Our approach combines a concept-based architecture and adversarial\nlearning and we demonstrate through empirical results that it achieves\ncomparable performance to state-of-the-art debiasing methods while performing\nminimal and interpretable prediction changes.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}