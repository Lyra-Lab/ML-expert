{
  "id": "arxiv_2502.20633v1",
  "text": "Are LLMs Ready for Practical\nAdoption for Assertion Generation?\nVaishnavi Pulavarthi, Deeksha Nandal\nElectrical and Computer Engg. Dept.\nUniversity of Illinois Chicago\nChicago, USA\n{vpulav2, dnanda6}@uic.edu\nSoham Dan\nMicrosoft\nsohamdan@microsoft.com\nDebjit Pal\nElectrical and Computer Engg. Dept.\nUniversity of Illinois Chicago\nChicago, USA\ndpal2@uic.edu\nAbstract‚ÄîAssertions have been the de facto collateral for\nsimulation-based and formal verification of hardware designs for\nover a decade. The quality of hardware verification, i.e., detection\nand diagnosis of corner-case design bugs, is critically dependent on\nthe quality of the assertions. With the onset of generative AI such\nas Transformers and Large-Language Models (LLMs), there has\nbeen a renewed interest in developing novel, effective, and scalable\ntechniques of generating functional and security assertions from\ndesign source code. While there have been recent works that use\ncommercial-of-the-shelf (COTS) LLMs for assertion generation,\nthere is no comprehensive study in quantifying the effectiveness\nof LLMs in generating syntactically and semantically correct\nassertions. In this paper, we first discuss AssertionBench from\nour prior work, a comprehensive set of designs and assertions to\nquantify the goodness of a broad spectrum of COTS LLMs for\nthe task of assertion generations from hardware design source\ncode. Our key insight was that COTS LLMs are not yet ready for\nprime-time adoption for assertion generation as they generate a\nconsiderable fraction of syntactically and semantically incorrect\nassertions. Motivated by the insight, we propose AssertionLLM,\na first of its kind LLM model, specifically fine-tuned for as-\nsertion generation. Our initial experimental results show that\nAssertionLLM considerably improves the semantic and syntactic\ncorrectness of the generated assertions over COTS LLMs.\nIndex Terms‚Äîcomponent, formatting, style, styling, insert\nI. INTRODUCTION\nSystem-on-Chip (SoC) designs are crucial for many safety-\ncritical computing applications, including vehicular systems,\nmilitary, and industrial automation. SoCs often use sensitive and\nusers‚Äô private data to perform numerous complex computations.\nIt is crucial for our national and personal well-being to ensure\nthat the SoCs are functionally correct, safe, and secure.\nAssertions are mathematical encoding of desired design\nproperties that should hold True for a design. Assertions are\nwidely used for hardware design validation throughout its life\ncycle, e.g., pre-silicon formal verification and simulation-based\nverification, emulation, and often synthesized in a fabricated\nchip for post-silicon validation and in-field debug and diagno-\nsis. In the past decade, assertion-based Verification (ABV) [1]\nhas emerged as the de facto standard to verify the security and\nfunctional correctness of hardware designs. However, crafting a\nsuccinct yet expressive set of assertions that capture subtle and\nimportant hardware design behaviors is a tedious and time-\nconsuming task, requiring a considerable amount of human\ningenuity. Too many assertions (i) can negatively affect verifica-\ntion performance with a prolonged verification closure and (ii)\nmay require a substantial amount of on-chip resources, whereas\ntoo few assertions may result in insufficient design coverage\ncausing corner case design bugs to escape to production and\nmass manufacturing. The ever increasing hardware design com-\nplexity and rapidly broadening target applications (e.g., deep\nlearning, AI) have only worsened the problem. Consequently,\ndeveloping an automated and scalable technique is crucial to\nrapidly generate a succinct set of hardware design properties\ntargeting design functionality and security.\nA considerable amount of research has leveraged two dif-\nferent paradigms ‚Äì lightweight static analysis of design source\ncode and formal verification\n[2]‚Äì[4], and data-driven statis-\ntical analysis, e.g., data mining [5]‚Äì[10]. While static analy-\nsis can generalize and capture corner-case design behaviors,\nit suffers from prohibitive computational complexity limiting\nits scalability to industrial-scale designs. Alternatively, data-\ndriven dynamic analysis can scale to large designs with a\nconsiderable amount of trace data due to its computational\nefficiency, however, it often generates spurious design prop-\nerties due to the lack of design insights and domain context.\nMore recently, researchers have proposed assertion generation\ntechniques that combine static analysis and data-driven dynamic\nanalysis [11]‚Äì[13] and developed algorithms to induce ranking\non such automatically generated assertions based on the subtle\ndesign behavior they capture [14]. However, all such techniques\ngenerate a large number of assertions, many of which are re-\ndundant and do not capture model-level or system-level design\nbehaviors, and fail to scale to large industry-scale designs due\nto the algorithmic complexity of the underlying static analysis.\nDespite intense research across academia and industry over the\nlast decade, there is a widening gap between assertion solutions\nand the industry‚Äôs actual requirements in terms of assertion\nquality for complex hardware designs.\nWith recent advances in deep-learning (DL) and generative\nAI models, especially Large-Language Models (LLMs), e.g.,\nGPT-3.5, GPT-4o, CodeLLaMa 2, and LLaMa3-70B, there is\na renewed interest to harness the power of LLMs to tame the\never-widening gap. Most recent assertion generation approaches\n(c.f., Section VIII) treat a LLM model as a black box and use\nprompt engineering to iteratively refine the set of generated\nassertions. However, there is no in-depth study nor a dataset to\narXiv:2502.20633v1  [cs.LG]  28 Feb 2025\n\n\n1\nmodule arb2(clk, rst, req1, req2, gnt1, gnt2);\n2\ninput clk, rst, req1, re2;\n3\noutput gnt1, gnt2;\n4\nreg gnt_, gnt1, gnt2;\n5\nalways @(posedge clk or posedge rst)\n6\nif(rst)\n7\ngnt_ <= 0;\n8\nelse\n9\ngnt_ <= gnt1;\n10\nalways @(*)\n11\nif (gnt_)\n12\nbegin\n13\ngnt1 = req1 & req2;\n14\ngnt2 = req2;\n15\nend\n16\nelse\n17\nbegin\n18\ngnt1 = req1;\n19\ngnt2 = req2 & Àúreq1;\n20\nend\n21\nendmodule\nFig. 1: A Verilog code for a 2-port Arbiter [14].\nPre-condition\nCovered\nUnreachable\nTrue\nValid\nVacuous\nPost-\nCondition\nFalse\nCEX\nFig. 2: Assertion status based on pre-condition and post-\ncondition evaluation. CEX: Counter example.\nevaluate the fit of different state-of-the-art (SOTA) LLM models\nfor generating a succinct and correct set of assertions without\na considerable amount of designer-developed prompts.\nIn this work, first, we discuss our prior work AssertionBench\n[15], the first comprehensive benchmark consisting of 100\ncurated hardware designs of varying complexity and their\nformally verified assertions to quantify the efficacy of SOTA\nand upcoming LLMs for assertion generation. Our primary\nfocus is to quantify the quality of the generated assertions\nfrom SOTA LLMs learned on a set of labeled designs and their\nformally verified assertions. Our key insight is that almost all\nSOTA LLMs generate a considerable fraction of syntactically\nand semantically incorrect assertions. Leveraging this insight,\nwe develop AssertionLLM, a fine-tuned LLM model that\ncan automatically generate substantially higher fraction (up\nto 25%) of syntactically and semantically correct assertions\nfrom design source codes without any iterative inputs from\nthe verification engineer. We further outline several research\nchallenges and opportunities that are worth pursuing to truly\nexploit the potential of generative AI for assertion generation.\nII. BACKGROUND\nA. Assertions: Syntax, Semantics, and Validity\nWe consider a hardware design D in Verilog1 as a com-\nposition of a set of concurrent processes Pi, e.g., (always\nand assign blocks). Let V be the set of design signals,\nI ‚äÇV be the set of input signals, O ‚äÇV be the set\n1We consider Verilog as the demonstration vehicle for this work, however,\nour work can naturally be extended to other hardware design languages, e.g.,\nVHDL, SystemC, and other hardware description languages.\nof output signals, and R ‚äÇV be the set of registers. Fig-\nure 1 shows a Verilog design D of a 2-port Arbiter, con-\nsisting of two concurrent processes P1 (line 6) and P2 (line\n11), and V = {clk, rst, req1, req2, gnt1, gnt2, gnt }, I =\n{clk, rst, req1, req2}, O = {gnt1, gnt2}, and R = {gnt }.\nAn assertion is a temporal formula in LTL [16] of the\nformat P = G(A ‚ÜíC) where the antecedent A is of the\nform A = Vm\ni=0 X i(Ai) and consequent C is of the form\nC = X n(Cn), where n ‚â•m. Each Ai (Ci) is a proposition\nand is a (var, val) pair where var ‚ààV and val ‚àà{0, 1}. X is\ncalled the next-cycle operator and X i(i ‚â•0) is equal to a delay\nof i clock cycles XX . . . X\n|\n{z\n}\ni times\n. Although SystemVerilog Assertion\n(SVA) [17] defines a rich set of grammar for assertions, we\nconsider a restricted subset (sequential assertion) as captured\nby P. We say an assertion P is True (Valid) if D |= P (read\nas D models P), otherwise, the assertion is False, i.e., D Ã∏|= P\nand there exists a Boolean value assignment to a subset of\ndesign signals known as CEX (counter-example) that shows a\nrefutation of the assertion P on D. The implication operator\n‚Üíin P are of two types, overlapped and non-overlapped. The\noverlapped implication operator (‚Üí) implies if there is a match\non the antecedent A, then the consequent C is evaluated in\nthe same clock cycle whereas the non-overlapped implication\noperator (‚áí) implies if there is a match on the antecedent A,\nthen the consequent C is evaluated in the next clock cycle.\nIn Figure 2, we show the assertion evaluation status. Note\nA ‚ÜíC can be re-written as ¬¨A ‚à®C. Consequently, if pre-\ncondtion A is unreachable (or False), then the assertion P is\nvacuously True (i.e., ¬¨False ‚à®C). If pre-condition A is True\nand post-condition C is True as well, the assertion is reported\nto be Valid (i.e., D |= P), otherwise, if the post-condition C is\nFalse, then a counter example CEX is generated.\nFor the Arbiter of Figure 1, consider assertions P1\n:\nG((req1 == 1 ‚àßreq2 == 0) ‚Üí(gnt1 == 1)) and\nP2 : G((req2 == 0 ‚àßgnt\n== 1) ‚àßX(req1 == 1) ‚Üí\nX(gnt1 == 1)). The assertion P1 evaluates True if req1\nis 1‚Äôb1 and req2 is 1‚Äôb0 at the current clock cycle, then\ngnt1 is 1‚Äôb1 in the current clock cycle. The assertion P2\nevaluates True if req2 is 1‚Äôb0 and gnt is 1‚Äôb1 in the current\ncycle, req1 is 1‚Äôb1 in the next cycle, then gnt1 is 1‚Äôb1\nin the cycle after (i.e., in the 2nd cycle). Note that P2 can\nbe re-written using the non-overlapped implication operator,\nP2 : G((req2 == 0 ‚àßgnt\n== 1) ‚àßX(req1 == 1) ‚áí\n(gnt1 == 1)) where the ‚áísubsumes the X operator in the\nconsequent. On discharging a proof for P1 and P2 using a\nformal property verification (FPV) engine2, we find P1 is a\nvalid assertion whereas P2 generates a CEX.\nB. Large-Language Models\nLarge-Language Models (LLMs) are an instance of genera-\ntive AI built on top of encoder-decoder transformer architec-\ntures [19]. LLMs can be classified primarily in three classes,\n(i) encoder-only LLMs [20], (ii) decoder-only LLMs [21],\nand (iii) encoder-decoder LLMs [22]. Encoder-only LLMs\n2We use Cadence JasperGold [18], however, any other FPV tool will work.\n\n\nDesign\nLine No\n0\n250\n500\n750\n1000\n1250\nfht_1d_x8.v\nmtx_trps_8x8_dpsra\nbitNegator.v\ninputReg.v\ntcReset.v\nkey_expander.v\ncavlc_read_total_coe\nPSGBusArb.v\nPSGOutputSummer.v\ncrc_control_unit.v\nqadd.v\nnode.v\nclean_rst.v\nge_1000baseX_rx.v\neth_l3_checksum.v\neth_clockgen.v\nflow_ctrl.v\nreg_int_sim.v\ncounter.v\nrxStateMachine.v\ncan_crc.v\ncan_register_asyn_s\neth_fifo.v\nphasecomparator.v\nfifo_mem.v\nFig. 3: Design details in the test set in terms of the number\nof lines of code (excluding comments and blank lines).\nAssertion \nGeneration via \ntrained LLM Model\nAssertion Syntax \nCorrector\nLLM Model ICL\n(1-shot / 5-shot)\nùë®ùüè‚Üíùë™ùüè\nTrain Examples\nTest \nDesigns\nùë®‚Ä≤ùüè‚Üíùë™‚Ä≤ùüè\nùë®ùíé‚Ä≤ ‚Üíùë™‚Ä≤ùíé\nùë®ùíè‚Üíùë™ùíè\nFormal Verification\nEngine (JG)\nPASS\nFAIL\n1\n2\n3\n4\nFig. 4: Framework to evaluate LLMs for assertion genera-\ntion [15]. JG: JasperGold Formal Property Verification Engine.\nemploy a bi-directional transformer during pre-training for each\ntoken to attend every other token, decoder-only LLMs employ\nunidirectional language modeling for each token to attend its\npredecessor tokens, and where tokens can only participate in\nprevious tokens, and encoder-decoder LLMs employ denoising\nsequence-to-sequence pre-training objectives. The decoder-only\nLLM performs excellently in auto-regressive tasks such as\ncode completion and generation. Since assertion generation is\na special kind of code generation, in this work, we employ\ndecoder-only LLMs, e.g., GPT-3.5, GPT-4o, etc.\nThere are two distinct paradigms for LLM usage for different\ntasks ‚Äì (i) in-context learning (ICL), where a foundational\nLLM (e.g., GPT-4o) is seeded with a few examples of the\ndesired task followed by deployment and (ii) fine tuning where\na foundational LLM is trained with a small amount of high-\nquality downstream task-specific data to construct a task-\nspecific LLM. In this work, we use ICL to evaluate the fitness\nof COTS LLMs for assertion generation and use finetuning to\ndevelop specialized LLMs for assertion generation tasks.\nIII. ASSERTIONBENCH: BENCHMARK TO QUANTIFY\nGOODNESS OF LLMS FOR ASSERTION GENERATION\nAssertionBench3 is a comprehensive suite of Verilog design\nand associated formally verified assertions to evaluate the\ngoodness of the COTS LLMs for assertion generation [15].\nAssertionBench consists of designs from OpenCores [23].\nFigure 3 and Table I show representative details of the designs.\nOur benchmark consists of five ICL examples for 1-shot and\n5-shot learning, where each example is a tuple consisting of\na Verilog design and its formally verified assertions, generated\nfrom GOLDMINE [11] and HARM [13], and verified using\nCadence JasperGold [18]. The training set comprises funda-\nmental designs such as Arbiter, Half Adder, Full Adder, T-flip-\nflop, and Full Subtractor. Among these designs, Arbiter and\nT flip-flops are sequential, while the others are combinational.\nOur training set assertions contain both overlapped and non-\noverlapped implication operators. The test design set contains\n3https://github.com/achieve-lab/assertion data for LLM.\n1 You are an expert in SystemVerilog Assertions.\n2 Your task is to generate the list of assertions to\nthe given verilog design. An example is shown\nbelow. Generate only the list of assertions for\nthe test program with no additional text.\n3 Program 1: module arb2(clk, rst, req1, req2, gnt1,\ngnt2); input clk, rst; ...\n4 Assertions 1: (state == 1 & req2 == 1) |-> (gnt1 ==\n0);...\n5 Test Program:\n6 module fifo_mem #(parameter DEPTH=8, DATA_WIDTH=8,\nPTR_WIDTH=3) ( input wclk, w_en, rclk, r_en,\ninput [PTR_WIDTH:0] b_wptr,\n...\n7 Test Assertions:\nFig. 5: An example of the prompt for 1-shot learning [15].\nThe example consists of a tuple, a Verilog design (Program\n1) and a set of formally verified assertions for the design\n(Assertions 1). The Test Program is the Verilog de-\nsign for which we generate assertions using the trained LLM.\n100 Verilog designs (split in combinational and sequential\ndesigns) from OpenCores [23] that are more complex than\nthose in the training set, to evaluate LLMs‚Äô 1-shot and 5-\nshot learning. The set cover a wide variety of hardware in-\ncluding communication controllers, random number generators\n(RNG) for security hardware, Floating Point Unit (FPU), state\nmachines, and flow control hardware. The test designs code\nsize varies from 10 lines to 1150 lines (excluding blanks and\ncomments) as measured by cloc [24].\nIV. EXPERIMENTAL SETUP\nEvaluation Protocol: Figure 4 shows our evaluation frame-\nwork. To evaluate the effectiveness of the different LLMs, our\nk-shot ICL consists of 1-shot and 5-shot in-context examples\n(ICE) ( 1 in Figure 4). Each ICE is a tuple ‚ü®D, A‚ü©, where D\nis a Verilog source code and A is a set of formally verified\nassertions containing a minimum of two (2) and a maximum\nof 10 assertions with an average of 4.8 assertions per source\ncode. We use a prompt as shown in Figure 5 consisting of four\nparts ‚Äì (i) an English language description of the task in hand,\n(ii) an example Verilog design with newlines and comments\nremoved, (iii) an example assertion in SVA format, and (iv)\na test Verilog design with new lines and comments removed.\nFollowed by training, we provide each trained model with 100\ntest Verilog programs to infer assertions ( 2 in Figure 4). In our\nexperiments, we have found all of the LLM models generate\nsyntactically erroneous assertions, i.e., each LLM fails to learn\nthe SVA syntax from the training examples. Consequently, we\nuse a syntax corrector ( 3 in Figure 4) using GPT-3.5 and feed\nthe output of the syntax corrector to Cadence JasperGold FPV\nengine to evaluate the quality of the generated assertions. Note\nany other FPV engine compatible with SVA will work as well.\nICL Compute Platform: We use UIUC (University of Illinois\nUrbana-Champaign) NCSA‚Äôs (National Center for Supercom-\nputing Applications) Delta Cluster [25] to run our experiments.\nWe use GPU nodes containing 1-way, 4-way, and 8-way\nNVIDIA A40 (with 48GB GDDR6) and A100 (with 40GB\nSXM ) GPUs to perform k-shot learning.\n\n\nTABLE I: Details of a few representative designs in the test set of AssertionBench benchmark.\nVerilog Design\n# of Lines\nDesign Type\nDesign Functionality\nca prng\n1144\nSequential\nA compact Pattern Generator\ncavlc read total coeffs\n1090\nSequential\nVideo Encoder for generic audio visual.\ncavlc read total zeros\n637\nCombinational\nVideo Encoder for generic audio visual.\nge 1000baseX rx\n544\nSequential\nVerilog implementation of Physical Coding\nSublayer (PCS) type.\nMAC tx Ctrl\n504\nSequential\nAn Ethernet MAC controller.\nPre-trained Models and EDA Tools: We use pre-trained\nLLMs from the HuggingFace [26] for evaluation and Cadence\nJasperGold version 2022.06p002 to formally verify the asser-\ntions generated from the test Verilog designs. We use two\nSOTA tools GOLDMINE [11], [14] and HARM [13] to generate\nassertions for Verilog designs in the ICE. Below, we summarize\nthe COTS LLMs that we evaluate using AssertionBench.\n1) GPT-3.5 is a commercial LLM built using the GPT ar-\nchitecture [27]. It is part of OpenAI‚Äôs GPT (Generative Pre-\ntrained Transformer) series of models designed to understand\nand generate text based on the input it receives.\n2) GPT-4o (‚Äòo‚Äô for ‚Äúomni‚Äù) is the newest model of OpenAI‚Äôs\nGPT, which accepts any combination of input audio, image,\nvideo, and text and responds with an output consisting of\nimage, audio, video, and text [21]. With larger training data,\nincreased model size, and faster response than other GPT\nmodels, GPT-4o is a unified model for text, vision, and audio.\n3) CodeLLaMa 2 is a collection of generative text models\ndeveloped by Meta [28] with parameters ranging from 7B to\n70B. The model accepts only text as input and output. It is an\nauto-regressive language model. The context window length for\nCodeLLaMa 2 is 4096. The large 70B model uses Grouped-\nQuery Attention for improved inference scalability.\n4) LLaMa3-70B is available in two parameter sizes ‚Äì 8B and\n70B. The context window length for LLaMa3-70B is 8192\nand is pre-trained with 15 Trillion tokens of publicly available\ndata [29]. LLaMa3-70B excels at translation, contextual under-\nstanding, and dialogue generation. It has enhanced capabilities\nsuch as code generation, reasoning, and following instructions.\nICL Hyperparameters: For all LLMs under consideration, the\nhyperparameters have been set to their default values. Specifi-\ncally, the maximum output tokens is set to 1024, employing a\ngreedy decoding strategy and maintaining a temperature of 1.0\n(most creative), top p of 0.95. The random seed is set to 50.\nMetrics: We evaluate the generated assertions from the test\ndesigns using following metrics for each k-shot ICL per LLM.\n1) Pass quantifies the fraction of generated assertions that\nFPV engine attests as valid for the design. This includes the\nVacuous and the Pass cases from Figure 2.\n2) Fail quantifies the fraction of generated assertions that FPV\nengine attests as wrong for the design and generates a coun-\nterexample trace. This includes the Fail case from Figure 2.\n3) Error: It quantifies the fraction of generated assertions for\nwhich the FPV engine identifies one or more syntactic errors\nin the assertions even after syntax correction by the GPT-3.5.\nV. OBSERVATIONS AND INSIGHTS FROM ASSERTIONBENCH\nWe depict our observations and insights [15] in Figure 6\nand Figure 7 and discuss them below.\nObservation 1: Most LLMs generate valid assertions with\nan increasing number of ICL examples. For the assertion\ngeneration task, all LLMs progressively generate more valid\nassertions when the number of ICL examples is increased as\nseen in Figure 6. GPT-3.5, GPT-4o, and CodeLLaMa 2 show\non average an improvement of 2√ó, 1.2√ó, and 1.12√ó for valid\nassertion generation, respectively, when moved from 1-shot\nlearning to 5-shot learning. However, the LLaMa3-70B model\nloses accuracy from 31% to 24% on the same dataset. Our\nin-depth analysis shows in many cases, LLaMa3-70B either\nfails to generate assertions or generates syntactically wrong\nassertions (which even a syntax corrector fails to correct) or\ntries to generate codes in a new programming language (such\nas Java). This experiment shows that there is a considerable\nscope for improving the LLaMa3-70B model for this task,\nlikely via fine-tuning the pre-trained LLaMa3-70B model.\nObservation 2: An enhanced LLM does not necessarily\nensure a better semantic or syntactic understanding. In Fig-\nure 6, we do not see a correlation between the sophistication\n(in terms of the number of model parameters) of the LLMs and\ntheir ability to generate good assertions. For GPT-3.5 (c.f., Fig-\nure 6a), with an increase in the number of ICL examples, the\nLLM was able to produce more syntactically correct assertions,\nhowever, after such corrections, the majority of assertions (on\naverage up to 24%) generated a CEX when verified with\nJasperGold. For GPT-4o, the results were more consistent in\nterms of syntactically (in)correct assertions for both 1-shot\nand 5-shot learning (c.f., Figure 6b). For CodeLLaMa 2 and\nLLaMa3-70B, with an increase in the number of ICL examples,\nthe number of failed assertions decreased (on average up\nto 12% for CodeLLaMa 2 and LLaMa3-70B, c.f., Figure 6c\nand Figure 6d), however, both models generated more syn-\ntactically wrong assertions (on average up to 19% more for\nLLaMa3-70B). This observation is perplexing as one would\nexpect with more number of parameters, LLaMa3-70B would\nbe able to learn better to predict syntactically correct assertions.\nOur in-depth analysis shows that with a 1-shot, the variations\nin types of assertions in examples were limited. Consequently,\nLLaMa3-70B learned the assertion syntax. However, in 5-\nshot learning, we have more variations in the which made\nLLaMa3-70B‚Äôs learning task difficult. This experiment sug-\ngests that increasing the ICL examples alone will not neces-\n\n\nAccuracy\n0.0\n0.2\n0.4\n0.6\n1-shot\n5-shot\nPass\nCEX\nError\n(a)\nAccuracy\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n1-shot\n5-shot\nPass\nCEX\nError\n(b)\nAccuracy\n0.0\n0.2\n0.4\n0.6\n0.8\n1-shot\n5-shot\nPass\nCEX\nError\n(c)\nAccuracy\n0.0\n0.2\n0.4\n0.6\n1-shot\n5-shot\nPass\nCEX\nError\n(d)\nFig. 6: Comparison of accuracy of generated assertions. (a) Assertion accuracy comparison for GPT-3.5. (b) Assertion\naccuracy comparison for GPT-4o. (c) Assertion accuracy comparison for CodeLLaMa 2. (d) Assertion accuracy comparison for\nLLaMa3-70B. (a) k = 1-shot assertion accuracy. (b) k = 5-shot assertion accuracy. CEX: Counter Example trace.\nAccuracy\n0.0\n0.2\n0.4\n0.6\n0.8\nPass\nCEX\nError\nGPT3.5\nGPT 4o\nCodeLLaMa 2\nLLaMa 3\n(a) 1-shot.\nAccuracy\n0.0\n0.2\n0.4\n0.6\nPass\nCEX\nError\nGPT3.5\nGPT 4o\nCodeLLaMa 2\nLLaMa 3\n(b) 5-shot.\nFig. 7: Comparison of accuracy of generated assertions in\nterms of passing, failing, (generating a counter example),\nand syntactically wrong assertions between different LLMs\nper k-shot learning where k = 1 and k = 5.\nsarily improve LLM‚Äôs consistency in generating syntactically\nand semantically correct assertions.\nObservation 3: GPT-4o is relatively more consistent for\nassertion generation task. In Figure 7, we compare different\nLLMs in terms of generating valid assertions for 1-shot and\n5-shot learning. Our experiments show that GPT-4o is consis-\ntently superior in generating valid assertions for both 1-shot\nand 5-shot learning and generates, on average, up to 15.6%\nmore valid assertions as compared to other LLMs. This trend\nremains valid with respect to assertions generating CEX and\nsyntactically wrong assertions, i.e., GPT-4o produced less CEX\ngenerating assertions and syntactically incorrect assertions as\ncompared to other LLMs. This experiment shows that GPT-4o\nperforms relatively better than other LLMs.\nObservation 4: All LLMs need considerable improvement\nfor assertion generation task: In-depth analysis of Figure 6\nand Figure 7 show that none of the LLMs can generate valid\nassertions with an average of no more than 44% accuracy\nwhereas up to 63% generated assertions produces CEX and\non average up to 33% of generated assertions are syntactically\nwrong. Clearly, for LLMs to be of practical usage for any real-\nistic industrial-scale design, considerable improvement needs to\nbe made. Specifically, the LLMs need to capture the semantic\nmeaning of the hardware description languages, e.g., Verilog,\nfor generating higher fraction of valid assertions automatically\nwithout iterative human prompting. Our prior work [11], [14]\nshows that such critical insights are not directly available from\nthe raw design source code and need auxiliary artifacts, such\nas Control-Data Flow Graph (CDFG), Variable Dependency\nGraph (VDG), Cone of Influence (COI), etc. Future research\nin applying LLMs for assertion generation should consider\nsuch auxiliary artifacts to develop assertion-specific LLMs.\nEvaluation of the four COTS LLMs using AssertionBench\nshows that no LLM consistently outperforms other LLMs.\nùë®ùüè‚Üíùë™ùüè\nk Train Examples\nùë®ùíè‚Üíùë™ùíè\nAssertion \nGeneration via \ntrained LLM Model\nFine-tuned LLM \nModel ICL\n(1-shot / 5-shot)\nTest \nDesigns\nùë®‚Ä≤ùüè‚Üíùë™‚Ä≤ùüè\nùë®ùíé‚Ä≤ ‚Üíùë™‚Ä≤ùíé\nFormal Verification\nEngine (JG)\nPASS\nFAIL\nLLM Model\nFine-tuning\nùë®ùíè‚Üíùë™ùíè\nùë©ùüê‚Üíùë´ùíè\nùë¨ùíè‚Üíùë≠ùíè\nùëÆùíè‚ÜíùëØùíè\nn Train Examples\n1\n2\n3\n4\nFig. 8: Our evaluation framework for AssertionLLM.\nOur analysis emphasizes that there is a considerable scope to\nenhance LLMs for assertion generation. There are two different\nways ‚Äì (i) enhance ICL with diverse ICL examples or (ii) de-\nvelop an LLM specifically for assertion generation. Using this\ninsight, we develop AssertionLLM as detailed in Section VI.\nVI. ASSERTIONLLM: LLM FOR ASSERTION GENERATION\nAssertionLLM is an LLM specifically developed for the task\nof assertion generation. This is inline with the recent findings\nin other domains, e.g., financial analysis, where a task-specific\nLLM has excelled for downstream tasks as compared to a\nfoundational LLM. We use two different LLM foundational\nmodels ‚Äì (i) CodeLLaMa 2 and (ii) LLaMa3-70B and finetune\neach of them using a large amount of data where each data point\nconsists of a Verilog design and its formally verified assertions.\nFigure 8 shows the end-to-end flow to finetune LLM\nfor assertion generation and how we use the finetuned LLM\nto evaluate its goodness for assertion generation. Compared\nto Figure 4, we have removed the syntax corrector block\n( 3 in Figure 4), and instead of using foundational LLM ( 1\nin Figure 4), we are using the fined-tuned LLM model for ICL\n( 2 and 3 in Figure 8).\nWe use the same compute platform and and hyperparameters\nas detailed in Section IV for finetuning. Additionally, we use\n20 epochs to finetune each foundational LLM. We split the\nAssertionBench and use 75% of data for training and the\nremaining 25% for testing the goodness of the tuned LLM.\nVII. EXPERIMENTAL RESULTS ON ASSERTIONLLM\nFigure 9 shows our experimental results on assertion gen-\neration task using finetuned CodeLLaMa 2 and LLaMa3-70B.\nWe compare these results to that of results in Figure 6.\nObservation 5: Finetuning LLMs considerably improves\nfraction of correct assertions: We observe that the finetuned\n\n\nCodeLLaMa 2 increased proven assertions by 29% and 38%\nfor 1-shot and 5-shot ICL, decreased assertions generating\nCEX by 48% and 33% for 1-shot and 5-shot ICL, respectively\n(c.f., Figure 6c and Figure 9a). With respect to LLaMa3-70B,\nfine tuning has increased proven assertions by 24% for 5-shot\nlearning (c.f., Figure 6d and Figure 9b). However, for 1-shot\nICL, the fraction of proven assertions has decreased by 4.7%\nand has increased the fraction of assertions generating CEX by\n5.4% and 12% for 1-shot and 5-shot ICL, respectively. Further\nanalysis shows that as the foundational CodeLLaMa 2 was\ntrained on large corpora of codes, it learned assertion syntax\nand semantics better during finetuning. In contrast, foundational\nLLaMa3-70B training on general text corpora struggles to\nlearn assertion syntax and semantics during finetuning. This\nexperiment shows that it is crucial to select appropriate\nfoundational LLM and dataset for an effective fine-tuned\nLLM for assertion generation.\nObservation 6: Fine-tuning LLMs does not necessarily guar-\nantee syntactic error-free assertions: Fine-tuning LLMs does\nnot necessarily nullify the fraction of syntactically erroneous\nassertions. Figure 9 shows that both finetuned CodeLLaMa 2\nand LLaMa3-70B generate a considerable fraction (upto 38%)\nof syntactically erroneous assertions. In order to reduce the\nfraction of erroneous assertions, we speculate that we will\nrequire a more comprehensive dataset with a sufficient\nnumber of examples for the diverse syntax of assertions.\nVIII. RELATED WORK\nAutomatic generation of assertions in hardware has been\nan active area of research for the past decade. IODINE is\none of the earliest works for hardware assertion generation by\nanalyzing dynamic program behavior with respect to a set of\nstandard property templates [5]. Prior works have used static\nanalysis [6], [7], dynamic simulation execution data [8]‚Äì[10],\nand data-driven statistical analysis guided by the lightweight\nstatic analysis of design source code [11], [30] for assertion\ngeneration. Following GOLDMINE, researchers have developed\na wide variety of assertion generation techniques targeting\nhardware design functionality [12]‚Äì[14], [31]‚Äì[33] and hard-\nware design security [34], [35], and to evaluate the quality of\nnumerous assertions that automatic methods generate to aid\nthe downstream verification tasks [14], [33], [36]. However, all\nthese works suffer from following shortcomings ‚Äì they (i)\ndo not scale for industrial-scale designs, (ii) require a massive\namount of trace data to generate assertions, (iii) generate\nnumerous redundant design properties without any explanation\non their usability for downstream verification tasks, (iv) fail to\ngeneralize the properties beyond what is seen in the trace, and\n(v) encompass an extremely small subset of SVA, limiting the\nexpressibility and richness of the generated assertions.\nRecently,\nmassive\nsuccess\nof LLMs,\ne.g.,\nGPT\n[21],\nLLaMa [37], Gemini [38], in diverse scientific, engineering,\nand medical applications have led researchers to investigate\napplication of LLMs for hardware property generation [39]‚Äì\n[44]. However, almost all recent works on property generation\nusing LLMs suffer from the following shortcomings ‚Äì they (i)\nAccuracy\n0.0\n0.2\n0.4\n0.6\n1-shot\n5-shot\nPass\nCEX\nError\n(a)\nAccuracy\n0.0\n0.2\n0.4\n0.6\n1-shot\n5-shot\nPass\nCEX\nError\n(b)\nFig. 9: Comparison of accuracy of generated assertions. (a)\nAssertion accuracy comparison for finetuned CodeLLaMa 2.\n(b) Assertion accuracy comparison for finetuned LLaMa3-70B.\nCEX: Counter Example trace.\nrequire considerable human efforts and deep understanding of\nthe target hardware designs to devise hand-crafted prompts to\ngenerate and refine hardware properties, (ii) do not generalize\nthe assertions, and (iii) do not consider execution traces,\nrisking potentially missing subtle incorrect design behaviours or\nsecurity vulnerabilities that are not obvious in the design source\ncode. In fact, there is a lack of a systematic study comparing the\neffectiveness of different commercial and open-source LLMs\nin generating valid assertions from hardware design source\ncode. AssertionBench aims to fill in the gap and provides novel\ninsights for future research on LLMs for assertion generation.\nIX. LIMITATIONS\n‚Ä¢ Dataset: In this study, we focus on Verilog designs, given its\npredominance in hardware design language. Moving forward,\nit will be intriguing to develop benchmarks for assertions in\nother HDLs, e.g., VHDL, SystemC, to expand the scope of our\nanalysis to broader design paradigms.\n‚Ä¢ Modeling: In this paper, we assessed the assertion generation\ncapabilities of k-shot and finetuned SOTA LLMs. There is\na considerable scope for improvement in terms of assertion\nquality and correctness. Future work should focus on modeling\nto capture design coverage of generated assertions and quantify\ntheir goodness in terms of captured design behavior.\n‚Ä¢ Evaluation: In future work, it will be valuable to conduct\na more detailed evaluation of model errors to better understand\nthe specific limitations of each LLM for assertion generation.\nX. CONCLUSION AND FUTURE WORK\nThis work introduces AssertionBench to evaluate the current\nand future commercial and open-source LLMs for the assertion\ngeneration task and AssertionLLM to fully automate assertion\ngeneration using generative AI without the designer‚Äôs itera-\ntive intervention. Although there is no LLM that consistently\noutperforms other LLMs, we notice several promising trends\nand research directions such as (i) to quantify the goodness\nof assertion in terms of captured design behavior, (ii) to\nquantify the design coverage of the assertions, (iii) to model and\ncapture likely design security vulnerabilities as assertions, and\n(iv) going beyond temporal/sequential assertions to generate\nassertions encompassing richer set of SVA, to enhance the\npractical applicability of LLMs for assertion generation task for\nindustrial-scale designs. Pursuing these directions will further\naccelerate SoC and hardware design verification.\n\n\nREFERENCES\n[1] Hasini Witharana, Yangdi Lyu, Subodha Charles, and Prabhat Mishra. A\nSurvey on Assertion-based Hardware Verification. ACM Comput. Surv.\n(CS), 2022.\n[2] Saddek Bensalem, Yassine Lakhnech, and Hassen Saidi.\nPowerful\nTechniques for The Automatic Generation of Invariants. Int‚Äôl Conf. on\nComputer-Aided Verification (CAV), 1996.\n[3] A. Tiwari, H. Rue√ü, H. Sa¬®ƒ±di, and N. Shankar. A Technique for Invariant\nGeneration. Int‚Äôl Conf. on Tools and Algorithms for the Construction and\nAnalysis of Systems (TACAS), 2001.\n[4] Corina S. PÀòasÀòareanu and Willem Visser. Verification of Java Programs\nUsing Symbolic Execution and Invariant Generation. Int‚Äôl SPIN Workshop\non Model Checking of Software (SPIN), 2004.\n[5] Sudheendra Hangal, Sridhar Narayanan, Naveen Chandra, and Sandeep\nChakravorty. IODINE: A Tool to Automatically Infer Dynamic Invariants\nfor Hardware Designs. Design Automation Conf. (DAC), 2005.\n[6] G. Pinter and I. Majzik. Automatic Generation of Executable Assertions\nfor Runtime Checking Temporal Requirements.\nIEEE Int‚Äôl Symp. on\nHigh-Assurance Systems Engineering (HASE), 2005.\n[7] A. Hekmatpour and A. Salehi.\nBlock-based Schema-driven Assertion\nGeneration for Functional Verification. Asian Test Symp. (ATS), 2005.\n[8] Andrew DeOrio, Adam B. Bauserman, Valeria Bertacco, and Beth C.\nIsaksen.\nInferno: Streamlining Verification With Inferred Semantics.\nIEEE Trans. on Computer-Aided Design of Integrated Circuits and\nSystems (TCAD), 2009.\n[9] Po-Hsien Chang and Li.-C Wang. Automatic Assertion Extraction via\nSequential Data Mining of Simulation Traces. Asia and South Pacific\nDesign Automation Conf. (ASP-DAC), 2010.\n[10] Chih-Neng Chung, Chia-Wei Chang, Kai-Hui Chang, and Sy-Yen Kuo.\nApplying Verification Intention for Design Customization via Property\nMining Under Constrained Testbenches. Int‚Äôl Conf. on Computer Design:\nVLSI in Computers and Processors, (ICCD), 2011.\n[11] GoldMine. GOLDMINE: An Automatic Assertion Generation Tool. http:\n//goldmine.csl.illinois.edu/, 2024. Accessed: March 3, 2025.\n[12] Mohammad Reza Heidari Iman, Gert Jervan, and Tara Ghasempouri.\nARTmine: Automatic Association Rule Mining with Temporal Behavior\nfor Hardware Verification.\nDesign, Automation, and Test in Europe\n(DATE), 2024.\n[13] Samuele Germiniani and Graziano Pravadelli.\nHARM: A Hint-Based\nAssertion Miner. IEEE Trans. on Computer-Aided Design of Integrated\nCircuits and Systems (TCAD), 2022.\n[14] Debjit Pal, Spencer Offenberger, and Shobha Vasudevan.\nAssertion\nRanking Using RTL Source Code Analysis. IEEE Trans. on Computer-\nAided Design of Integrated Circuits and Systems (TCAD), 2020.\n[15] Vaishnavi Pulavarthi, Deeksha Nandal, Soham Dan, and Debjit Pal.\nAssertionBench: A Benchmark to Evaluate Large-Language Models for\nAssertion Generation for Hardware Design. Conf. of the Nations of the\nAmericas Chapter of the Assoc. for Computational Linguistics (NAACL\nFindings), 2025.\n[16] Amir Pnueli.\nThe Temporal Logic of Programs.\nAnnual Symp. on\nFoundations of Computer Science (SFCS), 1977.\n[17] SystemVerilog. 1800-2017 - IEEE Standard for SystemVerilog‚ÄìUnified\nHardware Design, Specification, and Verification Language.\nhttps://\nieeexplore.ieee.org/document/8299595, 2024. Accessed: March 3, 2025.\n[18] Cadence.\nJasperGold Apps.\nhttps://www.cadence.com/en US/home/\ntools/system-design-and-verification/formal-and-static-verification/\njasper-gold-verification-platform.html, 2024. Accessed: March 3, 2025.\n[19] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N. Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. Attention\nis All You Need.\nInt‚Äôl Conference on Neural Information Processing\nSystems (NeurIPS), 2017.\n[20] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng,\nMing Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming\nZhou. CodeBERT: A Pre-Trained Model for Programming and Natural\nLanguages. Conf. on Empirical Methods in Natural Language Processing\n(EMNLP Findings), 2020.\n[21] OpenAI, Josh Achiam, and et al. GPT-4 Technical Report. arXiv, 2024.\n[22] Antonio\nMastropaolo,\nSimone\nScalabrino,\nNathan\nCooper,\nDavid\nNader Palacio, Denys Poshyvanyk, Rocco Oliveto, and Gabriele Bavota.\nStudying the Usage of Text-To-Text Transfer Transformer to Support\nCode-Related Tasks. Int‚Äôl Conf. on Software Engineering (ICSE), 2021.\n[23] OpenCores. https://opencores.org/, 2024. Accessed: March 3, 2025.\n[24] CLOC. https://github.com/AlDanial/cloc. Accessed: March 3, 2025.\n[25] NCSA.\nNCSA\nDelta.\nhttps://www.ncsa.illinois.edu/research/\nproject-highlights/delta/, 2024. Accessed: March 3, 2025.\n[26] HuggingFace.\nModel Repository.\nhttps://huggingface.co/, 2024.\nAc-\ncessed: March 3, 2025.\n[27] Junjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao, Shichun Liu,\nYuhan Cui, Zeyang Zhou, Chao Gong, Yang Shen, Jie Zhou, Siming\nChen, Tao Gui, Qi Zhang, and Xuanjing Huang.\nA Comprehensive\nCapability Analysis of GPT-3 and GPT-3.5 Series Models. arXiv, 2023.\n[28] Baptiste Rozi`ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai\nGat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre,\nTal Remez, J¬¥er¬¥emy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna\nBitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan\nXiong, Alexandre D¬¥efossez, Jade Copet, Faisal Azhar, Hugo Touvron,\nLouis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve.\nCode Llama: Open Foundation Models for Code. 2024.\n[29] Meta.\nIntroducing Meta Llama 3: The most capable openly available\nLLM to date. https://ai.meta.com/blog/meta-llama-3/, 2024. Accessed:\nMarch 3, 2025.\n[30] Samuel Hertz, David Sheridan, and Shobha Vasudevan. Mining Hardware\nAssertions With Guidance From Static Analysis.\nIEEE Trans. on\nComputer-Aided Design of Integrated Circuits and Systems (TCAD),\n2013.\n[31] Samuel Hertz, Debjit Pal, Spencer Offenberger, and Shobha Vasudevan.\nA Figure of Merit for Assertions in Verification. Asia and South Pacific\nDesign Automation Conf. (ASP-DAC), 2019.\n[32] Samuele Germiniani and Graziano Pravadelli.\nExploiting Clustering\nand Decision-Tree Algorithms to Mine LTL Assertions Containing Non-\nboolean Expressions. IFIP/IEEE Int‚Äôl Conf. on Very Large Scale Inte-\ngration (VLSI-SoC), 2022.\n[33] Mohammad Reza Heidari Iman, Jaan Raik, Gert Jervan, and Tara\nGhasempouri. IMMizer: An Innovative Cost-Effective Method for Mini-\nmizing Assertion Sets. Euromicro Conference on Digital System Design\n(DSD), 2022.\n[34] Calvin Deutschbein, Andres Meza, Francesco Restuccia, Ryan Kastner,\nand Cynthia Sturton.\nIsadora: Automated Information Flow Property\nGeneration for Hardware Designs. Workshop on Attacks and Solutions in\nHardware Security (ASHES), 2021.\n[35] Hasini Witharana, Aruna Jayasena, Andrew Whigham, and Prabhat\nMishra. Automated Generation of Security Assertions for RTL Models.\nACM Journal on Emerging Technologies in Computing Systems (JETC),\n2023.\n[36] Avinash Ayalasomayajula, Nusrat Farzana, Debjit Pal, and Farimah\nFarahmandi.\nPrioritizing Information Flow Violations: Generation of\nRanked Security Assertions for Hardware Designs.\nIEEE Int‚Äôl Symp.\non Hardware Oriented Security and Trust (HOST), 2024.\n[37] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-\nAnne Lachaux, Timoth¬¥ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric\nHambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. LLaMA: Open and Efficient Foundation\nLanguage Models. arXiv, 2023.\n[38] Google Gemini.\nhttps://gemini.google.com/app.\nAccessed: March 3,\n2025.\n[39] Mingjie Liu, Nathaniel Pinckney, Brucek Khailany, and Haoxing Ren.\nVerilogEval: Evaluating Large Language Models for Verilog Code Gen-\neration. Int‚Äôl Conf. on Computer-Aided Design (ICCAD), 2023.\n[40] Marcelo Orenes-Vera, Margaret Martonosi, and David Wentzlaff. Using\nLLMs to Facilitate Formal Verification of RTL. arXiv, 2023.\n[41] Rahul Kande, Hammond Pearce, Benjamin Tan, Brendan Dolan-Gavitt,\nShailja Thakur, Ramesh Karri, and Jeyavijayan Rajendran. LLM-assisted\nGeneration of Hardware Assertions. arXiv, 2023.\n[42] Wenji Fang, Mengming Li, Min Li, Zhiyuan Yan, Shang Liu, Hongce\nZhang, and Zhiyao Xie. AssertLLM: Generating and Evaluating Hard-\nware Verification Assertions from Design Specifications via Multi-LLMs.\n2024 IEEE LLM Aided Design Workshop (LAD), 2024.\n[43] Bhabesh Mali, Karthik Maddala, Sweeya Reddy, Vatsal Gupta, Chandan\nKarfa, and Ramesh Karri.\nChIRAAG: ChatGPT Informed Rapid and\nAutomated Assertion Generation. IEEE Computer Society Annual Symp.\non VLSI (ISVLSI), 2024.\n[44] Kexin Pei, David Bieber, Kensen Shi, Charles Sutton, and Pengcheng\nYin. Can Large Language Models Reason About Program Invariants?\nInt‚Äôl Conf. on Machine Learning (ICML), 2023.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20633v1.pdf",
    "total_pages": 7,
    "title": "Are LLMs Ready for Practical Adoption for Assertion Generation?",
    "authors": [
      "Vaishnavi Pulavarthi",
      "Deeksha Nandal",
      "Soham Dan",
      "Debjit Pal"
    ],
    "abstract": "Assertions have been the de facto collateral for simulation-based and formal\nverification of hardware designs for over a decade. The quality of hardware\nverification, i.e., detection and diagnosis of corner-case design bugs, is\ncritically dependent on the quality of the assertions. With the onset of\ngenerative AI such as Transformers and Large-Language Models (LLMs), there has\nbeen a renewed interest in developing novel, effective, and scalable techniques\nof generating functional and security assertions from design source code. While\nthere have been recent works that use commercial-of-the-shelf (COTS) LLMs for\nassertion generation, there is no comprehensive study in quantifying the\neffectiveness of LLMs in generating syntactically and semantically correct\nassertions. In this paper, we first discuss AssertionBench from our prior work,\na comprehensive set of designs and assertions to quantify the goodness of a\nbroad spectrum of COTS LLMs for the task of assertion generations from hardware\ndesign source code. Our key insight was that COTS LLMs are not yet ready for\nprime-time adoption for assertion generation as they generate a considerable\nfraction of syntactically and semantically incorrect assertions. Motivated by\nthe insight, we propose AssertionLLM, a first of its kind LLM model,\nspecifically fine-tuned for assertion generation. Our initial experimental\nresults show that AssertionLLM considerably improves the semantic and syntactic\ncorrectness of the generated assertions over COTS LLMs.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}