{
  "id": "arxiv_2502.21271v1",
  "text": "Adaptive Keyframe Sampling for Long Video Understanding\nXi Tang1*, Jihao Qiu1*, Lingxi Xie, Yunjie Tian2, Jianbin Jiao1, Qixiang Ye1\n1University of Chinese Academy of Sciences\n2University at Buffalo, SUNY\n{tangxi19,qiujihao19}@mails.ucas.ac.cn\n{jiaojb,qxye}@ucas.ac.cn\nAbstract\nMultimodal large language models (MLLMs) have enabled\nopen-world visual understanding by injecting visual input\nas extra tokens into large language models (LLMs) as con-\ntexts. However, when the visual input changes from a sin-\ngle image to a long video, the above paradigm encoun-\nters difficulty because the vast amount of video tokens has\nsignificantly exceeded the maximal capacity of MLLMs.\nTherefore, existing video-based MLLMs are mostly estab-\nlished upon sampling a small portion of tokens from in-\nput data, which can cause key information to be lost and\nthus produce incorrect answers.\nThis paper presents a\nsimple yet effective algorithm named Adaptive Keyframe\nSampling (AKS). It inserts a plug-and-play module known\nas keyframe selection, which aims to maximize the use-\nful information with a fixed number of video tokens. We\nformulate keyframe selection as an optimization involving\n(1) the relevance between the keyframes and the prompt,\nand (2) the coverage of the keyframes over the video,\nand present an adaptive algorithm to approximate the best\nsolution.\nExperiments on two long video understanding\nbenchmarks validate that AKS improves video QA accu-\nracy (beyond strong baselines) upon selecting informative\nkeyframes. Our study reveals the importance of information\npre-filtering in video-based MLLMs.Our codes are avail-\nable at https://github.com/ncTimTang/AKS.\n1. Introduction\n“You can’t manage what you can’t measure.”\n— Peter Drucker\nRecent years have witnessed a rapid development of mul-\ntimodal large language models (MLLMs) [14, 22, 39, 53]\nfor open-world visual understanding. Among the large cor-\npus of research in MLLMs, a straightforward and important\ndirection is to generalize them to video data. Compared to\nstill images, videos contain richer and more complex visual\n*Equal contribution.\nA: The panda is eating food.\nA: The panda is rolling down.\nQ: What does the panda do in the video?\nUniform Sampling\nAdaptive Keyframe \nSampling\nMLLM\nMLLM\nFigure 1.\nThe accuracy of video-based MLLMs heavily relies\non the quality of keyframes. The above example shows a long\nvideo from VideoMME [10] where keyframes are marked with\ngreen stars. The same MLLM (i.e., LLaVA-Video [55]) is used\nfor answering the question. Uniform sampling (the default set-\nting in [55]) finds irrelevant frames (the MLLM mostly performs\na random guess), while our algorithm (AKS) finds keyframes and\nproduces the correct answer.\ncontent, thus raising serious challenges to MLLMs includ-\ning key information retrieval, summarization, logical infer-\nence, etc. Many benchmarks [10, 43] have been established\nto evaluate MLLMs for video understanding.\nA typical framework of image-based MLLMs involves\nencoding the input image into a set of visual tokens and\nfeeding them as the context of LLMs. When this frame-\nwork was transplanted to videos, especially long videos, a\ndifficulty arose from the limited capacity of MLLMs, i.e.,\nthe maximal number of visual tokens that MLLMs can pro-\ncess is much fewer than that of an entire video; in other\nwords, not all video tokens can be perceived by MLLMs.\nTo bridge the gap, recent approaches [20, 44] often sam-\n1\narXiv:2502.21271v1  [cs.CV]  28 Feb 2025\n\n\npled a small portion of frames from the input video; conse-\nquently, the performance of these MLLMs heavily relies on\nthe quality of selected frames (i.e., keyframes). Despite its\nimportance, the keyframe selection algorithm has not been\ncarefully designed, e.g., LLaVA-Video [55] simply applied\na uniform sampling strategy which, as shown in Figure 1,\nis prone to losing important information and thus leads to\nincorrect outputs of video understanding.\nThis paper presents a systematic study on keyframe se-\nlection and reveals its importance to video understanding\nand beyond. We formulate keyframe selection as a plug-\nand-play module before the MLLM’s visual encoder; its\ngoal is to maximize the usefulness of the keyframes in\nvideo understanding. Intuitively, we propose two key as-\npects to be considered, namely, (1) relevance (i.e., how the\nkeyframes are related to the question) and (2) coverage (i.e.,\nhow the keyframe set covers the useful information in the\nentire video). Specifically, we quantify the target by (1)\ncomputing relevance between each candidate frame and the\nprompt using a vision-language (VL) model, and (2) esti-\nmating coverage by recursively partitioning the video into\nbins and counting the number of keyframes within each bin.\nWe show that maximizing relevance and coverage alone\nproduces simple baselines for keyframe selection, while a\nproper tradeoff between them, obtained by the proposed\nAdaptive Keyframe Sampling (AKS) algorithm, leads to the\nbest practice of video understanding.\nWe evaluate our approach on LongVideoBench [43]\nand VideoMME [10], two benchmarks for long video un-\nderstanding.\nWe investigate three frame-based MLLMs\n(Qwen2VL [41], LLaVA-OV [15], and LLaVA-Video [55])\nas the baseline and insert AKS as an off-the-shelf module to\nimprove the quality of keyframes. Our approach achieves\nconsistent accuracy gain throughout all tests. Specifically,\nwhen AKS is integrated with LLaVA-Video-7B, we set new\nrecords on these two benchmarks with 7B models. We fur-\nther validate that the improvement owes to higher-quality\nkeyframes found by AKS, demonstrating that MLLMs be-\ncome stronger with more informative visual contexts. Our\nstudy reveals that pre-filtering visual data is crucial and will\nbe a long-lasting research topic for MLLMs in perceiving\nhigh-dimensional data, e.g., long videos, and even 4D data.\n2. Related Work\nLarge language models (LLMs) and multimodal LLMs\n(MLLMs). LLMs [3–6, 8, 36, 40, 48, 52] have marked a\nnew era in AI, showcasing significant potential in unifying\nvarious tasks covering language understanding and genera-\ntion. To extend LLMs for visual understanding, the com-\nmunity has focused on aligning visual and language data\nwithin a unified feature space [32]. There are generally two\ntypes of approaches, (1) internal adaptation, such as [1],\nthat integrates cross-attention mechanisms within LLMs to\nachieve vision-language alignment, and (2) external adap-\ntation, such as [7, 17, 22], that trains additional modules for\nthe same purpose. Consequently, vision foundation mod-\nels [9, 13, 24, 32, 37, 38, 54] have evolved into multimodal\nLLMs (MLLMs) [14, 22, 39, 53], enabling them to perform\nlanguage-guided visual understanding tasks.\nVideo-based MLLMs.\nResearchers have extended\nMLLMs to video understanding.\nEarly efforts in this\narea include VideoChat [18], Video-ChatGPT [26], Video-\nLLaMA [50], Video-LLaVA [20], LanguageBind [56], and\nValley [25], etc. Different from still images, videos contain\nrich content that, when encoded as visual tokens, exceed the\nmaximal context capacity of MLLMs. Most of the above\nmethods have sampled video frames to fit MLLMs; some\nof them, such as Video-ChatGPT [26], introduced more ef-\nficient video features. There are also studies on instance-\nlevel video understanding have been proposed, such as\nLEGO [19] for moment retrieval, PG-Video-LLaVA [27]\nfor video grounding, and Artemis [31] for video referring,\nenriching the corpus of video understanding.\nMLLMs for Long Video Understanding. Going one step\nfurther, long video understanding faces greater challenges\ndue to the increased difficulty of keyframe selection, lead-\ning to significant loss of critical information. While some\nMLLMs (e.g., Kangaroo [23] and LLaVA-Video [55]) uti-\nlize language models with larger context capacities to al-\nlow more frames to be encoded and processed, many others\nhave designed specific strategies to mitigate this issue. For\nexample, MovieChat [34] employed both short-term and\nlong-term memory banks to compress and preserve video\ncontent. Similarly, MA-LMM [11] VideoStreaming [30]\nused a Q-former and a small language model (phi-2 [12]),\nto condense video data, while LongVLM [42] adopted to-\nken merging to decrease the number of video tokens. Gold-\nfish [2] integrated short video understanding with informa-\ntion retrieval to answer complex queries. In summary, these\napproaches aim to reduce the number of video tokens, but\nthere is often no guarantee that key information in the video\ncan be preserved. This work presents a simple yet effective\nalgorithm that maximally preserves important information\nfor long video understanding.\n3. Method\n3.1. Preliminaries\nWithin a broad range of video understanding tasks, the\nmodel receives a video clip and a text instruction as input\nand is required to output a text answer. Without loss of gen-\nerality, we denote the video as V ∈RT ×W ×H×C, where T\nis the number of frames and W, H, and C denote the width,\nheight, and number of channels, respectively. We consider\neach frame an image Vt (t ∈{1, 2, . . . , T}) and apply a\npre-trained encoder (e.g., the CLIP ViT-L model [32]) to\n2\n\n\nImage-Text Matching\nTime (s)\nSampled Frames\nUser Query\nMultimodal Large Language Model\nResponse\nA: The sun is seen four times in this video.\nJudge & Split\nAdaptive Keyframe Sampling\nSample Frames\nLinear Projection\nVisual Encoder\nJudge & Split\nMatching Score\nUser Query\nInput Video\nMatching Score\nJudge & Split\nQ: How many times is the \nsun visible in the video?\nFigure 2. The overall framework of our approach. We insert a plug-and-play module, Adaptive Keyframe Sampling (AKS, marked in\ngreen frames) into the MLLM to improve the quality of sampled keyframes. Each red dot indicates a prompt-frame matching score (i.e.,\ns(Q, Ft), see Section 3.2). AKS follows a recursive, judge-and-split optimization for keyframe selection (see Section 3.3).\nextract a set of visual tokens Ft from it. The text instruc-\ntion (a.k.a., prompt) is denoted as Q.\nThe overall pipeline of our algorithm is illustrated in Fig-\nure 2. We use a regular MLLM that addresses video under-\nstanding with the template [ User: ⟨video-tokens⟩⟨text-\ninstruction⟩Assistant: ], where the video tokens and text\ninstruction are projected into the same feature space using\nan MLP. For simplicity, we denote the MLLM as a function\nof G({Ft}) where we omit the LLM part and only focus\non the visual tokens as contexts. With a limited capacity\nof visual contexts (i.e., the number of video tokens cannot\nexceed a specific value), the above pipeline encounters diffi-\nculty in dealing with long videos where not all video content\ncan be perceived by the MLLM.\nA straightforward solution is to select keyframes from\nthe input video for token extraction. In other words, the goal\nis to design a selection function KSM(Q, F) that outputs an\nindex set, I ⊆{1, 2, . . . , T} and |I| = M, indicating the\nM best keyframes (M is pre-defined according to the con-\ntext capacity of the MLLM). Video tokens extracted from\nthe keyframes (i.e., {Ft | t ∈KSM(Q, F)} compose of\nthe context of the MLLM. As shown in Figure 1, the qual-\nity of keyframe selection is crucial for video understanding,\nbut the function KSM(Q, F) has not been well studied in\nthe community. As an example, a recent MLLM for video\nunderstanding [55] simply performed uniform sampling for\nkeyframe selection; with the function KSM(·) not using Q\nand F at all, it cannot guarantee to find useful information\nfor question answering.\nIn what follows, we establish two principles of keyframe\nselection (Section 3.2), after which we will present AKS,\nour optimization algorithm (Section 3.3).\n3.2. Principles of Keyframe Selection\nThe keyframe selection function KSM(Q, F) is to maxi-\nmize the amount of useful information, i.e.,\nKSM(Q, F) = arg max\n|I|=M G′({Ft | t ∈I}).\n(1)\nHere, we assume G′(·) to be a complementary function of\nG(·), indicating the MLLM’s confidence about its output.\nEqn (1) is mathematically intractable due to two reasons.\nFirst, the optimization involves exponentially many candi-\ndates of I. Second and more importantly, the function G′(·)\nis difficult to estimate because there is no supervision for\nkeyframe selection – even when a training set is available\nand one can compare the output of G(·) with the ground-\ntruth answer, it is not guaranteed that a correct answer cor-\nresponds to a perfect set of keyframes, and vice versa.\nWe propose a heuristic method to approximate Eqn (1).\nIntuitively, a set of keyframes is informative when the fol-\nlowing conditions are satisfied. (1) The relevance between\neach frame and the prompt is high, i.e., the visual data is\n3\n\n\nQ: How many outfits did the male protagonist change in total in the video?\nA: The male protagonist changed 5 outfits in total in the video.\nSampled \nFrames\n47\n57\n67\n77\n0.6\n0.3\n82\n92\n0.6\n0.3\n23\n33\n43\n53\n0.6\n0.3\n71\n81\n0.6\n0.3\n0\n10\n20\n30\n0.6\n0.3\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n0.6\n0.3\nSplit\nSample\nSample\nSample\nSample\nSample\nJudge & Split\n47\n57\n67\n77\n87\n98\n0.6\n0.3\nSplit\n0\n10\n20\n30\n40\n50\n0.6\n0.3\nSplit\n71\n81\n91\n101\n0.6\n0.3\nSplit\nLevel 0\nLevel 1\nLevel 2\nLevel 3\nFigure 3. An example of adaptive sampling (ADA). 8 keyframes are to be selected from the input video. Each red dot indicates a prompt-\nframe matching score, s(Q, Ft). At Level-0 and Level-1, all bins are split into two sub-bins; at Level-2, only the rightmost bin is further\npartitioned while the top-2 scores are sampled from the other three bins. Level-3 has reached the maximal depth.\nuseful for question answering. (2) The coverage of the se-\nlected frames is sufficient to comprehensively answer the\nquestion. Note that the coverage is difficult to quantify, and\nthe second principle is closely related to preventing redun-\ndant frames (e.g., neighboring frames with almost the same\nvisual content) from being selected, because (when the size\nof |I| is fixed) they can potentially reduce the amount of\nother useful information and thus harm the coverage of the\nentire keyframe set.\nFollowing the analysis above, we reformulate the right-\nhand side of Eqn (1), yielding:\nKSM(Q, F) = arg max\n|I|=M\nX\nt∈I\ns(Q, Ft) + λ · c(I).\n(2)\nHere we introduce two quantities, r(Q, Ft) as the relation-\nship between the prompt Q and the t-th frame Ft, and c(I)\nas the coverage of the entire keyframe set over the time axis.\nλ is the balancing hyper-parameter.\nComputing s(Q, Ft).\nThis involves a vision-language\n(VL) module to measure whether Ft contains information\nfor answering Q. Although the target MLLM itself G(·)\ncan play the role, its high computational cost can bring a\nmajor burden. In practice, we choose a cheaper VL model\n(e.g., CLIP [32] or BLIP ITM [16]) for replacement.\nEstimating c(I). Measuring coverage is an open problem\nwhich is related to the homogeneity of data distribution. In\nmathematics, Ripley’s K-function [33] is a popular way to\nmeasure homogeneity. Given the timestamp set I and any\nsearch radius r < T, the K-function of r, denoted as ˆK(r),\nis proportional to the number of (ti, tj) pairs satisfying |ti−\ntj| < r. The distribution of I is considered homogeneous\n(i.e., covering the entire time axis) if ˆK(r) is approximately\nproportional to r2.\nTo adapt K-function to computing coverage (closely re-\nlated but a bit different from homogeneity) as well as reduc-\ning computational overhead, we introduce bins with width\nr and approximate I(|ti −tj| < r) as whether ti and tj fall\ninto the same bin. We perform a recursive partition. At the\nfirst level, we set 2 bins with the bin width being T/2, i.e.,\nthe time axis [0, T) is partitioned into 2 non-overlapping\nbins, [0, T/2) and [T/2, T). With the numbers of keyframes\nfalling within these bins being m1 and m2, c(I) adds a\npenalty term |m1 −m2| since an uneven distribution im-\nplies weak coverage in the bin with fewer keyframes. At the\nsecond level, each of [0, T/2) and [T/2, T) is further par-\ntitioned into two bins, and the same calculation continues.\nThe recursion stops at the L-th level, where L ⩽⌈log2 M⌉\nis a hyper-parameter.\n3.3. Adaptive Keyframe Sampling\nWith the complex definition of c(I), it is difficult to find\na closed-form or accurate optimization for Eqn (2). This\npart discusses an approximation. Compared to the baseline\nthat only relies on s(Q, Ft) scores, we name such meth-\nods timestamp-aware optimization for its ability to consider\ntimestamps for better keyframe selection results.\n4\n\n\nWe first discuss two special cases. (1) When λ = 0 (i.e.,\ncoverage is neglected), Eqn (2) is solved by simply selecting\nthe top-M frames of the largest scores. We name this strat-\negy TOP, short for ‘top sampling’; as shown in Figure 5, in\nsome cases, it results in all keyframes being located within a\nsmall range of time and the MLLM missing important infor-\nmation in other moments. (2) When λ →+∞(i.e., cover-\nage is strictly guaranteed), Eqn (2) is solved by selecting the\nframe within the highest score in each bin as the keyframe\n(when the number of bins exceeds M, the champion frames\nwith the highest scores are preserved). We name this strat-\negy BIN, short for ‘binned sampling’. This situation fur-\nther degenerates to the uniform sampling baseline [55] if\na dummy VL model is used for scoring (i.e., s(Q, Ft) is\na constant over t). We name this strategy UNI, short for\n‘uniform sampling’.\nIn other cases (0 < λ ≪+∞), we adopt a hierarchical\noptimization method that follows the definition of c(I). At\nthe first level, we determine how to allocate M keyframes\ninto two bins, [0, T/2) and [T/2, T). We recall the rele-\nvance scores of all frames, s(Q, Ft), and compute the av-\nerage scores over all frames (denoted as sall) and over M\nframes with the highest scores (denoted as stop). If there is\nonly one keyframe to be selected, or stop −sall surpasses a\nthreshold, sthr, we believe that it is important to guarantee\nthe top-scored frames to be sampled (i.e., maximizing the\nfirst term of Eqn (2)), so the algorithm directly returns the\ntop-M frames as keyframes. Otherwise, we split the current\nbin into two sub-bins with the number of keyframes evenly\nallocated (i.e., maximizing the second term of Eqn (2)), and\nthen recursively call the above programs in the sub-bins.\nWe name this strategy ADA, short for ‘adaptive sampling’.\nNote that the hyper-parameter λ is not explicitly tuned; its\nrole is replaced by sthr.\nFigure 3 uses an example to show how adaptive sampling\n(ADA) works. ADA is a compromise between the special\ncases, TOP and BIN. As we shall see in experiments (see\nSection 4.4), ADA absorbs the advantages of TOP and BIN\nand achieves the best practice of video understanding.\n4. Experiments\n4.1. Experimental Setup and Details\nDataset and evaluation. We utilize the popular LMMs-\nEval [51] to evaluate the performance of AKS. We\nuse two popular benchmarks LongVideoBench [43], and\nVideoMME [10], for long video understanding. The length\nof videos in this dataset can exceed one hour, so the qual-\nity of keyframe selection plays a crucial role in visual un-\nderstanding. We establish AKS beyond three recent video-\nbased MLLMs (see the next paragraph). We do not tune\nthe parameters of these MLLMs, but only change the input\nframes into those selected by AKS. To highlight the impor-\nTable 1. Video-based question answering accuracy (%) of different\napproaches on LongVideoBench (LVB) val and VideoMME (V-\nMME). AKS is applied upon three baseline approaches. Frames\nand LLM indicate the number of video frames fed into the MLLM\nand the number of parameters in the LLM part, respectively.\nMethod\nFrames\nLLM\nLVB val\nV-MME\nGPT-4V [28]\n256\n–\n61.3\n59.9\nGPT-4o [29]\n256\n–\n66.7\n71.9\nGemini-1.5-Flash [35]\n256\n–\n61.6\n70.3\nGemini-1.5-Pro [35]\n256\n–\n64.0\n75.0\nVideoLLaVA [20]\n8\n7B\n39.1\n39.9\nMiniCPM-V 2.6 [46]\n64\n8B\n54.9\n60.9\nPLLaVA [44]\n32\n34B\n53.2\n-\nVILA [21]\n-\n40B\n-\n60.1\nQwen2-VL [41]\n32\n7B\n55.5\n57.6\nQwen2-VL w/ AKS\n32\n7B\n60.5\n59.9\nLLaVA-OV [15]\n32\n7B\n54.8\n56.5\nLLaVA-OV w/ AKS\n32\n7B\n59.3\n58.4\nLLaVA-Video [55]\n64\n7B\n58.9\n64.4\nLLaVA-Video w/ AKS\n64\n7B\n62.7\n65.3\ntance of keyframe selection, we do not use video subtitles\nto assist question answering. This setting also allows us to\nweaken the impact of the LLM’s strength and maximally\nfocus on visual understanding.\nImplementation details. We investigate three video-based\nMLLMs as our baseline, namely, Qwen2VL [41], LLaVA-\nOV [15], and LLaVA-Video [55]. LongVideoBench and\nVideoMME contain multi-choice questions; to answer these\nquestions, these MLLMs followed a similar prompt involv-\ning the question (in text), video frames (as tokens), and\noptions (in text).\nSpecifically, as the strongest baseline,\nLLaVA-Video used SigLIP [49] as its vision encoder and\nQwen2-7B [45] as its large language model. With capaci-\nties to process up to 32 or 64 video frames, these MLLMs\noffer basic abilities of video understanding, but they were\nbuilt upon uniformly sampled keyframes and can miss im-\nportant information.\nTo reduce computational costs, we sample the candidate\nframes from the raw video at 1 frame per second.\nThe\nprompt (in text) and each t-th frame (as image) are fed into\nthe text and visual encoders of BLIP [16] to obtain Q and\nFt, after which s(Q, Ft) is computed via image-text match-\ning (ITM), i.e., the similarity between Q and Ft. One can\nalso replace BLIP with other vision-language models (e.g.,\nCLIP [32]); see the ablation in Section 4.4. We use ADA\nsampling unless otherwise specified.\n4.2. Comparison to the State-of-the-Art\nQuantitative results.\nWe first compare the accuracy\nof video question answering between our approach and\n5\n\n\nQ: After entering the museum, where did the short-haired woman in\n     a black coat, black long skirt, and black mask visit first? \nQ: A white-lettered title says 'How much data do we need?’. The word\n    'dog' appears on the far right, When the image of a shaking dog\n    appears, what changes occur in the black scene?\nQ: Two men wearing straw hats and grey clothes stand in a grass field\n     holding long knives. What does the house behind them look like?\nA: Wooden house.\nLLaVA-Video-7B w/ AKS\nLLaVA-Video-7B  \nA: Straw hut.\nQ: How long does it take to drive from the Earth to the Moon\n     according to the video?\nA: Bronze statues.\nQwen2-VL-7B w/ AKS\nQwen2-VL-7B  \nA: Sculptures.\nA: 19 days.\nQwen2-VL-7B w/ AKS\nQwen2-VL-7B  \nA: 160 days.\nQ: What are the moves in the last scene of this dance?\nA: The image of the shaking \ndog gradually enlarges.\nLLaVA-OV-7B w/ AKS\nLLaVA-OV-7B  \nA: The arrow on the right \nside is labeled 'not dog'.\nA: Passe and then chasse.\nLLaVA-OV-7B w/ AKS\nLLaVA-OV-7B  \nA: Kneel down on one \nknee and lean back.\nQ: After entering the museum, where did the short-haired woman\n     in a black coat, black long skirt, and black mask visit first? \nA: Climbing on her.\nLLaVA-Video-7B w/ AKS\nLLaVA-Video-7B  \nA: Pulling her hair.\nFigure 4. AKS improves the baseline MLLMs for video understanding. The left three examples come from LongVideoBench while the\nright three come from VideoMME. Green stars indicate keyframes selected by AKS (note that 64 keyframes are selected for each video).\nsome recent MLLMs.\nResults are summarized in Ta-\nble 1.\nAKS brings consistent accuracy gain over three\nbaselines, e.g., upon Qwen2VL, the improvement is 5.0%\non LongVideoBench and 2.3% on VideoMME; even upon\nLLaVA-Video, the strongest baseline, these numbers are\n3.8% and 0.9%, respectively. These improvements not only\nmake our method surpass other competitors with a simi-\nlar computational complexity (i.e., input no more than 64\nframes, LLM no larger than 7B), but also allow it to achieve\nhigher levels set by larger models (e.g., with AKS, LLaVA-\nVideo-7B reports 62.7% on LongVideoBench, which is\n0.8% higher than the LLaVA-Video-72B model without\nAKS, and 1.4% and 1.1% higher than GPT-4V and Gemini-\n1.5-Flash, two proprietary models using 256 input frames).\nQualitative results. In Figure 4, we display representa-\ntive video understanding results of AKS (based on LLaVA-\nVideo-7B) and others.\nOne can see that the selected\nkeyframes are closely related to the question; this allows the\nMLLM, with a limited capacity of context, to get a com-\nprehensive view of question-related content and thus ob-\ntain the correct answer. As a side comment, we find that\nVideoMME contains many questions that require a high-\nlevel comprehension of the video content in which uniform\nsampling is a safe choice; nevertheless, AKS still finds more\ninformative frames and improves the accuracy, although the\ngain is smaller than that on LongVideoBench. Please also\nTable 2. Video-based question answering accuracy (%) of different\nsampling strategies. LLaVA-Video-7B with AKS is tested. Please\nrefer to Section 3.3 for the explanations of these abbreviations and\nSection 4.3 for the analysis of results.\nSampling\nLongVideoBench val\nVideoMME\nUNI\n58.9\n64.4\nTOP\n62.4\n63.7\nBIN\n60.2\n65.2\nADA\n62.7\n65.3\nsee the appendix for more examples.\n4.3. Diagnostic on Keyframe Selection\nThis part aims to diagnose how AKS works and ablates de-\nsign choices of AKS. We build our test upon the strongest\nbaseline, LLaVA-Video.\nMLLMs benefit from better keyframes. To show how\nkeyframe selection impacts video understanding, we test\ndifferent strategies described in Section 3.3. Table 2 lists\nthe results. Beyond the baseline (i.e., UNI sampling), ADA\nsampling (our default choice in Section 4.2) achieves the\nbest practice, while each of TOP and BIN samplings is bet-\nter than the other in one benchmark. Note that the MLLM\n(i.e., LLaVA-Video-7B) remains unchanged throughout all\nthese tests. In other words, all the improvements owe to\n6\n\n\nInput \nVideo\nUNI\nTOP\nAKS\nA: He closed the door.\nAKS Split\nBIN\n[No key frames]\n[4 key frames]\nQ: What is this man\n    wearing a black coat \n    doing?\nQ:How many different  \n    pictures are edited in\n    this video?\nA: 3 pictures. \n[8 key frames]\n[1 key frame]\nA: 1 picture.\nA: 3 pictures.\nA: 4 pictures. \nA: He is preparing\n    to sit on the chair.\nA: He is standing \n    up from the chair.\nA: He is preparing to\n    sit on the chair.\n[8 key frames]\n[3 key frames]\n[1 key frame]\n[3 key frames]\nBIN Split\nFigure 5. Two examples of how different sampling strategies impact video understanding. The left case comes from LongVideoBench\n(focusing on one moment) and the right one comes from VideoMME (relying on multiple moments). Each curve shows the s(Q, Ft) score\nover time, and the yellow circles indicate the position of sampled keyframes. We also annotate the number of true keyframes and the\nreason for each failure case below the answer.\nAKS in selecting higher-quality keyframes.\nVisualizing keyframe selection. Figure 5 shows two repre-\nsentative examples and explains how the style of questions\nvaries across LongVideoBench and VideoMME and how it\nimpacts the preference between TOP and BIN. Many ques-\ntions of LongVideoBench are focused on a simple moment\n(e.g., ‘What is a person doing at a specific time point?’), so\nTOP sampling (i.e., without constraints in temporal distri-\nbution) often works well in locating these keyframes, while\nBIN sampling limits the number of keyframes within each\nbin and results in information loss. On the contrary, the\nquestions of VideoMME often require the model to collect\ninformation from multiple moments (e.g., ‘How many times\ndoes something happen?’), so BIN sampling is a safe choice\nto locate keyframes in different bins, while TOP sampling\ncan lose information in weak peaks. ADA sampling absorbs\nthe advantages of TOP and BIN strategies and adaptively\nallocates keyframes to the desired position (see the exam-\nple in Figure 3) – this is why it achieves the best results in\nboth benchmarks.\nFigure 6 shows an interesting example that, on the same\ninput video, AKS selects different sets of keyframes based\non the prompt. This increases the flexibility that a frozen\nMLLM can adapt to different scenarios.\n4.4. Ablative Studies\nThe frequency of sampling keyframe candidates.\nTo\ndecrease the extra computational cost brought by AKS,\nwe sample fewer keyframe candidates (i.e., one frame per\n2/4/8/10 seconds, and compare the results with the stan-\ndard 1-fps method.\nResults are summarized in Table 3.\nQ: How many cubs is the mother bear with when the filmer encounters her \ndownstream of the river?\nA: 2 cubs are with the mother bear.\nA: Salmon.\nA: Because she spotted two adult male brown bears that might kill the cubs.\nInput Video\nQ: What is the main food of the brown bear in the video?\nQ: Why does the mother brown bear in make her cubs run to higher ground?\nFigure 6. AKS selects different keyframe sets to answer different\nquestions. All answers are correct.\nOn LongVideoBench, even at 0.1 fps, all results are higher\nthan the baseline (i.e., 57.4%, 57.9%, 58.9% at 16, 32, 64\nframes). VideoMME shows a similar trend, and 0.25 fps\nseems a safe option to surpass the baseline.\nIt is worth\nexploring more efficient pre-filtering algorithms towards a\nbetter tradeoff between accuracy and efficiency.\nThe VL model for computing s(Q, Ft) scores. We ana-\nlyze the impact of using different VL models for comput-\ning prompt-frame relevance. We study three options, i.e.,\nBLIP [16] (the default choice in this paper), Sevila (used\nin [47]), and CLIP [32]. Results are summarized in Ta-\nble 4. We find that BLIP works better on LongVideoBench\n7\n\n\nTable 3. Question answering accuracy (%) w.r.t. different sam-\npling frequencies. LLaVA-Video-7B is used as the MLLM.\nFrames\nSampling Frequency (fps)\nof MLLM\n1\n0.5\n0.25\n0.125\n0.1\nLongVideoBench val\n16\n61.6\n60.7\n60.6\n61.1\n59.4\n32\n61.9\n62.1\n59.8\n60.2\n58.5\n64\n62.7\n62.2\n61.8\n60.1\n60.1\nVideoMME\n16\n62.2\n63.0\n62.2\n61.0\n61.6\n32\n64.6\n64.7\n65.1\n64.4\n64.4\n64\n65.3\n65.1\n64.9\n64.0\n64.2\nTable 4. Question answering accuracy (%) w.r.t. different VL scor-\ners. LLaVA-Video-7B is used as the MLLM.\nFrames\nUniform\nBLIP\nSevila\nCLIP\nLongVideoBench val\n16\n57.4\n61.6\n59.2\n60.2\n32\n57.9\n61.9\n60.9\n61.9\n64\n58.9\n62.7\n61.5\n62.2\nVideoMME\n16\n60.6\n62.2\n63.0\n63.1\n32\n63.9\n64.6\n63.7\n65.0\n64\n64.4\n65.3\n65.1\n65.6\nTable 5. Ablating L and sthr together. Left: LVB, Right: V-MME.\nHHHH\nL\nsthr\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1\n62.4/63.8 62.4/64.0 62.5/64.2 62.0/64.1 61.8/63.8 61.9/64.0\n2\n62.4/63.8 62.0/64.0 62.4/64.0 61.8/63.5 61.7/63.4 62.0/63.6\n3\n62.4/63.8 62.8/64.0 62.6/54.5 62.1/64.4 62.2/64.4 62.1/64.4\n4\n62.4/63.8 62.7/64.1 62.7/64.3 62.2/64.9 62.1/65.0 62.2/65.0\n5\n62.4/63.8 62.7/64.1 62.2/64.7 61.7/65.0 61.3/65.3 61.7/65.2\n6\n62.4/63.8 62.7/64.0 62.3/64.5 61.8/65.0 61.3/65.1 61.4/65.1\nwhile CLIP works better on VideoMME. This is because\nCLIP was trained on generic image-text pairs while BLIP\nlearned from object-level data – correspondingly, questions\nin LongVideoBench and VideoMME are more focused on\nobjects and global perception, respectively.\nADA hyper-parameters, L and sthr. Lastly, we study the\nimpact of L and sthr. Results are summarized in Table 5.\nOne can see that LongVideoBench prefers smaller L and\nsthr values than VideoMME. This is because the key in-\nformation on LongVideoBench is more concentrated (i.e.,\nmany questions are related to single moments) while that\non VideoMME is more diverse (multi-moment data are re-\nquired for question answering). AKS offers a flexible abil-\nity to switch between different ‘modes’ and achieves better\nresults in both datasets.\nQ: What is the woman in sunglasses doing in the video?\nw/AKS: \nThe woman in sunglasses is sitting at a table with a drink, possibly \n \n \nenjoying the view.\nQ: Please describe this video about a temple.\nw/AKS: \nThe video opens with a blurry shot of a temple structure, followed \nby a clearer view of the temple's intricate carvings and statues. The temple \nappears to be made of stone and is adorned with detailed sculptures of \nmythical creatures and deities. The sky is visible in the background, \nsuggesting that the temple is outdoors. The video then transitions to a close-up \nof a statue, highlighting its craftsmanship and the texture of the stone. The \nfinal frames show a wider shot of the temple, emphasizing its grandeur and \nthe surrounding natural environment.\nBaseline: The video opens with a view of a lush tropical landscape, featuring \ntall palm trees and a mountain range in the background. The sky is overcast, \nand there's a sense of tranquility. As the video progresses, the camera pans to \nreveal more of the forested area, with dense foliage and a few buildings visible \nin the distance. The colors are vibrant, with greens of various shades \ndominating the scene. There are no visible texts or subtitles in the video. \nBaseline: The woman in sunglasses is walking through a lush garden      \n \n \nwith greenery and flowers.\nQ: What is the shirtless man doing in the video?\nw/AKS: \nThe shirtless man is standing in a small, rectangular pool with \n           \nturquoise water.\nBaseline: The shirtless man shirt is leading the donkeys.\nReferring\nReferring\nCaptioning\nFigure 7. Extending AKS for video referring and captioning. The\nbaseline results are produced upon uniform keyframe sampling.\nRed and green texts indicate incorrect and correct descriptions.\n4.5. Generalization to Other Tasks\nBeing an off-the-shelf algorithm, AKS is easily applied to\nother video understanding tasks. Here we showcase two ex-\namples known as video referring and captioning. For this\npurpose, we use the LLaVA-Video-7B model, switch the\ntext prompt into ‘What is [target] doing in the video?’ or\n‘Please describe this video.’, and remove the options. Qual-\nitative results are shown in Figure 7. As seen, to obtain a\ncomprehensive description of long videos, it is crucial to\nlocate keyframes and feed them into the MLLM as visual\ncontexts. AKS benefits from its keyframe selection ability\nand helps the MLLM to generate much better answers.\n5. Conclusions\nThis paper focuses on improving the ability of MLLMs for\nlong video understanding. The main difficulty arises from\nthe limited capacity of MLLMs which urges us to feed in-\nformative visual tokens into the model. For this purpose, we\npresent the Adaptive Keyframe Sampling (AKS) algorithm\nwhich (1) uses a vision-language model to estimate the rel-\nevance and (2) applies an adaptive optimization algorithm\nto facilitate the coverage of selected keyframes. Quanti-\ntative and qualitative studies validate the effectiveness of\n8\n\n\nAKS over different baselines and benchmarks. Our work\nreveals that a pre-filtering stage brings considerable benefit\nto video understanding and advocates for further studies in\nthis direction.\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\nKatherine Millican, Malcolm Reynolds, et al. Flamingo: a\nvisual language model for few-shot learning. Advances in\nNeural Information Processing Systems, 35:23716–23736,\n2022. 2\n[2] Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Es-\nsam Sleiman, Mingchen Zhuge, Jian Ding, Deyao Zhu,\nJ¨urgen Schmidhuber, and Mohamed Elhoseiny.\nGoldfish:\nVision-language understanding of arbitrarily long videos.\narXiv preprint arXiv:2407.12679, 2024. 2\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in neural in-\nformation processing systems, 33:1877–1901, 2020. 2\n[4] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao\nWu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao\nZhuang, Joseph E Gonzalez, et al. Vicuna: An open-source\nchatbot impressing gpt-4 with 90%* chatgpt quality.\nSee\nhttps://vicuna. lmsys. org (accessed 14 April 2023), 2023.\n[5] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, et al.\nPalm: Scaling language modeling with\npathways. arXiv preprint arXiv:2204.02311, 2022.\n[6] Hyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, et al.\nScaling\ninstruction-finetuned language models.\narXiv preprint\narXiv:2210.11416, 2022. 2\n[7] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven C. H. Hoi. Instructblip: Towards general-\npurpose vision-language models with instruction tuning.\narXiv preprint arXiv:2305.06500, 2023. 2\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova.\nBert:\nPre-training of deep bidirectional\ntransformers for language understanding.\narXiv preprint\narXiv:1810.04805, 2018. 2\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020. 2\n[10] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai\nRen, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang\nShen, Mengdan Zhang, et al.\nVideo-mme: The first-ever\ncomprehensive evaluation benchmark of multi-modal llms in\nvideo analysis. arXiv preprint arXiv:2405.21075, 2024. 1,\n2, 5, 12\n[11] Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xue-\nfei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam\nLim. Ma-lmm: Memory-augmented large multimodal model\nfor long-term video understanding. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 13504–13514, 2024. 2\n[12] Mojan Javaheripi, S´ebastien Bubeck, Marah Abdin, Jy-\noti Aneja, Sebastien Bubeck, Caio C´esar Teodoro Mendes,\nWeizhu Chen, Allie Del Giorno, Ronen Eldan, Sivakanth\nGopi, et al. Phi-2: The surprising power of small language\nmodels. Microsoft Research Blog, 2023. 2\n[13] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi\nMao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer\nWhitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll´ar,\nand Ross Girshick.\nSegment Anything.\narXiv preprint\narXiv:2304.02643, 2023. 2\n[14] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui\nYuan, Shu Liu, and Jiaya Jia.\nLISA: Reasoning Seg-\nmentation via Large Language Model.\narXiv preprint\narXiv:2308.00692, 2023. 1, 2\n[15] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng\nLi, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and\nChunyuan Li.\nLlava-onevision: Easy visual task transfer.\narXiv preprint arXiv:2408.03326, 2024. 2, 5, 11\n[16] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for uni-\nfied vision-language understanding and generation. In In-\nternational Conference on Machine Learning, pages 12888–\n12900. PMLR, 2022. 4, 5, 7\n[17] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBLIP-2: Bootstrapping Language-Image Pre-training with\nFrozen Image Encoders and Large Language Models. arXiv\npreprint arXiv:2301.12597, 2023. 2\n[18] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai\nWang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.\nVideochat: Chat-centric video understanding, 2024. 2\n[19] Zhaowei Li, Qi Xu, Dong Zhang, Hang Song, Yiqing Cai,\nQi Qi, Ran Zhou, Junting Pan, Zefeng Li, Van Tu Vu, Zhida\nHuang, and Tao Wang.\nGroundinggpt:language enhanced\nmulti-modal grounding model, 2024. 2\n[20] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng\nJin, and Li Yuan. Video-llava: Learning united visual repre-\nsentation by alignment before projection, 2023. 1, 2, 5\n[21] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Moham-\nmad Shoeybi, and Song Han. Vila: On pre-training for vi-\nsual language models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n26689–26699, 2024. 5\n[22] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual Instruction Tuning. arXiv preprint arXiv:2304.08485,\n2023. 1, 2\n[23] Jiajun Liu, Yibing Wang, Hanghang Ma, Xiaoping Wu, Xi-\naoqi Ma, Xiaoming Wei, Jianbin Jiao, Enhua Wu, and Jie Hu.\nKangaroo: A powerful video-language model supporting\nlong-context video input. arXiv preprint arXiv:2408.15542,\n2024. 2\n9\n\n\n[24] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nProceedings of the IEEE/CVF international conference on\ncomputer vision, pages 10012–10022, 2021. 2\n[25] Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Da Li,\nPengcheng Lu, Tao Wang, Linmei Hu, Minghui Qiu, and\nZhongyu Wei. Valley: Video assistant with large language\nmodel enhanced ability, 2023. 2\n[26] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fa-\nhad Shahbaz Khan. Video-chatgpt: Towards detailed video\nunderstanding via large vision and language models, 2023. 2\n[27] Shehan Munasinghe, Rusiru Thushara, Muhammad Maaz,\nHanoona Abdul Rasheed, Salman Khan, Mubarak Shah, and\nFahad Khan. Pg-video-llava: Pixel grounding large video-\nlanguage models, 2023. 2\n[28] OpenAI.\nGpt-4v.\nhttps://openai.com/index/\ngpt-4v-system-card/, 2023. 5\n[29] OpenAI.\nHello gpt-4o.\nhttps://openai.com/\nindex/hello-gpt-4o/, 2024. 5\n[30] Rui Qian, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Shuan-\ngrui Ding, Dahua Lin, and Jiaqi Wang. Streaming long video\nunderstanding with large language models. arXiv preprint\narXiv:2405.16009, 2024. 2\n[31] Jihao Qiu, Yuan Zhang, Xi Tang, Lingxi Xie, Tianren Ma,\nPengyu Yan, David Doermann, Qixiang Ye, and Yunjie\nTian. Artemis: Towards referential understanding in com-\nplex videos. arXiv preprint arXiv:2406.00258, 2024. 2\n[32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748–8763. PMLR, 2021. 2, 4, 5, 7\n[33] Brian D Ripley.\nThe second-order analysis of stationary\npoint processes. Journal of applied probability, 13(2):255–\n266, 1976. 4\n[34] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng\nZhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo,\nTian Ye, Yanting Zhang, et al. Moviechat: From dense token\nto sparse memory for long video understanding. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 18221–18232, 2024. 2\n[35] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui\nWu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan\nSchalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a\nfamily of highly capable multimodal models. arXiv preprint\narXiv:2312.11805, 2023. 5\n[36] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia\nJin, Taylor Bos, Leslie Baker, Yu Du, et al.\nLamda:\nLanguage models for dialog applications.\narXiv preprint\narXiv:2201.08239, 2022. 2\n[37] Yunjie Tian, Lingxi Xie, Xiaopeng Zhang, Jiemin Fang,\nHaohang Xu, Wei Huang, Jianbin Jiao, Qi Tian, and Qixiang\nYe.\nSemantic-aware generation for self-supervised visual\nrepresentation learning. arXiv preprint arXiv:2111.13163,\n2021. 2\n[38] Yunjie Tian, Lingxi Xie, Zhaozhi Wang, Longhui Wei, Xi-\naopeng Zhang, Jianbin Jiao, Yaowei Wang, Qi Tian, and Qix-\niang Ye. Integrally Pre-Trained Transformer Pyramid Net-\nworks. In 2023 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 18610–18620. IEEE, 2023.\n2\n[39] Yunjie Tian, Tianren Ma, Lingxi Xie, Jihao Qiu, Xi Tang,\nYuan Zhang, Jianbin Jiao, Qi Tian, and Qixiang Ye. Chatter-\nbox: Multi-round multimodal referring and grounding. arXiv\npreprint arXiv:2401.13307, 2024. 1, 2\n[40] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth´ee Lacroix, Baptiste\nRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama:\nOpen and efficient foundation language models.\narXiv preprint arXiv:2302.13971, 2023. 2\n[41] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan,\nJinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin\nGe, et al. Qwen2-vl: Enhancing vision-language model’s\nperception of the world at any resolution.\narXiv preprint\narXiv:2409.12191, 2024. 2, 5, 11\n[42] Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang,\nand Bohan Zhuang.\nLongvlm: Efficient long video un-\nderstanding via large language models.\narXiv preprint\narXiv:2404.03384, 2024. 2\n[43] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li.\nLongvideobench:\nA benchmark for long-context inter-\nleaved video-language understanding.\narXiv preprint\narXiv:2407.15754, 2024. 1, 2, 5, 12\n[44] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng,\nand Jiashi Feng. Pllava: Parameter-free llava extension from\nimages to videos for video dense captioning. arXiv preprint\narXiv:2404.16994, 2024. 1, 5\n[45] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen\nYu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng\nLiu, Fei Huang, et al. Qwen2 technical report. arXiv preprint\narXiv:2407.10671, 2024. 5\n[46] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui,\nHongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He,\net al. Minicpm-v: A gpt-4v level mllm on your phone. arXiv\npreprint arXiv:2408.01800, 2024. 5\n[47] Shoubin Yu, Jaemin Cho, Prateek Yadav, and Mohit Bansal.\nSelf-chained image-language model for video localization\nand question answering.\nAdvances in Neural Information\nProcessing Systems, 36, 2024. 7\n[48] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu\nLai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao\nXia, et al. Glm-130b: An open bilingual pre-trained model.\narXiv preprint arXiv:2210.02414, 2022. 2\n[49] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and\nLucas Beyer. Sigmoid loss for language image pre-training.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 11975–11986, 2023. 5\n[50] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An\ninstruction-tuned audio-visual language model for video un-\nderstanding, 2023. 2\n[51] Kaichen\nZhang,\nBo\nLi,\nPeiyuan\nZhang,\nFanyi\nPu,\nJoshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan\n10\n\n\nZhang, Jingkang Yang, Chunyuan Li, et al. Lmms-eval: Re-\nality check on the evaluation of large multimodal models.\narXiv preprint arXiv:2407.12772, 2024. 5\n[52] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,\nMoya Chen, Shuohui Chen, Christopher Dewan, Mona Diab,\nXian Li, Xi Victoria Lin, et al. Opt: Open pre-trained trans-\nformer language models. arXiv preprint arXiv:2205.01068,\n2022. 2\n[53] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi\nShao, Wenwei Zhang, Kai Chen, and Ping Luo. GPT4RoI:\nInstruction Tuning Large Language Model on Region-of-\nInterest. arXiv preprint arXiv:2307.03601, 2023. 1, 2\n[54] Xiaosong Zhang, Yunjie Tian, Lingxi Xie, Wei Huang, Qi\nDai, Qixiang Ye, and Qi Tian. Hivit: A simpler and more\nefficient design of hierarchical vision transformer. In The\nEleventh International Conference on Learning Representa-\ntions, 2022. 2\n[55] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Zi-\nwei Liu, and Chunyuan Li. Video instruction tuning with\nsynthetic data. arXiv preprint arXiv:2410.02713, 2024. 1, 2,\n3, 5, 11\n[56] Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa\nWang, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zong-\nwei Li, Wancai Zhang, Zhifeng Li, Wei Liu, and Li Yuan.\nLanguagebind: Extending video-language pretraining to n-\nmodality by language-based semantic alignment, 2024. 2\nA. Details of the ADA Algorithm\nIn Algorithm 1, we present the detailed pseudocode of our\nADA algorithm. To accelerate the experimental process, we\npre-process the video frames (sampled at 1 frame per sec-\nond along with the corresponding questions) by inputting\nthem into the VL scorer to obtain the corresponding scores.\nThese scores are then stored in a list referred to as\nmatching score. Each element in matching score con-\nsists of the matching score for a specific video frame and the\ncorresponding question. We begin by employing a recur-\nsive strategy to partition the matching scores list into sub-\nlists of varying lengths, according to the partitioning rule\noutlined in Section 3.3. Subsequently, based on the lengths\nof these sublists, we select different numbers of frames with\nthe highest matching scores from each sublist to construct\nthe final set of video frames. This final set is then sent to\nthe language model for visual understanding.\nB. More Visualization Results\nIn Figure 8, we show more examples of video understanding\nresults of AKS (based on three baselines, LLaVA-Video-\n7B [55], Qwen2-VL-7B [41], and LLaVA-OV-7B [15]).\nAs shown, our approach benefits from the ability to locate\nkeyframes so that the MLLM receives effective visual in-\nformation for understanding. The ability easily transfers to\nvarious MLLMs in a plug-and-play manner.\nAlgorithm 1: ADA: Adaptive Keyframe Selection\nInput: matching scores: A list, where each\nelement is the matching score of a frame and\nthe corresponding question\nlevel: Current recursion level\nmax level: Maximum recursion level\nsthr: Threshold\nM: Number of frames to select\nOutput: selected frames: Indices of the selected\nM frames\nFunction SplitSegments(matching scores,\nlevel, max level, sthr, M):\nsplit scores ←[] // List of completed\nsegments\nnew scores ←[] // List of segments\nto further split\nforeach matching score in matching scores\ndo\nsall ←mean(matching score)\nstop ←mean(topk(matching score, M))\nm ←stop −sall\nif m ≥sthr then\nAppend matching score to split scores\nelse if level < max level then\nSplit matching score into two bins\nfrom the center, denoted as split1 and\nsplit2\nAppend split1 and split2 to new scores\nif new scores is not empty then\ndeeper scores ←SplitSegments\n(new scores, level + 1, max level, sthr,\nM//2level)\nsplit scores ←\nmerge(split scores, deeper scores)\nreturn split scores\nFunction SelectFrames(segments, M):\ntotal length ←Total length of all segments\nselected frames ←[]\nforeach segment in segments do\nmi ←\n⌊M × length(segment)/total length⌋\nSelect the top mi highest-scoring frame\nindices from segment\nAppend the selected indices to\nselected frames\nreturn selected frames\nMain:\nmatching scores ←[matching score]\nsegments ←SplitSegments\n(matching scores, level, max level, sthr)\nselected frames ←SelectFrames\n(segments, M)\nreturn selected frames\n11\n\n\nQ: What is the first food item displayed in the video?\nQ: A person wearing an embroidered dress, whose face is not visible, is\n     cutting a tomato on a wooden board with a vegetable knife. What\n     objects are present in this scene?\nQ: In an oil painting depicting many people, there is a man in blue clothes\n    holding an object and kneeling towards a man in black clothes. Which\n    character appears in this scene?\nA: A person in black clothes\n   leaning on a crutch.\nLLaVA-Video-7B w/ AKS\nLLaVA-Video-7B  \nA: A person wearing\n   a red robe.\nQ: What card does the male judge pick?\nA: Avocado.\nQwen2-VL-7B w/ AKS\nQwen2-VL-7B  \nA: Beverage with ice cubes.\nA: 2 of clubs.\nQwen2-VL-7B w/ AKS\nQwen2-VL-7B  \nA: 2 of spades.\nQ: What is the stage background where several male\n     performers are holding long sticks?\nA: parsley.\nLLaVA-OV-7B w/ AKS\nLLaVA-OV-7B  \nA: ring.\nA: A forest.\nLLaVA-OV-7B w/ AKS\nLLaVA-OV-7B  \nA: A sailboat.\nQ: How many times does the interviewed girl appear in the video?\nA: 4.\nLLaVA-Video-7B w/ AKS\nLLaVA-Video-7B  \nA: 2.\nFigure 8.\nMore examples of AKS enhance the baseline MLLMs for video understanding.\nThe left three examples come from\nLongVideoBench [43] while the right three are from VideoMME [10]. Green stars indicate keyframes selected by AKS.\n12\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21271v1.pdf",
    "total_pages": 12,
    "title": "Adaptive Keyframe Sampling for Long Video Understanding",
    "authors": [
      "Xi Tang",
      "Jihao Qiu",
      "Lingxi Xie",
      "Yunjie Tian",
      "Jianbin Jiao",
      "Qixiang Ye"
    ],
    "abstract": "Multimodal large language models (MLLMs) have enabled open-world visual\nunderstanding by injecting visual input as extra tokens into large language\nmodels (LLMs) as contexts. However, when the visual input changes from a single\nimage to a long video, the above paradigm encounters difficulty because the\nvast amount of video tokens has significantly exceeded the maximal capacity of\nMLLMs. Therefore, existing video-based MLLMs are mostly established upon\nsampling a small portion of tokens from input data, which can cause key\ninformation to be lost and thus produce incorrect answers. This paper presents\na simple yet effective algorithm named Adaptive Keyframe Sampling (AKS). It\ninserts a plug-and-play module known as keyframe selection, which aims to\nmaximize the useful information with a fixed number of video tokens. We\nformulate keyframe selection as an optimization involving (1) the relevance\nbetween the keyframes and the prompt, and (2) the coverage of the keyframes\nover the video, and present an adaptive algorithm to approximate the best\nsolution. Experiments on two long video understanding benchmarks validate that\nAdaptive Keyframe Sampling improves video QA accuracy (beyond strong baselines)\nupon selecting informative keyframes. Our study reveals the importance of\ninformation pre-filtering in video-based MLLMs. Code is available at\nhttps://github.com/ncTimTang/AKS.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}