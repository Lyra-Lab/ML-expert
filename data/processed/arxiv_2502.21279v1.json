{
  "id": "arxiv_2502.21279v1",
  "text": "IEEE TRANSACTIONS ON XXX XXXX, XXX, XXX, SEPTEMBER 2023\n1\nL-Lipschitz Gershgorin ResNet Network\nMarius F. R. Juston1, William R. Norris2, Dustin Nottage3, Ahmet Soylemezoglu3\nAbstract—Deep residual networks (ResNets) have demon-\nstrated outstanding success in computer vision tasks, attributed to\ntheir ability to maintain gradient flow through deep architectures.\nSimultaneously, controlling the Lipschitz bound in neural net-\nworks has emerged as an essential area of research for enhancing\nadversarial robustness and network certifiability. This paper uses\na rigorous approach to design L-Lipschitz deep residual networks\nusing a Linear Matrix Inequality (LMI) framework. The ResNet\narchitecture was reformulated as a pseudo-tri-diagonal LMI with\noff-diagonal elements and derived closed-form constraints on\nnetwork parameters to ensure L-Lipschitz continuity. To address\nthe lack of explicit eigenvalue computations for such matrix\nstructures, the Gershgorin circle theorem was employed to ap-\nproximate eigenvalue locations, guaranteeing the LMI’s negative\nsemi-definiteness. Our contributions include a provable param-\neterization methodology for constructing Lipschitz-constrained\nnetworks and a compositional framework for managing recursive\nsystems within hierarchical architectures. These findings enable\nrobust network designs applicable to adversarial robustness,\ncertified training, and control systems. However, a limitation\nwas identified in the Gershgorin-based approximations, which\nover-constrain the system, suppressing non-linear dynamics and\ndiminishing the network’s expressive capacity.\nIndex Terms—Linear Matrix Inequalities, Lipschitz continuity,\nDeep residual networks, Adversarial robustness, Gershgorin\ncircle theorem, semi-definite programming\nI. INTRODUCTION\nT\nHE robustness of deep neural networks (DNNs) is a crit-\nical challenge, mainly when applied in safety-sensitive\ndomains where small adversarial perturbations can lead to\ndangerous situations such as the misclassification of important\nobjects. One approach to address this issue is by enforcing\nLipschitz constraints on the network architectures. These con-\nstraints guarantee that small changes in the input will not cause\nsignificant changes in the output. This property is vital for\ncertifying robustness against adversarial attacks, which involve\nintroducing slight noise to modify the expected classification\noutput result [1], [2]. The Lipschitz constant is a key measure\nto bound the network’s sensitivity to input perturbations.\nSpecifically, a L-Lipschitz network can be theoretically guar-\nanteed that the output remains stable within a defined ”stability\nMarius F. R. Juston1 is with The Grainger College of Engineering,\nIndustrial and Enterprise Systems Engineering Department, University of\nIllinois Urbana-Champaign, Urbana, IL 61801-3080 USA (email: mjus-\nton2@illinois.edu).\nWilliam R Norris2 is with The Grainger College of Engineering, In-\ndustrial and Enterprise Systems Engineering Department, University of\nIllinois Urbana-Champaign, Urbana, IL 61801-3080 USA (email: wrnor-\nris@illinois.edu).\nConstruction Engineering Research Laboratory3, U.S. Army Corps of\nEngineers Engineering Research and Development Center, IL, 61822, USA\nThis research was supported by the U.S. Army Corps of Engineers\nEngineering Research and Development Center, Construction Engineering\nResearch Laboratory.\nsphere” around each input, making it resistant to adversarial\nattacks up to a certain magnitude [3].\nTo achieve this, several methods have been proposed to\nenforce Lipschitz constraints on neural networks, including\nspectral normalization [4], [5], orthogonal parameterization\n[6], and more recent approaches such as Convex Potential\nLayers (CPL) and Almost-Orthogonal Layers (AOL) [6],\n[7]. The previous works have been shown to be formulated\nunder a unifying semi-definitive programming architecture,\nwhich possesses the constraints on the networks as LMIs [8].\nHowever, ensuring Lipschitz constraints in deep architectures,\nparticularly residual networks (ResNets), presents unique chal-\nlenges due to their recursive structure. While prior work has\nmade strides in constraining individual layers [8], [9] and\ngenerating a unifying semi-definite programming approach,\nthe generalized deep residual network formulation presents\nissues in the pseudo-tri-diagonal structure of its imposed LMI.\nFurthermore, multi-layered general Feedforward Neural\nNetworks (FNN) have been shown to generate block tri-\ndiagonal matrix LMi formulations [10] due to their inherent\nnetwork structure, which in contrast to the residual formula-\ntion, yield explicit solutions [11], [12]. However, due to the\noff-diagonal structure of the network, the direct application of\nthe exact eigenvalue computation is not feasible, making the\nsolution process significantly more complex.\nPrevious work has also demonstrated an iterative approach\nby utilizing projected gradient descent optimization or a reg-\nularization term on the estimated Lipschitz constant to ensure\na constraint on the Lipschitz constraint [13]–[15]. While this\nguarantees an iterative enforcement of the Lipschitz constraint,\nit does not ensure a theoretical Lipschitz guarantee across the\nentire network until this convergence. However, the advantage\nof this technique is its generalizability, which allows for the\nutilization of more general network structures.\nA. Contributions\nThis paper introduces the formulation of deep residual net-\nworks as Linear Matrix Inequalities (LMI). It derives closed-\nform constraints on network parameters to ensure theoretical\nL-Lipschitz constraints. The LMI was structured as a tri-\ndiagonal matrix with off-diagonal components, which inher-\nently complicates the derivation of closed-form eigenvalue\ncomputations. To address this limitation, the Gershgorin circle\ntheorem was employed to approximate the eigenvalue loca-\ntions. The Gershgorin circles enabled the derivation of closed-\nform constraints that guaranteed the negative semi-definiteness\nof the LMI.\nAdditionally, this paper demonstrates a significant limita-\ntion of the Gershgorin circle theorem in this context: the\narXiv:2502.21279v1  [cs.LG]  28 Feb 2025\n\n\nIEEE TRANSACTIONS ON XXX XXXX, XXX, XXX, SEPTEMBER 2023\n2\nderived approximations lead to over-constraining the system,\neffectively suppressing the network’s non-linear components.\nThis, in turn, makes the network act as a simple linear\ntransformation instead.\nMoreover, while [8]’s work generates a closed-form solution\nfor a residual network, it is limited to considering a single\ninner layer. In contrast, this paper presents a more general\nformulation that accommodates a more expressive inner layers\nsystem within the residual network system, offering greater\nflexibility and broader applicability.\nII. LMI FORMULATION\nFollowing the works for [8], who defined a Lipschitz\nneural network as a constrained LMI problem to define a\nresidual network, limitations in their approach were identified.\nSpecifically, their formulation resulted in a single-layered\nresidual network, which is inherently less expressive compared\nto the generalized deep-layered residual network popularized\nby architectures such as ResNet and its variants [16]–[20].\nThese deeper networks perform better due to the multiple\ninner layers that compose the modules, which allows for more\ncomplex latent space transformations and thus increases the\nnetwork’s expressiveness. This research focuses on establish-\ning constraints for the inner layers to maintain the L-Lipschitz\ncondition while maximizing the expressiveness of the residual\nnetwork for larger inner layers. As such, the inner layers of\nthe residual network were represented as a recursive system\nof linear equations:\nxk+1 = Akxk + Bkwk,n\nwk,n = σn(Cnwk,n−1 + bn)\n⋮\nwk,1 = σ1(C1xk + b1).\n(1)\nWhere each of the layer parameters were defined as Cl ∈\nRdl×dl−1,bl ∈Rdl for l ∈{1,⋯,n}. When n = 1, the formula-\ntion reduced to the one presented in [8], rendering it redundant\nin its derivation. The goal of the LMI was to maintain the Lip-\nschitz constraint formulated as ∥x′\nk+1 −xk+1∥≤L∥x′\nk −xk∥.\nGiven that this system could be represented as a large\nrecursive system, it was possible to split all the constraints\nof the inner layers as a set of LMI conditions similar to [8],\n[10], [21]. For the most general LMI constraint definition, the\nactivation functions were assumed to not necessarily be the\nReLU function but a general element-wise activation function,\nwhich were L-smooth and m-strongly convex, where Li ≥mi.\nThus, the general activation function quadratic constraint was\nused [8], [10]:\n[ vk −v′\nk\nw′\nk,i −wk,i]\n⊺\n[ −2LimiΛi\n(mi + Li)Λi\n(mi + Li)Λi\n−2Λi\n] [ vk −v′\nk\nw′\nk,i −wk,i] ≤0\n(2)\nwhere Λn must be a positive definitive diagonal matrix.\nGiven that vk −v′\nk = Cn (wk,n−1 −w′\nk,n−1) the inequality thus\nbecomes the following quadratic constraints, where ∆wk,i was\ndefined as ∆wk,i = w′\nk,i −wk,i,\n⎡⎢⎢⎢⎢⎣\nC1 (x′\nk −xk)\n∆wk,1\n⎤⎥⎥⎥⎥⎦\n⊺\n[ −2L1m1Λ1\n(m1 + L1)Λ1\n(m1 + L1)Λ1\n−2Λ1\n]\n⎡⎢⎢⎢⎢⎣\nC1 (x′\nk −xk)\n∆wk,1\n⎤⎥⎥⎥⎥⎦\n≤0,\n⎡⎢⎢⎢⎢⎣\nC2 (∆wk,1)\n∆wk,2\n⎤⎥⎥⎥⎥⎦\n⊺\n[ −2L2m2Λ2\n(m2 + L2)Λ2\n(m2 + L2)Λ2\n−2Λ2\n]\n⎡⎢⎢⎢⎢⎣\nC2 (∆wk,1)\n∆wk,2\n⎤⎥⎥⎥⎥⎦\n≤0,\n⋮\n⎡⎢⎢⎢⎢⎣\nCn (∆wk,n−1)\n∆wk,n\n⎤⎥⎥⎥⎥⎦\n⊺\n[ −2LnmnΛn\n(mn + Ln)Λn\n(mn + Ln)Λn\n−2Λn\n]\n⎡⎢⎢⎢⎢⎣\nCn (∆wk,n−1)\n∆wk,n\n⎤⎥⎥⎥⎥⎦\n≤0.\n(3)\nTo combine the LMIs, a concatenated vector of all the wk,n\nand xk was created to sum all the conditions and solve them\nall together. The following LMI could thus be formulated as\nthe summation in Equation (4). Where,\nDl =\nl\n∑\ni=1\ndi,\n(5)\nEl ∶{0,1}(dl+dl−1)×Dn,\n(6)\n[El]ij =\n⎧⎪⎪⎨⎪⎪⎩\n1\nif j −Dl = i\n0\nelse\n,\nmoreover, i and j represented the row and column, re-\nspectively. The El matrix represented a ”selection” vector\nto ensure that the proper variables were used for the pa-\nrameterization. Which gave the following resultant LMI in\nEquation (7). The question then became what parameterization\nof {Λ1,⋯,Λn},{C1,⋯,Cn}, and B would be needed to ensure\nthat the LMI was indeed negative semi-definitive to satisfy\nthe Lipschitz constraint where ideally {C1,⋯,Cn} would be\nas unconstrained as possible to ensure expressive inner layers.\nFrom the LMI, it could be noticed that it was exceedingly\ncomplex to derive the constraint of the network explicitly\nbased on the eigenvalues of the network. As such, although it\nonly provided loose bounds on the eigenvalues, the Gershgorin\ncircle theorem could be used to derive bounds on the network.\nTheorem II.1. Let A be a complex matrix n × n matrix, with\nentries aij. For i ∈{1,⋯,n} let Ri be the sum of the absolute\nvalues of the non-diagonal entries of the i-th row:\nRi = ∑\nj≠i\n∣aij∣.\n(8)\nLet D(aii,Ri) ⊆C be a closed disc centered at aii with\nradius Ri, every eigenvalue of A lies within at least one of\nthe Gershgorin discs D(aii,Ri).\nThe following corollary was thus derived to generate con-\nditions to ensure the LMI would be negative semi-definitive.\nCorollary 1. If all the Gershgorin discs of a matrix A are\ndefined in the negative real plane, R−, for i ∈{1,⋯,n}\nR(aii + Ri) ≤0, then the matrix A must be negative semi-\ndefinitive.\nThe conditions necessary to ensure that the overall LMI\nmatrix M was negative semi-definite were derived by analyz-\ning its Gershgorin discs. The analysis required demonstrating\nthat all Gershgorin discs were entirely contained within the\nleft half-plane, ensuring that the eigenvalues of M were non-\npositive. Given the structure of the LMI, the matrix could\nbe decomposed into three distinct sections: the first block,\nthe middle blocks, and the last block. For each block, a\ncorresponding set of constraints on the desired parameters was\ndetermined to ensure the feasibility of the problem.\nAs the LMI matrix was symmetric, the Gershgorin discs\nderived from the rows were shown to coincide with those\n\n\nIEEE TRANSACTIONS ON XXX XXXX, XXX, XXX, SEPTEMBER 2023\n3\n⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣\nx′\nk −xk\nw′\nk,1 −wk,1\nw′\nk,2 −wk,2\n⋮\nw′\nk,n−1 −wk,n−1\nw′\nk,n −wk,n\n⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦\n⊺⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣\n0dx\nIdx\n⋮\n0d1\n⋮\n⋮\n⋮\n⋮\n0dn−1\n⋮\nIdx\n0dx\n⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦\n[A⊺\nkAk −L2I\nA⊺\nkBk\nB⊺\nkAk\nB⊺\nkBk]\n⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣\n0dx\nIdx\n⋮\n0d1\n⋮\n⋮\n⋮\n⋮\n0dn−1\n⋮\nIdx\n0dx\n⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦\n⊺⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣\nx′\nk −xk\nw′\nk,1 −wk,1\nw′\nk,2 −wk,2\n⋮\nw′\nk,n−1 −wk,n−1\nw′\nk,n −wk,n\n⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦\n+\nn\n∑\nl=1\n⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣\nx′\nk −xk\nw′\nk,1 −wk,1\nw′\nk,2 −wk,2\n⋮\nw′\nk,n−1 −wk,n−1\nw′\nk,n −wk,n\n⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦\n⊺\nE⊺\ni [Cl\n0\n0\nI]\n⊺\n[ −2LlmlΛl\n(ml + Ll)Λi\n(ml + Ll)Λl\n−2Λl\n] [Cl\n0\n0\nI] Ei\n⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣\nx′\nk −xk\nw′\nk,1 −wk,1\nw′\nk,2 −wk,2\n⋮\nw′\nk,n−1 −wk,n−1\nw′\nk,n −wk,n\n⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦\n≤0,\n(4)\n⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣\nA⊺A −I −2L1m1C⊺\n1 Λ1C1\n(L1 + m1)C⊺\n1 Λ1\n0\n0\n0\nA⊺B\n(L1 + m1)Λ1C1\n−2L2m2C⊺\n2 Λ2C2 −2Λ1\n⋱\n0\n0\n0\n0\n⋱\n⋱\n⋱\n0\n0\n0\n0\n⋱\n⋱\n⋱\n0\n0\n0\n0\n⋱\n−2LnmnC⊺\nnΛnCn −2Λn−1\n(Ln + mn)C⊺\nnΛn\nB⊺A\n0\n0\n0\n(Ln + mn)ΛnCn\nB⊺B −2Λn\n⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦\n⪯0,\n(7)\nderived from the columns. This symmetry allowed the analysis\nto be carried out equivalently from either perspective without\nlosing generality.\nIII. GENERAL LMI SOLUTION\nFor notation, the parameters Sa and Pa were defined as\nSa = La + ma and Pa = Lama to help reduce the notation\nsize.\nA. Last block\nBelow is the derivation of the constraints for the parameters\ndefined in the last block portion of the LMI.\nTheorem III.1. For the parameter Cn, the norm of the rows\nmust be upper bounded by,\n∥Cn,i∥1 <\n2\n∣Ln + mn∣,\n(9)\nwhile λn,i must be lower bounded by,\nλn,i ≥\nb2\ni + ∣ai∣∣bi∣\n2 −∣Ln + mn∣∥Cn,i∥1\n.\n(10)\nProof. The final matrix row block was represented through\nthe parameters where l = n:\n{B⊺A,(Ln + mn)ΛnCn,B⊺B −2Λn}.\nWhich gave the following Gershgorin discs for ∀i{1,⋯,mn}\n(where mx = mn):\nD ⎛\n⎝b2\ni −2λn,i,∣ai∣∣bi∣+\ndn−1\n∑\nj=1\n∣(Ln + mn)cn,i,jλn,i∣⎞\n⎠,\n(11)\nFor which the upper bound constraint was thus:\nϵ3,n,i,max = b2\ni −2λn,i + ∣ai∣∣bi∣+ ∣Sn∣\ndn−1\n∑\nj=1\n∣cn,ijλn,i∣,\n(12)\nApplying the negative-semi definitive constraint, the following\nconstraint was derived:\n0 ≥b2\ni −2λn,i + ∣ai∣∣bi∣+ ∣Ln + mn∣\ndn−1\n∑\nj=1\n∣cn,i,jλn,i∣,\n≥b2\ni + ∣ai∣∣bi∣+ λn,i\n⎛\n⎝∣Ln + mn∣\ndn−1\n∑\nj=1\n∣cn,ij∣−2⎞\n⎠,\nλn,i ≥\nb2\ni + ∣ai∣∣bi∣\n2 −∣Ln + mn∣∑dn−1\nj=1 ∣cn,ij∣\n.\n(13)\nGiven that all λn,i must be positive definitive, the only way\nto ensure this was to ensure that:\ndn−1\n∑\nj=1\n∣cn,ij∣= ∥Cn,i∥1 <\n2\n∣Ln + mn∣.\n(14)\nThus enforcing that all the rows of Cn must be strictly upper\nbound by\n2\n∣Ln+mn∣.\nQED\nB. Middle blocks\nBelow is the derivation of the constraints for the parameters\ndefined in the middle blocks of the LMI.\nTheorem III.2. For all l = 1,⋯,n −1 the parameter Cl must\nhave its row norm be upper bounded by,\n∥Cn,i∥1 <\n2\n∣Ln + mn∣,\n(15)\nand element-wise upper bounded by\n∣cl+1,ji∣≤\n∣Sl+1∣2 + 4∣Pl+1∣\n2(∣Pl+1∣+ Pl+1)∣Sl+1∣.\n(16)\nwhile λl,i must be lower bounded by,\nλl,i ≥∑dl+1\nj=1 λl+1,j (∣Sl+1∣∣cl+1,ji∣−2Pl+1c2\nl+1,ji)\n2 −∣Sl∣∑dl−1\nj=1 ∣cl,ij∣\n+\n2∣Pl+1∣∑dl\nz=1,z≠i ∣∑dl+1\nj=1 λl+1,jcl+1,jicl+1,jz∣\n2 −∣Sl∣∑dl−1\nj=1 ∣cl,ij∣\n.\n(17)\n\n\nIEEE TRANSACTIONS ON XXX XXXX, XXX, XXX, SEPTEMBER 2023\n4\nProof. The set of block matrices that represent the middle\nblock represented the parameters where ∀l{1,⋯,n −1} were:\n{SlΛlCl,−2Pl+1C⊺\nl+1Λl+1Cl+1 −2Λl,Sl+1C⊺\nl+1Λn}.\nWhich gave the following Gershgorin discs, D (aii,Ri), for\n∀i{1⋯ml}∀l{1⋯n −1}:\naii = −2(Ll+1ml+1)\ndl+1\n∑\nj=1\nλl+1,jc2\nl+1,ji −2λl,i,\n(18)\nRi =λl,i∣Ll + ml∣\ndl−1\n∑\nj=1\n∣cl,ij∣\n+ ∣Ll+1 + ml+1∣\ndl+1\n∑\nj=1\nλl+1,j∣cl+1,ji∣\n+ 2∣Ll+1ml+1∣\ndl\n∑\nz=1,z≠i\nRRRRRRRRRRR\ndl+1\n∑\nj=1\nλl+1,jcl+1,jicl+1,jz\nRRRRRRRRRRR\n.\n(19)\nFor which the upper bound constraint was thus:\nϵ2,l,i,max =aii + Ri,\n(20)\n=λl,i\n⎛\n⎝∣Sl∣\ndl−1\n∑\nj=1\n∣cl,ij∣−2⎞\n⎠\n+\ndl+1\n∑\nj=1\nλl+1,j (∣Sl+1∣∣cl+1,ji∣−2Pl+1c2\nl+1,ji)\n+ 2∣Pl+1∣\ndl\n∑\nz=1,z≠i\nRRRRRRRRRRR\ndl+1\n∑\nj=1\nλl+1,jcl+1,jicl+1,jz\nRRRRRRRRRRR\n.\n(21)\nApplying the negative-semi definitive constraint, the following\nconstraint was derived:\nλl,i ≥∑dl+1\nj=1 λl+1,j (∣Sl+1∣∣cl+1,ji∣−2Pl+1c2\nl+1,ji)\n2 −∣Sl∣∑dl−1\nj=1 ∣cl,ij∣\n+\n2∣Pl+1∣∑dl\nz=1,z≠i ∣∑dl+1\nj=1 λl+1,jcl+1,jicl+1,jz∣\n2 −∣Sl∣∑dl−1\nj=1 ∣cl,ij∣\n,\n(22)\nGiven that all λ must be positive definitive and that the\nnumerator and denominator are independent, the following\nconstraints could thus be derived:\ndn−1\n∑\nj=1\n∣cl,ij∣= ∥Cl,i∥1 <\n2\n∣Ll + ml∣.\n(23)\nThus enforcing that all the rows of Cl had to be strictly upper\nbound by\n2\n∣Ll+ml∣.\nIn addition, the numerator was analyzed to ensure that the\nsystem remained positive and definite. A simplified approach\nwas adopted by imposing the following conditions:\n∣Sl+1∣∣cl+1,ji∣≥2Pl+1c2\nl+1,ji,\n∣Sl+1∣≥2Pl+1∣cl+1,ji∣,\n∣cl+1,ji∣≤∣Sl+1∣\n2Pl+1\n.\n(24)\nThe off-diagonal terms were examined to derive a less re-\nstrictive upper bound for the variable Cl. The radius Ri was\nincreased to produce a more conservative estimate, which,\nalthough broader, facilitated the inclusion of the off-diagonal\nterms within the inequality framework.\n=\ndl+1\n∑\nj=1\nλl+1,j (∣Sl+1∣∣cl+1,ji∣−2Pl+1c2\nl+1,ji)\n+ 2∣Pl+1∣\ndl\n∑\nz=1,z≠i\nRRRRRRRRRRR\ndl+1\n∑\nj=1\nλl+1,jcl+1,jicl+1,jz\nRRRRRRRRRRR\n,\n≤\ndl+1\n∑\nj=1\nλl+1,j (∣Sl+1∣∣cl+1,ji∣−2Pl+1c2\nl+1,ji)\n+ 2∣Pl+1∣\ndl\n∑\nz=1,z≠i\ndl+1\n∑\nj=1\nλl+1,j∣cl+1,jicl+1,jz∣,\n=\ndl+1\n∑\nj=1\nλl+1,j (∣Sl+1∣∣cl+1,ji∣−2Pl+1c2\nl+1,ji)\n+ 2∣Pl+1∣\ndl+1\n∑\nj=1\nλl+1,j∣cl+1,ji∣\ndl\n∑\nz=1,z≠i\n∣cl+1,jz∣,\n=\ndl+1\n∑\nj=1\nλl+1,j (∣Sl+1∣∣cl+1,ji∣−2Pl+1c2\nl+1,ji)\n+ 2∣Pl+1∣\ndl+1\n∑\nj=1\nλl+1,j∣cl+1,ji∣(\ndl\n∑\nz=1\n∣cl+1,jz∣−∣cl+1,ji∣),\n≤\ndl+1\n∑\nj=1\nλl+1,j (∣Sl+1∣∣cl+1,ji∣−2Pl+1c2\nl+1,ji)\n+ 2∣Pl+1∣\ndl+1\n∑\nj=1\nλl+1,j∣cl+1,ji∣(\n2\n∣Sl+1∣−∣cl+1,ji∣),\n=\ndl+1\n∑\nj=1\nλl+1,j\n⎡⎢⎢⎢⎢⎣\n∣Sl+1∣∣cl+1,ji∣−2Pl+1c2\nl+1,ji\n+ 2∣Pl+1∣∣cl+1,ji∣(\n2\n∣Sl+1∣−∣cl+1,ji∣)\n⎤⎥⎥⎥⎥⎦\n.\n(25)\nWhere the inner term needed to be constrained:\n0 ≤∣Sl+1∣∣cl+1,ji∣−2Pl+1c2\nl+1,ji\n+ 2∣Pl+1∣∣cl+1,ji∣(\n2\n∣Sl+1∣−∣cl+1,ji∣),\n=∣Sl+1∣∣cl+1,ji∣−2Pl+1c2\nl+1,ji\n+ 2∣Pl+1∣(2∣cl+1,ji∣\n∣Sl+1∣\n−c2\nl+1,ji),\n=∣Sl+1∣∣cl+1,ji∣+ 4∣Pl+1∣\n∣Sl+1∣∣cl+1,ji∣\n−2(Pl+1 + ∣Pl+1∣)c2\nl+1,ji,\n0 ≤∣Sl+1∣+ 4∣Pl+1∣\n∣Sl+1∣−2(Pl+1 + ∣Pl+1∣)∣cl+1,ji∣\n∣cl+1,ji∣≤\n∣Sl+1∣2 + 4∣Pl+1∣\n2(∣Pl+1∣+ Pl+1)∣Sl+1∣.\n(26)\nGiven the additional element-wise constraint, it was observed\nthat there were two situations in which the constraint became\nirrelevant. The first scenario occurred when Ll+1ml+1 ≤0,\nand the second arose when Ll+1 + ml+1 = 0. This condition\nwas satisfied, for instance, in the case of a ReLU activation\nfunction, where L = 1 and m = 0.\nQED\n\n\nIEEE TRANSACTIONS ON XXX XXXX, XXX, XXX, SEPTEMBER 2023\n5\nC. First layer\nFinally, below is the derivation of the constraints for the\nparameters defined in the first-row block of the LMI.\nTheorem III.3. The parameter C1 must be element-wise\nupper bounded by\n∣c1,ji∣≤\n(L2 −a2\ni −∣ai∣∣bi∣)∣S1∣\nd1λ1,j (∣S1∣2 + 4∣P1∣)\n,\n(27)\nProof. This block represented the parameters where l = 1:\n{A⊺A −L2I −2L1m1C⊺\n1 Λ1C1,(L1 + m1)C⊺\n1 Λ1,A⊺B}.\nWhich gave the following Gershgorin discs for ∀i{1⋯mx}\n(where mx = mn):\naii =a2\ni −L2 −2(L1m1)\nd1\n∑\nj=1\nλ1,jc2\n1,ji,\n(28)\nRi =∣ai∣∣bi∣+ ∣L1 + m1∣\nd1\n∑\nj=1\nλ1,j∣c1,ji∣\n+ 2∣L1m1∣\ndx\n∑\nz=1,z≠i\nRRRRRRRRRRR\nd1\n∑\nj=1\nλ1,jc1,jic1,jz\nRRRRRRRRRRR\n.\n(29)\nFor which the upper bound constraint was thus:\nϵ1,1,i,max =a2\ni −L2 −2P1\nd1\n∑\nj=1\nλ1,jc2\n1,ji + ∣ai∣∣bi∣\n+ ∣S1∣\nd1\n∑\nj=1\nλ1,j∣c1,ji∣\n+ 2∣P1∣\ndx\n∑\nz=1,z≠i\nRRRRRRRRRRR\nd1\n∑\nj=1\nλ1,jc1,jic1,jz\nRRRRRRRRRRR\n.\n(30)\nApplying the negative-semi definitive constraint, the following\nconstraint was derived:\n0 ≥a2\ni −L2 −2P1\nd1\n∑\nj=1\nλ1,jc2\n1,ji + ∣ai∣∣bi∣+ ∣S1∣\nd1\n∑\nj=1\nλ1,j∣c1,ji∣\n+ 2∣P1∣\ndx\n∑\nz=1,z≠i\nRRRRRRRRRRR\nd1\n∑\nj=1\nλ1,jc1,jic1,jz\nRRRRRRRRRRR\n,\n≤a2\ni −L2 −2P1\nd1\n∑\nj=1\nλ1,jc2\n1,ji + ∣ai∣∣bi∣+ ∣S1∣\nd1\n∑\nj=1\nλ1,j∣c1,ji∣\n+ 2∣P1∣\ndx\n∑\nz=1,z≠i\nd1\n∑\nj=1\nλ1,j∣c1,jic1,jz∣,\n=a2\ni −L2 −2P1\nd1\n∑\nj=1\nλ1,jc2\n1,ji + ∣ai∣∣bi∣+ ∣S1∣\nd1\n∑\nj=1\nλ1,j∣c1,ji∣\n+ 2∣P1∣\nd1\n∑\nj=1\nλ1,j∣c1,ji∣\ndx\n∑\nz=1,z≠i\n∣c1,jz∣,\n=a2\ni −L2 + ∣ai∣∣bi∣+\nd1\n∑\nj=1\nλ1,j\n⎛\n⎝∣S1∣∣c1,ji∣−2P1c2\n1,ji\n+ 2∣P1∣∣c1,ji∣\ndx\n∑\nz=1,z≠i\n∣c1,jz∣⎞\n⎠,\n≤a2\ni −L2 + ∣ai∣∣bi∣+\nd1\n∑\nj=1\nλ1,j\n⎡⎢⎢⎢⎢⎣\n∣S1∣∣c1,ji∣−2P1c2\n1,ji\n+ 2∣P1∣∣c1,ji∣( 2\n∣S1∣−∣cl+1,ji∣)\n⎤⎥⎥⎥⎥⎦\n,\n=\nd1\n∑\nj=1\n⎡⎢⎢⎢⎢⎣\na2\ni −L2 + ∣ai∣∣bi∣\nd1\n+ λ1,j\n⎛\n⎝∣S1∣∣c1,ji∣−2P1c2\n1,ji\n+ 2∣P1∣∣c1,ji∣( 2\n∣S1∣−∣c1,ji∣)⎞\n⎠\n⎤⎥⎥⎥⎥⎦\n. (31)\nThe constraint L1m1 ≤0 was enforced to ensure the solvability\nof the system. Consequently, the system of equations was\nformulated as follows:\n0 ≥a2\ni −L2 + ∣ai∣∣bi∣\nd1\n+ λ1,j\n⎛\n⎝∣S1∣∣c1,ji∣−2P1c2\n1,ji\n+ 2∣P1∣∣c1,ji∣( 2\n∣S1∣−∣c1,ji∣)⎞\n⎠,\n=a2\ni −L2 + ∣ai∣∣bi∣\nd1\n+ λ1,j (∣S1∣∣c1,ji∣+ 4∣P1∣\n∣S1∣∣c1,ji∣),\n∣c1,ji∣≤L2 −a2\ni −∣ai∣∣bi∣\nd1λ1,j\n1\n∣S1∣+ 4∣P1∣\n∣S1∣\n,\n=\n(L2 −a2\ni −∣ai∣∣bi∣)∣S1∣\nd1λ1,j (∣S1∣2 + 4∣P1∣)\n.\n(32)\nWhich then induced the inequality that L2 −a2\ni −∣ai∣∣bi∣≥0,\nwhich in turn gave the consrtaints that ai ∈(−L,L) with ∣bi∣<\nL2−a2\ni\n∣ai∣. This thus completed the LMI constraints.\nQED\nGiven all the derived constraints, the complete set of con-\nstraints of the neural network was listed in Table II.\nThe generalized versions of the equations could additionally\nbe presented in matrix forms in Table II, where the absolute\nvalue function is applied element-wise, i.e., ∣A∣= {∣aij∣}, for\nsimplified computation and practicality, the diagonal matrices\nΛl,A,B are represented as column vectors where the elements\nare the diagonal values.\nD. Weighted norm constraint\nFor a weighted ℓ1-norm, it was desired to derive an unpa-\nrameterized optimization formulation scheme for xi while en-\nsuring the system remained upper bounded. Where vi > 0,∀i.\na ≤∥x∥1,v =\nn\n∑\ni=1\nvi∣xi∣\n(33)\nThe reparameterization ∣xi∣≤± a\nn\n1\nvi was introduced, such that:\na ≤\nn\n∑\ni=1\nvi∣xi∣≤\nn\n∑\ni=1\nvi∣a\nn\n1\nvi\n∣= a\nn\nn\n∑\ni=1\n1 = a\n(34)\nWhere the constraint,\nxi = a\nn\n1\nvi\npi,\n(35)\nwhere pi ∈(−1,1). As such, to parameterize xi, the op-\ntimization parameter for the network becomes optimizing\n\n\nIEEE TRANSACTIONS ON XXX XXXX, XXX, XXX, SEPTEMBER 2023\n6\nTable I: General LMI Condensed Constraints\nParameter\nInequality\nConstraint\nIndexing\nSl\n=\nLl + ml\n∀l{1, ⋯, n}\nPl\n=\nLlml\n∀l{1, ⋯, n}\nλn,i\n≥\nb2\ni +aibi\n2−∣Sn∣∑\ndn−1\nj=1\n∣cn,ij∣\n∀i{1, ⋯, dn}\n∥Cn,i∥1\n<\n2\n∣Sl∣\n∀i{1, ⋯, dl}∀l{1, ⋯, n}\nλl,i\n≥\n∑\ndl+1\nj=1\nλl+1,j(∣Sl+1∣∣cl+1,ji∣−2Pl+1c2\nl+1,ji)+2∣Pl+1∣∑\ndl\nz=1,z≠i ∣∑\ndl+1\nj=1\nλl+1,jcl+1,jicl+1,jz∣\n2−∣Sl∣∑\ndl−1\nj=1\n∣cl,ij∣\n∀i{1, ⋯, dl}∀l{1, ⋯, n −1}\n∣cl,ij∣\n≤\n∣Sl∣2+4∣Pl∣\n2(∣Pl∣+Pl)∣Sl∣\n∀i{1, ⋯, dl}∀j{1, ⋯, dl−1}∀l{2, ⋯, n}\n∣c1,ij∣\n≤\n(L2−a2\ni −aibi)∣S1∣\nd1λ1,i(∣S1∣2+4∣P1∣)\n∀i{1, ⋯, dx}\n∣ai∣\n∈\n(0, L)\n∀i{1, ⋯, dx}\n∣bi∣\n∈\n[0, L2−a2\ni\n∣ai∣\n)\n∀i{1, ⋯, dx}\nTable II: General LMI Matrix Condensed Constraints\nParameter\nInequality\nConstraint\nIndexing\nSl\n=\nLl + ml\nPl\n=\nLlml\nDl\n=\n[∥Cl,1∥1\n⋯\n∥Cl,ml∥1]\n⊺\n∀l{1, ⋯, n}\nΛn\n≥\nB2+∣A∣∣B∣\n2−∣Sn∣Dn\nDl\n<\n2\n∣Sl∣\n∀l{1, ⋯, n}\nQl\n=\nC⊺\nl diag(Λl)Cl\nΛl\n≥\nΛ⊺\nl+1(∣Sl+1∣∣Cl+1∣−2Pl+1C○2\nl+1)+2∣Pl+1∣1⊺∣Ql+1−diag(diag(Ql+1))∣\n2−∣Sl∣Dl\n∀l{1, ⋯, n −1}\n∣Cl∣\n≤\n∣Sl∣2+4∣Pl∣\n2(∣Pl∣+Pl)∣Sl∣\n∀l{2, ⋯, n}\n∣C1∣\n≤\n∣S1∣(L2−A2−∣A∣∣B∣)\nd1(∣S1∣2+4∣P1∣)\nΛ−1\n1\n∣A∣\n∈\n(0, L)\n∣B∣\n∈\n[0, L2\n∣A∣−∣A∣)\npi, where pi = tanh(wi), where wi was an unconstrained\noptimization parameter. As demonstrated by the normalization\nfactor ∂xi\n∂pi ∝O( 1\nnvi ), which implied that the gradients of xi\nbecame proportionally smaller as the dimension of the vector\nbecame smaller. Small or vanishing gradients could cause\nproblems for large and deeper networks.\nE. Elementwise vs. row constraint bound switching\n1) Cl constraints: For the matrices Cl with l ∈{2,⋯,n},\ntwo simultaneous constraints were imposed on the system: the\nrow-wise and element-wise constraints. In this context, the\nobjective was to derive the upper bounds for the values of Cl\nthat the optimization would be based on.\nThe following constraint was derived from the norm con-\nstraints established in the unparameterized optimization for-\nmulation given by Equation (35), where the upper bound was\nset as a =\n2\n∣Sl∣and vi = 1. The goal, therefore, was to identify\nthe conditions under which the row-wise element constraint\nwould dominate over the overall element-wise constraint.\n2\n∣Sl∣dl−1\n≤\n∣Sl∣2 + 4∣Pl∣\n2(∣Pl∣+ Pl)∣Sl∣,\ndl−1 ≥4(∣Pl∣+ Pl)\n∣Sl∣2 + 4∣Pl∣\n.\n(36)\nThis thus informed us that when Pl ≤0 (∀l,dl ≥0), the\nelement-wise constraint would always be greater than the\nelement-wise, and if Pl > 0 and,\ndl−1 ≥\n8Pl\n∣Sl∣2 + 4Pl\n.\n(37)\nBy examining the maximum value of the bound, it was found\nthat, due to the equation’s symmetry concerning Ll and ml,\nsolving for either the optimal value of ml or Ll led to the\noptimal solution. This symmetry implied that both parameters\ncontributed equivalently to the system, and thus, optimizing\none in isolation was sufficient to determine the overall optimal\nconfiguration.\n∂\n∂ml\n8Llml\n(Ll + ml)2 + 4Llml\n= 8Ll(Ll −ml)(Ll + ml)\n(L2\nl + 6Llml + m2\nl )\n2 ,\n(38)\nsolving for 0 the optimal value was obtained when ml =\n{−Ll,Ll}, where only the ml = Ll solution was kept due\nto the Pl > 0 constraint. Which gave the solution that (when\nml = Ll, Sl = 2L,Pl = L2\nl ):\n2\n∣Sl∣dl−1\n≥\n∣Sl∣2 + 4∣Pl∣\n2(∣Pl∣+ Pl)∣Sl∣,\n1\n∣Ll∣dl−1\n≥4L2\nl + 4L2\nl\n8L2\nl ∣Ll∣,\n\n\nIEEE TRANSACTIONS ON XXX XXXX, XXX, XXX, SEPTEMBER 2023\n7\n1\n∣Ll∣dl−1\n≥\n1\n∣Ll∣.\n(39)\nThis demonstrated that even in the specific condition when\nml = Ll and dl ≤1, the element and row-wise constraints\nwould be equivalent to each other. This implied that the row-\nwise constraint would always be smaller than the element-wise\nconstraint and should thus be the only one considered when\nconstraining Cl for ∀{2,⋯,n}.\n2) C1 constraints: Upon analyzing the constraints derived\nfor C1, it was observed that a mutual dependence existed\nbetween C1 and Λ1. Specifically, the definition of C1 ne-\ncessitated the prior specification of Λ1, and conversely, the\ndetermination of Λ1 was contingent upon the specification of\nC1. This interdependence introduced significant complexity in\nderiving an appropriate parameterization for C1. As a result,\nan additional constraint was imposed on C1 to address this\nissue, such that:\n∣Sl∣\ndx\n∑\nj=1\n∣c1,ij∣≤1,\n(40)\nWhich thus enforced the constraint that,\nGl =Λ⊺\nl (∣Sl∣∣Cl∣−2PlC○2\nl )\n+ 2∣Pl∣1⊺∣Ql −diag(diag(Ql))∣,\n(41)\nλ1,i ≥\nG2\n2 −∣Sl∣∑dx\nj=1 ∣c1,ij∣\n≥G2,\n(42)\nwhere, Gl represented the numerator of the Λl parameteri-\nzation. Enforcing this additional constraint on the row norm\nof C1 thus imposed an upper bound of Λ1, which no longer\ncontained a dependence on C1, breaking the cyclical parame-\nterization.\nF. LMI parameterization\nThe eigenvalue distribution of the LMI was displayed below\nin Figure 1 (The eigenvalue range was truncated to 10 times\nthe quartile range; however, some of the eigenvalues have\nreached a magnitude of −1011). To generate this distribution,\nall the parameters, the weights Cl ( parameterized by pl), and\nthe biases bl were initialized with a uniform distribution. The\nbiases and weights were initialized using the standard Kaiming\ninitialization scheme, where the weights used tanh gains (i.e.,\nscaling of 1 for tanh [22]) given that the variables pl were\nconstrained by tanh.\nThe constraints above, when implemented, thus generated\nthe following example of Gershgorin circles for the LMI\nillustrated in Figure 2. The Figure demonstrates that the\nGershgorin circles were all constrained on the negative real\nplane.\nIt was also interesting to observe that due to the recursive\nnature of the Λl parameterization, the Gershgorin circles ended\nup encapsulating each other most of the time (this is not a\ngeneral statement).\nFor the sake of completeness, the L and m constants of\nthe activation functions defined in PyTorch (assuming default\nvalues if not specified) were derived and defined in Table III.\nIt should be noted that the Hardshrink and RReLU could not\nFigure 1: Eigenvalue distribution\nFigure 2: LMI Gershgorin Circles\nbe used due to their infinite L,m constants; Hardshink has\ninfinite L,m due to its noncontinuous piece-wise definition,\nand PReLU due to its stochastic definition, which no longer\nmade it’s L,m computable. Where,\nerfc(z) = 1 −erf(z),\n(43)\nerf(z) =\n2\n√π ∫\nz\n0 e−t2dt.\n(44)\nIV. EXPERIMENT\nBased on the designed network constraints, a network\nwas thus generated. To run such a network due to the co-\ndependence of the Λl,Ll and ml from the next layer and the\nfirst layer, the evaluation of the network needed to be run in\ntwo passes, a backward pass which computed the Λl and Cl\nparameters, as illustrated in Figure 3, and then, the forward\npass performed the inferences using the computed parameters\nas a standard residual network as illustrated in 4. It should\nbe noted that it was not possible to make use of techniques\n\n\nIEEE TRANSACTIONS ON XXX XXXX, XXX, XXX, SEPTEMBER 2023\n8\nTable III: Convexity constants of the element-wise activation functions in PyTorch\nActivation Function\nL\nm\nS\nP\nELU (α = 1) [23]\nmax(1, α)\n0\nmax(1, α)\n0\nHardshrink [24]\n∞\n0\n∞\n∞\nHardsigmoid [25]\n1\n6\n0\n1\n6\n0\nHardtanh [26]\n1\n0\n1\n0\nHardswish [27]\n1.5\n-0.5\n1\n-0.75\nLeakyReLU (α = 1e−2) [28]\n1\nα\n1 + α\nα\nLogSigmoid\n1\n0\n1\n0\nPReLU (α = 1\n4 ) [29]\n1\nα\n1 + α\nα\nReLU [30]\n1\n0\n1\n0\nReLU6 [31]\n1\n0\n1\n0\nRReLU [32]\n∞\n−∞\n∞\n∞\nSELU [33]\nα × scale ≈1.758099341\n0\nα × scale ≈1.758099341\n0.0\nCELU [34]\n1\n0\n1\n0\nGELU [35]\nerfc(1)\n2\n−\n1\ne√π\n1\n2 (erf(1) + 1) +\n1\ne√π\n1\n(e√π(erf(1)+1)+2)(e√πerfc(1)−2)\n4e2π\n≈1.128904145\n≈−0.1289041452\n≈−0.145520424\nSigmoid [36]\n1\n0\n1\n0\nSiLU [37]\n1.099839320\n-0.09983932013\n1\n-0.1098072100\nSoftplus [38]\n1\n0\n1\n0\nMish (α ≥1\n2 ) [39]\n1.199678640\n-0.2157287822\n0.8060623125\n-0.2204297485\nSoftshrink [24]\n1\n0\n1\n0\nSoftsign [40]\n1\n0\n1\n0\nTanh [36]\n1\n0\n1\n0\nTanhshrink\n1\n0\n1\n0\nThreshold\n1\n0\n1\n0\nFigure 3: Backwards pass\nFigure 4: Forward pass\nsuch as a batch normalization [17], [41], [42], which is a\ncommon practice in more modern ResNet architectures. This\nwas because normalization was not a constrained L-Lipschitz\nformulation as the normalized features were computed as [13],\n[43]:\nˆx(k) = x(k) −E[x(k)]\n√\nVar[x(k)]\n,\n(45)\nWhich could be represented as a linear layer where,\nCb = diag(\n√\nVar[x(1)],⋯,\n√\nVar[x(d)])\n−1\n,\n(46)\nbb = −[\nE[x(1)]\n√\nVar[x(1)]\n⋯\nE[x(d)]\n√\nVar[x(d)]]\n⊺\n.\n(47)\nWhere it would only be in particular conditions that the batch\nnormalization would follow the constraints posed by Table II;\nthis is due to the variance scaling term being very hard to\ncontrol and is defined by the dataset that is inputted into the\nsystem.\nTo test the network’s capabilities, it was initially tested\non a straightforward dataset to fit y =\n1\n2 sin(x) on x ∈\n(−2π,2π), which is a\n1\n2\nLipchitz bounded function as\narg maxx ∂\n∂x\n1\n2 sin(x) = 1\n2arg maxx cos(x) = 1\n2, which should\nthus make it possible to train the network on. However, it\nwas noticed that no matter what optimizer, activation function,\nsize or number of hidden layers, learning rate, or other hyper-\nparameters used, the system would be unable to fit the function\nto any degree of accuracy using the MSE loss function. This\n\n\nIEEE TRANSACTIONS ON XXX XXXX, XXX, XXX, SEPTEMBER 2023\n9\nis illustrated from the output results in Figures 5 and 6.\nFigure 5: Trained L-Lipschitz network output over multiple\noptimizers\nFigure 6: MSE training loss over multiple optimizers\nThe network’s output looked like a single regression line, and\nno expressiveness of the network could be observed. For a\nminimum error to fit with this line, it would have to be a line\ndefined as:\nLl = 1\n4π ∫\n2π\n−2π (ax + b −sin(x))2dx,\n= 1\n6 (4a(2π2a + 3) + 6b2 + 3).\n(48)\nWhose minimum would be defined when a = −\n3\n4π2 and b = 0,\nwith a total error of Ll = 1\n2 −\n3\n4π2 . After further inspection\nof the network, the main culprit in the decay issue was C1’s\nmagnitude as the Λ1 magnitude in the network became very\nlarge, which caused an over-constraining of the C1 matrix\nparameter. Given that the Gershgorin circle theorem is only\nan approximation of the eigenvalue locations, this caused\nthe overall network’s compounding approximations to over-\nconstrain the network and thus disable the non-linear portion\nof the system as such, the network comes the simple y ≈Ax\nformula, where A is the parameterized diagonal matrix. It is\nthus sadly noted that this type of network with the current\ntype of parameterization for the weights and biases of the\nsystem does not function as a universal function approximator.\nAs such this paper is only able to elaborate on the current\nmethodology for solving the LMI using the Gershgorin circle\nfor more complicated general LMI structures; however, it\nshould be noted that if the LMI follows a more standard\nmatrix structure such as a tri-diagonal form [10] common in\na standard Feedforward Neural Network (FNN) or the likes it\nis possible to derive more exact eigenvalue constraints on the\nsystem.\nV. CONCLUSION\nThis study rigorously derived constraints for the pseudo-tri-\ndiagonal matrix LMI representing a residual network. Given\nthe absence of explicit eigenvalue computations for the tri-\ndiagonal matrix with off-diagonal elements, the Gershgorin\ncircle theorem was employed to approximate the eigenvalue\nlocations of this complex recursive system. The system was\ndecomposed into three distinct blocks, and weight parameter-\nizations were systematically derived and summarized in Table\nII.\nA two-step process was detailed once the constraints were\nestablished and the network was constructed. Due to the resid-\nual network’s recursive nature, the normalization parameters\nneeded to be computed and propagated in advance to enable\nthe creation of layer weight parameterizations. This stage was\ndefined as the backward pass. The forward pass involved\nperforming inference on the network.\nUpon evaluating the implemented network, it was observed\nthat the Gershgorin circle approximations caused the normal-\nization factors of the inner layers to deactivate the network’s\nnon-linear components. Consequently, based on the Gersh-\ngorin formulation, the final implementation proved ineffective\nand unsuitable as a functional approximation. This study\nestablishes a foundation for future research into alternative\neigenvalue approximations and refined parameterization strate-\ngies, advancing robust deep learning architectures’ theoretical\nand practical development.\nREFERENCES\n[1] M. Inkawhich, Y. Chen, and H. Li, “Snooping attacks on deep\nreinforcement\nlearning,”\nProceedings\nof\nthe\nInternational\nJoint\nConference on Autonomous Agents and Multiagent Systems, AAMAS,\nvol. 2020-May, pp. 557–565, 5 2019. [Online]. Available: https:\n//arxiv.org/abs/1905.11832v2\n[2] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing\nadversarial examples,” 3rd International Conference on Learning\nRepresentations, ICLR 2015 - Conference Track Proceedings, 12 2014.\n[Online]. Available: https://arxiv.org/abs/1412.6572v3\n[3] Y. Tsuzuku, I. Sato, and M. Sugiyama, “Lipschitz-margin training:\nScalable\ncertification\nof\nperturbation\ninvariance\nfor\ndeep\nneural\nnetworks,” CoRR, vol. abs/1802.04034, 2018. [Online]. Available:\nhttp://arxiv.org/abs/1802.04034\n[4] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida, “Spectral\nnormalization for generative adversarial networks,” 6th International\nConference on Learning Representations, ICLR 2018 - Conference\nTrack Proceedings, 2 2018. [Online]. Available: https://arxiv.org/abs/\n1802.05957v1\n\n\nIEEE TRANSACTIONS ON XXX XXXX, XXX, XXX, SEPTEMBER 2023\n10\n[5] P. L. Bartlett, D. J. Foster, and M. Telgarsky, “Spectrally-normalized\nmargin bounds for neural networks,” Advances in Neural Information\nProcessing Systems, vol. 2017-December, pp. 6241–6250, 6 2017.\n[Online]. Available: https://arxiv.org/abs/1706.08498v2\n[6] B. Prach and C. H. Lampert, “Almost-orthogonal layers for efficient\ngeneral-purpose lipschitz networks,” 8 2022. [Online]. Available:\nhttps://arxiv.org/abs/2208.03160v2\n[7] L.\nMeunier,\nB.\nJ.\nDelattre,\nA.\nAraujo,\nand\nA.\nAllauzen,\n“A\ndynamical system perspective for lipschitz neural networks,” pp.\n15 484–15 500, 6 2022. [Online]. Available: https://proceedings.mlr.\npress/v162/meunier22a.html\n[8] A. Araujo, A. Havens, B. Delattre, A. Allauzen, and B. Hu, “A unified\nalgebraic perspective on lipschitz neural networks,” 3 2023. [Online].\nAvailable: http://arxiv.org/abs/2303.03169\n[9] L. Meunier, B. Delattre, A. Araujo, and A. Allauzen, “A dynamical\nsystem perspective for lipschitz neural networks,” Proceedings of\nMachine Learning Research, vol. 162, pp. 15 484–15 500, 10 2021.\n[Online]. Available: https://arxiv.org/abs/2110.12690v2\n[10] Y. Xu and S. Sivaranjani, “Eclipse: Efficient compositional lipschitz\nconstant estimation for deep neural networks,” 4 2024. [Online].\nAvailable: https://arxiv.org/abs/2404.04375v2\n[11] A.\nSandryhaila\nand\nJ.\nM.\nF.\nMoura,\n“Eigendecomposition\nof\nblock\ntridiagonal\nmatrices,”\n6\n2013.\n[Online].\nAvailable:\nhttps:\n//arxiv.org/abs/1306.0217v1\n[12] E. Agarwal, S. Sivaranjani, V. Gupta, and P. Antsaklis, “Sequential\nsynthesis of distributed controllers for cascade interconnected systems,”\nProceedings of the American Control Conference, vol. 2019-July, pp.\n5816–5821, 7 2019.\n[13] H. Gouk, E. Frank, B. Pfahringer, and M. J. Cree, “Regularisation\nof\nneural\nnetworks\nby\nenforcing\nlipschitz\ncontinuity,”\nMachine\nLearning,\nvol.\n110,\npp.\n393–416,\n2\n2021.\n[Online].\nAvailable:\nhttp://link.springer.com/10.1007/s10994-020-05929-w\n[14] S. Aziznejad, H. Gupta, J. Campos, and M. Unser, “Deep neural\nnetworks with trainable activations and controlled lipschitz constant,”\nIEEE\nTransactions\non\nSignal\nProcessing,\nvol.\n68,\npp.\n4688–\n4699, 1 2020. [Online]. Available: http://arxiv.org/abs/2001.06263http:\n//dx.doi.org/10.1109/TSP.2020.3014611\n[15] J. Bear, A. Pr¨ugel-Bennett, and J. Hare, “Rethinking deep thinking:\nStable learning of algorithms using lipschitz constraints,” 10 2024.\n[Online]. Available: https://arxiv.org/abs/2410.23451v1\n[16] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning\nfor image recognition,” Proceedings of the IEEE Computer Society\nConference\non\nComputer\nVision\nand\nPattern\nRecognition,\nvol.\n2016-December, pp. 770–778, 12 2015. [Online]. Available: https:\n//arxiv.org/abs/1512.03385v1\n[17] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi, “Inception-v4,\ninception-resnet and the impact of residual connections on learning,”\n31st AAAI Conference on Artificial Intelligence, AAAI 2017, pp. 4278–\n4284, 2 2016. [Online]. Available: https://arxiv.org/abs/1602.07261v2\n[18] S. Zagoruyko and N. Komodakis, “Wide residual networks,” British\nMachine Vision Conference 2016, BMVC 2016, vol. 2016-September,\npp. 87.1–87.12, 5 2016. [Online]. Available: https://arxiv.org/abs/1605.\n07146v4\n[19] J. Hu, L. Shen, S. Albanie, G. Sun, and E. Wu, “Squeeze-and-excitation\nnetworks,” IEEE Transactions on Pattern Analysis and Machine\nIntelligence, vol. 42, pp. 2011–2023, 9 2017. [Online]. Available:\nhttps://arxiv.org/abs/1709.01507v4\n[20] S. Xie, R. Girshick, P. Doll´ar, Z. Tu, and K. He, “Aggregated\nresidual transformations for deep neural networks,” Proceedings - 30th\nIEEE Conference on Computer Vision and Pattern Recognition, CVPR\n2017, vol. 2017-January, pp. 5987–5995, 11 2016. [Online]. Available:\nhttps://arxiv.org/abs/1611.05431v2\n[21] M. Fazlyab, A. Robey, H. Hassani, M. Morari, and G. J. Pappas, Efficient\nand accurate estimation of lipschitz constants for deep neural networks.\nRed Hook, NY, USA: Curran Associates Inc., 2019.\n[22] S. K. Kumar, “On weight initialization in deep neural networks,” 4\n2017. [Online]. Available: https://arxiv.org/abs/1704.08863v2\n[23] D.-A. Clevert, T. Unterthiner, and S. Hochreiter, “Fast and accurate\ndeep\nnetwork\nlearning\nby\nexponential\nlinear\nunits\n(elus),”\n4th\nInternational Conference on Learning Representations, ICLR 2016\n-\nConference\nTrack\nProceedings,\n11\n2015.\n[Online].\nAvailable:\nhttp://arxiv.org/abs/1511.07289\n[24] H. F. Cancino-De-Greiff, R. Ramos-Garcia, and J. V. Lorenzo-Ginori,\n“Signal de-noising in magnetic resonance spectroscopy using wavelet\ntransforms,” Concepts in Magnetic Resonance, vol. 14, pp. 388–401, 1\n2002. [Online]. Available: https://onlinelibrary.wiley.com/doi/10.1002/\ncmr.10043\n[25] M. Courbariaux, Y. Bengio, and J. P. David, “Binaryconnect: Training\ndeep neural networks with binary weights during propagations,”\nAdvances in Neural Information Processing Systems, vol. 2015-January,\npp. 3123–3131, 11 2015. [Online]. Available: https://arxiv.org/abs/1511.\n00363v3\n[26] R. Collobert, “Large scale machine learning,” Ph.D. dissertation, Uni-\nversit´e de Paris VI, 2004.\n[27] A. Howard, M. Sandler, B. Chen, W. Wang, L. C. Chen, M. Tan,\nG. Chu, V. Vasudevan, Y. Zhu, R. Pang, Q. Le, and H. Adam,\n“Searching for mobilenetv3,” Proceedings of the IEEE International\nConference on Computer Vision, vol. 2019-October, pp. 1314–1324, 5\n2019. [Online]. Available: https://arxiv.org/abs/1905.02244v5\n[28] A. L. Maas, Y. H. Awni, and A. Y. Ng, “Rectifier nonlinearities improve\nneural network acoustic models,” Proceedings of the 30th International\nConference on Machine Learning, vol. 28, 6 2013.\n[29] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectifiers:\nSurpassing\nhuman-level\nperformance\non\nimagenet\nclassification,”\nCoRR,\nvol.\nabs/1502.01852,\n2\n2015.\n[Online].\nAvailable:\nhttp:\n//arxiv.org/abs/1502.01852\n[30] W.\nS.\nMcCulloch\nand\nW.\nPitts,\n“A\nlogical\ncalculus\nof\nthe\nideas immanent in nervous activity,” The Bulletin of Mathematical\nBiophysics,\nvol.\n5,\npp.\n115–133,\n12\n1943.\n[Online].\nAvailable:\nhttps://link.springer.com/article/10.1007/BF02478259\n[31] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,\nT. Weyand, M. Andreetto, and H. Adam, “Mobilenets: Efficient\nconvolutional neural networks for mobile vision applications,” 4 2017.\n[Online]. Available: http://arxiv.org/abs/1704.04861\n[32] B. Xu, N. Wang, T. Chen, and M. Li, “Empirical evaluation of rectified\nactivations in convolutional network,” 5 2015. [Online]. Available:\nhttp://arxiv.org/abs/1505.00853\n[33] G. Klambauer, T. Unterthiner, A. Mayr, and S. Hochreiter, “Self-\nnormalizing neural networks,” Advances in Neural Information Process-\ning Systems, vol. 30, 2017.\n[34] J. T. Barron, “Continuously differentiable exponential linear units,” 4\n2017. [Online]. Available: http://arxiv.org/abs/1704.07483\n[35] D. Hendrycks and K. Gimpel, “Gaussian error linear units (gelus),” 6\n2016. [Online]. Available: https://arxiv.org/abs/1606.08415v5\n[36] H. Sak, A. Senior, and F. Beaufays, “Long short-term memory based\nrecurrent neural network architectures for large vocabulary speech\nrecognition,” 2 2014. [Online]. Available: https://arxiv.org/abs/1402.\n1128v1\n[37] S. Elfwing, E. Uchibe, and K. Doya, “Sigmoid-weighted linear units\nfor neural network function approximation in reinforcement learning,”\nNeural Networks, vol. 107, pp. 3–11, 2 2017. [Online]. Available:\nhttps://arxiv.org/abs/1702.03118v3\n[38] M. Zhou, “Softplus regressions and convex polytopes,” 8 2016.\n[Online]. Available: http://arxiv.org/abs/1608.06383\n[39] D. Misra, “Mish: A self regularized non-monotonic activation function,”\n31st British Machine Vision Conference, BMVC 2020, 8 2019. [Online].\nAvailable: http://arxiv.org/abs/1908.08681\n[40] W. Ping, K. Peng, A. Gibiansky, S. O. Arik, A. Kannan, S. Narang,\nJ. Raiman, and J. Miller, “Deep voice 3: Scaling text-to-speech with\nconvolutional sequence learning,” 6th International Conference on\nLearning Representations, ICLR 2018 - Conference Track Proceedings,\n10 2017. [Online]. Available: http://arxiv.org/abs/1710.07654\n[41] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, “Rethinking\natrous convolution for semantic image segmentation,” 6 2017. [Online].\nAvailable: https://arxiv.org/abs/1706.05587\n[42] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning\nfor image recognition,” Proceedings of the IEEE Computer Society\nConference\non\nComputer\nVision\nand\nPattern\nRecognition,\nvol.\n2016-December, pp. 770–778, 12 2015. [Online]. Available: https:\n//arxiv.org/abs/1512.03385v1\n[43] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift,” 32nd International\nConference on Machine Learning, ICML 2015, vol. 1, pp. 448–456, 2\n2015. [Online]. Available: https://arxiv.org/abs/1502.03167v3\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21279v1.pdf",
    "total_pages": 10,
    "title": "L-Lipschitz Gershgorin ResNet Network",
    "authors": [
      "Marius F. R. Juston",
      "William R. Norris",
      "Dustin Nottage",
      "Ahmet Soylemezoglu"
    ],
    "abstract": "Deep residual networks (ResNets) have demonstrated outstanding success in\ncomputer vision tasks, attributed to their ability to maintain gradient flow\nthrough deep architectures. Simultaneously, controlling the Lipschitz bound in\nneural networks has emerged as an essential area of research for enhancing\nadversarial robustness and network certifiability. This paper uses a rigorous\napproach to design $\\mathcal{L}$-Lipschitz deep residual networks using a\nLinear Matrix Inequality (LMI) framework. The ResNet architecture was\nreformulated as a pseudo-tri-diagonal LMI with off-diagonal elements and\nderived closed-form constraints on network parameters to ensure\n$\\mathcal{L}$-Lipschitz continuity. To address the lack of explicit eigenvalue\ncomputations for such matrix structures, the Gershgorin circle theorem was\nemployed to approximate eigenvalue locations, guaranteeing the LMI's negative\nsemi-definiteness. Our contributions include a provable parameterization\nmethodology for constructing Lipschitz-constrained networks and a compositional\nframework for managing recursive systems within hierarchical architectures.\nThese findings enable robust network designs applicable to adversarial\nrobustness, certified training, and control systems. However, a limitation was\nidentified in the Gershgorin-based approximations, which over-constrain the\nsystem, suppressing non-linear dynamics and diminishing the network's\nexpressive capacity.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}