{
  "id": "arxiv_2502.21051v1",
  "text": "Detection of anomalies in cow\nactivity using wavelet transform\nbased features\nValentin Guien1, Violaine Antoine1, Romain Lardy2,\nIsabelle Veissier2, and Luis E C Rocha3,4\nDOI not yet assigned\nAbstract\nIn Precision Livestock Farming, detecting deviations from optimal or baseline values\n– i.e. anomalies in time series – is essential to allow undertaking corrective actions\nrapidly. Here we aim at detecting anomalies in 24 h time series of cow activity, with\na view to detect cases of disease or oestrus. Deviations must be distinguished from\nnoise which can be very high in case of biological data. It is also important to detect\nthe anomaly early, e.g. before a farmer would notice it visually. Here, we investigate the\nbeneﬁt of using wavelet transforms to denoise data and we assess the performance of\nan anomaly detection algorithm considering the timing of the detection. We developed\nfeatures based on the comparisons between the wavelet transforms of the mean of the\ntime series and the wavelet transforms of individual time series instances. We hypoth-\nesized that these features contribute to the detection of anomalies in periodic time\nseries using a feature-based algorithm. We tested this hypothesis with two datasets\nrepresenting cow activity, which typically follows a daily pattern but can deviate due\nto speciﬁc physiological or pathological conditions. We applied features derived from\nwavelet transform as well as statistical features in an Isolation Forest algorithm. We\nmeasured the distance of detection between the days annotated abnormal by animal\ncaretakers days and the days predicted abnormal by the algorithm. The results show\nthat wavelet-based features are among the features most contributing to anomaly de-\ntection. They also show that detections are close to the annotated days, and often\nprecede it. In conclusion, using wavelet transforms on time series of cow activity data\nhelps to detect anomalies related to speciﬁc cow states. The detection is often ob-\ntained on days that precede the day annotated by caretakers, which oﬀer possibility\nto take corrective actions at an early stage.\nKeywords: Periodic times series, cattle, precision livestock farming, machine learning,\nperformance criteria\n1Université Clermont Auvergne, CNRS, Clermont Auvergne INP, Mines Saint-Étienne, LIMOS, 63000\nClermont-Ferrand, France,\n2Université Clermont Auvergne, INRAE, VetAgro Sup, UMR Herbivores, F-\n63122 Saint-Genès-Champanelle, France,\n3Ghent University, Department of Economics, 9000 Ghent,\nBelgium,\n4Ghent University, Department of Physics and Astronomy, 9000 Ghent, Belgium\nCorrespondence\nValentin.Guien@uca.fr\n1\narXiv:2502.21051v1  [cs.LG]  28 Feb 2025\n\n\n2\nValentin Guien et al.\n1. Introduction\nPrecision Livestock Farming is based on monitoring animals or their environment thanks to\nsensors to support farmers’ decisions (Berckmans, 2014). It requires that deviations from the\noptimal or baseline values – i.e., anomalies in time series – are detected. There is evidence that\nthe behavior of animals is modiﬁed under certain conditions such as oestrus, parturition, distur-\nbances or diseases. The variations in an animal activity during 24 h seem very sensitive to the\ncondition of that animal (Wagner et al., 2021). Still, it remains challenging to detect anomalies\nfrom the 24-hour variations of activity with high sensitivity and accuracy (e.g. Lardy et al., 2023).\nThis is likely due to the high variability between and within individuals (Fuchs et al., 2022), which\nmakes it essential to denoise the data.\nAnomaly detection has been extensively studied (Chandola et al., 2009) across various ﬁelds\nsuch as electricity consumption (Himeur et al., 2021), card fraud (Hilal et al., 2022), healthcare\n(Faverjon and Berezowski, 2018; Paragliola and Coronato, 2018), and agriculture (Catalano et al.,\n2022; Wagner et al., 2021). In some domains, prior knowledge provides insights into the charac-\nteristics of the time series, allowing the choice of speciﬁc algorithms for anomaly detection. In\nthis paper, we focus on the time series of cows’ activity with the following characteristics: the\nactivity follows a regular periodicity but is also highly variable; thus, distinguishing anomalies\nfrom spontaneous variations – i.e. noises – becomes challenging. Providing insight about how\nanomalies manifest in a time series – to explain the decision of the model and to understand the\nonset of the anomaly – can be done by transforming the time series into a set of features and\nusing these features as inputs in a standard anomaly detection algorithm (Jiang et al., 2019).\nFor instance, statistical features can be computed from time series, and a Principal Compo-\nnent Analysis (PCA) can be performed on these features (Hyndman et al., 2015). The study from\nHyndman et al., 2015 detected abnormal time series using the ﬁrst two principal components.\nPCA speeds up computation but comes at the cost of features interpretability. Random Forest\nhas been applied to predict diﬀerent conditions of cows from their daily activity (Lardy et al.,\n2023). This previous study extracted statistical and Fourier transform features from each 24-\nhour time window. The features described intensity (e.g., mean, maximum and mode, quantiles),\nvariability (e.g., standard deviation, root mean square of diﬀerences between successive hours)\nand periodicity (autocorrelations, Fourier transform). The contribution of each feature was regis-\ntered as weight, allowing us to identify which features contribute the most to the classiﬁcation.\nThe use of the Fourier transform may be outperformed by the Discrete Wavelet Transforms\n(DWT) that allow not only a frequency analysis of the signal but also a time-localization of the\nchanges in the time series (Daubechies, 1990; Farge, 1992). DWT is commonly used for data\ncompression (Chiarot and Silvestri, 2023), and by extension, for denoising, which makes these\nmethods promising for processing data on animal behavior.\nWavelet transforms can be incorporated at various stages of an anomaly detection process.\nFor example, the Wavelet-based Anomaly Detector (WAD) (Zhang et al., 2003) applies wavelet\ntransform to time series data to obtain the Trend data and the Residual data. Anomalies are then\nidentiﬁed in the Residual data from days when a value exceeds a threshold determined by the\nstatistical distribution of the Residual data. This approach shows the ability of wavelets to cap-\nture explicit trends in data. However, WAD primarily focuses on detecting anomalies as isolated\npoints, whereas, in many time series applications, anomalies are represented as sequences of\npoints rather than single points, as in the case of the daily ﬂuctuations of the activity of animals.\nThe AutoWave method (Yao et al., 2023) is a regularizing auto-encoder that uses the DWT in\nits frequency regularizer to reduce the latent space, so the anomalies are poorly reconstructed.\nThis method highlights the strength of the wavelet transform in detecting speciﬁc patterns in\ntime series. However, while AutoWave learns features during the training, its input consists of\na sequence of values, limiting the interpretability of the detection due to the lack of explicit\nfeatures.\nIn Precision Livestock Farming, the detection must be sensitive (it captures most of the anom-\nalies) and speciﬁc (it does not produce false alarms) and occurs early so that farmers can make\nmanagement decisions quickly. An animal’s behaviour can change several days before clinical\n2\n\n\nValentin Guien et al.\n3\nsigns of disease are observed (Marchesini et al., 2018; Wottlin et al., 2021) or one day before\novulation in case of oestrus. In this paper, we evaluate the eﬀectiveness of DWT-based features\nin reducing the inherent variability of the time series of cows’ activity while retaining the essen-\ntial information needed to detect anomalies (here, diseases and oestrus ). We also investigate\nways to measure how early detection takes place by measuring the delay between the detec-\ntion and the true anomaly (here, that is the time when caretakers detect a disease or oestrus by\nobserving animals).\n2. Materials and Methods\n2.1. Time series preprocessing and feature generation\n2.1.1. Periodic time series with labels. We consider a data set D composed of time series data\nfor nI individuals. Each individual has their own time series, such that D = [X1, ... , XnI ] where\nXj = [x1,j, ... , xnj,j] ∈Rnj is the time series of individual j (1 ≤j ≤nI) consisting of nj val-\nues. The values of nj can vary because the number of observations diﬀers between individuals.\nWe also assume these time series are periodic with a period length ∆T, i.e. [xi,j, ... , xi+p−1,j]::\n[xi+∆T,j, ... , xi+2∆T−1,j].\nEach value xi,j of the time series is associated with a label yi,j ∈{N, A, F}. If yi,j = N (or\nyi,j = A), then xi,j is labelled as normal (or abnormal). If yi,j = F, the value xi,j is labelled as fuzzy,\nindicating uncertainty regarding the true label. Other labels, i.e. normal and abnormal, typically\ncorrespond to consecutive values, as they represent states that persist longer than the interval\nbetween points in the time series.\n2.1.2. Windows. To follow the evolution in the time series, we consider windows of size q through\nthe whole time series. Each window Wi,j ∈Rq constitute a set of values such that Wi,j =\n[xi,j, ..., xi+q−1,j]. By extension to labels on values, we deﬁne for each window Wi,j a label Yi,j ∈\n{Normal, Abnormal, Fuzzy}. The window is labelled depending on whether the window contains\nat least one abnormal or fuzzy values. If ∃k ∈[[i, i+q−1]] such that yk,j = A, then Yi,j = Abnormal.\nIf ∀k ∈[[i, i + q −1]], yk,j ̸= A, and ∃k ∈[[i, i + q −1]] such that yk,j = F, then Yi,j = Fuzzy. Else\nif ∀k ∈[[i, i + q −1]], yk,j = N then Yi,j = Normal. The windows are labelled considering the\n‘worst-case scenario’. This determination of the windows labels is illustrated in Figure 1.\nWe deﬁne two windows Wi1,j1, Wi2,j2 ∈Rq as consecutive if i1 = i2 −1 and j1 = j2. A series of\nconsecutive windows corresponds to a set of windows [Wi1,j, ... , Win,j] where ∀k ∈[[1, n]], Wik,j\nand Wik+1,j are consecutive. As the windows slide through the time series, all consecutive win-\ndows are identiﬁed. Series of consecutive windows are formed to ensure that they remain con-\ntiguous when selected for the training or testing sets.\n2.1.3. Construction of the training and testing sets. The anomaly detection algorithm is trained\non data that does not contain anomalies, allowing it to learn the normal behavior of the time\nseries. When data in the testing set signiﬁcantly deviate from this learned pattern, the algorithm\nﬂags it as an anomaly. Both training and testing sets are built using windows of the time series,\nconsidering not just the labels but also the sequential nature of the windows.\nLet WAbnormal, WNormal and WFuzzy be the sets of windows labelled as Abnormal, Normal and\nFuzzy, respectively. Let Wtrain and Wtest represent the sets of windows used for building the\ntraining set and the testing set, respectively. All windows in WAbnormal and WFuzzy belong to\nWtest.\nSince we created instances using a sliding window with a small sliding step size, the cases with\nonly a few steps of diﬀerence are naturally similar. To avoid biases when evaluating the quality of\nanomaly detection, such instances, referred to as consecutive windows, are kept together either\nin the training or testing sets.\nThe set WNormal is composed of sets of consecutive windows SWNormal = [SW1, ... , SWl],\nwhere SWk represents a set of consecutive windows labeled as Normal. The testing set Wtest\nis then completed using consecutive windows to create a testing set with a balanced number\nof anomaly and normal instances. Since the size of the sets SWk and the size of WAbnormal de-\npend on the datasets, we approach the balance with the following procedure. While there are\n3\n\n\n4\nValentin Guien et al.\nFigure 1 – Window labelling. If at least one data from an abnormal period is in the win-\ndow, the window is labelled abnormal. If at least one data from a fuzzy period is in the\nwindow, without any abnormal data, the window is labelled fuzzy. Otherwise, the win-\ndow is labelled normal.\nmore abnormal windows than normal windows in Wtest, we randomly select a set of consecutive\nwindows from SWNormal and add all the windows it contains in Wtest. The overall procedure for\nconstructing the training and testing sets is described in Algorithm 1.\nAlgorithm 1 Construction of training set and testing sets\n1: function BuildTrainTest(WNormal, WAbnormal, WFuzzy)\n2:\nInitialize Wtrain ←∅\n3:\nInitialize Wtest ←WAbnormal ∪WFuzzy\n4:\nGet SWNormal from WNormal\n5:\nwhile |Wtest ∩WNormal| < |WAbnormal| do\n6:\nSelect randomly SW from SWNormal\n7:\nfor W ∈SW do\n8:\nWtest ←Wtest ∪{W }\n9:\nend for\n10:\nSWNormal = SWNormal ∖{SW }\n11:\nend while\n12:\nWtrain ←WNormal ∖(Wtest ∩WNormal)\n13:\nreturn Wtrain, Wtest\n14: end function\nThe number of fuzzy windows is not considered when constructing the training and testing\nsets, as fuzzy windows represent sequences where information about the anomalies is uncertain.\n2.1.4. Wavelet based features. The wavelet transform is appropriate for analyzing periodic time\nseries with high variability. Let the window size q = ∆T be chosen to correspond to the length of\nthe period, and let {t1, ... , t∆T} represent the timestamps where each window can start. These\ntimestamps can shift, meaning that a window starting at ti corresponds to measurements taken\nat {ti, ... , t∆T, t1, ... , ti−1}. Figure 2 illustrates the pipeline for creating wavelet-based features.\n4\n\n\nValentin Guien et al.\n5\nFigure 2 – Pipeline for the creation of wavelet-based features.\nLet us consider M = [mt1, ... , mt∆T ] the average period of the normal time series from the\ntraining set Wtrain. Each point mtk is the mean of the values taken at tk in the selected windows\nwhere the timestamp corresponds to t1.\nNext, the DWT is applied to M, yielding the approximated signal ˆM = [ bmt1, ... , bm∆T]. This\nsignal represents the transformed average period, serving as a reference to generate the wavelet-\nbased features.\nLet Wk be a window containing a sequence of values, and let d\nWk denote the DWT applied\non Wk. The objective is to compare the wavelet transform of this window d\nWk with the wavelet\ntransform of the average period. Since Wk starts at timestamp ti, c\nM is shifted to align with ti,\nresulting in a new sequence c\nM′.\nc\nM′ and d\nWk are compared using the Euclidean distance between the two windows. A large (vs\nlow) distance indicates signiﬁcant (vs no signiﬁcant) deviation from the average normal period.\nThe DWT of a signal requires two parameters. The ﬁrst is the choice of the wavelet used for\nthe transformation. Diﬀerent types of wavelet can be grouped into families (Graps, 1995). The\nsecond parameter is the level of approximation of the signal. In a DWT, the signal passes through\na series of low- and high-pass ﬁlters (Shensa, 1992). The level of approximation corresponds to\nhow many times the signal undergoes ﬁltering. The maximum approximation level depends on\nthe chosen wavelet and the length of the signal.\n2.2. Application to disease and oestrus detection from cow behavior\n2.2.1. Activity level data. The data used for the study are time series of dairy cow behavior (Lardy\net al., 2022). Two data sets from experimental farms are used (Table 1). The data concerns cows\nin a barn, with a sensor on their collar tracing their activity. The sensor locates the cow in real-\ntime. Three areas are considered in the barn: the feeding table, the cubicles, and the alleys. The\ncurrent activity of the cow is estimated from its position. If the cow is near the feeding table, we\nassume the cow eats. If the cow is in a cubicle, we assume the cow is resting. If the cow is in\nthe alley, we assume the cow is walking or standing still. A Factorial Analysis of Correspondence\nis performed on the three activities to get their weighted contribution for the hourly activity\n(Veissier et al., 2017). For each cow i and hour h of the day, an Activity Level (ALi(h)) is thus\ncomputed from the duration of activities within that hour:\n5\n\n\n6\nValentin Guien et al.\n(1)\nALi(h) = −0.23×(time spent resting)+0.16×(time spent in alleys)+0.42×(time spent eating)\nThe data is recorded as a time series representing the values of AL(h), where h is the hour of\nthe day. ALi(h) depicts the evolution of the cow’s level of arousal (Veissier et al., 2017). The time\nseries are periodic, and the length of the period is ∆T = 24 (i.e. one day). Speciﬁc states of the\ncow (e.g. disease, oestrus, calving) and events (e.g. mixing, management change, disturbances\ndue to external events such as electric failures) occurring in the barn are recorded manually\nby caretakers. We expect these events to change the cows’ activity, resulting in the behavioral\nanomaly we want to detect in the time series using our proposed methodology (Veissier et al.,\n2017). Figure 3 shows a portion of a time series without anomaly of a certain cow. These time\nseries data contain much noise because the activity can greatly vary between cows and also\nwithin cows between days. In addition, the exact onset of the anomaly is unknown: the caretak-\ners can miss an anomaly or the behavior of a cow can start changing before it becomes visible\nby caretakers.\nTable 1 – Summary information of the datasets.\nDataset 1\nDataset 2\nSource\nINRAE Herbipôle\nINRAE Herbipôle\nExperimentation length\n2 months\n6 months\nStart date\n2015-03-02\n25-10-2018\nEnd date\n2015-04-30\n17-04-2019\nNumber of cows\n28\n28\nNumber of cow × hour\n40 246\n107 665\nRate of missing data\n0.2%\n12.4%\nRate of speciﬁc states\n0.7%\n2.2%\nDays with events are removed from the datasets because the aim is to detect cow states.\nAny day × cow annotated with a speciﬁc state is considered abnormal. Days × cows with an\naverage activity level superior to 1000 are also removed because they correspond to a non-\nfunctioning sensor. When a cow is considered abnormal for a speciﬁc day, we assume ∆τb days\nbefore and ∆τa days after the abnormal day as uncertain days where the cow’s behavior can be\nmodiﬁed (Wagner et al., 2021). These days are annotated as fuzzy days. The number of fuzzy\ndays preceding or following an abnormal day depends on the speciﬁc state. In the case of an\noestrus, we consider ∆τb = 2 and ∆τa = 1. In the case of calving, mastitis, lameness and other\ndiseases, we consider ∆τb = 2 and ∆τa = 7. In the case of accidents and LPS injection, we\nconsider ∆τb = 0 and ∆τa = 7. Finally in the case of acidosis, we consider ∆τa = 0 and ∆τb = 1.\n2.2.2. Features to describe time series. We tested ﬁve families of wavelets: Haar, Daubechies,\nCoiﬂet, Biorthogonal, and Reverse Biorthogonal.\nThe Haar wavelet is the oldest and simplest, deﬁned as a step function. Its simplicity makes\nit an ideal starting point for studying wavelet transforms. Due to its structure, the Haar wavelet\nallows for various levels of approximation, oﬀering a wide range of approximations for time series\ndata. It is commonly used for denoising (Luisier et al., 2009; Pang, 2017) and for performing\nsimilarity searches in time series (Popivanov and Miller, 2002).\nThe Daubechies wavelets (Vonesch et al., 2007) form a family of wavelets characterized\nby speciﬁc mathematical properties related to vanishing moments. These wavelets have been\nwidely applied in various studies, including disease detection, such as Parkinson’s disease (Soumaya\net al., 2019; Zayrit et al., 2020).\nThe Coiﬂet wavelets have been used for anomaly detection in network traﬃc (Lu and Ghor-\nbani, 2008), for which they have been proven to be among the most eﬀective wavelets. Coiﬂet\nwavelets are also used for denoising (Tikkanen, 1999).\n6\n\n\nValentin Guien et al.\n7\nFigure 3 – Activity Level (AL) of Cow n°7163 from Dataset 2 from March 5, 2013 to\nMarch 9, 2013. The green background corresponds to days with no recorded anomaly,\nand the red background corresponds to the day with an anomaly recorded by caretakers.\nBiorthogonal and Reverse Biorthogonal wavelets are less common in the literature on wavelet\ntransform applications. However, Guo et al., 2019 showed the advantages of Biorthogonal wavelets\nwhen dealing with imperfect reconstructions after compression and noise ﬁltering in 3D models.\nBiorthogonal wavelets are also used in similarity searches in time series (Popivanov and Miller,\n2002).\nFor each wavelet family considered, we selected the wavelets and approximation levels so\nthat the DWT is feasible on a window of size 24 with the speciﬁed parameters (e.g., the Haar\nwavelet is used in four features, and the approximation level used ranges from 1 to 4). In total,\n23 wavelet-based features are generated (Table 2).\n2.2.3. Other features. The wavelet-based features are complemented by 27 features derived\nfrom descriptive statistics (Table 3). We use all the features from Lardy et al., 2023, excluding\nthe harmonics from the Fourier transform due to redundancy with the wavelet transforms.\n2.2.4. Anomaly detection algorithm. The anomaly detection algorithm used is Isolation Forest\n(Liu et al., 2008). The algorithm is based on the construction of Isolation Trees. For a given tree,\neach node represents a split on random value of a random feature. Each window of 24 h is iso-\nlated in a root of the tree. The Isolation Forest builds a deﬁned number of Isolation Trees, and\nmeasure the average path length for each window. Windows considered abnormal are those with\nthe smallest path length. The two main parameters to conﬁgure are the number of trees and the\nproportion of window to draw from the training set used to build each tree. We chose this algo-\nrithm for its widespread use, detailed documentation, and minimal parameter tuning. No matter\n7\n\n\n8\nValentin Guien et al.\nTable 2 – Wavelet-based features.\nWavelet name (acronym)\nLevels of approximation\nused for this paper\nHaar\n1,2,3,4\nDaubechies 2 (db2)\n1,2,3\nDaubechies 3 (db3)\n1,2\nCoiﬂet 1 (coif1)\n1,2\nBiorthogonal wavelet 1.3 (bior1.3)\n1,2\nBiorthogonal wavelet 2.2 (bior2.2)\n1,2\nBiorthogonal wavelet 3.1 (bior3.1)\n1,2,3\nReverse biorthogonal wavelet 2.2 (rbio2.2)\n1,2\nReverse biorthogonal wavelet 3.1 (rbio3.1)\n1,2,3\nthe absolute performance of the algorithm, if improvements are observed with executions of\nthe Isolation Forest algorithm with the wavelet-based features compared to executions with-\nout these features, it will indicate that incorporating the wavelet transform is useful to detect\nanomalies in our time series.\nWe varied the number of trees between 50 and 1000, and adjusted the proportion of the\ntraining set used to build each tree from 25% to 100%. Between iterations with 50 trees per\nforest and iterations with 1000 trees per forest, the mean accuracy score only improved by 0.01.\nTo reduce computation time, we kept the default parameters of Isolation Forest from scikit-learn,\ni.e., 100 trees per forest and 100% of the training set used for tree building.\n2.3. Performance of anomaly detection and features’ contributions\nThe performance of the anomaly detection was assessed by calculating recall, precision, ac-\ncuracy and F1-score, at each iteration and with or without wavelet-based features. The True\nPositives are denoted as TP and correspond to the number of abnormal windows detected\nas abnormal. The False Positives are denoted as FP and correspond to the number of normal\nwindows detected as abnormal. The True Negatives are denoted as TN and correspond to the\nnumber of normal windows detected as normal. The False Negatives are denoted as FN and\ncorrespond to the number of abnormal windows detected as normal.\nThe recall score corresponds to the rate of anomalies detected on the total number of anom-\nalies:\n(2)\nrecall =\nTP\nFN + TP\nThe precision score corresponds to the rate of anomaly detections:\n(3)\nprecision =\nTP\nFP + TP\nThe accuracy score corresponds to the rate of correct predictions:\n(4)\naccuracy =\nTP + TN\nTP + FN + TN + FP\nThe F1-score is a harmonic mean of the precision and the recall scores:\n(5)\nF1 =\n2TP\n2TP + FP + FN\nWe compared the results obtained by running the Isolation Forest algorithm with and with-\nout the wavelet-based features to assess the added value of wavelet-based features for anomaly\ndetection. To account for score variability across diﬀerent splits of the data, 70 separations of\ntraining and testing sets were performed. We ran up to 20 iterations of the Isolation Forest for\neach data split. If the average accuracy over all iterations stabilized before the 20 iterations oc-\ncurred (i.e., did not change by more than 0.001 in the last ﬁve iterations), the loop was terminated\n8\n\n\nValentin Guien et al.\n9\nTable 3 – Features derived from descriptive statistics.\nDeﬁnition\nName\nMinimum activity over the 24h\nMinimum\nMaximum activity over the 24 h\nMaximum\nMean activity over the 24h\nMean\nSquare root of the mean of squared activi-\nties over the 24h\nRMS\nStandard deviation of the activity over the\n24h\nSTD\nMean of the standard deviation of the 4\nnon-overlapping 6h windows composing\nthe 24h\nMeanSTD6h\nStandard deviation of the mean of the 4\nnon-overlapping 6h windows composing\nthe 24h\nSTDMean6h\nRoot mean square of successive diﬀerences,\ni.e. the diﬀerences between the activity at\na given hour h and the activity at the next\nhour h + 1\nRMSSD\nMost common value over the 24h\nMode\n10% and 90% quantiles, calculated from the\nvalues that divide the hours into 10 equal\ngroups from lower to higher activity. Q10,\nmaximum values of the lower group; Q90,\nmaximum value of the last but one higher\ngroup\nQ10, Q90\n25%, 50% and 75% quantiles, calculated\nfrom the values which divide the hours into\n4 equal groups from lower to higher activity.\nQ25, maximum values of the lower group;\nQ50, median; Q75, maximum value of the\nlast but one higher group\nQ25, Median, 75\nSymmetry of the activity distribution over\nthe 24h\nSkewness\nTailedness of the activity distribution over\nthe 24h\nKurtosis\nAutocorrelation, i.e. the correlation be-\ntween the activity at any hour h and the ac-\ntivity at hour h + d, where d is a ﬁxed inter-\nval (1h, 2h, ... 11h)\nAutocorr1-11\nearly, and the next train/test split continued. The same method was applied for both scenarios:\nwith and without the wavelet-based features.\nWe used the SHAP measure to evaluate the features’ contribution in the Isolation Forest\nmodel (Lundberg and Lee, 2017). To account for variability in the SHAP results across diﬀerent\niterations, we conducted 10 separations of the training and testing set. For each separation, we\nran 50 iterations of the Isolation Forest. In each iteration, the mean SHAP value was extracted\nfor all the features. At the end of the process, we generated a critical diﬀerence diagram (Demšar,\n2006). The diagram illustrates the average rank of each feature’s contribution to anomaly detec-\ntion when applied on a speciﬁc data set.\n9\n\n\n10\nValentin Guien et al.\n2.4. Measure of the distance of detection\nWe set up a new method to identify abnormal days from the abnormal windows to analyze\nthe distance between the detected and annotated anomalies. For each cow, we consider the\nwhole time series of its observation, regardless of the data in the training or testing set. Since\neach window in the series has a size q = 24, each hour in the series is contained in at most\n24 windows. For each hour, we compute the “predicted state” value. It represents the sum of\nthe windows containing this hour that are considered normal minus the sum of the windows\ncontaining this hour that are considered abnormal, and the whole divided by the number of\nwindows containing this hour. The predicted state value goes from -1 to +1. We then consider\na threshold θ ∈[−1, 1]. If the predicted state value is below θ, the hour is predicted as abnormal.\nIf at least 12 hours are predicted as abnormal for a certain day, the day is deﬁned as abnormal.\nFigure 4 shows the whole time series of a cow with the annotated and the predicted labels for\neach hour.\nThe threshold below which each hour is considered as abnormal is θ = 0. For each day\ndetected as abnormal, we compute the distance of detection, which represents the numbers\nof days of oﬀset with the nearest day annotated as abnormal by caretakers. If this distance is\nnegative, it is detected before. If this distance is positive, it is detected after.\n3. Results & Discussion\n3.1. Improvement in anomaly detection\nAfter computing features for all windows, the correlations between features were checked.\nIf two features were highly correlated (correlation coeﬃcient > 0.9), the feature with the highest\naverage correlation with other features was removed.\nTable 4 lists the mean and standard deviation of scores to measure the performance of the\ndetection of anomalies, with and without the wavelet-based features, for the two datasets.\nTable 4 – Mean and standard deviation of selected scores of anomaly detection perfor-\nmance, without and with wavelet-based features (WBF).\nDataset 1\nDataset 2\nWithout WBF\nWith WBF\nWithout WBF\nWith WBF\nAccuracy\n0.51 ± 0.01\n0.54 ± 0.01\n0.54 ± 0.03\n0.55 ± 0.03\nRecall\n0.12 ± 0.02\n0.13 ± 0.01\n0.14 ± 0.01\n0.12 ± 0.01\nPrecision\n0.54 ± 0.07\n0.67 ± 0.07\n0.64 ± 0.14\n0.68 ± 0.15\nF1-score\n0.17 ± 0.02\n0.22 ± 0.02\n0.22 ± 0.02\n0.21 ± 0.02\nWithout wavelet-based features, the accuracy of the detection is around 0.50 in both datasets,\nthe recall scores are slightly above 0.10, and the precision is around 0.60, resulting in F1-scores\naround 0.20. These values are deﬁnitively low and would not allow a proper detection of cows’\nspeciﬁc states. Adding wavelet-based features results in an improvement of some scores. The\naccuracy and the F1-score are slightly improved in Dataset 1 (+0.03 and +0.05 i.e. +5.9% and\n+29.4%). The use of wavelet-based features mostly improve the precision in Dataset 1 (+ 0.13\ni.e. +24.1%) but not in Dataset 2. As a consequence, F1-score increased with the use of wavelet-\nbased features only in Dataset 1 (+0.05 i.e. +29.4%). In general, the use of wavelet-based fea-\ntures improved the performance scores only in Dataset 1. Dataset 2 corresponds to cows sub-\njected to an experimental protocol requiring numerous manipulations that may have hidden be-\nhavioral changes due to speciﬁc conditions of cows (Lardy et al., 2023).\nIn Dataset 1, the improvement of the detection thanks to including wavelet-based features is\nessentially in precision. With a low rate of anomalies, as in the case of speciﬁc cow states (preva-\nlence, 0.7% in the whole dataset), a high precision is necessary to avoid many false detections\nwhich can result in users not paying attention to the anomalies detected by the system. There-\nfore, wavelet-based features are likely to improve the quality of the detection as perceived by a\nuser of a Precision Livestock system.\n10\n\n\nValentin Guien et al.\n11\n(a) States of the time series of Cow n°8605. In this time series, some anomalies are annotated by the caretakers.\n(b) States of the time series of the Cow n°162 of the Dataset 2. In this time series, no anomaly is annotated by the\ncaretakers.\nFigure 4 – States of the time series of two cows from Dataset 2. The time series begin on\n2 March 2015 and ends on 30 April 2015. The blue curves represent the states annotated\nby the caretakers. The green curves represent the predicted states. The red dotted line\nrepresents the threshold under which the hours are predicted as abnormal.\nIn our study, the testing dataset was constructed to include a balanced number of windows\nlabelled normal and abnormal. This approach gives a fair evaluation of the performance of the\nanomaly detection algorithm but it does not reﬂect real-world scenarios where the distribution\nof normal and abnormal instances is generally skewed. Consequently, the accuracy of our model\nshould not be compared to a random detection of anomalies. For instance, in Dataset 1, speciﬁc\nstates to be detected represented only 2.2 % of the cow*days. A precision of 0.67, as obtained\nwith wavelet-based features in Dataset 1, would result in the vast majority of alarms received\nby users as false alarms. However, this is not considering that a detection during fuzzy days can\nalso be valid (see next section).\n11\n\n\n12\nValentin Guien et al.\n3.2. Distance of detections of anomalies\nThe ﬁgures 5 and 6 show the histograms of the distance of detections for the two datasets,\nwith and without considering the wavelet-based features. The histograms have been normalized\naccounting for initial distribution of the data. The histograms of Dataset 2 form a approximate\nGaussian distribution around 0, that is the day when caretakers noted the speciﬁc cow state. In\nDataset 1, the distance of detection is more variable, nevertheless we notice that the four highest\nfrequencies of detected days are between -3 and +1, when using the wavelet-based features.\nIn Dataset 1, without the wavelet-based features (Figure 5a), 35.79% of the days detected as\nabnormal have a distance of detection between -3 and +1, and with the wavelet-based features\n(Figure 5b), this rate is 49.40%. In Dataset 2, without the wavelet-based features (Figure 6a),\n47.65% of the days detected as abnormal have a distance of detection between -3 and +1, and\nwith the wavelet-based features (Figure 6b), this rate is 50.77%. In previous studies (Wagner\net al., 2021), fuzzy days ranged from ∆τb = 2 to ∆τa = 7. Our results suggest that at least one\nmore day should be considered fuzzy, with ∆τb = 3, as the behaviour of the cow may already\nhave changed.\n3.3. Features contribution\nIn Dataset 1, four of the ten features that contribute the most to the detection are wavelet-\nbased features (Figure 7). Two of them are among the ﬁve top features: Daubechies 3 and Haar,\nboth with one level of approximation.\nIn Dataset 2, two of the ten features that contribute the most to the detection are wavelet-\nbased (Figure 8). Haar, with one level of approximation, is one of the ﬁve top features. In both\ndata sets, Q25, Minimum, Q10, Median, Kurtosis, and Haar|1 are among the top ten features with\nthe most contributions. Haar|1 makes a signiﬁcant contribution in both data sets. The wavelet-\nbased features we developed are a good indicator for anomaly detection. Despite being the\noldest, the Haar wavelet is the best for these features. In addition, one level of approximation\nworks best, which is the lowest level of approximation of the signal. This means that removing\nsome variations in the time series is relevant to smooth curves while avoiding the loss of too\nmuch information.\n4. Conclusions\nAnomaly detection in time series is a well-explored challenge, relevant across various applica-\ntions and of special interest for Precision Livestock Farming. In many cases, the objective beyond\nmerely detecting anomalies is to understand what makes the anomaly. Transforming time series\ndata into features provides such insights and enhances the explainability of detected anomalies.\nThis paper presents a structured approach to splitting training and testing datasets from time\nseries of cow behavior and proposes the introduction of new features based on wavelet trans-\nforms. The features allow to address periodic time series with high variability as they can reduce\nnoise while keeping the main features of a periodic signal. Although the datasets used are com-\nplex, we observed that incorporating wavelet-based features slightly increased the detection\nperformance, especially in terms of precision. Thus, the wavelet transform seems promising for\nthe detection of abnormal 24 h cycles of cow’s behavior. In addition, many apparent false pos-\nitive correspond to detection before the anomaly was detected by caretakers, which is of high\nimportance for livestock management. Detecting an anomaly before caretakers notice allows to\ntake action at an early stage, e.g. taking measures so that a cow health does not deteriorate.\nIn this study the Isolation Forest was chosen due to its straightforward implementation. More\neﬃcient algorithms need to be applied, such as convolutional neural networks (CNNs), includ-\ning Auto-Encoders (Yao et al., 2023) and Generative Adversarial Networks (GANs) (Jiang et al.,\n2019). These algorithms will be tested to include the whole procedure of the windows creations,\ndataset construction, and the wavelet-based features incorporation. Also, our scores of perfor-\nmance focus essentially on instances labeled as normal or abnormal. Fuzzy labels were neglected,\nexcept that we considered whether an apparently false detection occurred on a fuzzy day. Per-\nformance metrics should be designed to integrate fuzzy labels and a timing factor to provide a\n12\n\n\nValentin Guien et al.\n13\n(a) Without Wavelet-based features (WBF)\n(b) With Wavelet-based features (WBF)\nFigure 5 – Histograms of the mean ratio of the average distance of detection normalized\nby the initial distribution of the data, for Dataset 1. The black bars represent the standard\ndeviation.\nmore comprehensive assessment of detection quality, in line with Precision Livestock Farming\nwhere one wants to warn farmers as early as possible (Wagner et al., 2021).\n5. Aknowledgements\nThe authors gratefully acknowledge the support received from the ’Agence Nationale de\nla Recherche’ of the French government through the ‘Investissements d’Avenir’ program (16-\nIDEX-0001 CAP 20-25). L.E.C.R aknowledges partial ﬁnancial support from the TOURNESOL\nprogramme (FWO/French Embassy in Belgium – Campus France(VS00123N)).\nReferences\nBerckmans D (2014). Precision livestock farming technologies for welfare management in intensive\nlivestock systems. Revue scientiﬁque et technique 33 1, 189–96.\n13\n\n\n14\nValentin Guien et al.\n(a) Without Wavelet-based features (WBF)\n(b) With Wavelet-based features (WBF)\nFigure 6 – Histograms of the mean ratio of the average distance of detection normalized\nby the initial distribution of the data for Dataset 2. The black bars represent the standard\ndeviation.\nCatalano C, Paiano L, Calabrese F, Cataldo M, Mancarella L, Tommasi F (2022). Anomaly detection\nin smart agriculture systems. Computers in Industry 143, 103750. https://doi.org/10.1016/\nj.compind.2022.103750.\nChandola V, Banerjee A, Kumar V (2009). Anomaly detection: A survey. ACM Computing Surveys\n41, 1–58. https://doi.org/10.1145/1541880.1541882.\nChiarot G, Silvestri C (2023). Time Series Compression Survey. ACM Computing Surveys 55, 1–32.\nhttps://doi.org/10.1145/3560814.\nDaubechies I (1990). The wavelet transform, time-frequency localization and signal analysis. IEEE\nTransactions on Information Theory 36, 961–1005. https://doi.org/10.1109/18.57199.\nDemšar J (2006). Statistical Comparisons of Classiﬁers over Multiple Data Sets. J. Mach. Learn. Res.\n7, 1–30.\nFarge M (1992). Wavelet transforms and their applications to turbulence. Annual Review of Fluid\nMechanics 24, 395–457. https://doi.org/10.1146/annurev.fl.24.010192.002143.\n14\n\n\nValentin Guien et al.\n15\nFigure 7 – Critical diﬀerence diagram for Dataset 1 to measure feature contribution. The\nvalue associated with each feature corresponds to its average rank after one execution\nof the Isolation Forest. The black bars represent features whose contributions are not\nsigniﬁcantly diﬀerent. The wavelet-based features are those in the form \"waveletname|l\"\nwhere l is the level of approximation used. These results were obtained with the code in\nIsmail Fawaz et al., 2019.\nFigure 8 – Critical diﬀerence diagram for Dataset 2 to measure feature contribution. The\nvalue associated with each feature corresponds to its average rank after one execution\nof the Isolation Forest. The black bars represent features whose contributions are not\nsigniﬁcantly diﬀerent. The wavelet-based features are those in the form \"waveletname|l\"\nwhere l is the level of approximation used. These results were obtained with the code in\nIsmail Fawaz et al., 2019.\nFaverjon C, Berezowski J (2018). Choosing the best algorithm for event detection based on the\nintended application: A conceptual framework for syndromic surveillance. Journal of Biomedical\nInformatics 85, 126–135. https://doi.org/10.1016/j.jbi.2018.08.001.\n15\n\n\n16\nValentin Guien et al.\nFuchs P, Adrion F, Shaﬁullah AZ, Bruckmaier R, Umstaetter C (2022). Detecting Ultra- and Circa-\ndian Activity Rhythms of Dairy Cows in Automatic Milking Systems Using the Degree of Functional\nCoupling—A Pilot Study. Frontiers in Animal Science 3, 839906. https://doi.org/10.3389/\nfanim.2022.839906.\nGraps A (1995). An introduction to wavelets. IEEE Computational Science and Engineering 2, 50–61.\nhttps://doi.org/10.1109/99.388960.\nGuo H, Guan Y, Liu M, Qin K (2019). Biorthogonal Wavelet Transforms and Applications Based\non Generalized Progressive Catmull-Clark Subdivision with Shape Control. IEEE Transactions on\nVisualization and Computer Graphics 25, 2392–2403. https://doi.org/10.1109/TVCG.\n2018.2845887.\nHilal W, Gadsden SA, Yawney J (2022). Financial Fraud: A Review of Anomaly Detection Techniques\nand Recent Advances. Expert Systems with Applications 193, 116429. https://doi.org/10.\n1016/j.eswa.2021.116429.\nHimeur Y, Ghanem K, Alsalemi A, Bensaali F, Amira A (2021). Artiﬁcial intelligence based anom-\naly detection of energy consumption in buildings: A review, current trends and new perspectives.\nApplied Energy 287, 116601. https://doi.org/10.1016/j.apenergy.2021.116601.\nHyndman RJ, Wang E, Laptev N (2015). Large-Scale Unusual Time Series Detection. In: 2015 IEEE\nInternational Conference on Data Mining Workshop (ICDMW), pp. 1616–1619. https://doi.\norg/10.1109/ICDMW.2015.104.\nIsmail Fawaz H, Forestier G, Weber J, Idoumghar L, Muller PA (2019). Deep learning for time series\nclassiﬁcation: a review. Data Mining and Knowledge Discovery 33, 917–963. https://doi.org/\n10.1007/s10618-019-00619-1.\nJiang W, Hong Y, Zhou B, He X, Cheng C (2019). A GAN-Based Anomaly Detection Approach for\nImbalanced Industrial Time Series. IEEE Access 7, 143608–143619. https://doi.org/10.\n1109/ACCESS.2019.2944689.\nLardy R, Mialon MM, Wagner N, Gaudron Y, Meunier B, Helle Sloth K, Ledoux D, Silberberg M,\nDe Boyer Des Roches A, Ruin Q, Bouchon M, Cirié C, Antoine V, Koko J, Veissier I (2022). Un-\nderstanding anomalies in animal behaviour: data on cow activity in relation to health and welfare.\nAnimal - Open Space 1, 100004. https://doi.org/10.1016/j.anopes.2022.100004.\nLardy R, Ruin Q, Veissier I (2023). Discriminating pathological, reproductive or stress conditions in\ncows using machine learning on sensor-based activity data. Computers and Electronics in Agricul-\nture 204, 107556. https://doi.org/10.1016/j.compag.2022.107556.\nLiu FT, Ting KM, Zhou ZH (2008). Isolation Forest. In: 2008 Eighth IEEE International Conference\non Data Mining. Pisa, Italy: IEEE, pp. 413–422. https://doi.org/10.1109/ICDM.2008.17.\nLu W, Ghorbani AA (2008). Network Anomaly Detection Based on Wavelet Analysis. EURASIP Jour-\nnal on Advances in Signal Processing 2009, 837601. https://doi.org/10.1155/2009/\n837601.\nLuisier F, Vonesch C, Blu T, Unser M (2009). Fast Haar-wavelet denoising of multidimensional ﬂuo-\nrescence microscopy data. In: 2009 IEEE International Symposium on Biomedical Imaging: From\nNano to Macro, pp. 310–313. https://doi.org/10.1109/ISBI.2009.5193046.\nLundberg SM, Lee SI (2017). A Uniﬁed Approach to Interpreting Model Predictions. In: Neural Infor-\nmation Processing Systems.\nMarchesini G, Mottaran D, Contiero B, Schiavon E, Segato S, Garbin E, Tenti S, Andrighetto I\n(2018). Use of rumination and activity data as health status and performance indicators in beef\ncattle during the early fattening period. The Veterinary Journal 231, 41–47. https://doi.org/\n10.1016/j.tvjl.2017.11.013.\nPang J (2017). Improved image denoising based on Haar wavelet transform. In: 2017 IEEE Smart-\nWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computed, Scalable Computing\n& Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation\n(SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI), pp. 1–6. https://doi.org/10.1109/\nUIC-ATC.2017.8397456.\nParagliola G, Coronato A (2018). Gait Anomaly Detection of Subjects With Parkinson’s Disease Using\na Deep Time Series-Based Approach. IEEE Access 6, 73280–73292. https://doi.org/10.\n1109/ACCESS.2018.2882245.\n16\n\n\nValentin Guien et al.\n17\nPopivanov I, Miller R (2002). Similarity search over time-series data using wavelets. In: Proceed-\nings 18th International Conference on Data Engineering. San Jose, CA, USA: IEEE Comput. Soc,\npp. 212–221. https://doi.org/10.1109/ICDE.2002.994711.\nShensa MJ (1992). The discrete wavelet transform: wedding the a trous and Mallat algorithms. IEEE\nTrans. Signal Process. 40, 2464–2482.\nSoumaya Z, Taouﬁq BD, Nsiri B, Abdelkrim A (2019). Diagnosis of Parkinson disease using the\nwavelet transform and MFCC and SVM classiﬁer. In: 2019 4th World Conference on Complex\nSystems (WCCS), pp. 1–6. https://doi.org/10.1109/ICoCS.2019.8930802.\nTikkanen PE (1999). Nonlinear wavelet and wavelet packet denoising of electrocardiogram signal.\nBiological Cybernetics 80, 259–267. https://doi.org/10.1007/s004220050523.\nVeissier I, Mialon MM, Sloth KH (2017). Short communication: Early modiﬁcation of the circadian\norganization of cow activity in relation to disease or estrus. Journal of Dairy Science 100, 3969–\n3974. https://doi.org/10.3168/jds.2016-11853.\nVonesch C, Blu T, Unser M (2007). Generalized Daubechies Wavelet Families. IEEE Transactions on\nSignal Processing 55, 4415–4429. https://doi.org/10.1109/TSP.2007.896255.\nWagner N, Mialon MM, Sloth KH, Lardy R, Ledoux D, Silberberg M, De Boyer Des Roches A,\nVeissier I (2021). Detection of changes in the circadian rhythm of cattle in relation to disease,\nstress, and reproductive events. Methods 186, 14–21. https://doi.org/10.1016/j.ymeth.\n2020.09.003.\nWottlin LR, Carstens GE, Kayser WC, Pinchak WE, Pinedo PJ, Richeson JT (2021). Eﬃcacy of\nstatistical process control procedures to monitor deviations in physical behavior for preclinical\ndetection of bovine respiratory disease in feedlot cattle. Livestock Science 248, 104488. https:\n//doi.org/10.1016/j.livsci.2021.104488.\nYao Y, Ma J, Ye Y (2023). Regularizing autoencoders with wavelet transform for sequence anomaly\ndetection. Pattern Recognition 134. Publisher: Elsevier Ltd. https://doi.org/10.1016/j.\npatcog.2022.109084.\nZayrit S, Drissi TB, Ammoumou A, Nsiri B (2020). Daubechies Wavelet Cepstral Coeﬃcients for\nParkinson’s Disease Detection. Complex Syst. 29.\nZhang J, Tsui F, Wagner MM, Hogan WR (2003). Detection of outbreaks from time series data using\nwavelet transform. AMIA Annual Symposium proceedings. AMIA Symposium, 748–752.\n17\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21051v1.pdf",
    "total_pages": 17,
    "title": "Detection of anomalies in cow activity using wavelet transform based features",
    "authors": [
      "Valentin Guien",
      "Violaine Antoine",
      "Romain Lardy",
      "Isabelle Veissier",
      "Luis E C Rocha"
    ],
    "abstract": "In Precision Livestock Farming, detecting deviations from optimal or baseline\nvalues - i.e. anomalies in time series - is essential to allow undertaking\ncorrective actions rapidly. Here we aim at detecting anomalies in 24h time\nseries of cow activity, with a view to detect cases of disease or oestrus.\nDeviations must be distinguished from noise which can be very high in case of\nbiological data. It is also important to detect the anomaly early, e.g. before\na farmer would notice it visually. Here, we investigate the benefit of using\nwavelet transforms to denoise data and we assess the performance of an anomaly\ndetection algorithm considering the timing of the detection. We developed\nfeatures based on the comparisons between the wavelet transforms of the mean of\nthe time series and the wavelet transforms of individual time series instances.\nWe hypothesized that these features contribute to the detection of anomalies in\nperiodic time series using a feature-based algorithm. We tested this hypothesis\nwith two datasets representing cow activity, which typically follows a daily\npattern but can deviate due to specific physiological or pathological\nconditions. We applied features derived from wavelet transform as well as\nstatistical features in an Isolation Forest algorithm. We measured the distance\nof detection between the days annotated abnormal by animal caretakers days and\nthe days predicted abnormal by the algorithm. The results show that\nwavelet-based features are among the features most contributing to anomaly\ndetection. They also show that detections are close to the annotated days, and\noften precede it. In conclusion, using wavelet transforms on time series of cow\nactivity data helps to detect anomalies related to specific cow states. The\ndetection is often obtained on days that precede the day annotated by\ncaretakers, which offer possibility to take corrective actions at an early\nstage.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}