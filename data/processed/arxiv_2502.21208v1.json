{
  "id": "arxiv_2502.21208v1",
  "text": "Preprint, Under Review\nARIES: AUTONOMOUS REASONING WITH LLMS ON IN-\nTERACTIVE THOUGHT GRAPH ENVIRONMENTS\nPedro Gimenes1 ∗, Zeyu Cao2, Jeffrey Wong1, Yiren Zhao1\n1Department of Electrical & Electronic Engineering, Imperial College London\n2Department of Computer Science and Technology, University of Cambridge\n{pedro.gimenes19, tsz.wong20, a.zhao}@ic.ac.uk\nzeyu.cao@cl.cam.ac.uk\nABSTRACT\nRecent research has shown that LLM performance on reasoning tasks can be enhanced by\nscaling test-time compute. One promising approach, particularly with decomposable prob-\nlems, involves arranging intermediate solutions as a graph on which transformations are\nperformed to explore the solution space. However, prior works rely on pre-determined,\ntask-specific transformation schedules which are subject to a set of searched hyperparame-\nters. In this work, we view thought graph transformations as actions in a Markov decision\nprocess, and implement policy agents to drive effective action policies for the underlying\nreasoning LLM agent. In particular, we investigate the ability for another LLM to act as\na policy agent on thought graph environments and introduce ARIES, a multi-agent archi-\ntecture for reasoning with LLMs. In ARIES, reasoning LLM agents solve decomposed\nsubproblems, while policy LLM agents maintain visibility of the thought graph states, and\ndynamically adapt the problem-solving strategy. Through extensive experiments, we ob-\nserve that using off-the-shelf LLMs as policy agents with no supervised fine-tuning (SFT)\ncan yield up to 29% higher accuracy on HumanEval relative to static transformation sched-\nules, as well as reducing inference costs by 35% and avoid any search requirements. We\nalso conduct a thorough analysis of observed failure modes, highlighting that limitations\non LLM sizes and the depth of problem decomposition can be seen as challenges to scaling\nLLM-guided reasoning.\n1\nINTRODUCTION\nPrior works have shown that Large Language Models (LLMs) are subject to the emergence of abilities as\ntheir parameter count grows (Wei et al., 2022), which spurred significant interest in training increasingly\nlarger models. However, recent work showed that under a fixed compute budget for training and inference,\nLLM performance on reasoning tasks can be enhanced by allocating a higher proportion of compute to\ninference rather than training (Snell et al., 2024). This shift towards inference-time compute scaling can\nbe intuitively understood through the Dual Process Theory, which postulates the existence of two distinct\nmodes of reasoning in humans - (1) a fast, intuitive mode and (2) a slow, deliberate mode (Evans & Frankish,\n2009). While the autoregressive decoding procedure of LLMs resembles System 1, prior works used LLMs\nin System 2 reasoning by inducing models to thoroughly explore a problem, such as using chain of thoughts,\nahead of providing a solution to the user query (Wei et al., 2023).\nSystem 2 reasoning can be induced in LLMs by querying models fine-tuned on extensive reasoning traces\n(Muennighoff et al., 2025). While such single-query approaches have been shown effective in improving\nthe quality of complex sequential logic, an alternative approach involves performing multiple queries with\nthe same LLM and arranging intermediate solutions (or “thoughts”) in a specified topology, i.e. topological\nreasoning (Besta et al., 2024b). This approach yields benefits in problems where intermediate solutions can\nbe reliably scored through a Process Reward Model (PRM) (Snell et al., 2024) or using real feedback from\nexternal environments (Yao et al., 2023a). Additionally, a graph formulation has shown promising results\nin problems displaying the property of decomposability into subproblems that can be solved independently\nthen aggregated through a sequence of graph transformations (Besta et al., 2024a). In this work, we focus\n∗Corresponding author.\n1\narXiv:2502.21208v1  [cs.AI]  28 Feb 2025\n\n\nPreprint, Under Review\n def make_palindrome(string: str) -> str:\n     prefix_length = len(string)\n     postfix = longest_palindromic_postfix(string)\n     prefix_length -= len(postfix)\n     prefix = get_prefix(string, prefix_length)\n     reversed_prefix = reverse_string(prefix)\n     return string + reversed_prefix\n def longest_palindromic_postfix(string: str) -> str:\n     # Finds the longest postfix that is a palindrome.\n     ...\n def get_prefix(string: str, length: int) -> str:\n     # Returns the prefix with a specified length.\n     ...\n def reverse_string(string: str) -> str:\n     # Reverses a given string.\n     ...\nAction:split\nNodes:0\nAction:solve\nNodes:1,2,3\nAction:refine\nNodes:1\nPolicy\nAgent\nReasoning\nAgent\n...\nFigure 1: ARIES workflow in answering the HumanEval prompt: ”Find the shortest palindrome that begins\nwith a supplied string”. The policy agent selects an action based on the thought graph state, which is executed\nby the reasoning agent. First, the split action generates a skeleton implementation calling yet-to-implement\nsubfunctions, decomposing the problem. Then, the agent is instructed to generate a solution for each sub-\nfunction. Since one of the solutions doesn’t pass its testcases, the reasoning agent is instructed to refine it\nbased on execution feedback.\non problems with the decomposability property and in environments where external feedback is viable and\nuseful, such as using LLMs to solve coding problems.\nDespite the benefits of topological reasoning, prior works rely on pre-determined traversal strategies\nparametrized by a discrete set of hyperparameters. This approach lacks generality, as these parameters must\nbe tuned manually or through extensive Bayesian search to achieve high query efficiency, due to the varying\ncharacteristics of each task. With this limitation in mind, we hypothesize that the generalization of artifi-\ncial problem-solving towards (or beyond) human-like abilities in arbitrary domains requires a mechanism for\nautonomous traversal of a solution space, falling outside the constrained scope of static schedules shown in\nTree-of-Thoughts (Yao et al., 2023a) and Graph-of-Thoughts (Besta et al., 2024a).\nTo this end, we propose viewing thought graphs as an interactive environment where a sequence of graph\ntransformations is seen as actions in a Markov Decision Process (MDP). Considering this state-action for-\nmulation, an effective action policy should explore the solution space to yield a solution while learning from\nexternal feedback. Such a mechanism would present a step towards general intelligent agents capable of\nleveraging existing world knowledge while adapting to out-of-distribution tasks.\nMotivated by recent improvements in LLM planning and reasoning (Wei et al., 2023; Yao et al., 2023b),\nwe aim to investigate whether existing LLMs have the capability to act as autonomous reasoning agents\nby formulating thought graphs as interactive environments. We propose the use of LLM policy agents (i.e.\nLLM-based action planners) to autonomously execute a set of transformations, including thought proposal,\nevaluation, aggregation and refinement. As such, we consider the following research questions: (1) Can\nLLMs act as policy agents and effectively utilize feedback from thought graph environments to dynami-\ncally tune their exploration strategies? (2) Can this approach match the performance of static transformation\nschedules extensively optimized for a given task? And finally, (3) What are the failure modes of using exist-\ning LLMs as policy agents in guiding thought graph exploration (i.e. factors affecting the ability to produce\ncoherent exploration plans)?\nWe investigate the aforementioned questions by implementing ARIES, a multi-agent framework for solving\nreasoning problems formulated as thought graphs. Figure 1 provides a summary of our approach - in each\niteration, the policy agent monitors the thought graph state and samples from the action space to choose\na graph transformation. The reasoning agent then performs these transformations and updates the thought\ngraph state. In summary, our contributions are as follows.\n• We introduce ARIES, a novel formulation to autonomous topological reasoning, making the whole\nreasoning task LLM-guided. We frame the topological reasoning task as a collaboration between\ntwo agents within a topological thought graph. The LLM policy agent assesses states and determines\nthe actions, while the LLM reasoning agent carries out these actions, executing transformations on\nthe thought graph.\n• We show that LLMs exhibit planning capacity and can serve effectively as policy agents on topolog-\nical reasoning tasks, thus eliminating the requirement for predefined, task-specific scheduling of the\n2\n\n\nPreprint, Under Review\nreasoning agents, as seen in Tree-of-Thoughts (ToT) and Graph-of-Thoughts (GoT). Additionally,\nwe identify and discuss the limitations and failure modes of their planning abilities.\n• We perform carefully controlled experiments against a number of benchmarks, showing that LLM-\nguided thought graph exploration can lead to up to 29% higher accuracy at 35% lower inference\ncost, as well as obviating any Bayesian search cost.\n2\nRELATED WORK\n2.1\nTOPOLOGICAL REASONING\nWei et al. (2023) pioneered the elicitation of step-by-step logical reasoning, with subsequent work by Wang\net al. (2023) demonstrating improved performance through the sampling and arbitration along multiple rea-\nsoning sequences. Yao et al. (2023a) formulate concurrent exploration of multiple reasoning paths by scoring\nreasoning steps, leveraging tree search algorithms (ToT). Finally, Besta et al. (2024a) generalize problem\nspace exploration by formulating thoughts as a graph, enabling the use of arbitrary transformations such as\nnode refinement and aggregation (GoT).\nSeveral works have explored methods of improving the query efficiency of topological reasoning, which suf-\nfers from high computational demand due to iterative LLM prompting (Hu et al., 2023; Sel et al., 2024; Ding\net al., 2024). Despite improvements, few works have targeted the generality of this approach by exploring\ndynamic transformations. While (Yao et al., 2023a) leverage standard tree search algorithms, (Long, 2023)\nhypothesize that tree search can be enhanced through trained policy networks to guide node backtracking.\nHowever, this idea is not explored fully and their evaluation is focused on heuristics-based rules. As such, our\nwork presents the first effort towards generalized topological reasoning through autonomous thought graph\nexploration.\n2.2\nLLMS AS ACTION POLICY AGENTS\nSignificant research has focused on leveraging LLMs for guiding action policies, such as in tasks requiring\ncoordination of heterogeneous model ensembles (Shen et al., 2023). LLMs have also been deployed as action\nplanners in interactive environments where feedback is provided to the action scheduler, such as solving\ncomputer tasks (Kim et al., 2023) and online shopping (Yao et al., 2023b). However, some works have\noutlined the instability in obtaining action plans over long-range horizons, where LLMs have been shown to\nrepeatedly generate invalid action plans (Xie et al., 2023). This limitation has been tackled by works such as\n(Shinn et al., 2023), which propose an episodic memory buffer of previous trials. However, to our knowledge,\nno prior work has investigated leveraging LLM planning abilities in the context of topological reasoning.\n3\nTOPOLOGICAL REASONING WITH LARGE LANGUAGE MODELS\nWe consider a reasoning problem to be stated in language as an ordered tuple of tokens p = (t1, . . . , tm),\nwhere each token t ∈V belongs to a vocabulary space V. We define a thought τ = (t1, . . . , tj) as a sequence\nof tokens sampled autoregressively from an LLM parametrized by θ, i.e. ti ∼P(ti | t1, . . . , ti−1; θ). This\nconsists of a language representation of an intermediate step towards the solution to the problem.\nA thought sequence can be represented as an ordered tuple of thoughts S = (τ 1, τ 2, . . . , τ k) of length k,\nsuch that the final thought τ k represents a candidate solution to the problem p. A thought tree Tτ can be\nrepresented as (V, E), where V is a set of thought nodes and E is a set of edges connecting them. The tree can\nbe parametrized with a depth of d and a width of w, denoting the number of nodes per level. Additionally,\neach thought τ ij (j-th thought at depth i) has a value λ(τ ij) such that nodes with higher values yield valid\nsolutions to the problem with higher probability. Hence, tree-based thought exploration involves finding a\npath P ⊂V that maximizes the cumulative value of thoughts, as follows.\nP ∗= arg max\nP\nX\nτ∈P\nλ(τ)\n(1)\nA thought graph Gτ can also be represented via the tuple (V, E), with no imposed restriction on the ar-\nrangement of thoughts and edges. Thought graph exploration can be regarded as a sequence of m graph\n3\n\n\nPreprint, Under Review\nTable 1: Thought graph transformations used to solve reasoning problems using a divide-and-conquer strat-\negy. See Appendix B for their complete definitions.\nTransformation\nSymbol\nDecompose\nϕdec\nSolve\nϕsol\nRefine\nϕref\nReduce\nϕred\nAggregate\nϕagg\nAlgorithm 1 Static Thought Graph Transformation Schedule\nRequire: Starting graph G0\nτ, allow reduce Red, allow refine Ref\nRequire: Solve multiplicity Sm, aggregate multiplicity Am, and refine multiplicity Rm\nef\nGdec\nτ\n←ϕdec(G0\nτ, 1, {0}))\nGsol\nτ\n←ϕsol(Gdec\nτ\n, Sm, ∆(Gdec\nτ\n, G0\nτ))\nGagg\nτ\n←ϕagg(Gsol\nτ , Am, ∆(Gsol\nτ , Gdec\nτ\n))\nif Red then\nGred\nτ\n←ϕred(Gagg\nτ\n, 1, ∆(Gagg\nτ\n, Gsol\nτ ))\nelse\nGred\nτ\n←Gagg\nτ\nend if\nif Ref then\nGref\nτ\n←ϕref(Gred\nτ\n, Rm\nef, ∆(Gred\nτ\n, Gagg\nτ\n))\nG∗\nτ ←ϕred(Gref\nτ\n, 1, ∆(Gref\nτ\n, Gred\nτ\n))\nelse\nG∗\nτ ←Gred\nτ\nend if\nReturn: G∗\nτ\ntransformations as follows, where each ϕi : Gi\nτ →Gi+1\nτ\nmodifies the set of nodes and edges. The full set of\nconsidered transformations and their formulations are shown in Table 6.\nG∗\nτ = ϕm(. . . (ϕ1(ϕ0(G0\nτ))))\n(2)\nTable 1 summarizes the thought graph transformations we consider in the rest of this work. ϕdec decomposes\na reasoning problem into subproblems to be solved individually, creating new nodes in the thought graph. ϕsol\ngenerates a candidate solution to a subproblem. ϕref considers an incorrect subproblem solution, utilizing\nfurther LLM queries to refine it. ϕred removes nodes in the graph according to their values. Finally, ϕagg\nperforms node merging to aggregate subproblem solutions into a coherent solution to the original problem.\nStatic Transformation Schedules: A static transformation schedule can be parametrized by the tuple\n(Red, Ref, Sm, Am, Rm\nef). Sm, Am, Rm\nref represents the multiplicity (i.e. number of attempts) of the solve,\naggregate and refine transformations, respectively. Red, Ref ∈{0, 1} indicate whether the ϕred and ϕref\ntransformations are applied after aggregation.\nIn Algorithm 1, each transformation is defined as ϕ(Gτ, m, S), where Gτ = (V, E) is a thought graph,\nS ⊂V is a subset of nodes and m is the multiplicity (number of attempts). Additionally, the function\n∆(Ga\nτ, Gb\nτ) outputs all nodes present in the first graph Ga\nτ = (Va, Ea) but not in the second Gb\nτ = (Vb, Eb),\ndefined formally as follows.\n∆(Ga\nτ, Gb\nτ) = {v|v ∈V1 & v /∈V2}\n(3)\nAlgorithm 1 represents a standard divide-and-conquer strategy. The ϕdec transformation decomposes the\nstarting problem into B subproblems, which are solved individually (ϕsol). The aggregation of the subprob-\nlem solutions is attempted Am times, as the ϕagg transformation has a non-zero probability of failure. If\nRed = 1, a single aggregation attempt is kept, while others are removed from the graph. If Ref = 1, the\nremaining aggregation attempts are then refined wth ϕref, and the highest-scoring attempt is kept as the final\nsolution.\n4\n\n\nPreprint, Under Review\n1\n2\n3\nLLM\n(i) System Prompt\n(ii) Action Space\n(iii) Thought Graph State\nPolicy Prompt Template\nDecompose the following list into\nsmaller sublists: [2,6,3,9,5,2,5,1]\nSort the following list: [2, 6, 3, 9]\nPolicy Agent\nReasoning Agent\nLLM\nThought\nGraph\nList 1: [2,6,3,9], List 2: [5,2,5,1]\n[2, 3, 6, 9]\n(iv) Action History\nAction: solve\nNodes: [1, 2]\nAttempts: 5\nFigure 2: Multi-agent framework for reasoning over thought graphs. First, (1) the policy agent an action and\nsubset of nodes given a prompt including (i-ii) general instructions and (iii-iv) an overview of the exploration\nstate. The sample is then (2) passed to the reasoning agent, which finally (3) updates the thought graph state.\n4\nTHOUGHT GRAPH EXPLORATION AS A MARKOV DECISION PROCESS\nBeyond the fixed schedule shown in Algorithm 1, the transformation of a thought graph can be generalized\nas a Markov decision process (S, A, Pa):\n• State st ∈S: represents an arrangement of nodes and edges in the thought graph, with the associated\nvalue of each node, i.e. st = (V, E, {λ(v)|v ∈V}).\n• Action a ∈A: indicates which transformation to perform on the thought graph, and which nodes to\nperform it on, i.e. A = {(Vs, ϕ) | Vs ⊂V, ϕ ∈Ω}, where Ωis the set of transformations (Table 6).\n• Transition probability Pa(s, s′): represents the probability that an action a applied at state s yields\nthe expected new state s′.\nThe optimal transformation sequence Φ is then defined as the sequence of actions that maximize the condi-\ntional probability of reaching a solution state s+, i.e. Φ = (ϕ0, . . . , ϕn) that solves the following optimization\nproblem.\nmax\nΦ\nP(s+ | s0, Φ)\ns.t.\n|Φ| < ϵ\nWe bound the number of queries by the constant ϵ, as in the limit |Φ| →∞, P(s+|s0, Φ) →1.\n4.1\nMULTI-AGENT REASONING\nIn this work, we hypothesize that LLMs can approximate a solution to the stated optimization problem by\nacting as policy agents. We develop an interactive framework consisting of a policy agent and a reasoning\nagent, as shown in Figure 2. In each iteration, (1) the policy agent selects an action from the action space,\n(i.e. the transformations in Table 6). The policy agent then (2) directs the reasoning agent to perform the\nselected action. Finally, (3) the reasoning agent updates the thought graph. The process is repeated until a\nsolution is found or a maximum number of iterations is reached.\nThe policy agent is invoked using the prompt template shown in Figure 2. (i) The system prompt outlines\nthe problem setting, input format and expected behaviour from the policy agent. (ii) A task-specific list of\nactions, describing the preconditions and effects of each transformation, provides a semantic understanding\nof the action space. (iii) The current state of the graph is provided in a textual format, enumerating all nodes\nand edges. Finally, (iv) the action history in the current trial is included, promoting continuity in the strategies\noutlined in previous steps.\n4.2\nIN-CONTEXT ACTION SELECTION\nPrior work has shown that reasoning abilities of LLMs are enhanced when prompted to output a verbose\nsequence of steps before the solution (Wei et al., 2023; Wang et al., 2023). This mechanism can be seen\n5\n\n\nPreprint, Under Review\nas enabling in-context task learning from some extracted innate world knowledge. Hence, our policy agent\nis instructed to generate a detailed analysis on the state of the thought graph and exploration history before\nsampling the action space. The analysis includes the following:\n1. Describe the action history and how each action relates to an exploration strategy.\n2. Describe the thought graph state, and how each node corresponds to previous actions.\n3. Discuss the outlined strategy, stating whether it is successful, unsucessful, or pending.\n4. Outline a number of options for the next action, detailing the expected outcome of each.\n4.3\nPOLICY AGENT ENSEMBLES\nGiven the stochastic nature of token prediction in LLMs, we observe high variability in the chosen action\nover several invocations of a policy agent under the same thought graph state. Given the preconditions and\neffects of each action are represented via text rather than any rigorous formulation, actions selected by the\npolicy agent can display flawed understanding of the problem constraints, leading to ineffective exploration\nof the thought graph. To overcome this limitation, we democratize action selection over an ensemble of\nagents, meaning a parametrizable number of LLM queries are performed concurrently at every iteration. The\nselected action is takes as the most frequent proposal among the ensemble. See Section 6 for ablation studies\non the impact of policy agent ensemble size on reasoning performance.\n5\nEXPERIMENTS\nThrough a range of controlled experiments, we evaluate the performance of LLM policy agents on interactive\nthought graphs. In Appendix D and Section 5.2, we define the benchmarks and baselines. We present the\ncore results across each benchmark task in Section 5.3. We profile the transition probabilities of each thought\ngraph transformation across tasks in Section 5.4. In Section 5.5, we provide empirical results demonstrating\ntwo main failure modes of LLMs as policy agents, namely model size and decomposition depth.\nExperimental Setup: We evaluate Llama-3.1-70B and Llama-3.1-405B as policy and reasoning agents,\nhosted with SGLang at a temperature of 1. Llama-3.1-70B was hosted with 8× A6000 GPUs. Llama-3.1-\n405B was hosted using 16× H100 GPUs distributed over 4 nodes. The total cost was approximately 3k GPU\nhours.\n5.1\nBENCHMARKS\nWe run our main evaluation on HumanEval, a widely used benchmark for assessing the functional correctness\nof code generation models through a set of Python programming problems with corresponding test cases\n(Chen et al., 2021). Additionally, we consider two popular tasks for topological reasoning with LLMs, list\nsorting and set intersection. Despite their simplicity, prior works have shown that these tasks are extremely\nchallenging for LLMs with direct prompting (Besta et al., 2024a), benefitting from a divide-and-conquer\nstrategy (i.e. decomposition, solving subproblems and merging). We evaluate these at various levels of\ndifficulty (quantified by the size of the lists and sets), resulting in six benchmarks: sorting32/64/128 and\nset-intersection32/64/128.\nFor HumanEval, we report the task accuracy, while for list sorting and set intersection we report error func-\ntion value E. Details on the definition for the error function for each task can be found in Appendix D.\nAdditionally, we report both the search Cs and inference cost Ci. We measure cost by the number of queries\nsince we observe a low standard deviation in the number of generated tokens across all LLM queries during\nour experiments.\n5.2\nBASELINES\nWe use static transformation schedules as the baseline, following (Besta et al., 2024a). As previously noted,\nstatic schedules require extensive, task-dependent hyperparameter tuning. For each individual task, we care-\nfully tune the hyperparameters using Bayesian optimization resulting in three variants: GoT25%, GoT50%\nand GoT100%. Here, the percentage corresponds to the number of trials spent until the hyperparameter search\nconverges. As such, we compare against baselines with several search compute budgets. See Appendix C for\n6\n\n\nPreprint, Under Review\nTable 2: Task accuracy (↑), search and inference costs (↓) on Human Eval. Cost is measured as the number\nof LLM queries. IO refers to direct prompting. Llama-405b was used for the reasoning and policy agents.\nAccuracy\nSearch\nInference\nMethod\n[%]\nCost (Cs)\nCost (Ci)\nIO\n77.4\n0\n1\nGoT25%\n66.3\n1160\n34.8\nGoT50%\n67.5\n2368\n24.3\nGoT100%\n60.1\n4742\n8.17\nARIES\n89.0\n0\n5.3\nFigure 3: Pareto frontiers in total query cost (Cs+i) and task error (E) for set intersection tasks at various\ndifficulty levels. The total cost is the number of queries expended at search and inference time. Llama-3.1-\n405B was used for the reasoning and policy agents. Our results (ARIES) have pushed the Pareto frontiers\nforward in each task.\ndetails on the full search methodology. We also consider an Direct IO (Input-Output) baseline, i.e. reasoning\nvia direct LLM prompting.\n5.3\nEVALUATION\nReplacing static transformation schedules with LLM policy agents offers generalization to arbitrary tasks at\nno tuning cost. However, performance is constrained by the LLM’s planning capabilities. As such, we eval-\nuate ARIES against the aforementioned benchmarks, demonstrating its advantages and identifying potential\nfailure modes. We set the policy agent ensemble size to 5 in all experiments, as explained in Section 6.\n5.3.1\nHUMANEVAL\nOur key findings for autonomous policy agents in the context of a coding task are shown in Table 2. It can\nbe seen that by formulating this code generation task as a Markov decision process with an off-the-shelf\nLLM policy agent, we achieve up to 28.9% higher accuracy than the most query-efficient static schedule\nbaseline. We also observe that as further trials are expended in the GoT baseline search, the query efficiency\nis increased, i.e. hyperparameter configurations are found that achieve similar performance levels at lower\nquery counts. Nevertheless, we achieve 54% lower inference cost on average compared to even the most\noptimized GoT baseline, and also avoids any search time requirement.\n5.3.2\nSET INTERSECTION\nIn Figure 3, we plot a Pareto curve showing viable trade-off points in task error and query cost for the set\nintersection task. Our approach extends the existing Pareto frontier constructed by considering static schedule\nbaselines and direct prompting. In the set-intersection32 task, we achieve a 2.3× error reduction relative to\nGoT25 while also achieving 116× lower overall cost.\n7\n\n\nPreprint, Under Review\nTable 3: Failure mode 1 results. Mean value of the error E (↓) for benchmarks with low decomposition depth.\nLlama-3.1-70B was used for the reasoning and policy agents.\nMethod\nDirect Prompting\nGoT25%\nGoT50%\nGoT100%\nARIES\nsorting32\n2.2\n0.82\n0.95\n0.73\n1.29\nset-intersection32\n1.05\n0.41\n0.0\n0.37\n1.22\nTable 4: Failure mode 2 results. Mean value of the error E (↓) and search cost C in terms of number of\nqueries (↓). Both the reasoning and policy agents are LLaMA-405B.\nMethod\nDirect Prompting\nGoT25%\nGoT50%\nGoT100%\nARIES\nMetrics\nE\nC\nE\nC\nE\nC\nE\nC\nE\nC\nsorting32\n0.6\n1\n0.74\n825\n0.82\n1650\n0.28\n3300\n0.22\n20\nsorting64\n5.07\n1\n2.22\n1671\n2.74\n3343\n3.46\n6687\n9.15\n48\nsorting128\n12.75\n1\n13.96\n2444\n12.65\n4888\n18.65\n9776\n32.74\n48\n5.4\nTRANSITION PROBABILITY PROFILING\nIn this section, we estimate the transition probabilities for each thought graph transformation across a number\nof tasks to gain insight into factors impacting a thought graph formulation of each reasoning problem. For\nϕref, we define a successful transition when E = 0 for the resulting node, considering only cases when the\ntransformation is executed on nodes previously containing errors. In transformations requiring LLM calls, the\ntransition probability between two states is a random process governed by the token distribution parametrized\nby the LLM. When LLM calls are not required, i.e. the transformation is implemented through simple node\nmanipulation, the transition probability is 1.\nTable 5: Esimated transition probabilities for each thought graph transformation, taken as the number of\nsuccessful state transitions in a static schedule.\nϕsol\nϕref\nϕred\nϕagg\nHumanEval\n0.77\n0.29\n1\n1\nsorting32\n0.57\n0.12\n1\n0.60\nset-intersection32\n0.75\n0.71\n1\n1\nThe results are summarized in Table 5. We observe the refinement transformation has notably low suc-\ncess probability, particularly in coding and sorting tasks. Additionally, sorting is the only task with non-\ndeterministic aggregation, which is a potential error source. We note that the performance of a thought graph\nformulation depends on the ability of the policy agent to capture the success profile of various transformations\nfor a task, and adapt the exploration strategy accordingly.\n5.5\nFAILURE MODES\nIn this section, we perform a number of empirical studies aiming to understand the main limiting factors\nimpacting the performance of LLM policy agents on interactive thought graphs. We find there are two major\nfailure modes, described as follows.\nFailure mode 1: LLM Parameter Count\nWe find that LLMs with insufficiently large parameter sizes exhibit limited performance when utilized as\npolicy agents on thought graph environments. We deploy Llama-3.1-70B as policy and reasoning agents in\nsorting and set intersection tasks, against which the larger LLM (Llama-405B) was shown to perform well\nas a policy agent. As shown in Table 3, LLM-guided graph exploration (ARIES) did not outperform static\nschedule baselines in this scenario. These findings are consistent with (Wei et al., 2022), which demonstrated\nthat zero-shot chain-of-thought reasoning abilities emerges in models beyond 175B parameters.\nFailure mode 2: Decomposition Depth\n8\n\n\nPreprint, Under Review\nFigure 4: Mean error (y-axis) obtained in the sorting32 task over a sweep of ensemble sizes (x-axis). Llama-\n3.1-70B was used as the policy agent.\nWe examine the impact of decomposition depth by analyzing the results in the sorting task, shown in Table 4.\nWe observe LLM policy agents lead to a 21% performance improvement relative to the most optimized static\nbaseline in sorting32, which has a decomposition depth of 2. However, as discussed in Section 5.4, the sorting\ntask presents a particular challenge due to the lower success probability of the aggregation transformation.\nAs the complexity and decomposition depth of a task increases, the policy agent is required to apply a higher\nnumber of aggregation transformations. Therefore, we observe up to 4.12× and 2.6× performance deteri-\noration in sorting64 and sorting128, respectively. Through empirical analysis, we observe that in the latter\ntasks, the ϕagg transformation constitutes 86% and 68% of all policy agent errors, respectively. As such, we\nconclude that high decomposition depths present a significant failure mode for LLM-guided thought graph\nexploration, particularly in tasks with low success transition probabilities for the aggregation transformation.\n6\nABLATION STUDIES\nAs discussed in Section 4, two factors that impact the performance of LLMs as policy agents in interactive\nthought graph environments are the size of the ensemble and the use of chain of thought reasoning to enhance\nthe planning abilities of the policy agent. In this section, we aim to understand the impact of each factor by\nevaluating sorting tasks over a range of ensemble sizes from 1 to 15, with and without CoT prompting in the\npolicy agent.\nAs shown in Figure 4, as the ensemble size increases to 5, CoT prompting leads to large performance im-\nprovements, though the benefits start diminishing beyond this point. Without CoT prompting, the trend is less\nconsistent, and larger ensemble sizes sometimes yield worse performance. Additionally, errors without CoT\nare higher for both tasks at any ensemble size. This highlights the necessity of CoT prompting in enhancing\nthe LLM policy agent’s ability to adapt from feedback and drive thought graph transformations.\n7\nCONCLUSION\nWe introduce ARIES, a multi-agent architecture for topological reasoning. By viewing thought graph trans-\nformations as actions in a Markov decision process, we show off-the-shelf LLMs can drive efficient action\npolicies without task-specific tuning. We show up to 29% higher accuracy on HumanEval while reducing\ninference costs by 35% compared to static schedules. We identified two key limitations: insufficient model\nsize and excessive decomposition depth on the task at hand. These constraints indicate that while LLMs show\npromise as reasoning agents, their effectiveness depends on parameter count and task complexity.\n9\n\n\nPreprint, Under Review\nREFERENCES\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna\nGajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. Graph of thoughts:\nSolving elaborate problems with large language models. Proceedings of the AAAI Conference on Artificial\nIntelligence, 38(16):17682–17690, March 2024a. ISSN 2159-5399. doi: 10.1609/aaai.v38i16.29720. URL\nhttp://dx.doi.org/10.1609/aaai.v38i16.29720.\nMaciej Besta, Florim Memedi, Zhenyu Zhang, Robert Gerstenberger, Guangyuan Piao, Nils Blach, Piotr\nNyczyk, Marcin Copik, Grzegorz Kwa´sniewski, J¨urgen M¨uller, Lukas Gianinazzi, Ales Kubicek, Hubert\nNiewiadomski, Aidan O’Mahony, Onur Mutlu, and Torsten Hoefler. Demystifying chains, trees, and graphs\nof thoughts, 2024b. URL https://arxiv.org/abs/2401.14295.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri\nEdwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael\nPetrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail\nPavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Pet-\nroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss,\nWilliam Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji,\nShantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant\nMisra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter\nWelinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evalu-\nating large language models trained on code, 2021. URL https://arxiv.org/abs/2107.03374.\nRuomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma, Wei Zhang, Si Qin, Saravan Rajmohan,\nQingwei Lin, and Dongmei Zhang. Everything of thoughts: Defying the law of penrose triangle for thought\ngeneration, 2024. URL https://arxiv.org/abs/2311.04254.\nJonathan Evans and Keith Frankish. In two minds: Dual processes and beyond. Oxford University Press,\n01 2009.\nISBN 9780199230167.\ndoi: 10.1093/acprof:oso/9780199230167.001.0001.\nURL https:\n//doi.org/10.1093/acprof:oso/9780199230167.001.0001.\nPengbo Hu, Ji Qi, Xingyu Li, Hong Li, Xinqi Wang, Bing Quan, Ruiyu Wang, and Yi Zhou. Tree-of-\nmixed-thought: Combining fast and slow thinking for multi-hop visual reasoning, 2023. URL https:\n//arxiv.org/abs/2308.09658.\nGeunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks, 2023. URL\nhttps://arxiv.org/abs/2303.17491.\nJieyi Long. Large language model guided tree-of-thought, 2023. URL https://arxiv.org/abs/\n2305.08291.\nNiklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettle-\nmoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025.\nURL https://arxiv.org/abs/2501.19393.\nBilgehan Sel, Ahmad Al-Tawaha, Vanshaj Khattar, Ruoxi Jia, and Ming Jin. Algorithm of thoughts: Enhanc-\ning exploration of ideas in large language models, 2024. URL https://arxiv.org/abs/2308.\n10379.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt:\nSolving ai tasks with chatgpt and its friends in hugging face, 2023. URL https://arxiv.org/abs/\n2303.17580.\nNoah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao.\nReflexion: Language agents with verbal reinforcement learning, 2023. URL https://arxiv.org/\nabs/2303.11366.\nCharlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can\nbe more effective than scaling model parameters, 2024. URL https://arxiv.org/abs/2408.\n03314.\n10\n\n\nPreprint, Under Review\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and\nDenny Zhou. Self-consistency improves chain of thought reasoning in language models, 2023. URL\nhttps://arxiv.org/abs/2203.11171.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus.\nEmergent abilities of large language models, 2022.\nURL\nhttps://arxiv.org/abs/2206.07682.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. URL https:\n//arxiv.org/abs/2201.11903.\nYaqi Xie, Chen Yu, Tongyao Zhu, Jinbin Bai, Ze Gong, and Harold Soh. Translating natural language to\nplanning goals with large-language models, 2023. URL https://arxiv.org/abs/2302.05128.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan.\nTree of thoughts: Deliberate problem solving with large language models, 2023a.\nURL https:\n//arxiv.org/abs/2305.10601.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Syn-\nergizing reasoning and acting in language models, 2023b. URL https://arxiv.org/abs/2210.\n03629.\n11\n\n\nPreprint, Under Review\nA\nLIMITATIONS\nA.1\nASSUMPTIONS AND ROBUSTNESS\nThe ARIES framework introduces a novel approach to reasoning with large language models (LLMs) through\ninteractive thought graph environments. However, several strong assumptions underlie our methodology.\nFirstly, we assume that thought graph transformations can be effectively modeled as a Markov decision pro-\ncess (MDP) with well-defined state transitions. While this formulation enables structured reasoning, it may\nnot fully capture the complexities of more ambiguous or highly interconnected problems. Additionally, our\napproach assumes that off-the-shelf LLMs can act as reliable policy agents without additional fine-tuning.\nThis assumption holds for certain problem domains but may degrade in tasks requiring domain-specific\nknowledge or long-horizon planning.\nOur empirical evaluation is constrained to specific reasoning tasks, including HumanEval, list sorting, and\nset intersection. While these benchmarks serve as valuable test cases for structured reasoning, they do not\nnecessarily generalize to all problem types, particularly those with weakly defined intermediate states or\nmulti-modal reasoning requirements. Furthermore, our evaluation primarily focuses on LLaMA-3.1 models,\nand results may not be directly transferable to other architectures.\nA.2\nPOTENTIAL RISKS\nThe ARIES framework introduces both opportunities and challenges in autonomous reasoning. One primary\nrisk is the potential for incorrect or biased reasoning paths due to the stochastic nature of LLM-generated\ndecisions. Although our policy agent ensembles mitigate some of this variability, they do not fully eliminate\nerroneous transformations, particularly in deeper decomposition settings. The framework’s reliance on ex-\nisting LLMs also means that any biases present in the underlying models could propagate into the reasoning\nprocess, potentially leading to unfair or misleading outcomes.\nAnother concern is the environmental impact associated with inference-heavy approaches. While ARIES im-\nproves query efficiency relative to static transformation schedules, it still necessitates a significant number of\nLLM queries to achieve high accuracy. As LLMs scale, the energy consumption required for these inference\ntasks could become a sustainability concern, particularly in high-throughput applications.\nA.3\nFAILURE MODES\nOur empirical findings highlight two major failure modes: (1) inadequate LLM parameter sizes and (2)\nincreasing decomposition depth. Smaller models (e.g., LLaMA-3.1-70B) struggle to act as policy agents\neffectively, demonstrating subpar reasoning capabilities compared to larger counterparts. This suggests that\nautonomous policy-driven thought graph exploration may require models beyond a certain scale threshold\nto function reliably. Additionally, as the depth of problem decomposition increases, ARIES exhibits a de-\ncline in performance, primarily due to errors in aggregating intermediate solutions. This limitation indicates\nthat current LLMs may have difficulties managing extended reasoning chains, which presents a barrier to\nscalability.\nB\nTHOUGHT GRAPH TRANSFORMATIONS\nThe full set of considered transformations is shown in Table 6.\nC\nSTATIC SCHEDULE PARAMETER SEARCH\nAs described in Section 3, a static transformation can be characterized using a set of discrete parameters.\nWe ran bayesian search using using Tree-structured Parzen Estimator (TPE) sampling to determine each\nparameter, establishing strong baselines for each task.\nThe search space is shown in Table 7. We run multi-objective search to concurrently minimize the task-\nspecific error function E (Section D) and associated cost, measured as |Φ(ω)| where Φ(ω) = (ϕ0, . . . , ϕm) is\na tuple enumerating thought graph transformations, as a function of the schedule parameters ω ∈Ω, where Ω\nis the search space. Note that |Φ(ω)| correlates with the number of LLM queries, meaning this formulation\naims to minimize exploration cost.\n12\n\n\nPreprint, Under Review\nTable 6: Thought graph transformations. Each transformation is defined as ϕ(Gτ, m, S) = (V ∪V +\\V −, E∪\nE+ \\E−), where Gτ = (V, E) is a thought graph, S ⊂V is a subset of nodes, m is the multiplicity (number\nof attempts), and E, R, A represent arbitrary functions for node expansion, refinement and aggregation,\nrespectively. The sets V +, V −, E+, E−are defined as follows.\nTransformation\nSymbol\nV+\nV−\nE+\nE−\nDecompose\nϕdec\n{E(v)|v ∈S}\n∅\n{(u, v)|u ∈S, v ∈V +}\n∅\nSolve\nϕsol\n{S(v)|v ∈S}\n∅\n{(u, v)|u ∈S, v ∈V +}\n∅\nRefine\nϕref\n{R(t)|t ∈S}\n∅\n{(u, v)|u ∈S, v ∈V +}\n∅\nReduce\nϕred\n∅\nS\n∅\n{(u, v)|u ∈S ∨v ∈S}\nAggregate\nϕagg\nA(S)\n∅\n{(u, v)|u ∈S, v ∈V +}\n∅\nTable 7: Search space for each parameter characterizing a static transformation.\nSearch\nParameter\nSpace\nRed\nAllow reduction\n{0, 1}\nRef\nAllow refinement\n{0, 1}\nSm\nSolve multiplicity\n{1, 5, 10, 15, 20}\nAm\nAggregate multiplicity\n{1, 5, 10, 15, 20}\nRm\nef\nRefine multiplicity\n{1, 5, 10, 15, 20}\nIn selecting parameter configurations, we use the cost function in Equation 4, such that the objectives of\ncost and error minimization are balanced through the scalar constant α ∈(0, 1). We aim to assign equal\nimportance to the cost and error objectives by tuning α independently for each task such that the mean value\nof the first term matches the second term, i.e. αE [E] = (1 −α)E [|Φ(ω)|)], or equivalently α =\nE[|Φ(ω)|]\nE[E+|Φ(ω)|]\nwhere E denotes the expected value. The expectations are obtained with random sampling.\nmin\nω [αE + (1 −α)|Φ(ω)|]\n(4)\nSearch was conducted separately on Llama-3.1-70B and Llama-3.1-405B. For sorting and set intersection\ntasks, search is conducted separately for each difficulty level, ensuring the chosen parameters are adapted to\nthe task. Note that we present three search checkpoints GoTn for n ∈{25, 50, 100}, where n corresponds\nto the percentage of trials until convergence. We define the convergeance point as the first iteration where\na rolling window J of size 20 matches the condition Jk = Jk−1. This enables comparing our proposed\nLLM-guided approach to optimized search schedules at various search budgets.\nTable 8: Results from GoT static schedule parameter search on Llama-3.1-405B.\nTask\nAlpha (α)\nGoT25\nGoT50\nGoT100\nsorting32\n0.99\n0.38\n0.38\n0.37\nsorting64\n0.96\n4.85\n4.49\n3.84\nsorting128\n0.84\n28.76\n25.76\n24.36\nset32\n0.99\n0.16\n0.16\n0.12\nset64\n0.99\n0.71\n0.51\n0.31\nset128\n0.98\n3.51\n3.51\n2.99\nThe complete search results for Llama-3.1-405B are shown in Table 8. It can be seen that tasks with higher\ndecomposition depth incur lower values of α due to the higher magnitude of the error function. sorting64,\nsorting128 and set-intersection64 show a smooth decline in the cost function, while the remaining tasks\nremain at local minima until close to the end of the search. The non-convexity of the search space highlights\nthe cost associated to optimize the parameter set associated with static transformations.\n13\n\n\nPreprint, Under Review\nTable 9: Core results for topological reasoning across all tasks and models. We show the mean value of\nthe score function E (↓), which is defined for each task in Section 5. GoT100, GoT50, GoT25 represent the\nobtained values from static schedule parameters obtained at convergeance, 50% and 25% of convergeance\ntrials, respectively.\nTask\nLlama-70b\nLlama-405b\nGoT25\nGoT50\nGoT100\nGoTLLM\nGoT25\nGoT50\nGoT100\nGoTLLM\nsorting32\n0.82\n0.95\n0.73\n1.29\n0.74\n0.82\n0.28\n0.22\nsorting64\n4.73\n4.73\n4.64\n10.04\n2.22\n2.74\n3.46\n9.15\nsorting128\n16.18\n13.86\n16.07\n31.79\n13.96\n12.65\n18.65\n32.74\nset-intersection32\n0.41\n0.0\n0.37\n1.22\n0.07\n0.0\n0.09\n0.03\nset-intersection64\n3.40\n2.66\n1.27\n7.34\n0.67\n0.64\n0.72\n1.08\nset-intersection128\n13.23\n12.92\n12.73\n22.98\n1.07\n0\n2.54\n4.62\nD\nBENCHMARKS\nWe choose two popular tasks for topological reasoning with LLMs, which are amenable to a divide-and-\nconquer strategy (i.e. decomposition, solving subproblems and merging): list sorting and set intersection.\nDespite their simplicity, prior works have shown that these tasks are extremely challenging for LLMs with\ndirect prompting (Besta et al., 2024a).\nSorting: involves sorting a list of numbers between 0 and 9 in ascending order. The error function E = X+Y\nhas its subterms defined in Equation 5, where a is the input list and b is a candidate solution. X corresponds\nto the number of incorrectly sorted pairs, while Y corresponds to the frequency difference between a and b\nfor each digit.\nX =\nm−1\nX\ni=1\nsign(max(bi −bi+1, 0))\nY =\n9\nX\ni=0\n||{bp : bp = i}| −|{aq : aq = i}||\n(5)\nSet Intersection: involves finding the intersection of sets A and B. The error function is defined in Equa-\ntion 6, where C is the candidate solution. The first and second terms correspond to missing and extra ele-\nments, respectively.\nE = |(A ∩B) \\ C| + |C \\ (A ∩B)|\n(6)\n14\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21208v1.pdf",
    "total_pages": 14,
    "title": "ARIES: Autonomous Reasoning with LLMs on Interactive Thought Graph Environments",
    "authors": [
      "Pedro Gimenes",
      "Zeyu Cao",
      "Jeffrey Wong",
      "Yiren Zhao"
    ],
    "abstract": "Recent research has shown that LLM performance on reasoning tasks can be\nenhanced by scaling test-time compute. One promising approach, particularly\nwith decomposable problems, involves arranging intermediate solutions as a\ngraph on which transformations are performed to explore the solution space.\nHowever, prior works rely on pre-determined, task-specific transformation\nschedules which are subject to a set of searched hyperparameters. In this work,\nwe view thought graph transformations as actions in a Markov decision process,\nand implement policy agents to drive effective action policies for the\nunderlying reasoning LLM agent. In particular, we investigate the ability for\nanother LLM to act as a policy agent on thought graph environments and\nintroduce ARIES, a multi-agent architecture for reasoning with LLMs. In ARIES,\nreasoning LLM agents solve decomposed subproblems, while policy LLM agents\nmaintain visibility of the thought graph states, and dynamically adapt the\nproblem-solving strategy. Through extensive experiments, we observe that using\noff-the-shelf LLMs as policy agents with no supervised fine-tuning (SFT) can\nyield up to $29\\%$ higher accuracy on HumanEval relative to static\ntransformation schedules, as well as reducing inference costs by $35\\%$ and\navoid any search requirements. We also conduct a thorough analysis of observed\nfailure modes, highlighting that limitations on LLM sizes and the depth of\nproblem decomposition can be seen as challenges to scaling LLM-guided\nreasoning.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}