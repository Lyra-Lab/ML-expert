{
  "id": "arxiv_2502.20933v1",
  "text": "Large Language Models Are Innate Crystal Structure Generators\nJingru Gan 1 Peichen Zhong 2 * Yuanqi Du 3 * Yanqiao Zhu 1 Chenru Duan 4 Haorui Wang 5\nCarla P. Gomes 3 Kristin A. Persson 2 Daniel Schwalbe-Koda 1 Wei Wang 1\nAbstract\nCrystal structure generation is fundamental to ma-\nterials discovery, enabling the prediction of novel\nmaterials with desired properties. While exist-\ning approaches leverage Large Language Models\n(LLMs) through extensive fine-tuning on mate-\nrials databases, we show that pre-trained LLMs\ncan inherently generate stable crystal structures\nwithout additional training. Our novel framework\nMatLLMSearch integrates pre-trained LLMs\nwith evolutionary search algorithms, achieving\na 78.38% metastable rate validated by machine\nlearning interatomic potentials and 31.7% DFT-\nverified stability via quantum mechanical calcula-\ntions, outperforming specialized models such as\nCrystalTextLLM. Beyond crystal structure gener-\nation, we further demonstrate that our framework\ncan be readily adapted to diverse materials design\ntasks, including crystal structure prediction and\nmulti-objective optimization of properties such as\ndeformation energy and bulk modulus, all without\nfine-tuning. These results establish pre-trained\nLLMs as versatile and effective tools for materi-\nals discovery, opening up new venues for crystal\nstructure generation with reduced computational\noverhead and broader accessibility.\n1. Introduction\nDiscovering materials with desired properties remains a fun-\ndamental challenge in materials science. The critical step\nis predicting thermodynamically stable crystal structures,\nwhich determine the physical and chemical characteristics of\na material (Bagayoko, 2014). While experimental synthesis\nand characterization remain the gold standard, computa-\ntional approaches have emerged as indispensable tools for\naccelerating materials discovery (Dunn et al., 2020; Eremin\n*Equal contribution 1University of California, Los Angeles, Los\nAngeles, CA, USA 2University of California, Berkeley, Berkeley,\nCA, USA 3Cornell University, Ithaca, NY, USA 4Deep Princi-\nple Inc., Cambridge, MA, USA 5Georgia Institute of Technol-\nogy, Atlanta, GA, USA. Correspondence to: Jingru Gan <jr-\ngan@cs.ucla.edu>.\net al., 2023). The field has evolved from evolutionary al-\ngorithms to deep learning approaches. Early evolutionary\nalgorithms provide effective strategies for exploring the vast\nchemical space of possible structures (Allahyari & Oganov,\n2020), enabling automated property-guided materials op-\ntimization. Recent advances in deep learning have intro-\nduced various generative models for structure prediction,\nranging from variational autoencoders that learn compact\ncrystalline representations to diffusion and flow models for\ndirect atomic configuration sampling (Flam-Shepherd &\nAspuru-Guzik, 2023; Gruver et al., 2024; Jiao et al., 2024;\nXie et al., 2022; Zeni et al., 2025). These models employ\ngraph neural networks to capture complex many-body inter-\nactions and crystallographic symmetries.\nMore recently, Large Language Models (LLMs) have\nemerged as promising tools for crystal structure genera-\ntion (Achiam et al., 2023; Antunes et al., 2023; Fu et al.,\n2023). The seminal work by Flam-Shepherd & Aspuru-\nGuzik (2023) demonstrates that auto-regressive models with\ncharacter-level tokenization can generate chemically valid\ncrystal structures. Subsequent work (Gruver et al., 2024)\nshows that fine-tuning pre-trained language models like\nLlama (Grattafiori et al., 2024) on materials datasets can\nproduce physically stable crystal structures. Given the vast\nscientific corpora that LLMs are pre-trained on, we hypothe-\nsize that these models already possess rich chemical knowl-\nedge that could enable direct crystal structure generation,\neliminating the the computational overhead of specialized\nfine-tuning. To verify this, we pose a challenging question:\nCan pre-trained LLMs be directly used to generate stable\ncrystal structures without additional fine-tuning?\nWhile promising, leveraging pre-trained LLMs for crys-\ntal structure generation faces several challenges: guiding\nthe LLMs to output valid crystal structure representations,\npreserving crystallographic constraints in proposed struc-\ntures, and ensuring thermodynamically stability of final\nconfigurations. To address them, we introduce MatLLM-\nSearch, a novel framework that synergistically integrates\nthe chemical space exploration capabilities of evolutionary\nalgorithms with the rich chemical knowledge embedded in\npre-trained LLMs. As illustrated in Figure 1, our framework\nimplements an iterative pipeline with three key stages: (1)\nSelection identifies promising candidate structures to guide\n1\narXiv:2502.20933v1  [cond-mat.mtrl-sci]  28 Feb 2025\n\n\nLarge Language Models Are Innate Crystal Structure Generators\nsubsequent generations, (2) Reproduction guides LLMs\nin breeding new candidates from parent structures via im-\nplicit crossover and mutations, and (3) Evaluation enforces\ncrystallographic constraints and assesses thermodynamic\nstability through a comprehensive validation pipeline.\nThrough comprehensive experiments, we show that our\nframework successfully generates diverse, thermodynam-\nically stable crystal structures while maintaining crystal-\nlographic validity. Guided by MatLLMSearch, the LLM\nachieves a 76.81% metastable structure generation rate, with\n31.70% of structures verified as stable through DFT cal-\nculations, surpassing the state-of-the-art fine-tuned model\nCrystalTextLLM (Gruver et al., 2024). Notably, this perfor-\nmance is achieved with minimal computational overhead,\nrequiring only LLM inference and stability evaluation rather\nthan extensive model training. Also, we only use thousands\nof reference structures, while CrystalTextLLM requires fine-\ntuning on the full Materials Project database of 45,231 stable\nstructures (Jain et al., 2013).\nBeyond crystal structure generation, our framework demon-\nstrates remarkable flexibility across various materials dis-\ncovery tasks. Through simple modifications in prompting\nand reference structure selection criteria, our method ex-\ntends to crystal structure prediction, which is validated by\nthe discovery of several metastable Na3AlCl6 polymorphs\nwith significantly higher stability than existing structures in\nthe Materials Project database. Furthermore, the framework\nenables multi-objective optimization of properties such as\nbulk modulus, suggesting its versatility across the spectrum\nof materials discovery challenges.\n2. Background: Computational Materials\nDiscovery with Machine Learning\n2.1. Problem Definition\nCrystal Structure Generation (CSG). The objective of\nCSG is to learn a probability distribution p(c, l, s) over crys-\ntalline materials, where c ∈RN×K represents the chemical\ncomposition matrix for N atoms of K distinct chemical\nspecies, l ∈R6 denotes the lattice parameters (lengths and\nangles), and s ∈RN×3 defines the spatial coordinates of\natoms within a periodic unit cell. Samples drawn from this\ndistribution should ideally satisfy fundamental thermody-\nnamic stability criteria (defined in Section 2.2).\nCrystal Structure Prediction (CSP). CSP addresses a more\nconstrained problem of determining stable crystal structures\nfor a specified chemical composition. Formally, it learns\na conditional probability distribution p(s, l | c) to iden-\ntify thermodynamically favorable atomic arrangements and\nlattice parameters given a fixed composition c. This formu-\nlation addresses the practical scenario of discovering stable\npolymorphs for a specified chemical formula.\nCrystal Structure Design (CSD). CSD extends beyond\nstructure prediction by incorporating property optimiza-\ntion and conditional generation. An example objective is\nfinding the optimal crystal structure that maximizes a tar-\nget property h(c, l, s): m∗= argmaxc,l,s∼p(c,l,s) h(c, l, s),\nwhere h : RN×K × R6 × RN×3 →R represents an ora-\ncle function evaluating the desired materials property. It\ncan also be formulated as sampling from a tilted distribu-\ntion p(c, l, s) exp(h(c, l, s)) (Rafailov et al., 2024). Addi-\ntional constraints can be integrated into the design process,\nallowing for flexible tasks such as compositional substitu-\ntion (learning p(c | l, s)) and composition/structure comple-\ntion (inpainting generation, learning p(cunknown, sunknown |\ncknown, l, sknown)) (Dai et al., 2024).\n2.2. (Meta)Stability of Materials\nAmong computational approaches for evaluating crystal\nstructure stability, Density Functional Theory (DFT) calcu-\nlations stand as the most reliable method for predicting for-\nmation energies in solid-state materials, showing close align-\nment with experimental measurements (Jain et al., 2011;\nSun et al., 2016). The thermodynamic stability of a struc-\nture is quantified through its decomposition energy (Ed)\nwith respect to the convex hull of known stable phases:\nEd = Es −P\ni xiEi, where Es represents the total energy\nper atom, xi denotes the molar fraction of the i-th compet-\ning phase, and Ei corresponds to its ground-state energy per\natom. While the convex hull serves as a fixed reference, the\nevaluated structure s need not be part of this hull. A negative\ndecomposition energy (Ed < 0) indicates a thermodynam-\nically stable state below the convex hull, while Ed > 0\nsuggests a metastable phase with a driving force for decom-\nposition into more stable compounds. Our main objective is\nto identify stable crystal structures where Ed ≤0.\nGiven the computational intensity of DFT calculations, uni-\nversal Machine Learning Interatomic Potentials (MLIPs),\ntrained on millions of DFT calculations, have emerged as\nefficient and reliable proxies for structure stability assess-\nment. Notable among these is CHGNet (Deng et al., 2023),\na Graph Neural Network (GNN)-based MLIP that uniquely\nincorporates magnetic moments to capture both atomic and\nelectronic interactions. M3GNet (Chen & Ong, 2022) offers\nan alternative approach, implementing three-body interac-\ntions in its graph architecture for accurate structural pre-\ndictions across diverse chemical spaces. Recent advances\nin universal MLIPs include MACE (Batatia et al., 2023),\nDPA-1 (Zhang et al., 2024), and JMP (Shoghi et al., 2024),\nwhich demonstrate high accuracy in predicting crystal ther-\nmodynamic stability, particularly when trained on industrial-\nscale datasets comprising millions of compounds and non-\nequilibrium atomic configurations (Barroso-Luque et al.,\n2024; Merchant et al., 2023; Yang et al., 2024). In this work,\nwe employ the pre-trained CHGNet as our universal MLIP\n2\n\n\nLarge Language Models Are Innate Crystal Structure Generators\ndue to its closer alignment with DFT results, using a fixed\nphase diagram derived from the Materials Project 2023 DFT\ncalculations (Jain et al., 2011; Wang et al., 2021).\n3. MatLLMSearch\nWe propose MatLLMSearch, an evolutionary workflow\nthat leverages pre-trained LLMs to search for stable and\noptimized crystal structures with. In this section, we intro-\nduce three key stages of the workflow as illustrated in Fig-\nure 1: (1) Selection, which identifies promising candidate\nstructures from existing pools based on stability and prop-\nerty metrics; (2) Reproduction, where the LLM generates\nnew candidates through implicit crossover and mutations of\nparent structures; and (3) Evaluation, which assesses pro-\nposed structures for validity, stability, and target properties.\nThe overall workflow, outlined in Algorithm 1, iteratively\nevolves a population of crystal structures while maintaining\nphysical constraints and optimizing desired properties.\nAlgorithm 1 The MatLLMSearch Framework\nRequire: Population size K, parent size P, reproduction\nsize C, number of iterations N, known stable\nstructures D, oracle function O, extra pool R\n1: ▷Initialization\n2: Form population P0 by sampling K groups of P struc-\ntures from D\n3: Initialize structure collection S ←∅\n4: for i ←0, 1, · · · , (N −1) do\n5:\n▷LLM-guided reproduction\n6:\nGenerate prompts from parent structures in Pi\n7:\nObtain offspring structures Ci via LLM inference\nand parsing\n8:\n▷Structure evaluation\n9:\nRelax structures Ci ←CHGNetRelax(Ci)\n10:\nCalculate decomposition energy Ed and properties\n11:\nEvaluate objective scores using oracle function O\n12:\nUpdate structure collection S ←S ∪Ci\n13:\n▷Selection\n14:\nForm candidate pool from parents Pi, offspring Ci,\nand extra pool R\n15:\nSelect top-(K × P) structures based on objective\nscores from the candidate pool\n16:\nConstruct next parent groups Pi+1\n17: Validate final structures via DFT\n18: return cumulated structures S\n3.1. Initialization\nOur evolutionary search begins by constructing a diverse\nand valid starting population. We sample (K×P) structures\nfrom a set of known stable structures D to form our initial\nparent pool P0, where K is the population size and P is\nthe number of parent structures per group. These structures\nare organized into K groups of P parents each to serve\nas reference examples in LLM prompts, with the LLM be-\ning queried K times to generate new candidate structures.\nThis grouping strategy enables the LLM to analyze multiple\nreference structures simultaneously when proposing new\ncandidates. Optionally, we can retrieve an extra pool of\nstructures R from D to expand the candidate space during\nthe selection stage. R can be customized to suit various\ndesign objectives, with more details and ablation studies\nprovided in Section 4.3. The initialization parameters and\ndetailed sampling strategy are described in Section 4.1.\n3.2. Reproduction\nGenetic algorithms traditionally mimic biological evolution\nthrough explicit crossover and mutation operations (Heiles\n& Johnston, 2013; Johnston, 2003). In crystal structure pre-\ndiction, crossover typically involves combining structural\nfragments from parent structures (e.g., swapping atomic\npositions or structural motifs), while mutation introduces\nrandom variations through predefined operations like atomic\ndisplacement, lattice transformation, or element substitu-\ntion (Curtis et al., 2018; Kadan et al., 2023). While effec-\ntive, these rigid operators can limit the exploration of the\ncomplex crystal structure space. In MatLLMSearch, we\nexplore the flexibility of LLMs for structure reproduction.\nThrough prompt-based guidance, we ask LLMs to perform\nimplicit crossover and mutation by analyzing and combining\nstructural information from parent materials. Specifically,\nLLMs are instructed to “modify or combine the base mate-\nrials”, while maintaining chemical validity and enhancing\ntarget properties. This approach allows LLMs to freely and\nsimultaneously introduce variations across multiple struc-\ntural aspects, including atomic positions, lattice parameters,\nand element substitutions, or even generate completely new\nstructures functionally relevant to parent structures.\n3.3. Evaluation\nIn genetic algorithms, evaluation serves as a crucial bridge\nbetween reproduction and selection by assessing the fit-\nness of offspring structures. Following LLM-guided re-\nproduction, we employ a two-stage evaluation pipeline to\nvalidate and evaluate the generated structures and ensure\nthey represent physically meaningful candidates for the next\ngeneration. Specifically, the evaluation process integrates\nrule-based filters for fundamental physical constraints and\nquantitative stability metrics, with optional additional prop-\nerty calculations to assess candidate performance.\nRule-based structure validation. We first apply a series\nof basic criteria to validate structural integrity. Each parsed\nstructure is extracted from LLM responses into standard-\nized crystallographic formats and must satisfy fundamental\nphysical requirements, most importantly three-dimensional\n3\n\n\nLarge Language Models Are Innate Crystal Structure Generators\nSelection\nCandidates\nRb2 Na1 Tm1 F6\n1.0\n6.2710 0.0000 3.5629\n2.0879 5.9153 3.6252\n0.0000 0.0000 7.3001\nRb Na Tm F\n2 1 1 6\ndirect\n0.2500 0.2500 0.2500 Rb\n0.7500 0.7500 0.7500 Rb\n0.5000 0.5000 0.5000 Na\n0.0000 0.0000 0.0000 Tm\n0.2040 0.7960 0.2040 F\n……\nReproduction\nEvaluation\nStability calculation\nProperty calculation \nFinal pool\nSelection\nReproduction\nEvaluation\nParsing & Validation\n• Validity & symmetry check\n• Compositional check\n• Charge balance check\nDFT\nMatBench\nParent pool Pi\nExtra pool R\nLLM responses\nChildren Ci\nGeneration\nLLM-proposed structures\nExtra Pool R\nTop K×P candidates\nPi+1 Ci+1\nRanking\nCSG\nCSP\nEd and other properties\nDecomposition energy Ed\nPre-trained LLMs\n…The proposed new materials \ncan be a modification or \ncombination of the base \nmaterials given below…\nPrompting\nChildren Ci\nEd\nInitial pool P0\nIndustrial\nevaluation\nProperties\ne.g., bulk modulus\nStructure Relaxation\nParent pool Pi–1\nValid children Ci–1\nPi→1 →Ci→1 →R\n!n\ni=1 Ci\nFigure 1. The workflow of MatLLMSearch for crystal structure generation. Starting from an initial population of known structures, our\nframework iteratively evolves new crystal structures through LLM-guided reproduction, evaluation, and selection.\nperiodicity with proper boundary conditions. Then, physical\nconnectivity is ensured by requiring valid bonding for each\natom, defined as interatomic distances between 0.6 to 1.3\ntimes the sum of constituent atomic radii. Chemical validity\nis verified through charge balance analysis based on for-\nmal valence states of the constituent elements. To maintain\nstructural diversity in the population, duplicate structures\ngenerated within the same iteration are eliminated.\nStability and property evaluation. Children structures\nthat satisfy the rule-based validation will undergo evalua-\ntion of stability and other specific target properties based\non the design objectives. Since LLM-proposed structures\nmay not be at their local energy minimum, each structure is\nfirst relaxed using CHGNet. We monitor the energy differ-\nence ∆E between relaxed and initial states, where a larger\n|∆E| indicates the initial structure required more significant\nrelaxation to reach stability. Notably, we show that LLM-\nproposed structures typically require minimal relaxation,\nwith 61.1% of structures exhibiting small energy changes\n(|∆E| < 0.5 eV/atom) during this process (detailed in Ap-\npendix G). The choice of evaluation metrics depends on the\noptimization objectives. For stability-focused optimization,\nwe quantify thermodynamic stability through the decomposi-\ntion energy Ed using CHGNet, calculated as the distance to\nthe convex hull from the Materials Project database (version\n2023-02-07-ppd-mp). For mechanical property-oriented\nobjectives, other metrics such as bulk modulus can be com-\nputed in this stage. These quantitative scores then guide the\nselection process for subsequent generations, allowing our\nframework to flexibly adapt to different design goals.\n3.4. Selection\nLast, the selection stage evolves a population of candidate\nstructures that meet the optimization objectives, such as\nthermodynamic stability or other desired physical proper-\nties. For each iteration i, we construct a new parent pool\nPi+1 of the same size (K × P) by selecting top-ranked\ncandidates from three sources: the current parent pool (Pi),\nnewly generated children structures (Ci), and an optional ex-\ntra pool (R) to improve diversity. Candidates in Pi ∪Ci ∪R\nare ranked according to optimization objectives. For single-\nobjective optimization, we can select based on either lower\ndecomposition energy Ed (for stability) or higher bulk mod-\nulus (as an example for property optimization). For multi-\nobjective optimization, we alternate among multiple objec-\ntives, with additional strategies detailed in Appendix D.\n3.5. Final DFT Verification\nAfter completing all evolutionary iterations, we collect\nthe cumulated offspring structures S = S\ni Ci for final\nvalidation using Density Functional Theory (DFT). To\nsave computational cost, we focus on meta-stable struc-\ntures with CHGNet-predicted decomposition energy Ed <\n0.1 eV/atom. DFT calculations are performed using VASP\n6 in the Generalized Gradient Approximation (GGA) with\nPBE functional (Perdew et al., 1996), using the projector-\naugmented wave method (Kresse & Furthmüller, 1996;\nKresse & Joubert, 1999). We employed a plane-wave basis\nset with an energy cutoff of 520 eV and a k-point mesh\nof 1,000 per reciprocal atom (Jain et al., 2013). The cal-\nculations converged to 10−6 eV in total energy for elec-\n4\n\n\nLarge Language Models Are Innate Crystal Structure Generators\ntronic self-consistent field cycles and 0.02 eV/Å in inter-\natomic forces for the ionic steps. The computational settings\nare consistent with MPGGARelaxSet and MPGGAStatic-\nSet (Jain et al., 2011).\n4. Experiments\n4.1. Experimental Settings\nWe use Llama 3.1 (70B) (Grattafiori et al., 2024) as the base\nLLM. We set temperature to 0.95 to balance creativity and\nreliability. All experiments use parent size P = 2, repro-\nduction size C = 5, and N = 10 iterations, with population\nsize K = 100 unless otherwise specified. Crystal structures\nare represented in POSCAR format with 12 decimal digits.\nInitialization. We use the MatBench dataset (Dunn et al.,\n2020) as the known stable structure set D. From D, we\nselect 3,500 known stable structures as the extra pool R,\nchosen based on their CHGNet-predicted band gaps closest\nto 3 eV. This selection criterion biases our pool towards\nsemiconductors and insulators, which often exhibit more\ndiverse and well-defined crystal structures compared to met-\nals. Detailed ablation studies regarding this selection policy\nare provided in Appendix A.\n4.2. Main Experimental Results\nCrystal structure generation. We first evaluate the ability\nof our framework to generate stable crystal structures by\noptimizing decomposition energy Ed as the sole objective.\nThe LLM prompting template is detailed in Appendix C.\nThe generation results are reported in Table 1. Following\nprevious work (Gruver et al., 2024; Xie et al., 2022), we re-\nport structural and compositional validity, which assess non-\noverlapping atomic radii and charge neutrality respectively.\nMetastability is evaluated using both CHGNet and M3GNet\nas surrogate models, measuring the percentage of struc-\ntures with decomposition energies below 0.1 eV/atom and\n0.03 eV/atom thresholds. Structures identified as metastable\n(Ed < 0.1 eV/atom) by CHGNet undergo further DFT cal-\nculations for stability assessment.\nWe compare our model against two baseline models CD-\nVAE (Xie et al., 2022) and CrystalTextLLM (Gruver et al.,\n2024). Among 1,479 generated structures, 76.8% and 81.1%\nare metastable based on CHGNet and M3GNet evaluations\nrespectively, outperforming the 49.8% metastability rate by\nM3GNet of the state-of-the-art CrystalTextLLM 70B model,\nwhich has a comparable model size to our base model. Un-\nder rigorous DFT validation, 31.7% of the metastable struc-\ntures remain stable, substantially improving the 10.6% sta-\nbility rate from CrystalTextLLM 70B.\nHowever, structures containing f-electron elements (ac-\ntinides and lanthanides, abbreviated as f-ele) lead to chal-\n−0.3\n−0.2\n−0.1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nEd (eV/atom)\n0\n50\n100\n150\n200\n250\n300\n350\nBulk Modulus (GPa)\nStability\nMulti-turn\nBulk Modulus\nFigure 2. Pareto frontiers of bulk modulus versus decomposition\nenergy (Ed) for structures optimized towards stability, bulk modu-\nlus and multi-objective (multi-turn). Ellipses indicate regions of\nhighest structure density.\nlenges in stability prediction due to their strongly correlated\nelectron interactions, which may not be adequately captured\nby DFT approaches under GGA and Hubbard U correc-\ntions (Anisimov et al., 1997). We find that structures with\nf-block elements consistently yield lower decomposition\nenergies (Ed), posting a potential computational shortcut in\nthe optimization process. To assess this effect, we report\nthe percentage of stable structures without f-ele (denoted\nas “w/o f-ele”) among the metastable structures.\nBased on this observation, we implemented a mitigation\nstrategy that excludes structures containing f-electron ele-\nments from being selected as parents. Under this interven-\ntion, the metastability rate improves to 78.4%, while the\nDFT-verified stability slightly decreases to 27.0%. Most no-\ntably, the proportion of stable structures without f-electrons\nincreases significantly from 14.0% to 24.6%, indicating our\napproach effectively explores alternative stable configura-\ntions. While this computational shortcut remains largely\nunaddressed by existing methods, our framework demon-\nstrates effective control over structural exploration through\nsimple interventions in the evolutionary process.\nWhile achieving better performance, our method also offers\nsignificant computational advantages. Compared to Crys-\ntalTextLLM which requires extensive fine-tuning on more\nthan 120K structures, we achieve higher stability rates using\nonly a few reference structures and direct LLM inference.\nThe computational cost is primarily from structure evalua-\ntion rather than model training or fine-tuning, making our\napproach more accessible.\nCrystal structure design. We also explore multi-objective\noptimization by extending our framework to balance sta-\nbility with desired material properties. We demonstrate\nthis capability by alternating between optimizing stability\n(Ed) and bulk modulus in each iteration. While this multi-\nobjective setting naturally yields lower stability rates (57.1%\nmetastable with Ed < 0.1 eV/atom and 15.6% DFT-verified\n5\n\n\nLarge Language Models Are Innate Crystal Structure Generators\nModel\nf-ele in Parents†\nValidity\nMetastability\nStability‡\nStructural\nComposition\nM3GNet\nCHGNet\nDFT\nEd < 0.1\nEd < 0.1\nEd < 0.03\nw/ f-ele\nw/o f-ele§\nCDVAE∗\n—\n100.0%\n86.7%\n28.8%\n—\n—\n5.4%\n—\nCrystalTextLLM-7B∗\n—\n96.4%\n93.3%\n35.0%\n—\n—\n8.7%\n—\nCrystalTextLLM-13B∗\n—\n95.5%\n92.4%\n38.0%\n—\n—\n14.4%\n—\nCrystalTextLLM-70B∗\n—\n99.6%\n95.4%\n49.8%\n—\n—\n10.6%\n—\nMatLLMSearch\n(Llama 3.1-70B)\n✓\n100.0%\n79.4%\n81.1%\n76.8%\n56.5%\n31.7%\n14.0%\n✗\n100.0%\n89.0%\n81.9%\n78.4%\n54.8%\n27.0%\n24.6%\nTable 1. Performance comparison of crystal structure generation. Metastability is first assessed using surrogate models, where we report\nboth M3GNet and CHGNet results for fair comparison with baselines CDVAE and CrystalTextLLM (which use M3GNet). ∗Results\ntaken from the original papers. †Indicates whether f-electron elements are excluded in parent structures (not applicable to CDVAE and\nCrystalTextLLM as they are trained on data including f-electron elements). ‡The stable fraction represents the percentage of DFT-verified\nstable structures (Ed < 0.0 eV/atom) over structures predicted to be metastable (Ed < 0.1 eV/atom) by respective surrogate models\n(M3GNet for CDVAE and CrystalTextLLM, CHGNet for ours, with CHGNet being more rigorous as evidenced by lower metastability\nrates). §We exclude structures containing f-electron in DFT verification while keeping the denominator as all metastable structures.\nstable structures with f-electron elements) compared to\nstability-only optimization, it enables the discovery of struc-\ntures with favorable property-stability trade-offs.\nAs shown in Figure 2, the Pareto frontiers under various opti-\nmization strategies converge in regions with high bulk mod-\nulus (> 200 GPa) and metastability (Ed ≤0.1 eV/atom) in\nthe stability-property space, indicating successful discovery\nof potentially valuable structures that balance both objec-\ntives. The regions of highest structure density, estimated\nusing Gaussian KDE and visualized as ellipses, reveal how\noptimization goals affect the distribution. Prioritizing bulk\nmodulus shifts the density distribution toward higher me-\nchanical strength at the cost of increased decomposition en-\nergy. We provide additional discussions of property-specific\nand multi-objective optimization strategies in Appendix D.\nCrystal structure prediction. We next evaluate our frame-\nwork on crystal structure prediction tasks, which aim to\npredict stable structure (i.e. lattice and atomic coordinates)\nfor a given composition.\nAs a case study, we prompt\nthe LLM to predict polymorphs of Na3AlCl6. For con-\ntext, the Materials Project database currently contains only\none structure for this composition (mp-1111450, Fm¯3m,\nEd = 0.142 eV/atom), which is significantly unstable.\nDuring the prompting process, we apply specific structural\nfilters to select seed structures containing only three distinct\nelements in a 3:1:6 ratio, matching the stoichiometry of\nNa3AlCl6. From MatBench, we identified 820 structures\nmeeting these criteria, which formed our initial and extra\nretrieval pool. Example structures proposed by the LLM\nfor this composition are visualized in Figure 3, with DFT-\nverified decomposition energies of 0.024 and 0.032 eV/atom\nrespectively. Although these predicted polymorphs remain\nmetastable, their decomposition energies Ed are signifi-\ncantly lower than the previously reported structure in Mat-\nBench (Ed reduced by up to 83%), exemplifying the poten-\ntial of our evolutionary pipeline for CSP applications.\nNa12Al4Cl24\nSpace group: Cc\nEd = 0.024 eV/atom\nNa12Al4Cl24\nSpace group: P21\nEd = 0.032 eV/atom\nFigure 3. Examples of predicted crystal structures with composi-\ntion Na3AlCl6.\n4.3. Detailed Analysis\nTo better understand the effectiveness of our framework, we\nconduct a comprehensive analysis by examining three key\naspects: the evolution of parent structure quality across iter-\nations, the impact of extra pool size on generation, and the\ndiversity of generated structures. Additional ablation studies\non factors affecting generation performance are discussed\nin Appendices H and I and ??.\nEvolution of parent structure quality. Figure 4 illustrates\nthe distribution of decomposition energy and the propor-\ntion of LLM-proposed structures using different extra pool\nsizes. The effectiveness of our proposed evolutionary search\nis demonstrated by the progressive improvement in parent\nstructure quality. We also observe a systematic transition\nfrom MatBench-sourced to LLM-generated parent struc-\ntures across successive generations, regardless of pool size\nconfigurations. This growing proportion of LLM-generated\nstructures in the parent pool indicates our framework effec-\ntively explores and optimizes the stability landscape.\nImpact of extra pool size. To evaluate how additional refer-\nence structures affect generation performance, we examined\n6\n\n\nLarge Language Models Are Innate Crystal Structure Generators\n2\n4\n6\n8\n10\nIteration\n−0.2\n−0.1\n0.0\n0.1\n0.2\n0.3\nDecomposition energy Ed (eV/atom)\nExtra Pool Size\nNo Extra Pool\n1,000\n3,500\nStatistics\nMedian\nPeak\n0\n20\n40\n60\n80\n100\nLLM-proposed Parents (%)\nExtra Pool Size\nNo Extra Pool\n1,000\n3,500\nStatistics\nMedian\nPeak\nFigure 4. Comparison across different extra pool sizes. (1) Decom-\nposition energy Ed distributions for generated structures (violin\nplots with solid median and dotted peak lines). (2) Percentage\nof LLM-proposed structures in the parent pool across iterations\n(dashed curves).\nExtra Pool Size\nEd < 0.1 eV/atom\nEd < 0.03 eV/atom\nNo Extra Pool\n71.25%\n37.45%\n1,000\n80.97%\n58.56%\n3,500\n76.81%\n56.52%\nTable 2. Metastability rates (percentage of generated structures)\nunder different extra pool size (decomposition energy evaluated by\nCHGNet at thresholds of Ed < 0.1 eV/atom and 0.03 eV/atom).\nthree configurations: (1) no extra pool, using only the initial\n(K×P) randomly selected structures, (2) an extra 1,000 ran-\ndomly selected structures, and (3) an extra 3,500 structures\nretrieved with band gaps closest to 3 eV.\nTable 2 and Figure 4 reveal that introducing a reference\npool significantly improves (meta)stability rate, but with\ndiminishing returns for larger pools. The metastability rate\n(Ed < 0.1 eV/atom) increases substantially from 71.25% to\n80.97% when adding the 1,000 extra structures, but plateaus\nwith further expansion to 3,500 structures. Beyond stability\nmetrics, each configuration exhibits distinct compositional\npatterns. Structures generated with no extra pool show di-\nverse combinations with transition metal compounds, while\nthe 1,000 extra pool configuration yields more balanced\ncation-anion distributions. The 3,500 pool demonstrates a\npreference for stable fluoride-based compounds, with Cs-\nF-Rb appearing as the most frequent combination (1.2%\noccurrence). This shift in compositional preferences sug-\ngests that larger pools enable more focused exploration of\nchemically favorable regions while maintaining structural\ndiversity. Further analyses showing specific crystal struc-\ntures and detailed compositional diversity across different\npool sizes are presented in Figure S3 in Appendix E.\nStructural and compositional diversity.\nTo evaluate\nthe diversity of our generated structures, we analyzed their\ncompositional and structural characteristics by comparing\nH\nHe\nLi\nBe\nB\nC\nN\nO\nF\nNe\nNa\nMg\nAl\nSi\nP\nS\nCl\nAr\nK\nCa\nSc\nTi\nV\nCr\nMn\nFe\nCo\nNi\nCu\nZn\nGa\nGe\nAs\nSe\nBr\nKr\nRb\nSr\nY\nZr\nNb\nMo\nTc\nRu\nRh\nPd\nAg\nCd\nIn\nSn\nSb\nTe\nI\nXe\nCs\nBa\nHf\nTa\nW\nRe\nOs\nIr\nPt\nAu\nHg\nTl\nPb\nBi\nPo\nAt\nRn\nFr\nRa\nRf\nDb\nSg\nBh\nHs\nMt\nDs\nRg\nCn\nNh\nFl\nMc\nLv\nTs\nOg\nLa\nCe\nPr\nNd\nPm\nSm\nEu\nGd\nTb\nDy\nHo\nEr\nTm\nYb\nLu\nAc\nTh\nPa\nU\nNp\nPu\nAm\nCm\nBk\nCf\nEs\nFm\nMd\nNo\nLr\nElement Frequencies (LLM-Proposed Structures)\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\nElement Frequency\nH\nHe\nLi\nBe\nB\nC\nN\nO\nF\nNe\nNa\nMg\nAl\nSi\nP\nS\nCl\nAr\nK\nCa\nSc\nTi\nV\nCr\nMn\nFe\nCo\nNi\nCu\nZn\nGa\nGe\nAs\nSe\nBr\nKr\nRb\nSr\nY\nZr\nNb\nMo\nTc\nRu\nRh\nPd\nAg\nCd\nIn\nSn\nSb\nTe\nI\nXe\nCs\nBa\nHf\nTa\nW\nRe\nOs\nIr\nPt\nAu\nHg\nTl\nPb\nBi\nPo\nAt\nRn\nFr\nRa\nRf\nDb\nSg\nBh\nHs\nMt\nDs\nRg\nCn\nNh\nFl\nMc\nLv\nTs\nOg\nLa\nCe\nPr\nNd\nPm\nSm\nEu\nGd\nTb\nDy\nHo\nEr\nTm\nYb\nLu\nAc\nTh\nPa\nU\nNp\nPu\nAm\nCm\nBk\nCf\nEs\nFm\nMd\nNo\nLr\nElement Frequencies (3,500 Extra Pool)\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\nElement Frequency\nFigure 5. Element frequencies in LLM-proposed structures (above)\nand extra pool structures (bottom).\nLLM-proposed structures and with the extra pool. Figure 5\npresents element frequency distributions for both sets. The\nresults show a compositional evolution from predominantly\ntransition metal oxides in reference structures to alkali met-\nals and halogens, with fluorine (F) appearing in 8.6% of the\nLLM-proposed structures.\nOur element co-occurrence analysis reveals high composi-\ntional diversity in the LLM-proposed structures, with even\nthe most frequent compositions appearing only twice (ap-\nproximately 0.14% of total structures). Examination of\nelement co-occurrences with F in Figure 6 highlights the ef-\nfectiveness of our evolutionary method in guiding structure\ngeneration toward stable F-based compounds particularly\nwith alkali metals and transition metals. The structural di-\nversity is further evidenced in Figure 7, which compares\ncrystal system distributions as determined by the Space-\ngroupAnalyzer from pymatgen (Ong et al., 2012). This dis-\ntribution confirms that our evolutionary method successfully\nnavigates toward stable regions of chemical space while\nmaintaining diverse structural motifs across different crystal\nsystems. Additional diversity and novelty evaluations and\nanalyses are provided in Appendix F.\n5. Related Work\n5.1. Language Models for Materials Science\nThe increasing capabilities of LLMs have prompted mate-\nrials science community to explore their potential for un-\n7\n\n\nLarge Language Models Are Innate Crystal Structure Generators\nAl Ba Cr Cs Er Eu Gd Ho K\nLi Lu Na Np O Rb Sc Tb Ti Tm V\nY Yb\nAl\nBa\nCr\nCs\nEr\nEu\nGd\nHo\nK\nLi\nLu\nNa\nNp\nO\nRb\nSc\nTb\nTi\nTm\nV\nY\nYb\nElement Co-occurrence with F (LLM-Proposed Structures)\n0\n5\n10\n15\n20\n25\nNumber of Unique Compositions\nInstance\nCount\n11\n23\n35\n47\nAl B Ba Be C Ca Co Cs Cu Fe H Hg K Li Mn N Na O\nP Rb Si V\nY\nAl\nB\nBa\nBe\nC\nCa\nCo\nCs\nCu\nFe\nH\nHg\nK\nLi\nMn\nN\nNa\nO\nP\nRb\nSi\nV\nY\nElement Co-occurrence with F (3,500 Extra Pool)\n0\n5\n10\n15\n20\n25\nNumber of Unique Compositions\nInstance\nCount\n10\n21\n32\n43\nFigure 6. Element co-occurrence patterns with fluorine (F) in LLM-proposed structures\n(left) versus 3,500 extra pool structures (right). Bubble size indicates frequency of\noccurrence for each element pair, while color intensity represents compositional diversity\n(darker indicates more unique compositions with that element pair).\nCubic\n11.4%\nHexagonal\n4.4%\nMonoclinic\n26.5%\nOrthorhombic 18.5%\nTetragonal\n14.5%\nTriclinic\n10.0%\nTrigonal\n14.7%\n13.0%\n7.3%\n22.0%\n14.7%\n16.5%\n15.6%\n10.9%\nOuter: Extra Pool\nInner: LLM Proposed\nFigure 7. Crystal systems distribution com-\nparison between extra pool of 3,500 structures\n(outer ring) and LLM-proposed structures (in-\nner pie).\nderstanding and predicting material properties (Jablonka\net al., 2023). However, benchmarking studies suggest fine-\ntuning LLMs over specific materials datasets is necessary\nto achieve performance comparable to or better than spe-\ncialized graph neural networks (Rubungo et al., 2024). Re-\nsearch in crystal structure generation has developed along\ntwo main paths. Flam-Shepherd & Aspuru-Guzik (2023)\ndemonstrate that autoregressive models trained from scratch\nwith character-level tokenization can generate chemically\nvalid crystal structures by directly tokenizing CIF files into\nstring sequences. Secondly, CrystalTextLLM (Gruver et al.,\n2024) fine-tunes a pre-trained LLM (over massive texts) on\ngenerating crystalline structures with task-specific prompts.\nWhile these approaches produce valid structures, they sac-\nrifice the general conversation capabilities of LLMs due\nto specialized training or fine-tuning on crystallographic\ndata.\nIn parallel developments within molecular chem-\nistry, MolLEO (Wang et al., 2025) successfully employs\npre-trained LLMs without domain-specific fine-tuning to\nsearch for small molecules. Subsequent work (Lu et al.,\n2024) extended this evolutionary optimization approach to\nmore complex transition metal chemistry using advanced\nbase LLMs with enhanced reasoning capabilities. However,\nthese applications benefit from natural string representations\nfor molecules (e.g., SMILES or SELFIES), which are con-\nsiderably simpler than the three-dimensional representations\nrequired for crystal structures. Our work bridges this gap\nby adapting the evolutionary approach to the more complex\ndomain of crystal structures without requiring fine-tuning.\n5.2. Generative Models for Materials Discovery\nBesides autoregressive language models, various generative\nmodels including variational autoencoders, diffusion mod-\nels, and flow models have emerged as promising solutions\nfor crystal structure generation. Early work proposes gener-\native crystal structures using variational autoencoders that\nrepresent crystal structures as 3D voxels (Court et al., 2020;\nNoh et al., 2019). CDVAE first proposes to generate crystal\nstructures with a score-based generative (diffusion) model\nand optimize crystal structure properties through gradient-\nbased optimization in the latent space (Xie et al., 2022).\nThis approach has been extended in several directions: Jiao\net al. (2024) developed Riemannian diffusion models to bet-\nter handle periodic coordinates, Zeni et al. (2025) scaled\nthe approach to encompass elements across the entire peri-\nodic table with various design criteria, and Dai et al. (2024)\napplied it to crystal inpainting tasks. Most recently, Sri-\nram et al. (2024) introduced Riemannian flow matching\nmodels to better address periodic boundary conditions with\nimproved performance.\n6. Conclusion\nIn this paper, we present an evolutionary workflow for com-\nputational materials discovery, encompassing crystal struc-\nture generation, prediction, and objective-based optimiza-\ntion. We demonstrate that a pre-trained LLM trained on\ngeneral text can identify a higher proportion of (meta)stable\nmaterials compared to state-of-the-art generative models\nspecifically trained on materials datasets. These findings\nsuggest that LLMs inherently function as effective crystal\nstructure generators, with both compositional and structural\ninformation naturally embedded within their text inference\ncapabilities. In conclusion, our method complements ex-\nisting structure discovery techniques by providing refined\noptimization capabilities while maintaining versatility in\naddressing various optimization objectives, offering an effi-\ncient approach for high-throughput materials discovery.\nLimitations and Future Work. Our study serves as a proof-\nof-concept and requires further validation in real-world ma-\n8\n\n\nLarge Language Models Are Innate Crystal Structure Generators\nterials discovery workflows. While we have demonstrated\nthat LLM inference is a powerful tool for searching materi-\nals under thermodynamic stability guidance, the practical\nrealization of new materials remains challenging, particu-\nlarly in terms of successful synthesis.\nOne limitation observed in our CSP tasks is that the gener-\nated structures exhibit similarities to the provided reference\nstructures. The evolutionary nature of the genetic algorithm\nnaturally favors incremental modifications over radical struc-\ntural changes. Additionally, LLMs exhibit an inductive bias\ntoward known stable structures, often resorting to their pre-\ntrained knowledge and simple atomic substitutions. Never-\ntheless, our approach can serve as an effective optimization\ntool in addition to the suggestion of novel structural proto-\ntypes, which can be more readily obtained through alterna-\ntive methods, including variational autoencoders (Ren et al.,\n2022; Zhu et al., 2024), diffusion models (Zeni et al., 2025),\nrandom structure searching (Pickard & Needs, 2011), or\nresponse-matching approaches (Cheng, 2024b). However,\nthe capability of these methods for comprehensive materials\ndiscovery across diverse chemical spaces remains under-\nexplored. In addition, it is an open question that whether\nthe LLM-proposed materials design hypotheses are free of\nintellectual property issues.\nLooking forward, a natural extension of this work would\nbe synthesis prediction based on the evolutionary method.\nImproved machine learning interatomic potentials will com-\nplement this process, as discussed in Appendix J. Such de-\nvelopment would benefit from integration with high-quality\nexperimental data from automated, high-throughput experi-\nments, bridging the gap between computational predictions\nand experimental synthesis, which would accelerate high-\nthroughput materials discovery.\nImpact Statement\nThis work aims to advance machine learning and compu-\ntational materials discovery by making crystal structure\ngeneration more accessible and efficient. This advancement\nwill particularly benefit researchers who have limited com-\nputational resources, enabling scientific discovery without\nthe need to train large ML models. Additionally, the oracle\nfunctions can be further adapted to incorporate experimental\ndata or high-fidelity property predictors for the generated\ncrystal structures when applying this pipeline to practical\nmaterials discovery.\nReferences\nAchiam, J., Adler, S., Agarwal, S., et al. GPT-4 Technical\nReport. arXiv.org, 2023. 1\nAllahyari, Z. and Oganov, A. R. Coevolutionary Search\nfor Optimal Materials in the Space of All Possible Com-\npounds. npj Comput. Mater., 2020. 1\nAnisimov, V. I., Aryasetiawan, F., and Lichtenstein, A. I.\nFirst-Principles Calculations of the Electronic Structure\nand Spectra of Strongly Correlated Systems: The LDA+\nU Method. J. Phys.: Condens. Matter, 1997. 5\nAntunes, L. M., Butler, K. T., and Grau-Crespo, R. Crys-\ntal Structure Generation with Autoregressive Large Lan-\nguage Modeling. Nat. Commun., 2023. 1\nBagayoko, D. Understanding Density Functional Theory\n(DFT) and Completing It in Practice. AIP Adv., 2014. 1\nBarroso-Luque, L., Shuaibi, M., Fu, X., et al. Open Mate-\nrials 2024 (OMAT24) Inorganic Materials Dataset and\nModels. arXiv.org, 2024. 2, 18\nBatatia, I., Benner, P., Chiang, Y., et al. A Foundation Model\nfor Atomistic Materials Chemistry. arXiv.org, 2023. 2,\n18\nBatzner, S., Musaelian, A., Sun, L., et al. E(3)-Equivariant\nGraph Neural Networks for Data-Efficient and Accurate\nInteratomic Potentials. Nat. Commun., 2022. 18\nBitzek, E., Koskinen, P., Gähler, F., et al. Structural Relax-\nation Made Simple. Phys. Rev. Lett., 2006. 18\nChen, C. and Ong, S. A Universal Graph Deep Learning\nInteratomic Potential for the Periodic Table. Nat. Comput.\nSci., 2022. 2, 18\nCheng, B. Cartesian Atomic Cluster Expansion for Machine\nLearning Interatomic Potentials. npj Comput. Mater.,\n2024a. 18\nCheng, B. Response Matching for Generating Materials and\nMolecules. J. Chem. Theory Comput., 2024b. 9\nCourt, C. J., Yildirim, B., Jain, A., et al. 3-D Inorganic\nCrystal Structure Generation and Property Prediction via\nRepresentation Learning. J. Chem. Inf. Model., 2020. 8\nCurtis, F., Li, X., Rose, T., et al. GAtor: A First-Principles\nGenetic Algorithm for Molecular Crystal Structure Pre-\ndiction. J. Chem. Theory Comput., 2018. 3\nDai, X., Zhong, P., Deng, B., et al. Inpainting Crystal\nStructure Generations with Score-Based Denoising. In\nICML Workshop on AI for Science, 2024. 2, 8\nDeng, B., Zhong, P., Jun, K., et al. CHGNet as a Pretrained\nUniversal Neural Network Potential for Charge-Informed\nAtomistic Modelling. Nat. Mach. Intell., 2023. 2, 18\nDu, Y., Wang, L., Feng, D., et al. A New Perspective on\nBuilding Efficient and Expressive 3D Equivariant Graph\nNeural Networks. NeurIPS, 2023a. 18\n9\n\n\nLarge Language Models Are Innate Crystal Structure Generators\nDu, Y., Wang, Y., Huang, Y., et al. M2Hub: Unlocking the\nPotential of Machine Learning for Materials Discovery.\nNeurIPS, 2023b. 18\nDunn, A., Wang, Q., Ganose, A., et al. Benchmarking\nMaterials Property Prediction Methods: The Matbench\nTest Set and Automatminer Reference Algorithm. npj\nComput. Mater., 2020. 1, 5, 12\nEremin, R., Humonen, I., Kazakov, A., et al. Graph Neural\nNetworks for Predicting Structural Stability of Cd- and\nZn-doped λ-CsPbI3. Comput. Mater. Sci., 2023. 1\nFlam-Shepherd, D. and Aspuru-Guzik, A. Language Models\nCan Generate Molecules, Materials, and Protein Binding\nSites Directly in Three Dimensions as XYZ, CIF, and\nPDB Files. arXiv.org, 2023. 1, 8, 16\nFu, N., Wei, L., Song, Y., et al. Material Transformers:\nDeep Learning Language Models for Generative Materi-\nals Design. Mach. Learn.: Sci. Technol., 2023. 1\nGrattafiori, A., Dubey, A., Jauhri, A., et al. The Llama 3\nHerd of Models. arXiv.org, 2024. 1, 5\nGruver, N., Sriram, A., Madotto, A., et al. Fine-Tuned\nLanguage Models Generate Stable Inorganic Materials as\nText. In ICLR, 2024. 1, 2, 5, 8, 15, 16\nHeiles, S. and Johnston, R. L. Global Optimization of Clus-\nters Using Electronic Structure Methods. Int. J. Quantum\nChem., 2013. 3\nJablonka, K. M., Ai, Q., Al-Feghali, A., et al. 14 Examples\nof How LLMs Can Transform Materials Science and\nChemistry: A Reflection on a Large Language Model\nHackathon. Digit. Discov., 2023. 8\nJain, A., Hautier, G., Ong, S. P., et al. Formation Enthalpies\nby Mixing GGA and GGA + U Calculations. Phys. Rev.\nB, 2011. 2, 3, 5\nJain, A., Ong, S. P., Hautier, G., et al. Commentary: The\nMaterials Project: A Materials Genome Approach to\nAccelerating Materials Innovation. APL Mater., 2013. 2,\n4\nJiao, R., Huang, W., Lin, P., et al. Crystal Structure Predic-\ntion by Joint Equivariant Diffusion. NeurIPS, 2024. 1, 8,\n16\nJohnston, R. L. Evolving Better Nanoparticles: Genetic\nAlgorithms for Optimising Cluster Geometries. Dalton\nTrans., 2003. 3\nKadan, A., Ryczko, K., Wildman, A., et al. Accelerated\nOrganic Crystal Structure Prediction with Genetic Algo-\nrithms and Machine Learning. J. Chem. Theory Comput.,\n2023. 3\nKresse, G. and Furthmüller, J. Efficient Iterative Schemes\nfor Ab Initio Total-Energy Calculations Using a Plane-\nWave Basis Set. Phys. Rev. B, 1996. 4\nKresse, G. and Joubert, D. From Ultrasoft Pseudopotentials\nto the Projector Augmented-Wave Method. Phys. Rev. B,\n1999. 4\nLiao, Y.-L., Wood, B., Das, A., et al. EquiformerV2: Im-\nproved Equivariant Transformer for Scaling to Higher-\nDegree Representations. In ICLR, 2024. 18\nL’opez-Zorrilla, J., Aretxabaleta, X. M., Yeu, I. W., et al.\nænet-PyTorch: A GPU-Supported Implementation for\nMachine Learning Atomic Potentials Training. J. Chem.\nPhys., 2023. 18\nLu, J., Song, Z., Zhao, Q., et al. Generative Design of Func-\ntional Metal Complexes Utilizing the Internal Knowledge\nof Large Language Models. arXiv.org, 2024. 8\nMerchant, A., Batzner, S., Schoenholz, S., et al. Scaling\nDeep Learning for Materials Discovery. Nature, 2023. 2,\n18\nNoh, J., Kim, J., Stein, H. S., et al. Inverse Design of Solid-\nState Materials via a Continuous Representation. Matter,\n2019. 8\nOng, S. P., Richards, W. D., Jain, A., et al. Python Materials\nGenomics (pymatgen): A Robust, Open-Source Python\nLibrary for Materials Analysis. Comput. Mater. Sci., 2012.\n7\nPark, Y., Kim, J., Hwang, S., et al. Scalable Parallel Algo-\nrithm for Graph Neural Network Interatomic Potentials\nin Molecular Dynamics Simulations. J. Chem. Theory\nComput., 2024. 18\nPeeperkorn, M., Kouwenhoven, T., Brown, D. G., et al. Is\nTemperature the Creativity Parameter of Large Language\nModels? In ICCC, 2024. 17\nPerdew, J. P., Burke, K., and Ernzerhof, M. Generalized\nGradient Approximation Made Simple. Phys. Rev. Lett.,\n1996. 4\nPickard, C. J. and Needs, R. J. Ab Initio Random Structure\nSearching. J. Phys.: Condens. Matter, 2011. 9\nRafailov, R., Sharma, A., Mitchell, E., et al. Direct Prefer-\nence Optimization: Your Language Model Is Secretly a\nReward Model. NeurIPS, 2024. 2\nRen, Z., Tian, S. I. P., Noh, J., et al. An Invertible Crys-\ntallographic Representation for General Inverse Design\nof Inorganic Crystals with Targeted Properties. Matter,\n2022. 9\n10\n\n\nLarge Language Models Are Innate Crystal Structure Generators\nRubungo, A. N., Li, K., Hattrick-Simpers, J., et al.\nLLM4Mat-Bench: Benchmarking Large Language Mod-\nels for Materials Property Prediction. arXiv.org, 2024.\n8\nShoghi, N., Kolluru, A., Kitchin, J. R., et al.\nFrom\nMolecules to Materials: Pre-Training Large Generaliz-\nable Models for Atomic Property Prediction. In ICLR,\n2024. 2, 18\nSriram, A., Miller, B. K., Chen, R. T. Q., et al. FlowLLM:\nFlow Matching for Material Generation with Large Lan-\nguage Models as Base Distributions. In NeurIPS, 2024.\n8, 16\nSun, W., Dacek, S. T., Ong, S. P., et al. The Thermodynamic\nScale of Inorganic Crystalline Metastability. Sci. Adv.,\n2016. 2\nWang, A., Kingsbury, R., McDermott, M., et al. A Frame-\nwork for Quantifying Uncertainty in DFT Energy Correc-\ntions. Sci. Rep., 2021. 3, 18\nWang, H., Skreta, M., Ser, C.-T., et al. Efficient Evolution-\nary Search over Chemical Space with Large Language\nModels. In ICLR, 2025. 8\nWen, M., Horton, M. K., Munro, J. M., et al. An Equivariant\nGraph Neural Network for the Elasticity Tensors of All\nSeven Crystal Systems. Digit. Discov., 2024. 13\nXie, T., Fu, X., Ganea, O.-E., et al.\nCrystal Diffusion\nVariational Autoencoder for Periodic Material Generation.\nIn ICLR, 2022. 1, 5, 8, 15, 16\nYang, H., Hu, C., Zhou, Y., et al. MatterSim: A Deep Learn-\ning Atomistic Model Across Elements, Temperatures and\nPressures. arXiv.org, 2024. 2, 18\nYin, B., Wang, J., Du, W., et al. AlphaNet: Scaling Up Local\nFrame-Based Atomistic Foundation Model. arXiv.org,\n2025. 18\nZeni, C., Pinsler, R., Z\"ugner, D., et al. A Generative Model\nfor Inorganic Materials Design. Nature, 2025. 1, 8, 9, 16\nZhang, D., Bi, H., Dai, F.-Z., et al. Pretraining of Attention-\nBased Deep Learning Potential Model for Molecular Sim-\nulation. npj Comput. Math., 2024. 2, 18\nZhang, L., Wang, H., Car, R., et al. Phase Diagram of a\nDeep Potential Water Model. Phys. Rev. Lett., 2021. 18\nZhu, R., Nong, W., Yamazaki, S., et al. WyCryst: Wyckoff\nInorganic Crystal Generator Framework. Matter, 2024. 9\n11\n\n\nLarge Language Models Are Innate Crystal Structure Generators\nSupplementary Material for MatLLMSearch\nA. Experimental Details of Population Initialization\nThe retrieval set R used consists of 3,500 stable structures sampled from known stable structures–Matbench-bandgap dataset\n(Dunn et al., 2020), which consists of 106,113 crystal structures in total. To initialize the parent structures for the first\niteration, we applied a simple rule-based structure sampling to the structures. First, we checked if the composition was\ncharge-balanced. Second, we verified that for each atom in the crystal structure, there exists at least one valid bond with\nanother site. In addition, we removed structures with simple or overly complicated compositions, i.e., keeping candidate\nstructures with 3 to 6 elements. Finally, we applied random shuffling and de-duplication by composition to the candidate\nstructures. For computational efficiency, we took the top 3,500 structures with a bandgap closest to 3 eV from the pool as\nextra pool of reference structures during the selection step. The analysis of how the size and sampling rule of the extra pool\naffect the performance is provided in Section 4.3. To further enhance the structure generation, we envision future work that\ncould explore how structures can be ensembled to form a larger candidate pool for parent selection.\nB. Reproducibility\nThe crystal structures generated by MatLLMSearch can be downloaded here. The implementation of our evolutionary\nsearch pipeline is available here.\nC. Prompt for CSG\nYou are an expert material scientist. Your task is to propose hypotheses for {reproduction_size} new\nmaterials with valid stable structures and compositions. No isolated or overlapped atoms are allowed.\nThe proposed new materials can be a modification or combination of the base materials given below.\nFormat requirements:\n1. Each proposed structure must be formatted in JSON with the following structure:\n{{\n\"i\":\n{{\n\"formula\":\n\"composition_formula\",\n\"POSCAR\":\n\"POSCAR_format_string\"\n}}\n}}\n2. Use proper JSON escaping for newlines (\\n) and other special characters\nBase material structure for reference:\n{reference_structures}\nYour task:\n1. Generate {reproduction_size} new structure hypotheses\n2. Each structure should be stable and physically reasonable\n3. Format each structure exactly as shown in the input\nOutput your hypotheses below:\n12\n\n\nLarge Language Models Are Innate Crystal Structure Generators\nModel\nf-ele in Parents\nValidity\nMetastability\nStructural\nComposition\nM3GNet\nCHGNet\nEd < 0.1\nEd < 0.1\nEd < 0.03\nCDVAE\n—\n100.0%\n86.7%\n28.8%\n—\n—\nCrystalTextLLM-7B\n—\n96.4%\n93.3%\n35.0%\n—\n—\nCrystalTextLLM-13B\n—\n95.5%\n92.4%\n38.0%\n—\n—\nCrystalTextLLM-70B\n—\n99.6%\n95.4%\n49.8%\n—\n—\nMatLLMSearch\n(Llama 3.1-70B)\nStability\n100.0%\n79.4%\n81.1%\n76.8%\n56.5%\nBulk Modulus\n100.0%\n82.9%\n27.0%\n43.3%\n8.3%\nMulti-turn\n100.0%\n84.2%\n70.9%\n57.1%\n29.8%\nWeighted sum\n100.0%\n85.1%\n61.8%\n52.3%\n27.4%\nTable S1. Compare experimental results under various optimization goals. We explored multi-objective optimization for stability and bulk\nmodulus in two different ways.\nEd\nBulk Modulus\n−0.2\n0.0\n0.2\n0.4\n0.6\nEd (eV/atom)\n0\n50\n100\n150\n200\n250\n300\nBulk Modulus (GPa)\nStability\nMulti-turn\nWeighted Sum\nBulk Modulus\nFigure S1. Comparison of optimization strategies targeting different objec-\ntives evaluated based on thermodynamic stability (decomposition energy\nEd) and mechanical property (bulk modulus).\nCubic\n13.0%\nHexagonal\n7.3%\nMonoclinic\n22.0%\nOrthorhombic\n14.7%\nTetragonal\n16.5%\nTriclinic\n15.6%\nTrigonal\n10.9%\n17.6%\n3.5%\n20.7%\n16.5%\n16.5%\n13.7%\n11.5%\n20.8%\n3.0%\n15.3%\n13.5%\n19.2%\n16.8%\n11.4%\n22.9%\n5.0%\n13.9%\n11.9%\n17.3%\n13.7%\n15.4%\nBulk Modulus\nWeighted Sum\nMulti-turn\nStability\nFigure S2. Crystal systems distribution under var-\nied objectives.\nD. Additional Experiments of Stable and Optimized Crystal Structure Generation\nThe flexibility of our evolutionary pipeline is demonstrated by its ability to guide LLMs in proposing novel crystal structures\nwith diverse mechanical characteristics. We further evaluate model performance under four distinct optimization strategies:\n(1) stability-oriented optimization (“Stability”), (2) property-oriented optimization (“Bulk Modulus”), (3) alternating multi-\nobjective optimization (“Multi-turn”), and (4) weighted-sum optimization (“Weighted Sum”). As shown in Table S1, all four\noptimization strategies maintain high metastability rates for the proposed structures, which demonstrate that our algorithm\ncan optimize specific properties while maintaining structural validity and stability. Our multi-objective strategies successfully\nnavigate the inherent trade-offs, maintaining reasonable stability while achieving improved mechanical properties.\nBulk modulus optimization. To validate the capability of MatLLMSearch for property-guided generation, we conduct\nsingle-property optimization by modifying the selection criteria from decomposition energy (Ed) to bulk modulus. In\ncrystalline solids, bulk modulus serves as a key indicator for designing materials with enhanced mechanical hardness. Our\nexperiments used bulk modulus values derived from the Birch-Murnaghan equation of state as a proof of concept. For more\ncomprehensive materials design applications, this approach can be extended to include elastic tensors from DFT calculations\nor tensorial predictions using equivariant graph neural networks (Wen et al., 2024).\nFigure S1 presents the distribution comparison of decomposition energy (Ed) and bulk modulus for structures generated\nunder varied optimization strategies, revealing distinct performance trade-offs. The bulk modulus optimization generated\nmore structures with larger bulk modulus values, reaching a peak density at 194 GPa compared to only 19 GPa in stability-\noriented optimization. However, this enhancement comes at the cost of increased decomposition energy, with the Ed density\npeaks shifting from 0.0 eV/atom in stability-oriented optimization to 0.1 eV/atom in bulk modulus optimization, indicating\n13\n\n\nLarge Language Models Are Innate Crystal Structure Generators\nAs B BaCaCoCrCsCu F FeGe H K LiMnMoNaNi P Rb S Sb Se Si Sn Sr Ti V Zn\nAs\nB\nBa\nCa\nCo\nCr\nCs\nCu\nF\nFe\nGe\nH\nK\nLi\nMn\nMo\nNa\nNi\nP\nRb\nS\nSb\nSe\nSi\nSn\nSr\nTi\nV\nZn\nElement Co-occurrence with O (No Extra Pool)\n0\n5\n10\n15\n20\nNumber of Unique Compositions\nInstance\nCount\n7\n14\n21\n28\nAs B BaCaCoCrCsCu F FeGe H K LiMnMoNaNi P Rb S Sb Se Si Sn Sr Ti V Zn\nAs\nB\nBa\nCa\nCo\nCr\nCs\nCu\nF\nFe\nGe\nH\nK\nLi\nMn\nMo\nNa\nNi\nP\nRb\nS\nSb\nSe\nSi\nSn\nSr\nTi\nV\nZn\nElement Co-occurrence with O (1K)\n0\n5\n10\n15\n20\nNumber of Unique Compositions\nInstance\nCount\n7\n14\n21\n28\nAs B BaCaCoCrCsCu F FeGe H K LiMnMoNaNi P Rb S Sb Se Si Sn Sr Ti V Zn\nAs\nB\nBa\nCa\nCo\nCr\nCs\nCu\nF\nFe\nGe\nH\nK\nLi\nMn\nMo\nNa\nNi\nP\nRb\nS\nSb\nSe\nSi\nSn\nSr\nTi\nV\nZn\nElement Co-occurrence with O (3.5K)\n0\n5\n10\n15\n20\nNumber of Unique Compositions\nInstance\nCount\n7\n14\n21\n28\nFigure S3. Element co-occurrence patterns with oxygen (O) in LLM-proposed structures across three different extra pool configurations:\nno extra pool (left), 1,000 random structures (middle), and 3,500 structures with band gaps closest to 3 eV (right). Bubble size represents\nfrequency of occurrence while color intensity indicates compositional diversity.\nreduced thermodynamically stability across iterations.\nMulti-objective optimization. Beyond single-objective optimization, we explored multi-objective optimization approaches\nto simultaneously target both thermodynamic stability and mechanical properties using two different multi-objective\noptimization strategies.\nThe first approach implements an alternating optimization strategy (“Multi-turn”), where the algorithm alternates between\noptimizing stability and property in successive iterations. Stability is optimized in the first iteration to set a foundation for\nproperty optimization. For customized multi-objective optimization, the number of iterations for each optimization goal can\nbe adjusted. As shown in Figure S1, this method achieves balanced performance in optimizing stability and bulk modulus,\nwith Ed centered around 0.037 eV/atom. We observe that bulk modulus distribution separates structures into groups with\nhigh mechanical strength at moderate stability versus high stability with lower mechanical strength, suggesting the inherent\ntrade-off in crystal structure generation.\nOur second methodology employs a weighted sum approach, combining decomposition energy Ed and bulk modulus in a\nsingle objective function J = 10Ed −BulkModulus. After sorting the candidate pool by this objective, we select the top\nstructures as parents for subsequent generations. The weighted sum strategy produces crystal structures with bulk modulus\ncentered around 141 GPa and Ed densely centered at 0.034 eV/atom. While single-objective stability optimization achieves\nthe highest metastability rate of 76.81%, both multi-objective approaches maintain rates above 50% while enhancing\nmechanical properties.\nIn addition, the analysis of crystal system distributions in Figure S2 reveals relatively uniform representation across all\noptimization strategies, indicating that our framework preserves structural diversity regardless of the optimization objective.\nE. Analysis of Extra Reference Pools\nIn Section 4.3, we examined three configurations of extra pool: (1) no extra pool, using only the initial randomly selected\nstructures (K × P), (2) an extra 1,000 randomly selected structures, and (3) an extra 3,500 structures retrieved with band\ngaps closest to 3 eV.\nStability performance. Our analysis reveals that structure generation achieves optimal metastability rates with a moderate-\nsized extra pool of reference structures, as demonstrated by the rates of 80.97% and 76.81% with 1,000 and 3,500 extra\nreference structures, respectively. These results indicate that while additional reference structures improve stability outcomes\nover using no extra pool (71.25%), the returns diminish as the pool size increases beyond a few thousand structures.\nEvolution of parent source. As shown in Figure 4, larger extra pools demonstrate more gradual adoption of LLM-generated\nparents across iterations. This pattern indicates more thorough exploration of the reference space before transitioning to\nLLM-generated structures, suggesting that larger pools provide a broader foundation for structure generation.\nCompositional diversity. Analysis of element combinations reveals distinct patterns in LLM-proposed structures across\n14\n\n\nLarge Language Models Are Innate Crystal Structure Generators\nComposition Diversity\nStructure Diversity\nStructure Novelty\nComposition Novelty\nOverall Novelty\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nScore\n0.82\n0.78\n0.47\n0.72\n0.81\n0.80\n0.71\n0.64\n0.80\n0.90\n0.82\n0.81\n0.24\n0.68\n0.72\n0.82\n0.79\n0.37\n0.62\n0.72\n0.82\n0.77\n0.23\n0.71\n0.75\n0.81\n0.69\n0.75\n0.91\n0.96\n0.78\n0.62\n0.85\n0.96\n0.98\nDiversity Metrics\nNovelty Metrics\nStability\nw/o f-ele\nBulk Modulus\nMulti-obj (Multi-turn)\nMulti-obj (Weighted Sum)\nExtra Pool (1,000)\nNo Extra Pool\nFigure S4. Diversity and novelty evaluation results for structures proposed under different experimental settings.\ndifferent extra pool configurations. Structures generated with no extra pool show diverse combinations with transition\nmetal compounds, while the 1,000-structure extra pool exhibits more balanced cation-anion distributions. The 3,500-\nstructure pool demonstrates a preference for stable fluoride-based compounds, with Cs-F-Rb appearing as the most frequent\ncombination (1.2% occurrence). Figure S3 illustrates the oxygen-containing compounds proposed by LLMs across the three\nconfigurations. With no extra pool or a small extra pool, the LLM tends to propose safer and less novel oxygen-containing\ncompositions. In contrast, larger pools enable greater exploration into chemically diverse spaces, particularly stable fluorine\ncompounds. This shift in compositional preferences suggests that larger pools enable more focused exploration of chemically\nfavorable regions while maintaining structural diversity.\nF. Evaluation on Diversity and Novelty of Generated Structures\nWe quantitatively evaluate the diversity and novelty of structures generated by our framework across configurations using\nestablished metrics from prior work (Gruver et al., 2024; Xie et al., 2022). Crystal diversity is measured by computing\npairwise distances between their structural and compositional fingerprints. Additionally, we apply log normalization to\ncomposition diversity for 0-1 scale standardization. The novelty measures the distance between generated samples and their\nclosest neighbors in the extra pool of reference structures. The structural distance cutoff and composition distance cutoff\nused for novelty calculation are 0.1 and 2 respectively. To align with previous work, all metrics are computed on structures\npredicted to be metastable.\nThe results are summarized in Figure S4. Across different optimization goals, we observe an interesting trade-off between\nproperty-specific optimization and novelty, balancing targeted enhancement against chemical space exploration. When\noptimizing beyond stability alone, such as targeting bulk modulus or performing multi-objective crystal structure design, we\nobserve decreased novelty while diversity remains consistently high across all optimization goals.\nOur investigation of extra pool sizes produced a seemingly contradictory finding: smaller reference pools yield higher novelty\nscores numerically, while larger extra pools lead to structures with distributions better aligned with stable compositions\nbeyond simple oxygen compounds, as analyzed in Appendix E. This apparent contradiction highlights limitations of these\nmetrics in our specific context. Since these metrics primarily measure overlap between training and generated structures, and\nour extra pools are substantially smaller than typical training datasets used in previous work, they cannot comprehensively\ncharacterize the quality of the generated distributions. This underscores the need for more nuanced evaluation metrics that\naccount for the evolutionary nature of our framework and its guided exploration of the chemical space.\nG. Impact of Structure Relaxation\nTo measure the contribution of structural relaxation in our framework, we introduce a quantity ∆E to represent the energy\ndifference after and before structural relaxation using CHGNet. Figure S5 reveals that the majority of the proposed structures\nproposed by LLMs exhibit a relatively small ∆E, with 61.1% showing minimal energy changes (|∆E| < 0.5 eV/atom)\nduring relaxation. This distribution indicates that our framework generates physically meaningful structures that are already\nclose to their local energy minima, requiring only modest refinements through relaxation.\n15\n\n\nLarge Language Models Are Innate Crystal Structure Generators\n−0.4\n−0.2\n0.0\n0.2\n0.4\n0.6\n0.8\nEd (eV/atom)\n−4\n−3\n−2\n−1\n0\n∆E (eV/atom)\nEnergy Distribution\nEd ≥0.1\nEd < 0.1 (Metastable)\nMetastable threshold\n0\n200\n400\n600\nCount\n−4\n−3\n−2\n−1\n0\n∆E (eV/atom)\n6\n7\n12\n49\n∆E Distribution\nFigure S5. Distribution of energy change ∆E before/after structural relaxation and decomposition energy (Ed) for structures proposed by\nLLM, evaluated using the pretrained CHGNet.\nMethod\nPrimary Format\nGenerative\nModel\nTraining\nCDVAE (Xie et al., 2022)\n3D\nDiffusion\nGNN\nTraining\nMatterGen (Zeni et al., 2025)\n3D\nDiffusion\nGNN\nTraining\nFlam-Shepherd & Aspuru-Guzik (2023)\n3D\nAR\nTransformer\nTraining\nDiffCSP (Jiao et al., 2024)\n3D\nDiffusion\nGNN\nTraining\nCrystalTextLLM (Gruver et al., 2024)\nText/CIF\nLLM\nTransformer\nFine-tuning\nFlowMM (Sriram et al., 2024)\n3D\nFlow\nGNN\nTraining\nMatLLMSearch (Ours)\nText/CIF/POSCAR\nLLM\nLlama 3.1\nN/A\nTable S2. A collection of generative models on computational materials discovery. Training denotes if training/fine-tuning is required on\nmaterials databases. CSG, CSP, and CSD are abbreviations for three tasks considered (Section 2.1).\nH. Impact of Structure String Formatting\nA number of computational methods has emerged for crystal structure generation using machine learning approaches,\nas shown in Table S2. Most methods represent crystal structures using 3D information processed through either Graph\nNeural Networks (GNN) or Transformer architectures, employing various generative strategies like diffusion models or\nautoregressive approaches. More recently, text-based formats and Large Language Models (LLMs) have emerged as an\nalternative approach, signaling a promising shift in crystal structure generation and analysis techniques.\nThe encoding of crystallographic structures into text-based format is essential for LLM processing, making the structural\nrepresentation an important consideration in our framework design. We investigated the impact of different formatting\nstrategies on generation efficiency and performance: CIF format and POSCAR format with either 4 or 12 decimal places of\nprecision. See Figure S7 for examples.\nFirst, we examine the token efficiency by analyzing the MatBench dataset for token length distribution as shown in Figure S6.\nThe distribution indicates that the POSCAR format with 4 decimal places offers the most token-efficient representation\nwhile maintaining reasonable precision, followed by the POSCAR with 12 digits and CIF format. CIF format requires more\ntokens than POSCAR format, given that CIF uses a more verbose structure and additional metadata.\nPerformance evaluation shown in Table S3 suggests that POSCAR formatting in 12 decimal places demonstrates slightly\nbetter overall performance in the rate of (meta)stability of generated structures under different criteria (Ed < 0.03 or 0.1\neV/atom). Therefore, we employ POSCAR of 12 decimal places as a trade-off results of token efficiency and informativeness.\nThe marginal difference across format may be attributed to the crystallographic data exposed to the LLMs during pre-training.\nHowever, it is noteworthy that performance differences across formats remain modest, suggesting the resilience of our\napproach across different structural representations.\n16\n\n\nLarge Language Models Are Innate Crystal Structure Generators\nFormat\n# Unique / # Total generated\nEd < 0.1 eV/atom\nEd < 0.03 eV/atom\nPOSCAR (4)\n76.7%\n75.4%\n55.3%\nPOSCAR (12)\n72.3%\n76.8%\n56.5%\nCIF\n75.1%\n68.9%\n49.5%\nTable S3. Proportion of unique structures and their CHGNet-predicted metastability using different structure formats.\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\nNumber of Tokens\n0\n5000\n10000\n15000\nNumber of Structures\nPOSCAR (4 digits) µ=617\nPOSCAR (12 digits) µ=1080\nCIF µ=1221\nFigure S6. Token efficiency comparison under CIF formatting and\nPOSCAR formatting for the precision of 4 and 12 decimal. µ\nindicate the mean of token lengths.\nCIF Format\n# generated using pymatgen\ndata_Rb2NaTmF6\n_symmetry_space_group_name_H-M   'P 1'\n_cell_length_a\n7.21244860\n_cell_length_b\n7.24515020\n_cell_length_c\n7.30009700\n_cell_angle_alpha\n59.97563106\n_cell_angle_beta\n60.39638285\n_cell_angle_gamma\n60.14919667\n_symmetry_Int_Tables_number\n1\n_chemical_formula_structural\nRb2NaTmF6\n_chemical_formula_sum\n'Rb2 Na1 Tm1 F6'\n_cell_volume\n270.79301120\n_cell_formula_units_Z\n1\nloop_\n_symmetry_equiv_pos_site_id\n_symmetry_equiv_pos_as_xyz\n……\nPOSCAR Format\nRb2 Na1 Tm1 F6\n1.0\n6.2709625677875156    0.0000000000000000    3.5629262244747322\n2.0879142593840205    5.9152705413161168    3.6252434248004177\n0.0000000000000000    0.0000000000000000    7.3000970000000001\nRb Na Tm F\n2 1 1 6\ndirect\n0.2500000000000000    0.2500000000000000    0.2500000000000000 Rb\n0.7500000000000000    0.7500000000000000    0.7500000000000000 Rb\n0.5000000000000000    0.5000000000000000    0.5000000000000000 Na\n0.0000000000000000    0.0000000000000000    0.0000000000000000 Tm\n0.2039646300000000    0.7960353700000000    0.2039646300000000 F\n0.7960353700000000    0.7960353700000000    0.2039646300000000 F\n0.7960353700000000    0.2039646300000000    0.7960353700000000 F\n0.7960353700000000    0.2039646300000000    0.2039646300000000 F\n0.2039646300000000    0.7960353700000000    0.7960353700000000 F\n0.2039646300000000    0.2039646300000000    0.7960353700000000 F\n……\nFigure S7. Structure string examples of CIF format and POSCAR\nformat.\nI. Hyper-Parameter Studies\nReproduction parameters. Our training-free evolutionary framework significantly reduces hyperparameter sensitivity\ncompared to traditional machine learning methods. The reproduction phase introduces several key hyper-parameters that\ninfluence LLMs’ generation behavior and efficiency, including population size (K), context size (C), and children size\n(c). Our baseline configuration (C = 2, c = 5) leverages the Llama 3.1 (70B) model to achieve balanced performance,\ngenerating 72.29% unique structures while maintaining high stability rates.\nAnalysis of parent-to-children ratios reveals that increasing parent diversity (C = 5, c = 2) can enhance composition\nuniqueness of generated structures to 95.49%, though at the price of slight decrease in stability, as presented in Table S4.\nConversely, results with single parent demonstrates that crossover between multiple parent structures is beneficial for\nmaintaining structural diversity and stability in the generation process. Overall, we believe that higher parent-to-children\nratios can lead to better overall quality in generated structures.\nOur analysis also reveals that larger population sizes K can maintain high stability and validity rates comparable to smaller\npopulations. One potential benefit of increasing population size is the diversity introduced in the iteration process, which\ncan alleviate the overpopulation of f-ele structures but also lead to higher compositional diversity. However, the increased\ndiversity is offset by higher rates of structural duplication across iterations, suggesting earlier convergence may be needed.\nOur findings above enable application-specific optimization of the framework’s parameters.\nModel temperature. The temperature hyper-parameter controls sampling randomness in language models by scaling the\nlogits before softmax transformation. Higher temperatures flatten the probability distribution, increasing sampling diversity,\nwhile lower temperatures concentrate probability mass on the most likely tokens. While temperature is commonly associated\nwith model creativity, with higher temperatures generally producing slightly more novel outputs (Peeperkorn et al., 2024),\nthis relationship remains an active area of research.\nCrystal structure generation is a creative task that requires exploring diverse structural possibilities while maintaining\nphysical validity. We employed an LLM inference temperature of 0.95 in our baseline experiments to facilitate broader\nstructural exploration while maintaining reasonable generation stability. In Table S5, we present the metastability evaluated\nby CHGNet for structures generated with different LLM temperatures. At the temperature of 0.95, the LLM generated\n76.81% metastable structures with Ed < 0.1 eV/atom as evaluated by CHGNet. Reducing the temperature to 0.7 maintained\nrobust performance, producing 75.38% metastable structures. Further lowering the temperature to 0.5 yields 71.18%\nmetastable structures. If we choose Ed < 0.03 eV/atom as the stability criterion, the percentage of qualifying structures at\ntemperatures 0.95, 0.7, 0.5 and 0.2 are be 56.52%, 56.64%, 51.37% and 50.17% respectively. The consistent high stability\nrates across temperature settings demonstrate the robustness of our pipeline to LLM hyper-parameter variations.\n17\n\n\nLarge Language Models Are Innate Crystal Structure Generators\nReproduction Configuration\n# Unique / # Total generated\nEd < 0.1eV/atom\nEd < 0.03 eV/atom\n1 →5\n56.5%\n79.8%\n56.4%\n2 →5\n72.3%\n76.8%\n56.5%\n2 →2\n86.3%\n74.8%\n54.3%\n5 →5\n92.7%\n72.3%\n47.3%\n5 →2\n95.5%\n68.3%\n46.1%\nTable S4. Proportion of unique structures and their CHGNet-predicted metastability under varying reproduction configurations.\nLLM Temperature\n# Unique / # Total generated\nEd < 0.1 eV/atom\nEd < 0.03 eV/atom\n0.95\n72.3%\n76.8%\n56.5%\n0.7\n70.7%\n75.4%\n56.6%\n0.5\n70.7%\n71.2%\n51.4%\n0.2\n69.8%\n70.3%\n50.2%\nTable S5. Proportion of unique structures and their CHGNet-predicted metastability with different LLM temperatures.\nJ. Details of Machine Learning Interatomic Potentials\nA significant breakthrough in addressing computational cost challenges has emerged through the development of machine\nlearning interatomic potentials (MLIPs) trained based on high-fidelity quantum mechanical calculations (e.g., DFT) (Batzner\net al., 2022; Cheng, 2024a; Du et al., 2023a;b; Liao et al., 2024; L’opez-Zorrilla et al., 2023; Yin et al., 2025; Zhang et al.,\n2021). In MLIPs, the total energy is expressed as a sum of atomic contributions, where each atom’s energy depends on its\nlocal environment including the atomic coordinates and chemical species of neighboring atoms within a cutoff radius:\nˆE =\nn\nX\ni\nϕ({⃗rj}i, {Cj}i),\nˆf i = −∂ˆE\n∂ri\n,\nσ = 1\nV\n∂ˆE\n∂ε .\n(S1)\nHere, ϕ is a learnable function that maps the set of position vectors {⃗rj}i and chemical species {Cj}i of the neighboring\natoms j to the energy contribution of atom i. The forces f i and stress σ are calculated via auto-differentiation of the\ntotal energy with respect to the atomic Cartesian coordinates and strain. Recent advances have demonstrated that MLIPs,\ntrained on extensive density functional theory (DFT) calculations accumulated over the past decade across diverse materials\nsystems, exhibit remarkable transferability in performing atomistic simulations across various material and chemical systems.\nThese broadly applicable potentials are known as universal MLIPs (uMLIPs) (Batatia et al., 2023; Chen & Ong, 2022;\nDeng et al., 2023; Park et al., 2024). By leveraging uMLIPs as surrogate energy models, researchers can rapidly optimize\ncrystal structures and obtain structure-energy relationships for assessing thermodynamic stability. By leveraging uMLIPs\nas surrogate energy models, one can rapidly optimize crystal structure and obtain the structure-energy relationships for\nassessing thermodynamic stability. Recent benchmark studies, including MACE (Batatia et al., 2023), DPA-1 (Zhang et al.,\n2024) and JMP (joint multi-domain pretraining) (Shoghi et al., 2024), have demonstrated the high accuracy of these uMLIPs\nin predicting crystal thermodynamical stability, particularly for industrial-scale implementations trained on millions of\ncompounds and non-equilibrium atomic configurations (Barroso-Luque et al., 2024; Merchant et al., 2023; Yang et al.,\n2024).\nTo accelerate the oracle function evaluation in the evolutionary iterations, we performed all structure relaxations with\nthe FIRE optimizer (Bitzek et al., 2006) over the potential energy surface provided by CHGNet, where the atom po-\nsitions, cell shape, and cell volume were optimized to reach converged interatomic forces of 0.1 eV/atom (Deng\net al., 2023). The output energy prediction is directly compatible with the Materials Project phase diagrams with the\nMaterialsProject2020Compatibility (Wang et al., 2021).\n18\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20933v1.pdf",
    "total_pages": 18,
    "title": "Large Language Models Are Innate Crystal Structure Generators",
    "authors": [
      "Jingru Gan",
      "Peichen Zhong",
      "Yuanqi Du",
      "Yanqiao Zhu",
      "Chenru Duan",
      "Haorui Wang",
      "Carla P. Gomes",
      "Kristin A. Persson",
      "Daniel Schwalbe-Koda",
      "Wei Wang"
    ],
    "abstract": "Crystal structure generation is fundamental to materials discovery, enabling\nthe prediction of novel materials with desired properties. While existing\napproaches leverage Large Language Models (LLMs) through extensive fine-tuning\non materials databases, we show that pre-trained LLMs can inherently generate\nstable crystal structures without additional training. Our novel framework\nMatLLMSearch integrates pre-trained LLMs with evolutionary search algorithms,\nachieving a 78.38% metastable rate validated by machine learning interatomic\npotentials and 31.7% DFT-verified stability via quantum mechanical\ncalculations, outperforming specialized models such as CrystalTextLLM. Beyond\ncrystal structure generation, we further demonstrate that our framework can be\nreadily adapted to diverse materials design tasks, including crystal structure\nprediction and multi-objective optimization of properties such as deformation\nenergy and bulk modulus, all without fine-tuning. These results establish\npre-trained LLMs as versatile and effective tools for materials discovery,\nopening up new venues for crystal structure generation with reduced\ncomputational overhead and broader accessibility.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}