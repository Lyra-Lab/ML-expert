{
  "id": "arxiv_2502.20008v1",
  "text": "Joint Fusion and Encoding:\nAdvancing Multimodal Retrieval from the Ground Up\nLang Huang∗Qiyu Wu*†, Zhongtong Miao, Toshihiko Yamasaki\nThe University of Tokyo, Tokyo, Japan\nAbstract\nInformation retrieval is indispensable for to-\nday’s Internet applications, yet traditional se-\nmantic matching techniques often fall short in\ncapturing the fine-grained cross-modal interac-\ntions required for complex queries. Although\nlate-fusion two-tower architectures attempt to\nbridge this gap by independently encoding vi-\nsual and textual data before merging them at\na high level, they frequently overlook the sub-\ntle interplay essential for comprehensive un-\nderstanding. In this work, we rigorously as-\nsess these limitations and introduce a unified\nretrieval framework that fuses visual and tex-\ntual cues from the ground up, enabling early\ncross-modal interactions for enhancing con-\ntext interpretation. Through a two-stage train-\ning process—comprising post-training adapta-\ntion followed by instruction tuning—we adapt\nMLLMs as retrievers using a simple one-tower\narchitecture. Our approach outperforms con-\nventional methods across diverse retrieval sce-\nnarios, particularly when processing complex\nmulti-modal inputs. Notably, the joint fusion\nencoder yields greater improvements on tasks\nthat require modality fusion compared to those\nthat do not, underscoring the transformative po-\ntential of early integration strategies and point-\ning toward a promising direction for contextu-\nally aware and effective information retrieval.\n1\nIntroduction\nRetrieval is a cornerstone task in modern Inter-\nnet systems and artificial intelligence, tradition-\nally built on semantic matching to identify rele-\nvant information from vast datasets. Early retrieval\nmethods (Robertson et al., 1995; Gao et al., 2021;\nWu et al., 2022) focused exclusively on single\nmodalities–primarily text–which yielded effective\nsystems in limited settings but inherently lacked the\n*Equal contribution.\n†Work done in a personal capacity.\n†Correspondence to: {langhuang, yamasaki}@cvm.t.u-\ntokyo.ac.jp and wuqiyu576@gmail.com\nRequire \nFusion \nNo Fusion \n10\n50\n30\n70\n70\n70\n70\n30\n30\n30\nFigure 1: Multi-modal retrieval results on a collection\nof retrieval tasks, categorized as 1) Single-modal, 2)\nCross-modal, 3) Mixed-modal, 4) Multi-modal, and 5)\nConditional. CLIP (Radford et al., 2021) is two-tower\nmodel, UniIR (Wei et al., 2024) is two-legs model with\nlate-fusion, and ours is built as one-tower with early\nfusion. Our approach obtains moderate improvements\nin retrieval tasks requiring no modality fusion compared\nto state-of-the-art two-tower methods, the gains become\nprominent when the tasks need modality fusion that\ninvolve multiple modalities or conditional information\nin the query or candidate. Specific results are in §4.\ncapacity to process multi-modal inputs. As applica-\ntions evolved to require richer, cross-modal infor-\nmation, such as integrating visual content alongside\ntext, the limitations of these conventional meth-\nods became increasingly evident. In response, re-\nsearchers adapted traditional approaches (Radford\net al., 2021) using late-fusion (Wei et al., 2024),\ntwo-tower architectures, where each modality is\nencoded separately and only fused at a high level.\nAlthough this strategy bridges the gap between\nmodalities, it falls short in capturing the nuanced,\nearly interactions, and fine-grained details essential\nfor fully understanding complex user intentions. In\nemerging applications such as retrieval-augmented\ngeneration (Lewis et al., 2020; Gao et al., 2023),\ncontext-aware search (Han et al., 2017; Liu et al.,\n1\narXiv:2502.20008v1  [cs.CV]  27 Feb 2025\n\n\n“Closest upper \ntaxonomy of \nthis plant”\n(a) Two Towers\n(b) Two Legs\n(c) One Tower \nText \nEncoder\nImage \nEncoder\nFusion \nNetwork\n“Closest upper \ntaxonomy of \nthis plant”\n+\nText \nEncoder\nImage \nEncoder\n“Closest upper \ntaxonomy of \nthis plant”\nEarly \nFusion\nDirect Joint-space \nContrastive Learning\nImage Embedding\nText Embedding\nText + Image Embedding\nContrastive Learning\nJoint Fusion\nEncoder\nStage 1. Post-training \nAdaptation\nStage 2. Instruction \nTuning\nFigure 2: Conceptual illustration of multi-modal retrieval frameworks. (a) Traditional two-tower methods encode\ntext and images separately, limiting early cross-modal interactions and often overlooking nuanced user queries. (b)\nTwo-leg methods introduce an additional fusion network to merge single-modal embeddings, enabling more complex\ntasks yet still deferring cross-modal interplay until later stages. (c) Our proposed Joint Fusion Encoder (JFE), an\none-tower method, integrates visual and textual cues from the ground up, unifying the embedding space for a direct\ncontrastive learning and facilitating fine-grained multi-modal understanding. This approach, including two stages:\npost-training adaptation and instruction tuning, simplifies and improves the retriever when complex multi-modal\nunderstanding is required.\n2021a), and conditional search (Vaze et al., 2023),\nthe simple high-level fusion of independent repre-\nsentations is insufficient. Instead, these scenarios\ndemand an integrated approach that can blend vi-\nsual and textual cues from the ground up, thereby\nenabling more precise interpretation of intricate,\nmulti-modal queries.\nIn this work, we propose a unified multi-modal\nretrieval framework, Joint Fusion Encoder (JFE),\nthat seamlessly interweaves encoding and fusion\ninto a single process, fundamentally rethinking the\nway retrieval systems handle complex, heteroge-\nneous queries. As shown in Figure 2, conventional\napproaches retrofit single-modal systems to accom-\nmodate multi-modal tasks by independently en-\ncoding each modality and subsequently merging\ntheir representations–a late-fusion strategy that of-\nten fails to capture the nuanced interplay between\ntextual and visual cues. In contrast, one-tower ar-\nchitecture embraces a joint fusion and encoding\nparadigm, wherein fusion occurs concurrently with\nencoding. This early integration addresses the in-\ntrinsic challenge of harmonizing the disparate char-\nacteristics of multi-modal data, enabling the cap-\nture of fine-grained, inter-modal interactions and\npreserving the modality-specific nuances critical\nfor deciphering intricate user intentions.\nBesides above fundamental benefits (Jang et al.,\n2023; Li and Tang, 2024) of such one-tower\nparadigm for multi-modal representation, recent ad-\nvent of large-scale pre-trained large language mod-\nels (LLMs) and multi-modal LLMs (MLLMs) built\non them, has made the one-tower design become\neven more compelling. By repurposing MLLMs\nand adapting them into effective encoders through\ncontrastive learning and instruction-tuning, our ap-\nproach not only simplifies the retrieval pipeline\nby eliminating the need for separate fusion mod-\nules but also produces unified, powerful embed-\ndings that significantly enhance performance across\na range of multi-modal retrieval tasks. The pro-\nposed approach consists of two key stages: 1)\nPost-training adaptation: We begin by adapting\nthe MLLM, which was originally pre-trained as\nan autoregressive decoder, to function as an en-\ncoder. This is achieved through post-training using\ncontrastive learning loss, allowing the model to\neffectively process inputs into meaningful embed-\ndings; 2) Instruction tuning We then fine-tune the\nmodel with instructive information specific to vari-\nous cross-modal retrieval tasks. This step enables\nthe model to better understand and follow instruc-\ntions in both textual and visual contexts.\nTo thoroughly evaluate the capabilities of multi-\nmodal retrieval models, we have organized exten-\nsive experiments across a comprehensive set of\nbenchmarks, categorized by their input and output\nmodalities: 1) single-modal, both the query and\n2\n\n\ncandidate are in the same modality; 2) cross-modal,\nthe query and candidate are in different modali-\nties; 3) mixed-modal, either query and candidate is\nmulti-modal and the other remains single-modal;\n4) multi-modal, both query and candidate are multi-\nmodal; and 5) conditional, the query is supple-\nmented with additional contextual or conditional\ninformation. As shown in Figure 1, we can observe\nthat while our joint fusion and encoding method\nJFE obtains moderate improvements in standard\nsingle-modal and cross-modal retrieval compared\nto state-of-the-art two-tower methods, the gains\nbecome even prominent when the query and/or can-\ndidate involve multiple modalities or conditional\ninformation, the scenarios attaining mounting atten-\ntion as massive multi-modal contents and instruc-\ntions are produced every second.\n2\nRelated Work\nMulti-modal retrieval serves as a cornerstone for\nmulti-modal information systems. Exisiting studies\nin this field have been mostly based on two-towers\nor two-legs architectures (Radford et al., 2021; Bal-\ndrati et al., 2022; Liu et al., 2023; Koukounas et al.,\n2024) as we shown in Figure 2. A notable ex-\nample is CLIP (Radford et al., 2021), which has\nbeen widely adopted as a foundational representa-\ntion model in multi-modal tasks, including retrieval.\nCLIP employs separate encoders for text and im-\nages and aligns them within a shared embedding\nspace. Building on CLIP, Pic2Word (Saito et al.,\n2023) leverages pseudo language tokens to train a\nmapping network for zero-shot composed image\nretrieval, while SEARLE (Baldrati et al., 2023)\nadopts a similar strategy by pre-training a textual\ninversion network for zero-shot composed image\nretrieval. Additionally, UniIR (Wei et al., 2024) uti-\nlizes score-level fusion and feature-level fusion as\na comprehensive exploration of two-legs architec-\ntures, and improve CLIP/BLIP-based multi-modal\ninformation retrieval systems.\nMeanwhile, there are latest works applying one-\ntower visual LLM, given the strong multi-modal\nunderstanding ability from large-scale pre-training.\nJiang et al. (2024) and Zhang et al. (2024b) em-\nploy visual-language models as its backbone to\nfuse textual and visual spaces into a unified vision-\nlanguage embedding model, and they also intro-\nduces a comprehensive embedding dataset for eval-\nuation on such a complex multimodal scenairo.\nMeanwhile,\nLin et al. (2024) adopts a similar\napproach using a visual-language model for re-\ntrieval, but it focuses on hard-negative sampling\nand leverages an LLM for reranking. Both of these\nconcurrent works leverage visual-language mod-\nels to fuse text and image inputs—thereby adopt-\ning a one-tower architecture similar to ours. They\nimplicitly share our advocation that a one-tower\narchitecture is particularly effective for complex\nvision-language representation and retrieval tasks.\n3\nMethodology\n3.1\nMulti-modal retrieval\nThis section describes multi-modal retrieval, a\ntask designed to retrieve relevant information from\nmulti-modal databases by matching queries and\ncandidates across textual, visual, or combined\nmodalities. Formally, the query set Q and can-\ndidate set C can be defined as follows,\nQ = {q | q ∈{qi, qt, {qi, qt}}}\nC = {c | q ∈{ci, ct, {ci, ct}}}.\n(1)\nDepending on application scenarios as introduced\nin §1, the query can be a text query qt, an im-\nage query qi or a pair of a text and an image, i.e.,\n{qi, qt}. Similarly, ct, ci, {ci, ct} represent text can-\ndidate, image candidate, and Multi-modal candi-\ndate, respectively.\nConventional two-tower models.\nA conven-\ntional two-tower model, e.g., CLIP (Radford et al.,\n2021), encodes the inputs into meaningful embed-\ndings using encoders for texts and images sepa-\nrately, as follows,\nhq-i = fθi(qi) ∈RD\nhq-t = fθt(qt) ∈RD\n(2)\nIn the case where the inputs contain both texts\nand images, a combiner module, notated as gθc, is\ntypically needed to combine the features for tasks,\nsuch as composed image retrieval. The process can\nbe defined as follows,\nhq =\n\n\n\n\n\ngθc(hq-i, hq-t),\nif q = {qi, qt}\nhq-i,\nif q = qi\nhq-t,\nif q = qt\n(3)\nhq ∈RD is the embedding representing the query\nand the embedding of a candidate hc ∈RD can be\nobtained in the same way with identical f and g.\nOptimization objective.\nGiven a batch of pairs,\nB = {{qi, ci}}|B|\ni=1, where qi ∈Q and ci ∈C are\n3\n\n\ntermed query and the targeted candidate. As intro-\nduced in Equations (1), (2) and (3), we obtain em-\nbeddings as {{hq\ni, hc\ni}}|B|\ni=1. The set of parameters\nto be optimized is Θ = {θt, θi, θc}. Contrastive\nlearning objective (Oord et al., 2018) is used to op-\ntimize parameters Θ by minimizing the following\nInfoNCE loss,\nL = −1\n|B|\nX\n1≤i≤|B|\nlog\nexp(hq\ni · hc\ni/τ)\nP\n1≤j≤|B| exp(hq\ni · hc\nj/τ),\n(4)\nwhere τ is a temperature term controlling the dis-\ncrimination to the negative samples.\n3.2\nJoint Fusion Encoder\nWhile conventional two-tower models have signifi-\ncantly transformed the landscape of image-to-text\nand text-to-image retrieval and perform well on\nsimple text-image matching tasks, these models\ncan still be limited because\n• The inability to jointly model visual and language\ndata because of their separate encoding processes.\n• The requirement for task-specific combiners to\nhandle more complex tasks, such as composed\nimage retrieval, due to the single-modal input and\nembeddings.\nUnified encoding and embedding process.\nWe\npropose Joint Fusion Encoder (JFE), which builds\non an MLLM, e.g., (Beyer et al., 2024), as its\nbackbone encoder, unifying the encoding process\nfor both queries and candidates. Given an input,\neither a query q ∈Q or a candidate c ∈C, that\nmay contain multi-modal information, we augment\nthe token sequence by appending a special token\n[Emb] to its end. For example, for a query we\nform the augmented input xq = [q; [Emb]] which\nis processed by the MLLM’s shared Transformer\nencoder:\neq = fθ(xq) = {eq,1, eq,2, . . . , eq,Nq},\n(5)\nwhere Nq is the total number of tokens in the aug-\nmented sequence. We then extract the hidden state\ncorresponding to the [Emb] token as the final query\nembedding:\nhq ≜eq,Nq ∈RD.\n(6)\nSimilarly, for a candidate, we obtain its embedding\nby hc ≜ec,Nc ∈RD. These embeddings eq and ec\nserve as the representations for the contrastive loss\nin Equation (4), which encourages corresponding\nquery–candidate pairs to have similar embeddings\nwhile pushing apart those of unrelated pairs.\nAlthough MLLMs are originally trained on gen-\nerative tasks (e.g., visual question answering) that\nfocus on next-token prediction, they are not natu-\nrally designed to extract discriminative represen-\ntations for retrieval. To address this limitation,\nwe first perform post-training adaptation on the\nbackbone MLLM using large-scale paired data,\nfollowed by instruction tuning. Unlike the con-\nventional two-tower model described in §3.1, this\nunified encoding process eliminates the need for an\nadditional combiner—even when handling multi-\nmodal queries such as in composed image retrieval.\nPost-training Adaptation.\nIn this stage, we fine-\ntune the backbone MLLM using the image-caption\ndatasets to generate retrieval-specific embeddings.\nFor each image-caption pair, we create two training\ninstances by swapping roles: one instance treats\nthe image as the query and the caption as the can-\ndidate, while the other reverses these roles. Each\ninstance is processed by appending a special token\n(e.g., [Emb]) to the input sequence to designate the\nlocation for extracting the final embedding. The\nMLLM then encodes these augmented inputs, and\na contrastive learning objective aligns embeddings\nfrom matching image–caption pairs while distin-\nguishing non-matching pairs. This process ensures\nthat both images and captions are effectively repre-\nsented for retrieval tasks.\nInstruction Tuning.\nRecent studies in text\nretrieval have explored integrating instructions\ninto retrievers to better align with users’ inten-\ntions (Asai et al., 2023; Su et al., 2023). This\nchallenge is more pronounced in Multi-modal re-\ntrieval, where instructive information can be pre-\nsented in both textual and visual modalities for\ndifferent tasks. This complexity necessitates the\nsimultaneous processing of textual and visual in-\nstructive inputs during encoding. To further re-\nfine the MLLM for vision-language retrieval and\nbetter align it with human intent, we incorporate\nexplicit task-specific instructions into the input se-\nquences. In this stage, we leverage the instruction\ndata from UniIR (Wei et al., 2024) to tune the uni-\nfied MLLM for retrieval tasks. Specifically, for an\ninput query q (which may represent either visual or\ntextual content) and its corresponding instruction i,\nwe construct an augmented sequence by appending\nthe instruction to the context along with a special\n4\n\n\ntoken for embedding extraction:\nq′ = [q; i; [Emb]].\n(7)\nHere, [Emb] marks the position from which the\nfinal embedding is extracted. The MLLM pro-\ncesses this combined sequence as a single input\nas in Equations (5) and (6), thereby jointly encod-\ning the primary content and the instructive cues.\nThe training objective (as defined in Equation (4))\nthen aligns the embeddings of matching pairs while\ndistinguishing those of non-matching pairs. This\nintegrated learning approach enables the model to\neffectively interpret and execute retrieval tasks in\naccordance with human instructions.\nData sampling strategy.\nOur instruction data\noriginates from multiple datasets and tasks, each\nexhibiting unbalanced data volumes. Since con-\ntrastive learning is sensitive to both intra-dataset\nand inter-dataset batch composition—relying on\nthe effective mining of negative examples—we de-\nsign a sampling scheme that carefully controls the\nnumber of datasets included in each batch. Em-\npirically, we observe that limiting the number of\ndatasets per batch improves overall performance.\nTo achieve this, we sample the number of datasets\nper batch from a normal distribution (with round-\ning), Nd ∼N(4, 1), thereby ensuring that each\nbatch contains a small, balanced subset of datasets.\nThis strategy helps mitigate data imbalance while\nmaintaining a rich set of negative samples, ulti-\nmately enhancing the robustness of the contrastive\nlearning process.\nSummary.\nThrough the post-training adapta-\ntion and instruction tuning steps, we transform the\nMLLM into a powerful encoder for retrieval tasks.\nThis adapted model excels in encoding multi-modal\ninputs into meaningful embeddings. Our approach\nleverages the MLLM’s inherent ability to under-\nstand multi-modal instructive information, result-\ning in a unified encoding process that seamlessly\nhandles various input types - be it text-only, image-\nonly, or a combination of both.\n4\nExperiments\n4.1\nDatasets\nCC3M (Sharma et al., 2018).\nWe performed\nthe post-training adaption on the CC3M dataset\n(Sharma et al., 2018), which consists of 3.3 mil-\nlion image-text pairs from the web. Using the\nimg2dataset toolbox (Beaumont, 2021), we down-\nload the dataset based on the provided URL-caption\npairs, resulting in approximately 2.8 million image-\ntext pairs due to some expired links.\nM-BEIR (Wei et al., 2024).\nWe instruction-\ntune the models on the M-BEIR dataset, which is\na multimodal retrieval dataset encompassing eight\ntasks and ten datasets across domains like every-\nday imagery, fashion, Wikipedia, and news. It in-\ncludes 1.5 million queries and 5.6 million retrieval\ncandidates, despite being originally designed for\nvarious purposes. These include retrieval-focused\ndatasets (e.g., OVEN (Hu et al., 2023), CIRR (Liu\net al., 2021b), FashionIQ (Wu et al., 2021)),\nimage-caption datasets (e.g., MS-COCO (Lin et al.,\n2014), Fashion200K (Han et al., 2017)), an image-\nsimilarity dataset (NIGHTS\n(Fu et al., 2023)),\nand retrieval-based VQA datasets (InfoSeek (Chen\net al., 2023), WebQA (Chang et al., 2022)). For\neach dataset above, Wei et al. (2024) generated 4 in-\nstructions that describe a multimodal retrieval task\nby intent, domain, query modality, and target can-\ndidate modality. Thanks to its diverse input/output\nformats, MBEIR provides a suitable platform to\ntrain and evaluate multimodal retrieval systems.\n4.2\nTraining setups\nFor most of the experiments in this paper, we de-\nfault to PaliGemma (Beyer et al., 2024) as the\nchoice of MLLMs because of its relatively compact\nsize (∼3B) and competitive performance on var-\nious vision-language understanding benchmarks.\nFollowing the original recipe, the image input of\nthe model is simply scaled to size 256 × 256 and\nfed into a SigLIP vision encoder (a ViT with patch\nsize of 16 × 16) to obtain 256 vision tokens; the\ntext input is tokenized by the sentence piece tok-\nenizer. For training efficiency and reducing GPU\nmemory consumption, we truncate the input token\nwhen the total number of tokens is larger than 384,\nwhich means the maximal length of textual tokens\n(including those of textual instruction) is 128 when\nthe input contains an image or 378 otherwise.\nFor the post-training adaption, we train the\nMLLMs on CC3M (Sharma et al., 2018) datasets\nfor 1 epoch using Low-Rank Adapters (LoRA) (Hu\net al., 2022) with r = 128, α = 256, and a dropout\nprobability 0.05. We use the AdamW (Loshchilov\nand Hutter, 2017) optimizer with a learning rate\nof 2e-4, a batch size of 2048, and no weight de-\ncay for the LoRA training. The learning rate is\n5\n\n\nTable 1: Retrieval results on M-BEIR benchmark (Wei et al., 2024).\nTask\nDataset\nSoTA Zero-Shot\nSingle-task FT\nMulti-task (w/ instruction)\nCLIP\nSigLIP\nBLIP\nBLIP2\nCLIPSF\nBLIPFF\nCLIPSF\nBLIPFF\nOurs\nqt →ct\nWebQA\n36.2\n39.8\n44.9\n38.6\n81.7\n67.5\n84.1\n79.2\n88.7\nqi →ci\nNIGHTS\n26.1\n28.9\n27.4\n25.4\n33.5\n30.4\n31.1\n31.7\n27.8\nSINGLE-MODAL AVERAGE\n31.2\n34.4\n36.2\n32\n57.6\n49.0\n57.6\n55.5\n58.3\nqt →ci\nVisualNews\n43.3\n30.1\n16.4\n16.7\n43.5\n20.0\n42.5\n22.9\n34.6\nMSCOCO\n61.1\n75.7\n74.4\n63.8\n80.4\n77.3\n80.7\n79.5\n78.5\nFashion200K\n6.6\n36.5\n15.9\n14.0\n10.7\n17.1\n18.1\n26.2\n37.2\nqi →ct\nVisualNews\n41.3\n30.8\n17.2\n15.0\n42.7\n22.4\n42.5\n23.1\n33.1\nMSCOCO\n79.0\n88.2\n83.2\n80.0\n89.8\n86.0\n91.8\n90.8\n90.0\nFashion200K\n7.7\n34.2\n19.9\n14.2\n12.0\n15.6\n18.3\n28.6\n36.9\nCROSS-MODAL AVERAGE\n39.8\n49.3\n37.8\n34.0\n46.5\n39.7\n49.0\n45.2\n51.7\nqt →(ci, ct)\nEDIS\n43.3\n27.0\n26.8\n26.9\n58.8\n38.2\n53.6\n49.9\n54.3\nWebQA\n45.1\n43.5\n20.3\n24.5\n76.3\n67.8\n78.3\n78.1\n82.4\n(qi, qt) →ct\nOVEN\n24.2\n29.7\n16.1\n12.2\n45.4\n33.8\n46.0\n42.7\n46.0\nInfoSeek\n20.5\n25.1\n10.2\n5.5\n23.5\n18.5\n27.4\n23.3\n35.6\n(qi, qt) →ci\nFashionIQ\n7.0\n14.4\n2.3\n4.4\n25.9\n3.0\n24.8\n29.2\n31.8\nCIRR\n13.2\n22.7\n10.6\n11.8\n52.0\n13.9\n44.6\n50.7\n54.0\nMIXED-MODAL AVERAGE\n25.6\n27.1\n14.4\n14.2\n47.0\n29.2\n45.8\n45.7\n50.7\n(qi, qt) →(ci, ct)\nOVEN\n38.8\n41.7\n27.4\n27.3\n66.2\n49.9\n68.7\n56.5\n72.7\nInfoSeek\n26.4\n27.4\n16.6\n15.8\n47.4\n32.3\n48.8\n30.4\n61.1\nMULTI-MODAL AVERAGE\n32.6\n34.6\n22.0\n21.55\n56.8\n41.1\n58.8\n43.5\n66.9\nALL AVERAGE\n32.5\n37.2\n26.8\n24.8\n49.4\n37.1\n50.1\n46.4\n54.0\nlinearly warmed-up for the first 3% of training to\nthe specified value and then decayed using a cosine\nannealing schedule (Loshchilov and Hutter, 2016).\nFor the instruction-tuning stage, we first merge\nthe LoRA of the first stage to the base MLLM and\nthen reinitialize and train a new of new LoRA based\non the merged weights. We use r = 256, α =\n512, and a dropout probability 0.3 for LoRA at\nthis stage because we find it beneficial to use more\nparameters to enhance the instruction-following\ncapability (see Tab. 5). We train the LoRA with a\nbatch size of 1024, no weight decay, and a learning\nrate of 2e-4 which is warmed-up and decayed as in\nthe first stage.\n4.3\nEvaluation setups\nWe mainly evaluate JFE on the M-BEIR bench-\nmark (Wei et al., 2024) because it contains a di-\nverse set of input and target modalities and provides\nlarge-scale query and candidate sets (190K queries\nand 5.6M candidates) for reliable evaluations. We\nadopt the settings that perform retrieval from a\ntask-specific pool provided by the original dataset,\nenabling comparison with non-instruction-tuned re-\ntrievers. We report the Recall@5 for all the datasets\nexcept FashionIQ and Fashion 200K, where Re-\ncall@10 is used following Wu et al. (2021).\nIn addition, we also evaluate JFE on conditional\nimage similarity, which measures the capability\nof models not only in encoding the content of the\nquery but also in understanding users’ intent or\nconditions. We use the GeneCIS benchmark (Vaze\net al., 2023), an image-to-image retrieval task con-\nditioned on several keywords. GeneCIS consists\nof four sub-tasks about focusing or changing on a\nspecific attribute or object. For instance, for the\nsub-task about focusing on an object, the models\nneed to find the most relevant image with the same\nobject (specified in the condition) as the query.\n4.4\nSingle-modal and cross-modal retrieval\nWe begin by evaluating our method in single-modal\nand cross-modal settings, where both the query\nand candidate consist of a single modality. Al-\nthough the baseline models are specifically de-\nsigned to handle single-modal inputs, our multi-\nmodal input method slightly outperforms them in\nsingle-modal retrieval, achieving an average score\nof 58.3 compared to 57.6 for CLIPSF and 49.0 for\nBLIPFF. Similarly, in cross-modal retrieval, our\n6\n\n\nTable 2: Conditional Retrieval on GeneCIS benchmark (Vaze et al., 2023).\nMethod\nFocus Attribute\nChange Attribute\nFocus Object\nChange Object\nAvg\nR@1 R@2 R@3 R@1 R@2 R@3 R@1 R@2 R@3 R@1 R@2 R@3 R@1\nPic2Word (Saito et al., 2023)\n12.5\n23.4\n33.7\n11.7\n21.9\n30.9\n9.9\n19.3\n27.4\n8.6\n18.2\n26.1\n10.7\nSEARLE (Baldrati et al., 2023)\n16.3\n29.4\n40.7\n16.2\n27.3\n35.5\n10.8\n18.2\n27.9\n8.3\n15.6\n25.8\n12.9\nCompoDiff (Gu et al., 2023)\n14.3\n26.7\n38.4\n19.7\n28.8\n37.4\n9.2\n19.1\n25.8\n18.7\n31.7\n40.2\n15.5\nCIReVL (Karthik et al., 2024)\n20.5\n34.0\n44.5\n16.1\n28.6\n39.4\n14.7\n25.2\n33.0\n18.1\n31.2\n41.0\n17.4\nLinCIR (Gu et al., 2024)\n19.1\n33.0\n42.3\n17.6\n30.2\n38.1\n10.1\n19.1\n28.1\n7.9\n16.3\n25.7\n13.7\nMagicLens (Zhang et al., 2024a)\n16.6\n28.7\n39.3\n16.0\n27.5\n36.5\n15.7\n27.6\n37.3\n18.7\n31.7\n40.2\n16.7\nCLIPSF (Wei et al., 2024)\n21.1\n33.9\n44.6\n15.1\n27.6\n37.8\n15.0\n25.3\n35.0\n13.6\n24.8\n35.7\n16.2\nBLIPFF (Wei et al., 2024)\n19.4\n32.3\n44.0\n15.8\n26.9\n36.0\n18.0\n28.4\n37.0\n18.5\n29.4\n39.1\n17.9\nOurs\n18.9\n29.6\n40.7\n15.7\n28.0\n36.7\n21.5\n32.7\n40.5\n24.1\n37.9\n48.4\n20.1\nTable 3: Influence of two-stage training.\nStage 1\nStage 2\nRetrieval recall\nSingle\nCross\nMixed\nAverage\n✗\n✗\n6.7\n0.1\n0.0\n1.4\n✓\n✗\n59.3\n54.3\n6.7\n36.3\n✗\n✓\n84.8\n80.5\n44.3\n66.9\n✓\n✓\n88.2\n84.3\n42.3\n68.3\nmethod attains an average score of 51.7, surpass-\ning single-task fine-tuned models (46.5 for CLIPSF\nand 39.7 for BLIPFF) and multi-task models (49.0).\nThese results indicate that even in tasks where\ntwo-tower-based methods typically excel, our uni-\nfied multi-modal approach delivers competitive and\neven slightly superior performance.\n4.5\nMixed-modal and multi-modal retrieval\nWe further evaluate our model in mixed- and multi-\nmodal settings, where both queries and candidates\ncan be arbitrary combinations of images and text.\nFor mixed-modal retrieval, JFE achieves an av-\nerage score of 50.7, significantly outperforming\nmulti-task baselines (45.8 and 45.7) and single-task\nBLIPFF (29.2). In multi-modal tasks, our model\nachieves an average score of 66.9, surpassing both\nsingle-task FT (56.8) and multi-task models (58.8)\nby 8 points. These results demonstrate that, un-\nlike two-tower-based baselines which struggle to\ninterpret multi-modal inputs, our approach effec-\ntively integrates cross-modal and multi-task sig-\nnals, obtaining superior performance in complex\nmulti-modal retrieval scenarios. These experiments\nreiterate the importance of unified models, like JFE,\nfor vision-language retrieval.\n4.6\nConditional retrieval\nFollowing Vaze et al. (2023), we report the Re-\ncall@K, K = {1, 2, 3} for all four sub-tasks, as\nwell as the averaged Recall@1 in Tab. 2. We con-\nduct in-depth comparisons with various state-of-\nthe-art methods, all following the two-tower fash-\nion and potentially with a combiner. This includes\n1) Pic2Word (Saito et al., 2023), SEARLE (Baldrati\net al., 2023) and LinCIR (Gu et al., 2024) that map\nimages into a special text token inserted to the con-\ndition prompts; 2) CIReVL (Karthik et al., 2024)\nthat first captions the image and then merges the\ncaption and the condition using LLMs to a textual\nquery; 3) CompoDiff (Gu et al., 2023) and Magi-\ncLens (Zhang et al., 2024a) which curate or synthe-\nsize composed image retrieval data for training a\ntwo-tower model; and 4) UniIR variants (CLIPSF\nand BLIPFF) that are trained on the same data as\nours. From the table, we observe that JFE deliv-\ners competitive performance across all tasks com-\npared to SOTA methods, with exceptional results\nin object-centric conditions. JFE achieves an aver-\naged Recall@1 of 20.1, significantly outperforming\nall other methods by a large margin, despite not re-\nlying on any task-specific design for conditional\nretrieval. This underscores the effectiveness of us-\ning a unified model to jointly comprehend vision\nand language information. Additionally, although\nBLIPFF shows relatively competitive performance\nwith an average Recall@1 of 17.9, it still falls sig-\nnificantly short of the robustness demonstrated by\nour approach, especially in subtasks involving ob-\nject modification or composition. This reinforces\nthat the improvements achieved by JFE are primar-\nily due to its architectural design rather than any\nadvantage from the data.\n4.7\nAblation analysis\nImpact of Two-stage Training.\nIn Tab. 3, we\nstudy the impact of the two-stage training on re-\ntrieval performance using a subset of M-BEIR.\n7\n\n\nTable 4: Influence of data sampling strategy.\n#Dataset/Batch\nRetrieval recall\nSingle\nCross\nMixed\nAverage\nN/A\n81.1\n78.4\n37.9\n62.8\n2\n75.4\n81.5\n39.5\n63.4\n4\n84.5\n82.0\n41.3\n66.2\n8\n81.8\n77.9\n40.9\n63.9\nN(4, 1)\n84.8\n80.5\n44.3\n66.9\nTable 5: Influence of the hyper-parameters rank (r), α,\nand dropout probability d in LoRA.\nLoRA hyper-param.\nRetrieval recall\nr\nα\nd\nSingle\nCross\nMixed\nAverage\n16\n32\n0.05\n78.3\n77.7\n35.9\n61.1\n128\n256\n0.05\n81.5\n80.2\n38.3\n63.7\n128\n256\n0.3\n81.9\n83.8\n37.3\n64.8\n256\n512\n0.3\n84.8\n80.5\n44.3\n66.9\nWithout any training, the performance of the orig-\ninal MLLMs is no better than random guessing,\nindicating the necessity of carefully designed adap-\ntion steps. Applying stage 1 alone yields a signif-\nicant performance improvement, but falls behind\nstage 2 training alone, which is reasonable con-\nsidering the discrepancy between the CC3M and\nM-BEIR. The combination of both stages results\nin the best retrieval recall especially in the case of\nsingle-modal retrieval, suggesting the benefits of\nthe post-training adaption for tasks involving only\nsingle-modality query/candidate.\nBenefits of data sampling.\nTab. 4 investigates\nhow different data sampling strategies influence\nretrieval performance. Training without any sam-\npling strategy underperforms (62.8 average recall)\nthose with sampling operations, emphasizing the\nnecessity of batch diversity. As the dataset count\nper batch increases, performance improves up to\na peak at 4 datasets per batch. The use of Gaus-\nsian sampling N(4, 1) further improves this result,\nachieving an averaged score of 66.9. This sug-\ngests that properly balancing the data source within\nbatches benefits generalization.\nImpact of the LoRA hyper-parameters and\nbatch size.\nTabs. 5 and 6 explore the impor-\ntance of LoRA hyper-parameters and batch size\non retrieval performance. For LoRA, larger rank\nand scaling factors (r = 256, α = 512) consis-\ntently yield performance gains, achieving the best\nretrieval recall of 66.9. Additionally, moderate\ndropout regularization (0.3) outperforms smaller\nrates (0.05), suggesting that balancing parameter\nTable 6: Influence of the number of training batch size.\nBatch Size\nRetrieval recall\nSingle\nCross\nMixed\nAverage\n256\n80.2\n77.8\n41.4\n63.7\n512\n77.5\n82.9\n41.1\n65.1\n1024\n84.8\n80.5\n44.3\n66.9\nTable 7: Influence of the number of training epochs.\nNum. Epochs\nRetrieval recall\nSingle\nCross\nMixed\nAverage\n2\n87.1\n80.8\n44.6\n67.6\n3\n88.7\n84.3\n42.9\n68.6\n5\n86.9\n84.2\n44.0\n68.6\ncomplexity and overfitting is crucial for robust per-\nformance. Meanwhile, batch size plays a critical\nrole, with a larger batch size of 1024 reaching 66.9\naveraged score, showing that batch size is partic-\nularly impactful as larger batches typically lead\nto better negative sampling and stronger represen-\ntation learning. However, due to GPU memory\nlimitations, batch sizes beyond 1024 could not be\ntested. We would expect performance to further\nimprove with larger batch sizes.\nScaling the number of training epochs.\nTab. 7\nanalyzes how the number of training epochs affects\nretrieval performance. From the table, we can see\nthat the performance considerably increases from\n67.6 at 2 epochs to 68.6 at 3 epochs. Beyond this,\nthe performance plateaus, as both 3 and 5 epochs\nyield the same overall score (68.6 recall). These re-\nsults suggest that training saturation occurs beyond\n3 epochs, where additional epochs provide dimin-\nishing returns on retrieval effectiveness. We default\nto 3 epochs in our experiments for a good trade-off\nbetween performance and training efficiency.\n5\nConclusion\nIn this work, we introduced JFE, a one-tower\nmulti-modal retrieval framework that integrates fu-\nsion directly into the encoding process. By adapt-\ning MLLMs into effective encoders through post-\ntraining adaptation and instruction-tuning, JFE cap-\ntures fine-grained interactions between visual and\ntextual cues from the ground up. Our extensive\nevaluations demonstrate that JFE achieves moder-\nate gains in standard single-modal and cross-modal\nretrieval, its performance improvements become\nparticularly pronounced in complex scenarios in-\nvolving multi-modal or conditional queries, where\nthe modality fusion is required. Overall, the find-\n8\n\n\nTable 8: Retrieval results on M-BEIRglobal\nZero-shot\nMulti-task (w/ instruction)\nMulti-task (w/o instruction)\nTask\nDataset\nBLIP2\nCLIPSF\nBLIPFF\nCLIPSF\nBLIPFF\nOurs\n1. qt →ci\nVisualNews\n0.0\n12.7\n8.3\n42.2\n22.5\n31.5\nMSCOCO\n0.0\n27.3\n27.7\n71.4\n65.3\n62.1\nFashion200K\n0.0\n5.9\n9.0\n18.0\n26.1\n35.6\n2. qt →ct\nWebQA\n35.2\n82.3\n76.1\n83.5\n78.5\n87.6\n3. qt →(ci, ct)\nEDIS\n0.0\n41.1\n36.0\n52.7\n49.3\n51.7\nWebQA\n0.0\n68.2\n74.7\n77.5\n77.1\n81.0\n4. qi →ct\nVisualNews\n0.0\n12.1\n4.9\n38.8\n21.1\n30.3\nMSCOCO\n0.0\n84.6\n76.9\n91.4\n89.8\n89.0\nFashion200K\n0.0\n1.2\n3.6\n18.2\n27.4\n30.9\n5. qi →ci\nNIGHTS\n24.0\n31.0\n31.3\n39.5\n31.6\n27.8\n6. (qi, qt) →ct\nOVEN\n0.0\n36.8\n37.7\n22.2\n39.5\n42.4\nInfoSeek\n0.0\n18.3\n17.8\n24.6\n19.8\n31.9\n7. (qi, qt) →ci\nFashionIQ\n3.9\n22.8\n28.1\n43.1\n28.9\n31.1\nCIRR\n6.2\n32.0\n45.1\n59.8\n48.3\n50.4\n8. (qi, qt) →(ci, ct)\nOVEN\n13.8\n58.7\n51.6\n44.3\n55.9\n69.1\nInfoSeek\n11.4\n42.3\n25.4\n44.3\n26.2\n57.4\nAverage\n5.9\n36.1\n34.6\n47.4\n44.2\n50.6\nings underscore the superiority of joint fusion and\nencoding for advanced multi-modal retrieval appli-\ncations with inputs requiring complex multi-modal\nunderstanding.\nLimitations\nThis work is built upon large-scale pre-trained mod-\nels rather than developing two-tower and one-tower\narchitectures from scratch. Although these pre-\ntrained models have been widely adopted in the\ncommunity, their use introduces two primary limi-\ntations. First, the influence of the pre-trained mod-\nels cannot be fully isolated—since both the pre-\ntraining approach and the underlying data have not\nbeen entirely publicly disclosed, their contributions\nremain a confounding factor. Second, there are effi-\nciency concerns, particularly for retrieval tasks that\ndemand fast online inference. These issues could\nbe mitigated by further advances in efficient large\nmodels and the development of more streamlined\nbackbone architectures.\nEthical Statement\nThis research focuses on the daily task of informa-\ntion retrieval, which in itself does not pose ethi-\ncal concerns. Our approach employs an encoding\nmodel to compress information, thereby mitigat-\ning the risk of inappropriate data generation. All\ndatasets and pre-trained checkpoints used in this\nstudy are publicly available with free use for re-\nsearch and remain unaltered. However, as is com-\nmon in much of today’s AI research, the perfor-\nmance of large AI models is not yet fully under-\nstood. Our evaluation is limited to academic bench-\nmarks, and we do not endorse their deployment in\npractical applications at this stage.\nAppendix\nA\nAdditional Experimental Results\nWe also adopt an alternative retrieval setting as\ndescribed in (Wei et al., 2024), which conducts re-\ntrieval from a pool of 5.6 million candidates aggre-\ngated from eight tasks across ten M-BEIR datasets.\nAs demonstrated in Table 8, despite the signif-\nicantly varied evaluation settings, JFE not only\nachieves moderate improvements over baselines in\nsingle-modal and cross-modal retrieval but also de-\nlivers substantial gains in mixed-modal and multi-\nmodal scenarios—settings that are increasingly rel-\nevant in our multi-modal content-rich world. These\nresults further corroborate our findings in §4 and\nunderscore the advantages of the Joint Fusion and\nEncoding paradigm.\n9\n\n\nReferences\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen,\nGautier Izacard, Sebastian Riedel, Hannaneh Ha-\njishirzi, and Wen-tau Yih. 2023. Task-aware retrieval\nwith instructions. In Findings of the Association for\nComputational Linguistics: ACL 2023, pages 3650–\n3675, Toronto, Canada. Association for Computa-\ntional Linguistics.\nAlberto Baldrati, Lorenzo Agnolucci, Marco Bertini,\nand Alberto Del Bimbo. 2023. Zero-shot composed\nimage retrieval with textual inversion. In Proceed-\nings of the International Conference on Computer\nVision.\nAlberto Baldrati, Marco Bertini, Tiberio Uricchio, and\nAlberto Del Bimbo. 2022. Effective conditioned and\ncomposed image retrieval combining clip-based fea-\ntures. In 2022 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 21434–\n21442.\nRomain Beaumont. 2021. img2dataset: Easily turn\nlarge sets of image urls to an image dataset. https:\n//github.com/rom1504/img2dataset.\nLucas Beyer, Andreas Steiner, André Susano Pinto,\nAlexander Kolesnikov, Xiao Wang, Daniel Salz,\nMaxim Neumann, Ibrahim Alabdulmohsin, Michael\nTschannen, Emanuele Bugliarello, et al. 2024.\nPaligemma: A versatile 3b vlm for transfer. arXiv\npreprint arXiv:2407.07726.\nYingshan Chang, Mridu Narang, Hisami Suzuki, Gui-\nhong Cao, Jianfeng Gao, and Yonatan Bisk. 2022.\nWebqa: Multihop and multimodal qa. In Proceed-\nings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 16495–16504.\nYang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit\nChangpinyo, Alan Ritter, and Ming-Wei Chang. 2023.\nCan pre-trained vision and language models answer\nvisual information-seeking questions? In Proceed-\nings of the 2023 Conference on Empirical Methods in\nNatural Language Processing, pages 14948–14968.\nStephanie Fu, Netanel Y Tamir, Shobhita Sundaram,\nLucy Chai, Richard Zhang, Tali Dekel, and Phillip\nIsola. 2023. Dreamsim: learning new dimensions\nof human visual similarity using synthetic data. In\nAdvance on Neural Information Processing Systems,\npages 50742–50768.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimCSE: Simple contrastive learning of sentence em-\nbeddings. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 6894–6910, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,\nJinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen\nWang. 2023. Retrieval-augmented generation for\nlarge language models: A survey. arXiv preprint\narXiv:2312.10997.\nGeonmo Gu, Sanghyuk Chun, Wonjae Kim, , Yoohoon\nKang, and Sangdoo Yun. 2024. Language-only train-\ning of zero-shot composed image retrieval. In Pro-\nceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition.\nGeonmo Gu, Sanghyuk Chun, Wonjae Kim, HeeJae Jun,\nYoohoon Kang, and Sangdoo Yun. 2023. Compod-\niff: Versatile composed image retrieval with latent\ndiffusion. arXiv preprint arXiv:2303.11916.\nXintong Han, Zuxuan Wu, Phoenix X Huang, Xiao\nZhang, Menglong Zhu, Yuan Li, Yang Zhao, and\nLarry S Davis. 2017. Automatic spatially-aware fash-\nion concept discovery. In Proceedings of the IEEE\nInternational Conference on Computer Vision, pages\n1463–1471.\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu,\nYuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,\net al. 2022. Lora: Low-rank adaptation of large lan-\nguage models. In International Conference on Learn-\ning Representations.\nHexiang Hu, Yi Luan, Yang Chen, Urvashi Khandel-\nwal, Mandar Joshi, Kenton Lee, Kristina Toutanova,\nand Ming-Wei Chang. 2023. Open-domain visual\nentity recognition: Towards recognizing millions of\nwikipedia entities. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages\n12065–12075.\nJiho Jang, Chaerin Kong, Donghyeon Jeon, Seonhoon\nKim, and Nojun Kwak. 2023.\nUnifying vision-\nlanguage representation space with single-tower\ntransformer. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 37, pages 980–988.\nZiyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz,\nYingbo Zhou, and Wenhu Chen. 2024. Vlm2vec:\nTraining\nvision-language\nmodels\nfor\nmassive\nmultimodal embedding tasks.\narXiv preprint\narXiv:2410.05160.\nShyamgopal Karthik, Karsten Roth, Massimiliano\nMancini, and Zeynep Akata. 2024.\nVision-by-\nlanguage for training-free compositional image re-\ntrieval. In International Conference on Learning\nRepresentation.\nAndreas Koukounas, Georgios Mastrapas, Bo Wang,\nMohammad\nKalim\nAkram,\nSedigheh\nEslami,\nMichael Günther, Isabelle Mohr, Saba Sturua, Scott\nMartens, Nan Wang, et al. 2024. jina-clip-v2: Multi-\nlingual multimodal embeddings for text and images.\narXiv preprint arXiv:2412.08802.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459–9474.\nSongtao Li and Hao Tang. 2024. Multimodal align-\nment and fusion:\nA survey.\narXiv preprint\narXiv:2411.17040.\n10\n\n\nSheng-Chieh Lin, Chankyu Lee, Mohammad Shoeybi,\nJimmy Lin, Bryan Catanzaro, and Wei Ping. 2024.\nMm-embed: Universal multimodal retrieval with\nmultimodal llms. arXiv preprint arXiv:2411.02571.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C Lawrence Zitnick. 2014.\nMicrosoft coco:\nCommon objects in context. In Computer Vision–\nECCV 2014: 13th European Conference, Zurich,\nSwitzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740–755. Springer.\nZhenghao Liu, Chenyan Xiong, Yuanhuiyi Lv, Zhiyuan\nLiu, and Ge Yu. 2023. Universal vision-language\ndense retrieval: Learning a unified representation\nspace for multi-modal retrieval. In The Eleventh In-\nternational Conference on Learning Representations.\nZheyuan Liu, Cristian Rodriguez-Opazo, Damien Teney,\nand Stephen Gould. 2021a. Image retrieval on real-\nlife images with pre-trained vision-and-language\nmodels. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision (ICCV), pages\n2125–2134.\nZheyuan Liu, Cristian Rodriguez-Opazo, Damien Teney,\nand Stephen Gould. 2021b.\nImage retrieval on\nreal-life images with pre-trained vision-and-language\nmodels. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 2125–\n2134.\nIlya Loshchilov and Frank Hutter. 2016. Sgdr: Stochas-\ntic gradient descent with warm restarts.\narXiv\npreprint arXiv:1608.03983.\nIlya Loshchilov and Frank Hutter. 2017.\nDecou-\npled weight decay regularization.\narXiv preprint\narXiv:1711.05101.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018.\nRepresentation learning with contrastive predictive\ncoding. arXiv preprint arXiv:1807.03748.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from\nnatural language supervision. In International con-\nference on machine learning, pages 8748–8763.\nStephen E Robertson, Steve Walker, Susan Jones,\nMicheline M Hancock-Beaulieu, Mike Gatford, et al.\n1995. Okapi at trec-3. Nist Special Publication Sp,\n109:109.\nKuniaki Saito, Kihyuk Sohn, Xiang Zhang, Chun-Liang\nLi, Chen-Yu Lee, Kate Saenko, and Tomas Pfister.\n2023. Pic2word: Mapping pictures to words for zero-\nshot composed image retrieval. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern\nRecognition.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\nRadu Soricut. 2018. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic im-\nage captioning. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 2556–2565.\nHongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang,\nYushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A.\nSmith, Luke Zettlemoyer, and Tao Yu. 2023. One\nembedder, any task: Instruction-finetuned text em-\nbeddings. In Findings of the Association for Compu-\ntational Linguistics: ACL 2023, pages 1102–1121,\nToronto, Canada. Association for Computational Lin-\nguistics.\nSagar Vaze, Nicolas Carion, and Ishan Misra. 2023.\nGenecis: A benchmark for general conditional image\nsimilarity. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition.\nCong Wei, Yang Chen, Haonan Chen, Hexiang Hu,\nGe Zhang, Jie Fu, Alan Ritter, and Wenhu Chen.\n2024.\nUniir: Training and benchmarking univer-\nsal multimodal information retrievers. In European\nConference on Computer Vision, volume 15145 of\nLecture Notes in Computer Science, pages 387–404.\nSpringer.\nHui Wu, Yupeng Gao, Xiaoxiao Guo, Ziad Al-Halah,\nSteven Rennie, Kristen Grauman, and Rogerio Feris.\n2021. Fashion iq: A new dataset towards retrieving\nimages by natural language feedback. In Proceedings\nof the IEEE/CVF Conference on Computer Cision\nand Pattern Recognition, pages 11307–11317.\nQiyu Wu, Chongyang Tao, Tao Shen, Can Xu, Xiubo\nGeng, and Daxin Jiang. 2022. PCL: Peer-contrastive\nlearning with diverse augmentations for unsupervised\nsentence embeddings. In Proceedings of the 2022\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 12052–12066, Abu Dhabi,\nUnited Arab Emirates. Association for Computa-\ntional Linguistics.\nKai Zhang, Yi Luan, Hexiang Hu, Kenton Lee, Siyuan\nQiao, Wenhu Chen, Yu Su, and Ming-Wei Chang.\n2024a. Magiclens: Self-supervised image retrieval\nwith open-ended instructions. In International Con-\nference on Machine Learning.\nXin Zhang, Yanzhao Zhang, Wen Xie, Mingxin Li, Ziqi\nDai, Dingkun Long, Pengjun Xie, Meishan Zhang,\nWenjie Li, and Min Zhang. 2024b. Gme: Improving\nuniversal multimodal retrieval by multimodal llms.\narXiv preprint arXiv:2412.16855.\n11\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20008v1.pdf",
    "total_pages": 11,
    "title": "Joint Fusion and Encoding: Advancing Multimodal Retrieval from the Ground Up",
    "authors": [
      "Lang Huang",
      "Qiyu Wu",
      "Zhongtao Miao",
      "Toshihiko Yamasaki"
    ],
    "abstract": "Information retrieval is indispensable for today's Internet applications, yet\ntraditional semantic matching techniques often fall short in capturing the\nfine-grained cross-modal interactions required for complex queries. Although\nlate-fusion two-tower architectures attempt to bridge this gap by independently\nencoding visual and textual data before merging them at a high level, they\nfrequently overlook the subtle interplay essential for comprehensive\nunderstanding. In this work, we rigorously assess these limitations and\nintroduce a unified retrieval framework that fuses visual and textual cues from\nthe ground up, enabling early cross-modal interactions for enhancing context\ninterpretation. Through a two-stage training process--comprising post-training\nadaptation followed by instruction tuning--we adapt MLLMs as retrievers using a\nsimple one-tower architecture. Our approach outperforms conventional methods\nacross diverse retrieval scenarios, particularly when processing complex\nmulti-modal inputs. Notably, the joint fusion encoder yields greater\nimprovements on tasks that require modality fusion compared to those that do\nnot, underscoring the transformative potential of early integration strategies\nand pointing toward a promising direction for contextually aware and effective\ninformation retrieval.",
    "published_date": "2025-02-27",
    "source": "arxiv"
  }
}