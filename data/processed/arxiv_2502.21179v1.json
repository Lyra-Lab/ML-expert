{
  "id": "arxiv_2502.21179v1",
  "text": "∆-model correction of Foundation Model based on the models own understanding\nMads-Peter Verner Christiansen and Bjørk Hammer∗\nCenter for Interstellar Catalysis, Department of Physics and Astronomy,\nAarhus University, DK-8000 Aarhus C, Denmak\n(Dated: March 3, 2025)\nFoundation models of interatomic potentials, so called universal potentials, may require fine-\ntuning or residual corrections when applied to specific subclasses of materials. In the present work,\nwe demonstrate how such augmentation can be accomplished via ∆-learning based on the represen-\ntation already embedded in the universal potentials. The ∆-model introduced is a Gaussian Process\nRegression (GPR) model and various types of aggregation (global, species-separated, and atomic)\nof the representation vector are discussed. Employing a specific universal potential, CHGNet [Deng\net al., Nat. Mach. Intell. 5, 1031 (2023)], in a global structure optimization setting, we find that it\ncorrectly describes the energetics of the ”8” Cu oxide, which is an ultra-thin oxide film on Cu(111).\nThe universal potential model even predicts a more favorable structure compared to that discussed\nin recent DFT-based literature.\nMoving to sulfur adatom overlayers on Cu(111), Ag(111), and\nAu(111) the CHGNet model, however, requires corrections. We demonstrate that these are effi-\nciently provided via the GPR-based ∆-model formulated on the CHGNet’s own internal atomic\nembedding representation. The need for corrections is tracked to the scarcity of metal-sulfur atomic\nenvironments in the materials project database that CHGNet is trained on leading to an overre-\nliance on sulfur-sulfur atomic environments. Other universal potentials trained on the same data,\nMACE-MP0, SevenNet-0, and ORB-v2-only-MPtrj show similar behavior, but with varying degrees\nof error, demonstrating the general need for augmentation schemes for universal potential models.\nI.\nINTRODUCTION\nQuantum mechanical calculations of material proper-\nties have been a corner stone of material science for the\npast several decades.\nThis encompasses e.g.\nthe pre-\ndiction or theoretical explanation of phase diagrams of\nsolids or surfaces. In recent years the advancement of\nmachine learning (ML) techniques as a set of tools to re-\nduce the computational expense of such studies has be-\ncome widespread [1].\nPerhaps the most common use of ML is that of re-\nplacing expensive first-principles calculations of total en-\nergies with orders of magnitude faster machine-learning\ninteratomic potentials (MLIPs). Early works in this area\nincluded Behler-Parinello neural networks and the Gaus-\nsian Approximation Potentials of Bartok et.\nal [2, 3].\nSince then many advances have been made improving\nthe accuracy and data efficiency of these potentials [4–\n17]. With these improvements many tasks in computa-\ntional material science have benefitted from the efficiency\nthey offer, however often demanding the construction of\na task specific dataset, thus still requiring expensive ab-\ninitio calculations.\nRecently datasets containing ab-initio properties of\natomic configurations covering vast regions of chemical\nspace have become available [18–20].\nThese data sets\nhave enabled the creation of the atomistic equivalent\nof foundation models – universal potentials. One such\ndataset is the Materials Project Trajectory (MPtrj) con-\ntaining among others energies and forces for some ∼1.6\nmillion structures extracted from the Materials Project.\n∗hammer@phys.au.dk\nSeveral MLIPs have been trained on this dataset includ-\ning CHGNet, MACE-MP0, SevenNet-0, and ORB-v2-\nonly-MPtrj [18, 21–23].\nOne area that will benefit from these potentials is that\nestablishing which phases of a material are stable under\nphysically relevant conditions. One strategy in this do-\nmain is global structure optimization, finding the most\nenergetically, in terms of total energy, favorable geome-\ntries of an atomic system followed by a thermodynamic\nanalysis to identify the phases with the lowest Gibbs free\nenergy. The optimization step requires exploration of the\npotential energy surface, a task that has received much\nresearch interest in order to provide effective algorithms\n[24–29]\nThe construction of accurate phase diagrams depends\ncritically on the accuracy of the underlying total energy\ndescription. In our recent work on describing the global\noptimal structure of silicate clusters and ultra-thin oxide\nfilms on Ag(111) with CHGNet, it was found necessary\nto augment the CHGNet model with ∆-learning in order\nto get the correct order of stability of low-lying struc-\ntures [30].\nIn that work, the ∆-model was built on a\nrepresentation involving the SOAP descriptor for each\natom. In the present work, we introduce the necessary\nformalism for eliminating the need of such a descriptor\nand instead use the internal representation of the atoms\nin the CHGNet when constructing the ∆-model.\nEm-\nploying the method in a global optimization setting, we\nfind the resulting corrections for ultra-thin oxide films on\nCu(111)-c(8 × 4) to be small. The search results in an\noxide film structure that represents a reinterpretation of\nthe experimentally found structure. This result clearly\ntestifies to a high quality of a foundation model such as\nCHGNet.\nIn contrast, for sulfur ad-atom layers on Cu(111),\narXiv:2502.21179v1  [cond-mat.mtrl-sci]  28 Feb 2025\n\n\n2\nAg(111), and Au(111) we find that the correction terms\nresulting from the delta model are more critical, in par-\nticular for Ag(111) and Au(111). By analyzing the simi-\nlarity of local atomic environments in the sulfur ad-layer\nsystems and in the training database for CHGNet, we\ntrace the origin of the low accuracy in these systems to\nthe lack of relevant Ag-S and Au-S local environments in\nthe training data.\nThe paper is structured as follows: First we introduce\nthe methodologies employed, this includes an account\nof the descriptor we employ which has been extracted\nfrom CHGNet, the Gaussian Process Regression scheme\nthat we use, and a summary of the global optimization\n(GO) algorithm used for structural searches. Next, we\npresent the computed phase diagrams for O/Cu(111) and\nS/Au(111), showing the need for corrections in the lat-\nter case. The paper proceeds by comparing the CHGNet\nbehavior for a specific S-coverage on Cu(111), Ag(111),\nand Au(111) and relates that to the occurrence of rel-\nevant structures in the MPtrj dataset thereby shedding\nlight on why CHGNet gets the relatively simple sulfur\noverlayer structure so wrong when it is capable of pre-\ndicting a previously undiscovered phase of the copper\noxide. The paper ends by identifying that the issues for\nthe sulfur ad-layers pertains to other foundation models,\nMACE-MP0, SevenNet-0, and ORB-v2-only-MPtrj, that\nhave all been trained on the same dataset as CHGNet.\nII.\nMETHODOLOGY\nA.\nDescription of Atomic Environments\nThe development and success of machine learning tools\nin computational materials science has been driven by it-\nerative improvements to the description of environments.\nThe field initally relied on handcrafted descriptors, such\nas Behler-Parinello symmetry functions [31], the Valle-\nOganov fingerprint [32] or the Smooth Overlap of Atomic\nPositions (SOAP) formalism [33]. Since then neural net-\nworks such as SchNet that are capable of learning useful\nrepresentations directly from Cartesian coordinates have\nbecome widespread [8, 34, 35].\nRecently such neural networks have been trained on\nlarge datasets resulting in so-called universal potentials –\nthe atomistic equivalent of a foundation model. One such\nfoundation model is CHGNet, which has been trained\non the MPtrj dataset [18].\nCHGNet is a graph neu-\nral network where message-passing operations are used\nto iteratively update the representation of each atom\nwith information from its surrounding neighbors.\nThe\nlayout of CHGNet is shown schematically in the green\nbox of Fig. 1. A prediction from CHGNet, and likewise\nfor the majority of other graph based MLIPs, is calcu-\nlated by first updating the atomic representations with\nmessage-passing and then passing these refined descrip-\ntors to a prediction head, that transforms and aggregates\nthe descriptors in order to produce a prediction. Typi-\nFIG. 1. Illustration of our use of CHGNet, including a sum-\nmarized version of CHGNet and our additional ∆-model using\nthe CHGNet atomic representations.\ncally, an energy prediction is made by transforming the\nhigh-dimensional descriptors to a scalar for each atom\nand summing these together. We will use ⃗vi to denote\nrepresentations of atomic environments extracted from\nCHGNet. These are 64 dimensional vectors one for each\natom in an atomic configuration. These descriptors may\nbe extracted and used for other tasks, such as ∆-learning\nas shown schematically with the brown boxes in Fig. 1.\nFor a full description of the architecture of CHGNet,\nwe refer to Figure 1 of the original publication Ref. [18].\nFor the present purposes, it serves to summarize it as:\n1. Initialize\nrelevant\nproperties\nfor\nthe\nmessage-\npassing, including initial descriptors, ⃗vi\n0.\n2. Apply message-passing interaction blocks t −1\ntimes resulting in descriptors ⃗vi\nt−1 for each atom i.\n3. Predict magnetic moments using vectors ⃗vi\nt−1.\n4. Apply one more message-passing block to compute\nvectors ⃗vi\nt.\n5. Predict atomic energies ϵi using the vectors ⃗vi\nt.\n6. Sum local energies to predict total energy E =\nP\ni ϵi.\nCHGNet consists of a representation part, that uses\nmessage-passing to construct descriptors of atomic envi-\n\n\n3\nronments, steps 1 through 4, and a fully-connected neu-\nral network that converts each atomic descriptor to an\natomic energy and the atomic energies are summed to\ncompute the total energy, steps 5 and 6. The individ-\nual atomic descriptors are 64-dimensional vectors. We\nuse the final atomic descriptors ⃗vi\nt as the basis for a ∆-\nmodel described in the next section.\nB.\nAdditive Gaussian Process Regression\nWe follow our recent proposal [30] and employ a Gaus-\nsian Process Regression (GPR) model in a ∆-learning\ncontext for correcting the universal potential in regions\nwhere it makes incorrect predictions. However, in this\nwork, we introduce a different formalism that ties more\nclosely together with the neural network.\nCustomarily the predicted mean of a GPR is given by\nm(R) = k(R, X)[K(X, X) + σ2\nnI]−1y.\n(1)\nWhere R is the representation of a query object, X are\nrepresentations of the training data, typically a matrix\nwhere each row is a feature vector but can more generally\nbe considered as a set of representations of the training\nexamples.\nEach training example comes with a corre-\nsponding target y and σ2\nn is the variance of the assumed\nnoise and I is the identity matrix. The representation R\nmay describe the full object through descriptors of parts\nof the object. To facilitate this, an additive kernel may\nbe defined:\nKadd(Rv, Rw) =\nnv\nX\ni\nnw\nX\nj\nLv\ni Lw\nj K(⃗vi, ⃗wj).\n(2)\nHere Rv, the representation of object v, which consists of\n(\n⇒\ncv, Lv) where\n⇒\ncv= [⃗v1,⃗v2, ..,⃗vnv] is a collection of vectors\ndescribing v and Lv\ni is the number of contributions asso-\nciated with each vector ⃗vi – likewise Rw = (\n⇒\ncw, Lw). This\ntype of kernel can compare two objects represented by a\ndifferent number of vectors and with a different number\nof contributions. If\n⇒\ncv= [⃗v1],\n⇒\ncw= [⃗w1] and Lv\n1 = Lw\n1 = 1\nit is evident that the original kernel K(⃗v1, ⃗w1) is recov-\nered. This introduces the additional property L, which\nwe may also use to write the additive kernel in matrix\nform\nKadd(Rv, Rw) = LvK(\n⇒\ncv,\n⇒\ncw)LT\nw\n(3)\nWith this expression we can write Eq. (1) with an addi-\ntive kernel as,\nm(Rv) = Lvk(\n⇒\ncv, X)LT\nX[LXK(X, X)LT\nX + σ2\nnI]−1y. (4)\nFor the task of learning total energies of atomic config-\nurations starting from vectors describing the environment\nof individual atoms this leaves us with several options of\nhow to construct the representations R. This is essen-\ntially a choice of an aggregation procedure\n• Global aggregation:\n⇒c = [ 1\nN\nPN\ni ⃗vi] and L = [N] or\nL = [1].\n• Atomic aggregation:\n⇒c =\n[⃗v1, .., ⃗vN] and L\n=\n[1, .., 1]\n• Species aggregation:\n⇒c = [⃗c1, ⃗c2, .., ⃗\ncM] where ⃗cm =\n1\nNm\nPN\ni ⃗vi · δ(Zm, Zi) and L = [N1, N2, .., NM]\nWhere N is the total number of atoms in a configuration,\n⃗vi are atomic descriptors, M is the number of different\nspecies, Nm is the number of atoms of species Zm, Zi is\nthe species of atom i and δ is the Kronecker delta func-\ntion. These aggregation procedures are depicted in Fig-\nure 2. With a global aggregation scheme the model learns\nthe total property directly and by using L = [N] it is ca-\npable of learning from data involving different amounts\nof atoms (unlike the situation if L = [1] is chosen, as is\noften done). With atomic aggregation the model learns\natomic energies such that they sum to the total energy.\nSimilarly, with species aggregation the model learns the\naverage energy of each species which in a sum weighted\nby the number of atoms of each species yields the total\nenergy. Global aggregation amounts to attributing one\nfeature vector to the entire configuration in which case\nEq.\n(1) may be used.\nAtomic aggregation is equiva-\nlent to popular techniques such as GAP, but introduces\npoor computational scaling as the number of atomic en-\nvironments in a training set may be very large – generally\nnecessitating the introduction of approximate GPR tech-\nniques such as the use of a sparsified GPR, that uses a\nsubset of the data as inducing points to make training\nfeasible and limit prediction time. Finally using species\naggregation a configuration is described by as many vec-\ntors as there are unique atomic species with each vec-\ntor describing the average environment of that species\nand lm counting the number of atoms of each species.\nThis reduces the computational expense and eliminates\nthe need for approximate GPR techniques while offering\nimproved resolution compared to global aggregation. A\nsimilar scheme has previously been used for filtering of\natomic structures [36].\nWe use this species aggregation GPR model in com-\nbination with CHGNet in a ∆-learning scheme, where\nthe GPR learns to correct the errors of CHGNet, we will\nrefer to this as ∆GP-CHGNet for the remainder of the\narticle. A final note on the GPR model, in addition to\nextracting features from CHGNet we may also leverage\nthe automatic differentiation capabilities of PyTorch that\nCHGNet is written in. This means derivatives of the total\n∆GP-CHGNet energy can be computed at essentially no\nadditional expense compared to those of just CHGNet.\nAdditionally, this way of evaluating forces does not re-\nquire the implementation of any analytical derivatives,\nbe it of features or kernels, which is normally the most\nchallenging and error-prone part of implementing a GPR\nfor the prediction of atomistic properties.\n\n\n4\nConfigurations\n1.\n2.\nSpecies aggregation\nAtomic aggregation\nGlobal aggregation\nFIG. 2. Illustration of aggregation procedures for two configurations. For global aggregation each configuration is represented\nby a single vector and the additive kernel reduces to a simple kernel between the vectors for each configuration. For atomic\naggregation each configuration is represented by vectors describing the environment of each atom with unit weight L. Finally\nfor species aggregation each configuration is represented by two vectors each describing the average oxygen and copper, as the\nfirst configuration has more oxygen than the second they have different L.\nC.\nGlobal structure search\nIn the applications of the ∆-learning augmented\nCHGNet model, we will study the stability of surfaces\nconsisting of a fixed slab and an overlayer with variable\nstoichiometry.\nTo find the optimal geometry for each\nstoichiometry we employ the GOFEE optimization algo-\nrithm as implemented in AGOX [24, 26]. This algorithm\niteratively explores the potential energy surface guided\nby a surrogate model, an outline of each iteration is as\nfollows\n1. Create a number of structures.\n2. Locally optimize each structure in the lower-\nconfidence bound of the current surrogate model.\n3. Select the most promising candidate according to\nan acquisition function that takes the surrogate\nprediction into account.\n4. Perform a single-point DFT calculation for the se-\nlected candidate and store it in a database.\n5. Update the surrogate model with the new data\npoint.\nWe run a number of such searches for each stoichiom-\netry for a fixed number of iterations. This results in a\nnumber of configurations with total energies calculated\nat the DFT level for each stoichiometry. The DFT set-\ntings we employed are described in . To establish which\nstoichiometry/phase is stable under different conditions\nthe Gibbs free energy is calculates as\n∆G = ET −Eslab −\nX\nZ\nnZ(∆µZ + εZ).\n(5)\nWhere ET is the total energy, Eslab is the total energy\nof the clean slab, nZ is the number of atoms with atomic\nnumber Z in the overlayer and finally ∆µZ and εZ are\nthe chemical potential and reference energy of atoms with\natomic number Z.\nIII.\nRESULTS\nA.\nCopper-oxide\nThe first system we investigate is that of the ”8” Cu\noxide, which is an ultra-thin oxide film on Cu(111). For\n\n\n5\nCu4O5 -0.15 eV\nCu5O5 -0.01 eV\nCu6O5 -0.07 eV\nCu7O5 -0.10 eV\nCu8O5 0.01 eV\nCu4O4 -0.02 eV\nCu5O4 -0.02 eV\nCu6O4 -0.01 eV\nCu7O4 -0.08 eV\nCu8O4 -0.00 eV\nCu4O3 -0.02 eV\nCu5O3 -0.17 eV\nCu6O3 -0.06 eV\nCu7O3 -0.00 eV\nCu8O3 -0.00 eV\nFIG. 3. Minimum energy structures for each stoichiometry found with ∆GP-CHGNet, the energies reported in the insets\nare the difference between the lowest energy structure found when employing a ∆-model and those with just CHGNet, both\ncalculated (including relaxations) with DFT. Small and large circles represent Cu and O atoms, respectively. The atoms are\ncolored according to height above the slab with higher atoms being darker in color. The unit cell is shown in black.\nthis system, LEED shows evidence of a periodic struc-\nture forming within a c(8 × 4) surface cell, see Ref. 37\nand references therein. A structural model containing 8\nCu and 4 O atoms was originally proposed based on the\nexperimental evidence in 2008 by Moritani et al.[37]. Re-\ncently, the model was revised based on density functional\ntheory calculations by Kim et al.[38]. However, no full\nCu4O5\nCu5O5\nCu6O5\nCu7O5\nCu8O5\nCu4O4\nCu5O4\nCu6O4\nCu7O4\nCu8O4\nCu4O3\nCu5O3\nCu6O3\nCu7O3\nCu8O3\nG (\nO =\n1 eV)\n2\n1\n0\nO [eV]\n5\n0\nG [eV]\nCu5O4\nCu5O5\nCu8\n1.0\n0.5\nG [eV]\nFIG. 4.\nTop:\nRaster plot of the Gibbs free energy at\n∆µO = −1 eV. Bottom: Phase diagram showing the most\nstable phase as a function of the chemical potential of oxy-\ngen.\nphase diagram has been constructed leaving this system\nopen for discovery of new phases.\nTo supplement the previous studies, we have therefore\nconsidered stoichiometries of the ultra-thin oxide film,\nCuxOy, with x = [4, 5, 6, 7, 8] and y = [3, 4, 5]. The refer-\nence energy for Cu is calculated as the difference in the\ntotal energy of a four and three-layer slab divided by the\ndifference in the number of atoms. The reference energy\nfor oxygen is half of the total energy of an O2 molecule\nfrom a spin-polarized calculation.\nFor each stoichiometry we employ the optimization al-\ngorithm described in Section II C with CHGNet-v0.3.0\nand with ∆GP-CHGNet-v0.3.0. The lowest energy struc-\ntures found by the searches with the ∆GP model are re-\nported in Fig. 3. It is evident from this figure that for\nmost stoichiometries the structures found with the ∆GP\nare only slightly more stable than those found by just\nsearching in CHGNet. This shows the remarkable ability\nof CHGNet to predict new, unpublished, structures for\nmaterials solely based on being trained on a wide variety\nof examples, the vast majority of which are bulk and not\nsurfaces.\nThe phase diagram for this system is shown in Fig-\nure 4. Which shows that for a wide range of chemical\npotentials for oxygen the preferred phase is Cu5O4. We\nnote that by considering a wide range of CuxOy stoi-\nchiometries, the present search reveals a new structure\nthat challenges the previous assignment of Kim et al.\n[38], which however is also contained in Fig. 4 as the\nmost stable structure at that given stoichiometry, Cu8O4.\nWhile LEED-IV studies or surface X-ray diffraction ex-\nperiments would be required to conclusively discriminate\nwhich structural model conforms best with the actual\n\n\n6\nCHGNet\nS4\nS7\nS9\nGP-CHGNet\nE =\n1.58 eV\nE =\n2.29 eV\nE =\n1.95 eV\nFIG. 5. Most energetically favorable, as evaluated using DFT, structures for three selected values for the number of sulfur\natoms found with CHGNet (top) and ∆GP-CHGnet (bottom). The energies reported in the top row are calculated as ∆E =\nE∆GP−CHGNet −ECHGNet where the subscripts indicate the method used to find the configurations but both are evaluated with\nDFT. The unit cell is shown in red.\nCu ”8” oxide, it seems likely that the new Cu5O4 is the\ncorrect structure as this aligns with the findings of an\nidentical structure for Ag(111)[39, 40] and a similar non-\ncommensurate structure for Pd(111) [41].\nB.\nSulfur adsorption on Au(111)\nThe next system studied is that of sulfur adsorption\non Au(111). For this system, an adsorption phase hav-\ning a (5×5) super cell has been observed with LEED, see\nRef. 42, and a surface structure with 7 sulfur atoms in a\n’Rosette’ was proposed as the underlying structure. The\noccurrence of this pattern is attributed to slightly repul-\nsive interactions between adsorbed sulfur atoms. We in-\nvestigate the efficacy of CHGNet and our proposed ∆GP-\nCHGNet model for structural exploration of this system.\nAgain, we perform searches using only CHGNet and\nusing ∆GP-CHGNet, in Figure 5 the most stable config-\nurations found with the number of sulfur atoms NS =\n[2, 7, 9] for both methodologies is shown. For this sys-\ntem, the differences in stability between structures iden-\ntified purely with CHGNet and those found with ∆GP-\nCHGNet are very significant. Out of the box CHGNet-\nv0.3.0 generally prefers sulfur dimers whereas ∆GP-\nCHGNet learns that it is preferential for sulfur atoms\nto sit in separate hollow-fcc sites.\nThis tendency may\nbe observed in all three stoichoimetries depicted in Fig-\nure 5. With four sulfur atoms, two dimers are arranged\nin a small cluster, with seven sulfur atoms three dimers\nand a lone sulfur atom form a cluster and finally for nine\natoms four dimers and a lone sulfur form a cluster. In\ncontrast, for all these structures, DFT prefers separated\natoms forming extended overlayers – only forming dimers\nwhen the number of sulfur atoms added to the surface is\nrelatively high.\nGiven the large configurational discrepancies between\n4\n3\n2\n1\n0\nG [eV]\nCHGNet\nS0\nS3\nS5\nS7\nS9\n1.2\n1.0\n0.8\n0.6\nS [eV]\n4\n3\n2\n1\n0\nG [eV]\nGP-CHGNet\nS0\nS2\nS3 S4\nS7\nS10\nFIG. 6. Predicted Gibbs free energy phase diagrams as a func-\ntion of the chemical potential of sulfur for structures found\nusing out of the box CHGNet (top) and using ∆GP-CHGnet\n(bottom).\nthe two search methodologies it is unsurprising that the\nGibbs free energy diagrams differ extensively, as is shown\nin Figure 6.\nFor this system, if relying entirely on\nCHGNet no phases are predicted correctly except the\ntrivial bare surface for which the chemical potential ∆µS\nnecessary for its occurrence is rather inaccurate.\n\n\n7\nCHGNet\nS7-Cu\nS7-Ag\nS7-Au\nGP-CHGNet\nE =\n0.43 eV\nE =\n1.30 eV\nE =\n2.29 eV\nFIG. 7. Sulfur overlayer global minimums identified with CHGNet (top) and ∆-GP CHGNet (bottom). The energy difference,\nwith DFT, between the two structures is given. With ∆-GP CHGNet the same motif is found for all three metals, whereas\nwithout the correction dimers are formed for silver and gold. CHGNet predicts the correct motif on copper, but it is shifted to\nsulfur on hcp hollow sites.\nC.\nAnalysis\nSo far, we have seen results for two systems.\nOne\nwhere the universal potential CHGNet is able to pre-\ndict a rather complex surface-oxide structure. However,\nfor the other system the universal potential struggles sig-\nnificantly and consistently favors configurations with an\nerroneous bonding motif. For other metal-sulfur adsorp-\ntion structures, such as S/Cu and S/Ag discrepancies\nbetween structures preferred in CHGNet and DFT are\nalso present but to a lesser extent. This is illustrated in\nFig 7. In fact, for Cu(111)-(5×5)-7S the correct motif is\nthe global minimum of CHGNet, but it is shifted from\nsulfur sitting on fcc hollow sites to hcp hollow sites. For\nsilver a cluster containing several dimers is the preferred\nstructure according to CHGNet, whereas with DFT the\nsame motif as for gold and copper is obtained.\nTo facilitate an analysis of the origin of the poor\nCHGNet description of S/Ag(111) and S/Au(111) sys-\ntems, we have found it instructive to reduce the com-\nplexity of the sulfur structure by studying systems with\nonly two sulfur atoms present in the cell. For all three\nmetals Cu, Ag, and Au it can be found with DFT that\nconfigurations in which the two sulfur atoms are sepa-\nrated are more stable than configurations in which they\nform a dimer on the surface. A full pathway from two\nseparated sulfur atoms to a sulfur dimer can be calcu-\nlated, see Fig. 8 for an example for the Cu(111) surface.\nIn an effort to investigate the reasons for this differ-\nence in CHGNet’s ability to predict physically realistic\nresults we wish to understand which parts of the Mate-\nrials Project training data are likely to have influenced\nthe predictions for each system. For this reason, we com-\npute the CHGNet features for every sulfur for various\nmetal-sulfides present in the MPtrj dataset alongside the\nfeatures of sulfur atoms of the S2 system for trajectories\nstarting with the separated sulfur atoms and ending with\na sulfur dimer. Such features may be compared using a\nsimilarity metric, such as a normalized dot product. For\neach configuration of the S2-trajectories we compute\nS(k) =\n1\n2N\nN\nX\ni\n2\nX\nj\n⃗vi · ⃗wj(k)\n|⃗vi||⃗wj(k)|\n(6)\nWhere ⃗vi is the CHGNet representation of a sulfur-atom\nin the MPtrj dataset, either from configurations involv-\ning both sulfur and metal atoms or from configurations\ninvolving only sulfur atoms. ⃗wj(k) is the representation\nof the j’th sulfur atom for configuration k along the tra-\njectory. The result of this is shown in Fig. 8. On copper\nthe similarity towards sulfur atoms in environments con-\ntaining both Cu and S remains largely constant along the\npathway, while the similarity involving only S increases\nas the dimer is formed. In contrast, on both silver and\ngold the similarity towards sulfur atoms in environments\ncontaining the metal decreases sharply at the same time\nas the similarity towards sulfur in environments contain-\ning only sulfur increases. That is, for sulfur on silver and\ngold CHGNet relies too heavily on information gathered\nfrom pure sulfur configurations where dimers are very\nstable. This may explain why CHGNet can identify that\non copper the dimer is not preferential, while on silver\nand gold it is unable to do so – which then leads to er-\nroneous configurations when additional sulfur atoms are\nintroduced.\nFurther evidence for this explanation is found from the\nnumber of sulfur environments originating from configu-\nrations of different types present in the MPtrj training\ndataset, presented in Table I. For CuxSy the number of\nconfigurations is comparable to the number of configura-\ntions of pure sulfur, whereas for silver and especially for\ngold the number of configurations and environments is\nsubstantially less than those of pure sulfur. It stands to\n\n\n8\nEDFT\nECHG\nDFT\nCHG\n0\n1\n2\nEnergy [eV]\n0.7\n0.8\n0.9\n1.0\nSimilarity (k)\nS2 / Copper\nS in CuxSy\nS in Sy\n0.7\n0.8\n0.9\n1.0\nSimilarity (k)\nS2 / Silver\nS in AgxSy\nS in Sy\n0\n20\n40\n60\n80\n100\nk\n0.8\n0.9\n1.0\nSimilarity (k)\nS2 / Gold\nS in AuxSy\nS in Sy\nFIG. 8. Energy profiles for combining two sulfur atoms on\na copper surface. Average dot product similarity for sulfur\natoms for a trajectory starting with separately adsorbed sul-\nfur atoms ending with a S2-dimer. The blue lines shows the\naverage similarity against sulfur atoms present in MPTrj in\nstructures of the form XxSy where X is copper, silver or gold.\nThe orange lines measures the similarity against sulfur atoms\nfrom MPtrj in structures containing only sulfur.\nreason, that this imbalance has at least partially induced\nthe overreliance on sulfur environments from configura-\ntions containing only sulfur.\nCHGNet is not the only universal potential trained on\nMPtrj. In Figure 9 we show the relative stability of S2\ndimers compared to two separated S atoms on the three\ndifferent metals calculated with DFT and four machine\nlearning potentials.\nAll of them overestimate the sta-\nbility of the dimer, but by various amounts and as with\nElement Configurations Environments\nCu + O\n647\n3268\nCu + S\n548\n5154\nAg + S\n374\n828\nAu + S\n17\n42\nS only\n636\n20450\nTABLE I. Number of configurations in MPTrj of the form\nMxSy for different metals and the number of configurations\ncontaining only sulfur along with the total number of sulfur\nenvironments for each type of configuration. In addition, the\ntable lists the number of CuxOy configurations and the num-\nber of oxygen environments in those in MPtrj.\nDFT\nMACE\nOrb\nSevenNet\nCHGNet\nDFT\nOrb\nSevenNet\nCHGNet\nMACE\nDFT\nOrb\nSevenNet\nMACE\nCHGNet\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\nE [eV]\nS2-Cu\nS2-Ag\nS2-Au\nFIG. 9. Relative stability of sulfur-dimer compared to two\nseparated sulfur atoms on copper, silver and gold. Relative\nstabilities have been calculated using DFT, CHGNet-0.3.0,\nMACE-MP0, SevenNet-0 and ORB-v2-only-MPtrj – which\nall are trained on the MPtrj dataset. Positive values indicate\nthat the dimer is less stable than the two separated atoms.\nA visual representation of these numbers are depicted in Fig.\n8(a)\nCHGNet the errors are generally smallest when the metal\nis copper.\nIV.\nCONCLUSION\nWe have investigated the use of a universal poten-\ntial, CHGNet, for the task of global mimimum energy\nstructure prediction, specifically for two systems an oxy-\ngen induced surface reconstruction of copper and sulfur\noverlayers on group 11 metals.\nFurther, we introduce\na ∆-learning method on top of the universal potential\nin order to perform global optimization searches with\nboth the out-of-the-box CHGNet and this adapted ∆-\nGP-CHGNet model.\nFor the ”8” Cu surface oxide we find a new global\n\n\n9\nminimum structure and show that this is a discovery\nthat as-is CHGNet is capable of supporting.\nWhereas\nwe for the sulfur overlayer systems find that CHGNet\nhas flawed understanding that leads to false predictions\nfor the global minimum energy structures. Analyzing the\nbehavior of CHGNet for the S2 dimer in various metallic\nembeddings, we traced the origin of the false predictions\nto CHGNet overestimating the stability of the sulfur-\ndimer. By inspecting the MPtrj dataset using the rep-\nresentations CHGNet has learned during its training on\nthis dataset, we find that a possible cause for CHGNets\nmistaken understanding of adsorbed sulfur dimers is an\noverreliance on the parts of the training data that only\ninvolve sulfur.\nUniversal potentials provide the materials science com-\nmunity with the opportunity to investigate more and\nlarger systems and allows realistic materials modelling to\nbe done with much fewer computational resources. How-\never, they do not come with guarantees and may, as we\nhave shown, be severely mistaken. Users should be wary\nand make sure to test the correctness of any such po-\ntential, at least for a scaled-down version of their system\nof interest. Our efforts are one way of adding some ex-\nplainability to the predictions of a universal potential,\nfurther work may involve the application of other model\ninterpretability methods such as TracInCP for the iden-\ntification of influential training examples [43].\nV.\nDATA AVAILABILITY\nThe data and code supporting for this article is openly\navailable from gitlab.com/agox/agox-chg.\nVI.\nACKNOWLEDGEMENTS\nWe acknowledge support from VILLUM FONDEN\nthrough Investigator grant, project no. 16562, and by\nthe Danish National Research Foundation through the\nCenter of Excellence “InterCat” (Grant agreement no:\nDNRF150).\n[1] G. Wang, C. Wang, X. Zhang, Z. Li, J. Zhou, and Z. Sun,\niScience 27, 109673 (2024).\n[2] J. Behler and M. Parrinello, Phys. Rev. Lett. 98, 146401\n(2007).\n[3] A. P. Bart´ok, M. C. Payne, R. Kondor, and G. Cs´anyi,\nPhys. Rev. Lett. 104, 136403 (2010).\n[4] M. Rupp, A. Tkatchenko, K.-R. M¨uller,\nand O. A.\nVon Lilienfeld, Phys. Rev. Lett. 108, 058301 (2012).\n[5] J. Behler, Int. J. Quantum Chem. 115, 1032 (2015).\n[6] V. Botu, R. Batra, J. Chapman, and R. Ramprasad, J.\nPhys. Chem. C 121, 511 (2017).\n[7] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and\nG. E. Dahl, in Proceedings of the 34th International Con-\nference on Machine Learning, Proceedings of Machine\nLearning Research, Vol. 70 (PMLR, 2017).\n[8] K. Sch¨utt, P.-J. Kindermans, H. E. Sauceda Felix,\nS. Chmiela, A. Tkatchenko,\nand K.-R. M¨uller, in\nAdvances in Neural Information Processing Systems,\nVol. 30, edited by I. Guyon, U. V. Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett\n(Curran Associates, Inc., 2017).\n[9] N. Thomas, T. Smidt, S. Kearnes, L. Yang, L. Li,\nK. Kohlhoff,\nand P. Riley, “Tensor field networks:\nRotation- and translation-equivariant neural networks\nfor 3D point clouds,” (2018), arXiv:1802.08219 [cs].\n[10] B.\nAnderson,\nT.-S.\nHy,\nand\nR.\nKondor,\n“Cor-\nmorant: Covariant Molecular Neural Networks,” (2019),\narXiv:1906.04015 [physics, stat].\n[11] O. T. Unke and M. Meuwly, J. Chem. Theory Comput.\n15, 3678 (2019).\n[12] K. Sch¨utt, O. Unke, and M. Gastegger, in Proceedings of\nthe 38th International Conference on Machine Learning\n(PMLR, 2021) pp. 9377–9388.\n[13] V. L. Deringer, A. P. Bart´ok, N. Bernstein, D. M.\nWilkins, M. Ceriotti, and G. Cs´anyi, Chem. Rev. 121,\n10073 (2021).\n[14] S. Batzner, A. Musaelian, L. Sun, M. Geiger, J. P.\nMailoa, M. Kornbluth, N. Molinari, T. E. Smidt,\nand\nB. Kozinsky, Nat Commun 13, 2453 (2022).\n[15] H. Montes-Campos, J. Carrete, S. Bichelmaier, L. M.\nVarela, and G. K. H. Madsen, J. Chem. Inf. Model. 62,\n88 (2022).\n[16] I. Batatia, D. P. Kovacs, G. Simm, C. Ortner,\nand\nG. Csanyi, in Advances in Neural Information Process-\ning Systems, Vol. 35, edited by S. Koyejo, S. Mohamed,\nA. Agarwal, D. Belgrave, K. Cho,\nand A. Oh (Curran\nAssociates, Inc., 2022) pp. 11423–11436.\n[17] D. M. Anstine and O. Isayev, J. Phys. Chem. A 127,\n2417 (2023).\n[18] B. Deng, P. Zhong, K. Jun, J. Riebesell, K. Han, C. J.\nBartel, and G. Ceder, Nat. Mach. Intell. 5, 1031 (2023).\n[19] L. Chanussot, A. Das, S. Goyal, T. Lavril, M. Shuaibi,\nM. Riviere, K. Tran, J. Heras-Domingo, C. Ho, W. Hu,\nA. Palizhati, A. Sriram, B. Wood, J. Yoon, D. Parikh,\nC. L. Zitnick, and Z. Ulissi, ACS Catal 11, 6059 (2021).\n[20] R. Tran, J. Lan, M. Shuaibi, B. M. Wood, S. Goyal,\nA. Das,\nJ. Heras-Domingo,\nA. Kolluru,\nA. Rizvi,\nN. Shoghi, A. Sriram, F. Therrien, J. Abed, O. Voznyy,\nE. H. Sargent, Z. Ulissi,\nand C. L. Zitnick, ACS Catal\n13, 3066 (2023).\n[21] I. Batatia, P. Benner, Y. Chiang, A. M. Elena, D. P.\nKov´acs, J. Riebesell, X. R. Advincula, M. Asta, W. J.\nBaldwin,\nN. Bernstein,\nA. Bhowmik,\nS. M. Blau,\nV. C˘arare, J. P. Darby, S. De, F. D. Pia, V. L. De-\nringer, R. Elijoˇsius, Z. El-Machachi, E. Fako, A. C.\nFerrari, A. Genreith-Schriever, J. George, R. E. A.\nGoodall,\nC. P. Grey,\nS. Han,\nW. Handley,\nH. H.\nHeenen, K. Hermansson, C. Holm, J. Jaafar, S. Hof-\nmann, K. S. Jakob, H. Jung, V. Kapil, A. D. Ka-\nplan, N. Karimitari, N. Kroupa, J. Kullgren, M. C.\nKuner, D. Kuryla, G. Liepuoniute, J. T. Margraf, I.-\nB. Magd˘au, A. Michaelides, J. H. Moore, A. A. Naik,\n\n\n10\nS. P. Niblett, S. W. Norwood, N. O’Neill, C. Ortner,\nK. A. Persson, K. Reuter, A. S. Rosen, L. L. Schaaf,\nC. Schran, E. Sivonxay, T. K. Stenczel, V. Svahn, C. Sut-\nton, C. van der Oord, E. Varga-Umbrich, T. Vegge,\nM. Vondr´ak, Y. Wang, W. C. Witt, F. Zills,\nand\nG. Cs´anyi, (2023), arXiv:2401.00096 [physics.chem-ph].\n[22] J. Kim, J. Kim, J. Kim, J. Lee, Y. Park, Y. Kang, and\nS. Han, J. Am. Chem. Soc. 147, 1042 (2024).\n[23] M. Neumann, J. Gin, B. Rhodes, S. Bennett, Z. Li,\nH. Choubisa, A. Hussey,\nand J. Godwin, “Orb:\nA\nFast,\nScalable Neural Network Potential,”\n(2024),\narXiv:2410.22570 [cond-mat].\n[24] M. K. Bisbo and B. Hammer, Phys. Rev. Lett. 124,\n086102 (2020).\n[25] J. Timmermann,\nF. Kraushofer,\nN. Resch,\nP. Li,\nY. Wang,\nZ. Mao,\nM. Riva,\nY. Lee,\nC. Staacke,\nM. Schmid, C. Scheurer, G. S. Parkinson, U. Diebold,\nand K. Reuter, Phys. Rev. Lett. 125, 206101 (2020).\n[26] M.-P. V. Christiansen, N. Rønne,\nand B. Hammer, J.\nChem. Phys. 157, 054701 (2022).\n[27] D. Chen, C. Shang, and Z.-P. Liu, J. Chem. Phys. 156,\n094104 (2022).\n[28] C. Larsen, S. Kaappa, A. L. Vishart, T. Bligaard, and\nK. W. Jacobsen, Phys. Rev. B 107, 214101 (2023).\n[29] P. Lyngby, C. Larsen,\nand K. W. Jacobsen, Physical\nReview Materials 8, 123802 (2024).\n[30] J. Pitfield, F. Brix, Z. Tang, A. M. Slavensky, N. Rønne,\nM.-P. V. Christiansen, and B. Hammer, Phys. Rev. Lett.\n134, 056201 (2025).\n[31] J. Behler, J. Chem. Phys. 134, 074106 (2011).\n[32] A. R. Oganov and M. Valle, J. Chem. Phys. 130, 104504\n(2009).\n[33] A. P. Bart´ok, R. Kondor, and G. Cs´anyi, Phys. Rev. B\n87, 184115 (2013).\n[34] K. Sch¨utt, O. Unke,\nand M. Gastegger, in Proceedings\nof the 38th International Conference on Machine Learn-\ning, Proceedings of Machine Learning Research, Vol. 139,\nedited by M. Meila and T. Zhang (PMLR, 2021) pp.\n9377–9388.\n[35] S. Batzner, A. Musaelian, L. Sun, M. Geiger, J. P.\nMailoa, M. Kornbluth, N. Molinari, T. E. Smidt,\nand\nB. Kozinsky, Nat Commun 13, 2453 (2022).\n[36] P. Kov´acs, E. Heid, J. De Landsheere,\nand G. K. H.\nMadsen, “LoGAN: Local generative adversarial network\nfor novel structure prediction,” (2024).\n[37] K. Moritani, M. Okada, Y. Teraoka, A. Yoshigoe,\nand\nT. Kasai, J. Phys. Chem. C 112, 8662 (2008).\n[38] H. J. Kim, G. Lee, S.-H. V. Oh, C. Stampfl, and A. Soon,\nACS Nano 18, 4559 (2024).\n[39] J. Schnadt, J. Knudsen, X. L. Hu, A. Michaelides, R. T.\nVang, K. Reuter, Z. Li, E. Lægsgaard, M. Scheffler, and\nF. Besenbacher, Phys. Rev. B 80, 075424 (2009).\n[40] H. L. Mortensen, S. A. Meldgaard, M. K. Bisbo, M.-P. V.\nChristiansen, and B. Hammer, Physical Review B 102,\n075427 (2020).\n[41] E. Lundgren, G. Kresse, C. Klein, M. Borg, J. N. Ander-\nsen, M. De Santis, Y. Gauthier, C. Konvicka, M. Schmid,\nand P. Varga, Phys. Rev. Lett. 88, 246103 (2002).\n[42] M. Yu, H. Ascolani, G. Zampieri, D. P. Woodruff, C. J.\nSatterley, R. G. Jones,\nand V. R. Dhanak, J. Phys.\nChem. C 111, 10904 (2007).\n[43] G. Pruthi, F. Liu, S. Kale, and M. Sundararajan, in Ad-\nvances in Neural Information Processing Systems, Vol. 33\n(Curran Associates, Inc., 2020) pp. 19920–19930.\n[44] A. Hjorth Larsen, J. Jørgen Mortensen, J. Blomqvist,\nI. E. Castelli,\nR. Christensen,\nM. Du lak,\nJ. Friis,\nM. N. Groves, B. Hammer, C. Hargus, E. D. Hermes,\nP. C. Jennings, P. Bjerre Jensen, J. Kermode, J. R.\nKitchin, E. Leonhard Kolsbjerg, J. Kubal, K. Kaasb-\njerg, S. Lysgaard, J. Bergmann Maronsson, T. Max-\nson, T. Olsen, L. Pastewka, A. Peterson, C. Rostgaard,\nJ. Schiøtz, O. Sch¨utt, M. Strange, K. S. Thygesen,\nT. Vegge, L. Vilhelmsen, M. Walter, Z. Zeng, and K. W.\nJacobsen, J. Phys. Condens. Matter 29, 273002 (2017).\n[45] J. J. Mortensen, A. H. Larsen, M. Kuisma, A. V. Ivanov,\nA. Taghizadeh, A. Peterson, A. Haldar, A. O. Dohn,\nC. Sch¨afer, E. ¨O. J´onsson, E. D. Hermes, F. A. Nils-\nson, G. Kastlunger, G. Levi, H. J´onsson, H. H¨akkinen,\nJ. Fojt, J. Kangsabanik, J. Sødequist, J. Lehtom¨aki,\nJ. Heske, J. Enkovaara, K. T. Winther, M. Dulak,\nM. M. Melander, M. Ovesen, M. Louhivuori, M. Walter,\nM. Gjerding, O. Lopez-Acevedo, P. Erhart, R. Warm-\nbier, R. W¨urdemann, S. Kaappa, S. Latini, T. M. Boland,\nT. Bligaard, T. Skovhus, T. Susi, T. Maxson, T. Rossi,\nX. Chen, Y. L. A. Schmerwitz, J. Schiøtz, T. Olsen,\nK. W. Jacobsen,\nand K. S. Thygesen, J. Chem. Phys.\n160, 092503 (2024).\nAppendix A: Appendix: DFT Settings\nAll DFT calculations have been performed with the\nGPAW code [44, 45]. For copper-oxide a plane-wave cut-\noff of 400 eV was used, a (4×4) Monkhorst-Pack k-point\ngrid, with the PBE exchange-correlation functional. For\nthe metal-sulfides we employed a cutoff of 520 eV and\nMonkhorst Pack grids with a density of 3.5 points/˚A−1\nagain with the PBE functional.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21179v1.pdf",
    "total_pages": 10,
    "title": "$Δ$-model correction of Foundation Model based on the models own understanding",
    "authors": [
      "Mads-Peter Verner Christiansen",
      "Bjørk Hammer"
    ],
    "abstract": "Foundation models of interatomic potentials, so called universal potentials,\nmay require fine-tuning or residual corrections when applied to specific\nsubclasses of materials. In the present work, we demonstrate how such\naugmentation can be accomplished via $\\Delta$-learning based on the\nrepresentation already embedded in the universal potentials. The $\\Delta$-model\nintroduced is a Gaussian Process Regression (GPR) model and various types of\naggregation (global, species-separated, and atomic) of the representation\nvector are discussed. Employing a specific universal potential, CHGNet [Deng et\nal., Nat. Mach. Intell. 5, 1031 (2023)], in a global structure optimization\nsetting, we find that it correctly describes the energetics of the \"8\" Cu\noxide, which is an ultra-thin oxide film on Cu(111). The universal potential\nmodel even predicts a more favorable structure compared to that discussed in\nrecent DFT-based literature. Moving to sulfur adatom overlayers on Cu(111),\nAg(111), and Au(111) the CHGNet model, however, requires corrections. We\ndemonstrate that these are efficiently provided via the GPR-based\n$\\Delta$-model formulated on the CHGNet's own internal atomic embedding\nrepresentation. The need for corrections is tracked to the scarcity of\nmetal-sulfur atomic environments in the materials project database that CHGNet\nis trained on leading to an overreliance on sulfur-sulfur atomic environments.\nOther universal potentials trained on the same data, MACE-MP0, SevenNet-0, and\nORB-v2-only-MPtrj show similar behavior, but with varying degrees of error,\ndemonstrating the general need for augmentation schemes for universal potential\nmodels.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}