{
  "id": "arxiv_2502.21193v1",
  "text": "Towards High-performance Spiking Transformers from ANN to\nSNN Conversion\nZihan Huang\nPeking University\nBeijing, China\nhzh@stu.pku.edu.cn\nXinyu Shi\nPeking University\nBeijing, China\nxyshi@pku.edu.cn\nZecheng Hao\nPeking University\nBeijing, China\n1900012989@pku.edu.cn\nTong Bu\nPeking University\nBeijing, China\nputong30@pku.edu.cn\nJianhao Ding\nPeking University\nBeijing, China\ndjh01998@stu.pku.edu.cn\nZhaofei Yu∗\nPeking University\nBeijing, China\nyuzf12@pku.edu.cn\nTiejun Huang\nPeking University\nBeijing, China\ntjhuang@pku.edu.cn\nAbstract\nSpiking neural networks (SNNs) show great potential due to their\nenergy efficiency, fast processing capabilities, and robustness. There\nare two main approaches to constructing SNNs. Direct training\nmethods require much memory, while conversion methods offer\na simpler and more efficient option. However, current conversion\nmethods mainly focus on converting convolutional neural networks\n(CNNs) to SNNs. Converting Transformers to SNN is challenging\nbecause of the presence of non-linear modules. In this paper, we\npropose an Expectation Compensation Module to preserve the\naccuracy of the conversion. The core idea is to use information\nfrom the previous T time-steps to calculate the expected output\nat time-step T. We also propose a Multi-Threshold Neuron and\nthe corresponding Parallel Parameter normalization to address the\nchallenge of large time steps needed for high accuracy, aiming to\nreduce network latency and power consumption. Our experimental\nresults demonstrate that our approach achieves state-of-the-art\nperformance. For example, we achieve a top-1 accuracy of 88.60%\nwith only a 1% loss in accuracy using 4 time steps while consuming\nonly 35% of the original power of the Transformer. To our knowl-\nedge, this is the first successful Artificial Neural Network (ANN)\nto SNN conversion for Spiking Transformers that achieves high\naccuracy, low latency, and low power consumption on complex\ndatasets. The source codes of the proposed method are available at\nhttps://github.com/h-z-h-cell/Transformer-to-SNN-ECMT.\n∗Corresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nMM ’24, October 28-November 1, 2024, Melbourne, VIC, Australia\n© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0686-8/24/10\nhttps://doi.org/10.1145/3664647.3680620\nCCS Concepts\n• Computing methodologies →Artificial intelligence.\nKeywords\nSpiking Neural Networks, Spiking Transformer, ANN-SNN Con-\nversion, Expectation Compensation, Multi-Threshold Neurons\nACM Reference Format:\nZihan Huang, Xinyu Shi, Zecheng Hao, Tong Bu, Jianhao Ding, Zhaofei Yu,\nand Tiejun Huang. 2024. Towards High-performance Spiking Transformers\nfrom ANN to SNN Conversion. In Proceedings of the 32nd ACM International\nConference on Multimedia (MM ’24), October 28-November 1, 2024, Melbourne,\nVIC, Australia. ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/\n3664647.3680620\n1\nIntroduction\nSpiking neural networks(SNNs) are a type of neural network model\nthat imitates the mechanisms of biological neurons[1, 18]. They\nare called the third generation of neural networks [34] due to their\nbiological plausibility and computational efficiency[46, 57]. Neu-\nrons in SNNs do not produce output at each time step. Instead, they\nbecome active and emit spikes only when their membrane potential\nreaches a specific threshold. The sparse activity of spikes leads to\nsignificantly higher computational efficiency than traditional neural\nnetworks [40], especially on neuromorphic chips [6, 7, 35, 37]. How-\never, training large-scale, high-precision, and low-latency SNNs\nremains challenging due to the non-differentiable nature of spikes.\nCurrently, there are two main approaches to train SNNs. The\nfirst approach is direct training using backpropagation or local\nlearning[13, 15, 28, 36, 52–54, 56, 61, 65]. These methods utilize dif-\nferentiable continuous functions or spike-time-dependent plasticity\nstrategies to replace the non-differentiable spike emission rules.\nHowever, this training process still relies on standard GPUs that\nare not well-suited for the unique characteristics of SNNs, leading\nto significant resource consumption and limited performance. The\nsecond approach is ANN to SNN conversion [3, 4, 9, 30, 41]. This\nconversion method does not require any additional training. Instead,\nit uses pre-trained ANNs and replaces activation functions with\narXiv:2502.21193v1  [cs.CV]  28 Feb 2025\n\n\nMM ’24, October 28-November 1, 2024, Melbourne, VIC, Australia\nZihan Huang et al.\nspiking neurons, leveraging the similarity between ReLU activation\nfunctions and spike emission rates of integrate-and-fire models.\nThe result SNN model preserves the original ANN’s performance\nbut often leads to longer inference times, and the modules that can\nbe successfully converted are limited.\nAs is well known, Transformers have demonstrated exceptional\nperformance in various vision tasks [5, 12, 24, 33, 38]. Despite nu-\nmerous efforts to convert CNNs to SNNs, converting Transformer\nmodels remains a challenge. This is due to unique nonlinear mod-\nules such as layernorm and GELU in Transformers that differ from\nthe ReLU function in CNNs. These modules require interaction\nbetween neurons within the same layer and exhibit non-linear char-\nacteristics, making it challenging to achieve accurate conversion\nthrough the linear piecewise quantization of individual neurons.\nThis paper proposes a new method to convert Transformers to\nSNNs. The main challenge lies in handling non-linear modules. To\naddress this, we propose an Expectation Compensation Module\n(ECM) that calculates expectations and replaces each non-linear\nmodule. Specifically, a customized ECM is employed to substitute\nthe matrix product, performing most operations through accumu-\nlations. This reduces power consumption and ensures the total\noutput matches the expected result at each time step. To improve\nthe efficiency of minimal spikes, we introduce Multi-Threshold\nNeurons and the corresponding Parallel Parameter normalization,\nsignificantly reducing the required latency and power consumption\nfor inference with comparable accuracy.\nOur main contributions are summarized as follows:\n• We analyze the challenges of non-linear module conversion\nin Transformer and introduce a novel solution called the\nExpectation Compensation Module, which uses the informa-\ntion from the previous time steps to calculate the expected\noutput at the current time step. This module overcomes the\nlimitations of traditional methods with minimal power con-\nsumption increase.\n• To overcome the issue of slow accuracy improvement over\ntime during Transformer conversion, we propose a Multi-\nThreshold Neuron and the corresponding Parallel Parameter\nnormalization, substantially reducing power consumption\nrequirements and significantly decreasing latency.\n• The proposed method is effective on the ImageNet1k dataset,\noutperforming existing SNN models in accuracy and sig-\nnificantly reducing power consumption compared to other\nTransformer models. It achieves a top-1 accuracy of 88.60%,\nwith only a 1% accuracy loss compared to ANN, while reduc-\ning energy consumption by 65%.\n2\nRelated Works\nANN-SNN Conversion\nThe ANN-SNN conversion methods aim to replicate the perfor-\nmance of ANNs by converting pre-trained ANN weights into synap-\ntic weights of SNNs. Cao et al. [4] initially proposed training an\nANN with ReLU activation function and then replacing the activa-\ntion layer with IF neurons. Diehl et al. [10] further narrowed the\ngap by scaling and normalizing the weights. To address the spike\ncount errors resulting from the hard reset mechanism, soft reset\nneurons were proposed in Rueckauer et al. [41] and Han et al. [19].\nFurther research has aimed to minimize conversion errors through\nvarious optimization: (1) Optimizing thresholds: Sengupta et al. [43]\nand Zhang et al. [58] proposed dynamic threshold adjustment strate-\ngies during the conversion process. (2) Optimizing membrane po-\ntential: Bu et al. [2] demonstrated that setting the initial membrane\npotential at half the threshold can reduce errors. Hao et al. [21]\nfurther suggested analyzing residual membrane potential to elim-\ninate conversion errors. (3) Optimizing the pre-conversion ANN\nstructure: Esser et al. [14] suggested training ANNs with quan-\ntized activation values. Ho and Chang [22] introduced a trainable\nclipping layer (TCL) for threshold determination. Ding et al. [11]\nproposed a rate norm layer, while others [3, 20, 25, 48] suggested\nvarious activation functions to replace ReLU. (4) Optimizing spik-\ning neuronal models. Li et al. [32] introduced a neuron model for\nburst spikes. Wang et al. [50] proposed a memory-enhanced signed\nneuron. Li et al. [29] suggested incorporating negative spikes and\nextending simulation time to improve accuracy with minimal cost.\nPrevious methods for converting CNNs to SNNs were limited\nby CNN performance. Jiang et al.[26] introduced Universal Group\nOperators and a Temporal-Corrective Self-Attention Layer to ap-\nproximate original Transformers but faced long inference latency\nand accuracy gaps with the ANN.\nIn contrast, this paper presents a new method for converting\nTransformers to SNNs, achieving high accuracy and low latency\nwhile reducing network energy consumption.\nDirectly Trained Transformer in ANNs and SNNs\nThe Transformer architecture has performed well in the ANN and\nSNN domains. Initially, Transformers gained prominence in the\nANNs with their self-attention mechanisms as proposed by Vaswani\net al.[47]. Dosovitskiy et al. [12] then introduced the Vision Trans-\nformer (ViT), which divided images into fixed-size patches as token\ninputs, achieving significant success in computer vision. Fang et\nal. [16, 17] and Sun et al. [45] further expanded ViT models to one\nbillion parameters, pushing the limits of large-scale visual models.\nIn the SNN domain, spike-based Transformers quickly emerged,\nincorporating spike self-attention mechanisms with some floating-\npoint calculations [31, 64]. Subsequently, Zhou et al. [63], Yao et\nal. [55] and Shi et al.[44] introduced fully event-based Transform-\ners. Wang et al.[51] first trained a modified Transformer and then\nconverted it into a Spiking Transformer. Spike-based Transformers\nhave successfully applied to applications, such as monocular depth\nestimation [60], single-object tracking with event cameras [59], and\nautomatic speech recognition [49].\nIn contrast to previous methods that training Transformers from\nscratch, this paper focuses on converting pre-trained Transformers\ninto SNNs to reduce energy while maintaining performance.\n3\nPreliminaries\nIn this section, we first detail the theoretical basis of the conversion\nprocess from ANNs to SNNs. Then, we introduce the Vision Trans-\nformer (ViT), the ANN architecture we selected for conversion.\n3.1\nANN-SNN conversion theory\n3.1.1\nNeurons in ANNs. In ANNs, for linear or convolution layers\nin CNNs using the ReLU activation, the output 𝒂𝑙of neurons in\n\n\nTowards High-performance Spiking Transformers from ANN to SNN Conversion\nMM ’24, October 28-November 1, 2024, Melbourne, VIC, Australia\nlayer 𝑙can be formulated as:\n𝒂𝑙= ReLU(𝑾𝑙𝒂𝑙−1) = max(𝑾𝑙𝒂𝑙−1, 0),\n(1)\nwhere 𝑾𝑙represents the weights of the linear transformation or\nconvolution in this layer.\n3.1.2\nIntegrate-and-Fire Neurons in SNNs. For Integrate-and-Fire\n(IF) neurons in SNNs, let 𝒎𝑙(𝑡) and 𝒗𝑙(𝑡) denote the membrane\npotential of neurons in the 𝑙-th layer before and after firing spikes\nat time-step 𝑡, the neural dynamic can be formulated as follows:\n𝒎𝑙(𝑡) = 𝒗𝑙(𝑡−1) + 𝑾𝑙𝒙𝑙−1(𝑡),\n(2)\n𝒔𝑙(𝑡) = 𝐻(𝒎𝑙(𝑡) −𝜃𝑙),\n(3)\n𝒙𝑙(𝑡) = 𝜃𝑙𝒔𝑙(𝑡),\n(4)\n𝒗𝑙(𝑡) = 𝒎𝑙(𝑡) −𝒙𝑙(𝑡).\n(5)\nwhere 𝐻is the Heaviside step function and 𝜃𝑙is the neuron thresh-\nold in layer 𝑙. 𝒔𝑙(𝑡) is the output spike of layer 𝑙. 𝒙𝑙(𝑡) is the post-\nsynaptic potential and theoretical output of layer 𝑙, which equals\n𝜃𝑙if the neuron fires and 0 otherwise. Following [41] and [19], we\nuse the \"reset-by-subtraction\" mechanism, where 𝒗𝑙(𝑡) decreases\nby a value of 𝜃𝑙if the neuron fires.\n3.1.3\nANN-SNN Conversion. Combining Equations (2)-(5), we get\n𝒗𝑙(𝑡) −𝒗𝑙(𝑡−1) = 𝑾𝑙𝒙𝑙−1(𝑡) −𝒙𝑙(𝑡).\n(6)\nSumming from time-step 1 to time-step 𝑇, we have\n𝒗𝑙(𝑇) −𝒗𝑙(0)\n𝑇\n= 𝑾𝑙Í𝑇\n𝑖=1 𝒙𝑙−1(𝑖)\n𝑇\n−\nÍ𝑇\n𝑖=1 𝒙𝑙(𝑖)\n𝑇\n.\n(7)\nLetting Φ𝑙(𝑇) =\nÍ𝑇\n𝑖=1 𝒙𝑙(𝑖)\n𝑇\n, we have\nΦ𝑙(𝑇) = 𝑾𝑙Φ𝑙−1(𝑇) −𝒗𝑙(𝑇) −𝒗𝑙(0)\n𝑇\n.\n(8)\nComparing Equations (1) and (8), 𝒗𝑙(𝑇)−𝒗𝑙(0)\n𝑇\ntends to 0 as 𝑇be-\ncomes large. This allows Φ𝑙(𝑇) in SNNs to approximate 𝒂𝑙in ANNs.\n3.1.4\nParameter normalization. Due to the spike-based communi-\ncation in SNNs, approximation errors arise since SNN neurons can\nemit only one spike per time step, limited to a firing rate in the\nrange of [0,𝑟max], where ANNs do not have such constraints. To\nprevent approximation errors from excessively low or high firing\nrates, Diehl et al.[10] and Rueckauer et al.[41] introduced weight\nnormalization to rescale parameters using the following equations:\n𝑊𝑙\nSNN = 𝑊𝑙\nANN\n𝜆𝑙−1\n𝜆𝑙.\n(9)\nwhere 𝜆𝑙is determined by the 𝑝-th percentile of the total activity\ndistribution of layer 𝑙. Modifying Equation (9) and setting 𝜃𝑙\n𝑗to 1 is\nequivalent to adjusting the firing threshold on the soft-reset neuron\nto 𝜆𝑙[2]. This adjustment ensures that the output 𝒙𝑙(𝑡) is a spike\nmatrix equal to 𝒔𝑙(𝑡) and suits the operational dynamics of SNNs.\n3.2\nVision Transformer\nVision Transformer (ViT) architecture consists of three core compo-\nnents: Embeddings, Transformer Encoder, and Classification Head.\n3.2.1\nEmbeddings. The process starts by segmenting an image\ninto patches of specific dimensions, viewing them as a sequence\nof tokens. Each patch undergoes linear embedding with added\npositional embeddings, enriching the output token vectors with\nthe patch’s content and location within the image.\n3.2.2\nTransformer Encoder. Central to feature extraction, the Trans-\nformer Encoder plays a crucial role in various visual tasks. It is\ndivided into two primary segments:\n(1) Self-Attention Mechanism. This mechanism calculates a weighted\nsum of all the values 𝑉in a given sequence. The attention weights\nare determined based on the similarity between a query 𝑄and a\nkey 𝐾. The values 𝑄, 𝐾, and 𝑉are obtained through the input 𝑋\nusing weight matrices 𝑊𝑄, 𝑊𝐾, and 𝑊𝑉respectively. The follow-\ning equation describes the matrix form of the output calculation\nfor the self-attention mechanism:\n𝑂= Softmax\n\u0012𝑄𝑇𝐾\n√\n𝑑\n𝑉\n\u0013\n= Softmax\n \n(𝑊𝑄𝑋)𝑇𝑊𝐾𝑋\n√\n𝑑\n𝑊𝑉𝑋\n!\n. (10)\nwhere 𝑑is the dimension of the key and query vectors.\n(2) Feed-Forward Network. Here, the input vector passes through\ntwo linear layers and is activated by the GELU function between\nthem.\n3.2.3\nClassification Head. Features related to the CLS token are\ndirected toward the classification head, which then computes the\nprobabilities for the various classes.\n4\nMethod\nIn this section, we first analyze the main errors encountered in\nANN-SNN conversion. Following this, we propose the Expectation\nCompensation Module (EC) to preserve the accuracy of non-linear\nmodules. In particular, we detailed a lossless conversion method for\nthe matrix product layer, mainly using additional operations. Addi-\ntionally, a Multi-Threshold Neuron (MT) is designed to improve the\nefficiency of minimal spikes, which significantly reduces network\nlatency and energy consumption. The diagram shown in Figure 1\nprovides an overview of the architecture we utilized.\n4.1\nError Analysis of Nonlinear Module in\nANN-SNN Conversion\nExisting ANN-SNN conversion methods mainly focus on CNNs,\nwhich typically employ linear operations, such as linear transfor-\nmations and convolutions, combined with ReLU activation, as for-\nmulated in Equation (1). However, Transformer architecture uses\nmany non-linear operations, such as GELU, softmax, layernorm, and\nmatrix product, which cannot be directly formulated using Equa-\ntion (1). Consequently, the current conversion theory discussed\nin Section 3.1 does not apply to Transformers, which can lead to\nconversion errors.\nTo be specific, we assume that the outputs of layer 𝑙−1 in\nboth ANNs and SNNs are identical, denoted as 𝒂𝑙−1 = Φ𝑙−1(𝑇) =\nÍ𝑇\n𝑡=1 𝒙𝑙−1(𝑡)\n𝑇\n, and we will compare the outputs 𝒂𝑙and Φ𝑙in layer 𝑙.\nConsidering an arbitrary non-linear module in layer 𝑙of an ANN,\nits function can be formulated as:\n𝒂𝑙= 𝐹(𝒂𝑙−1),\n(11)\n\n\nMM ’24, October 28-November 1, 2024, Melbourne, VIC, Australia\nZihan Huang et al.\nClassification Head\nInput\nTransformer Encoder\nH W C\nEmbeddings\nPatch\nEmbedding\nCLS\nPosition\nEmbedding\n L\nAttention\nMLP\nLayerNorm-EC\nLayerNorm-EC\nLinear\nMT-N\nLinear\nMT-N\nLinear\nMT-N\nMT-N\nMT-N\nMatrix Product-EC\nSoftmax-EC\nMT-N\nMatrix Product-EC\nMT-N\nLinear\nMT-N\nAttention\nMLP\nLinear\nMT-N\nGELU-EC\nLinear\nMT-N\nFigure 1: An overview of the proposed architecture, including\nthe whole architecture, Attention, and MLP module.\nwhere 𝐹is the function of this layer. Obviously, it cannot be ex-\npressed equivalently using Equation (1). In this case, if we do not\nintroduce a further conversion method for this non-linear mod-\nule, the actual output of the SNN counterpart at time 𝑡will be\n𝒙𝑙(𝑡) = 𝐹(𝒙𝑙−1(𝑡)). The average output can be formulated as fol-\nlows:\nΦ𝑙(𝑇) =\nÍ𝑇\n𝑡=1 𝒙𝑙(𝑡)\n𝑇\n=\nÍ𝑇\n𝑡=1 𝐹(𝒙𝑙−1(𝑡))\n𝑇\n.\n(12)\nHowever, in the case of ANNs, the expected average output can\nbe formulated as:\n𝒂𝑙= 𝐹(𝒂𝑙−1) = 𝐹\n Í𝑇\n𝑡=1 𝒙𝑙−1(𝑡)\n𝑇\n!\n.\n(13)\nDue to the non-linear nature of the module, we have:\nÍ𝑇\n𝑡=1 𝐹(𝒙𝑙−1(𝑡))\n𝑇\n≠𝐹\n Í𝑇\n𝑡=1 𝒙𝑙−1(𝑡)\n𝑇\n!\n.\n(14)\nThis implies that the output Φ𝑙(𝑇) of SNNs in Equation (12) is\nnot equivalent to the output 𝒂𝑙of ANNs in Equation (13), posing\nchallenges for non-linear conversion.\nExpectation Compensation (EC)\nMatrix Product-EC\nFigure 2: The upper diagram shows the general Expectation\nCompensation module(EC). The lower diagram shows the Ex-\npectation Compensation module for Matrix Product(Matrix\nProduct-EC).\n4.2\nExpectation Compensation Module\nTo overcome the challenge of converting non-linear layers, we\npropose using Expectation Compensation Modules to preserve non-\nlinearity throughout the conversion process by leveraging prior\ninformation to compute expectations.\n4.2.1\nGeneral Expectation Compensation Module.\nThe theorem below calculates the expected output of the arbi-\ntrary non-linear layer at each time step in SNNs.\nTheorem 4.1. Consider a non-linear layer 𝑙with a function 𝐹.\nIn SNNs, the output of this layer at time 𝑡is denoted as 𝑶𝑙(𝑡). Let\n𝑺𝑙(𝑇) be the cumulative sum of layer 𝑙outputs up to time 𝑇, given\nby 𝑺𝑙(𝑇) = Í𝑇\n𝑡=1 𝑶𝑙(𝑡). The expected output of the SNNs at time 𝑇is\ngiven by:\n𝑶𝑙(𝑇) = 𝑇𝐹\n \n𝑺𝑙−1(𝑇)\n𝑇\n!\n−(𝑇−1)𝐹\n \n𝑺𝑙−1(𝑇−1)\n𝑇−1\n!\n.\n(15)\nThe detailed proof is provided in the supplementary materials.\nTheorem 7.1 indicates that lossless conversion can be achieved\nby an accumulator to records 𝑺𝑙−1(𝑇) and an optional variable to\nrecords 𝑇𝐹\n\u0010\n𝑺𝑙−1(𝑇)/𝑇\n\u0011\nas shown in Figure 2.\n4.2.2\nExpectation Compensation Module for Matrix Product.\nFor the matrix product layer, we can convert it into a specialized\nmodule that primarily uses additional operations to achieve loss-\nless conversion. The theorem below outlines how to calculate the\nexpected output of the matrix product layer at each time step in\nSNNs.\n\n\nTowards High-performance Spiking Transformers from ANN to SNN Conversion\nMM ’24, October 28-November 1, 2024, Melbourne, VIC, Australia\nMTH\n...\n...\nSynapse\nDendrite\nSoma\nAxon\nFigure 3: Diagram of MT neuron. MT neuron receives input\nfrom nonlinear/linear modules and emits up to one spike.\nTheorem 4.2. Consider a module for matrix product that receives\ntwo sets of spike inputs, denoted by 𝑨𝑣𝑎(𝑡) and 𝑩𝑣𝑏(𝑡). These inputs\nare generated by neurons 𝐴and 𝐵, respectively, and are characterized\nby multiple thresholds 𝑣𝑎and 𝑣𝑏, as described in Section 4.3.\nWe can integrate the input by 𝑨(𝑡) = Í\n𝑣𝑎𝑣𝑎𝑨𝑣𝑎(𝑡) and 𝑩(𝑡) =\nÍ\n𝑣𝑏𝑣𝑏𝑩𝑣𝑏(𝑡). Here, 𝑨(𝑡) and 𝑩(𝑡) are the sum matrices weighted\nby multiple thresholds 𝑣𝑎and 𝑣𝑏, respectively.\nLet 𝑺𝐴(𝑇) = Í𝑇\n𝑡=1 𝐴(𝑡) and 𝑺𝐵(𝑇) = Í𝑇\n𝑡=1 𝐵(𝑡) represent the cu-\nmulative sum of inputs up to time𝑇. We define 𝑺𝐾(𝑇) = 𝑺𝐴(𝑇)𝑺𝐵(𝑇).\nThen, the expected output at time T can be formulated as:\n𝑶(𝑇) = 1\n𝑇𝑺𝐾(𝑇) −\n1\n𝑇−1𝑺𝐾(𝑇−1),\n(16)\nwhere 𝑺𝐾(𝑇) can be calculated mainly using addition, as described\nby the following equation:\n𝑺𝐾(𝑇) = 𝑺𝐾(𝑇−1) + 𝑲(𝑇)\n(17)\n𝑲(𝑇) =\n∑︁\n𝑣𝑎,𝑣𝑏\n𝑣𝑎𝑣𝑏𝑨𝑣𝑎(𝑇)𝑩𝑣𝑏(𝑇) +\n∑︁\n𝑣𝑎\n𝑣𝑎𝑨𝑣𝑎(𝑇)𝑺𝐵(𝑇−1)\n+\n∑︁\n𝑣𝑏\n𝑣𝑏𝑺𝐴(𝑇−1)𝑩𝑣𝑏(𝑇).\n(18)\nThe detailed proof is provided in the supplementary materials.\nAccording to Theorem 8.1, the output 𝑶(𝑇) can be obtained through\nthe process illustrated in Figure 2. The main power consumption in\nthis process occurs during the matrix product calculation of 𝑲(𝑇)\nusing spike matrices, which can be implemented through accumula-\ntions. Since each position of the input matrix has only one effective\nthreshold at each time, it limits the total number of input spikes,\nthereby restricting the total number of operations. Combined with\nthe sparsity of spikes, this reduces power consumption at each time\nstep while achieving lossless conversion.\n4.3\nMulti-Threshold Neuron\n4.3.1\nProblem of Consumption and Latency.\nIf we only use the Expectation Compensation Module, neuron\ncommunication will remain in a floating-point format. As discussed\nin Section 5.5, most of the network’s power consumption occurs\nin the linear and matrix product layers. To reduce the network’s\nenergy consumption, we introduce spiking neurons before each\nlinear layer and matrix product layer. Thus, we can significantly\nreduce the network’s power consumption by adopting spiking com-\nmunication.\nHowever, if we only use one threshold, no matter how set, it will\nresult in excessively high firing rates or high inference latency. The\nfindings in Section 5.4 demonstrate the importance of having large\nand small thresholds in the Transformer.\n4.3.2\nThe Proposed Solution: Multi-Threshold Neuron.\nTo tackle the challenges of high power consumption and latency,\nwe propose a Multi-Threshold Neuron (MT neuron).\nThis neuron model has additional thresholds built upon the base\nthreshold, allowing it to process more information in a single time\nstep. The MT neuron is characterized by parameters including\nthe positive and negative base thresholds, represented as 𝜃1 and\n−𝜃2, respectively, and the number of thresholds denoted as 2𝑛.\nWe can refer to 𝜆𝑙𝑝as the 𝑝-th threshold value of the MT neuron\ncorresponding to index 𝑝.\n𝜆𝑙\n1 = 𝜃𝑙\n1, 𝜆𝑙\n2 = 2𝜃𝑙\n1, ..., 𝜆𝑙\n𝑛= 2𝑛−1𝜃𝑙\n1,\n𝜆𝑙\n𝑛+1 = −𝜃𝑙\n2, 𝜆𝑙\n𝑛+2 = −2𝜃𝑙\n2, ..., 𝜆𝑙\n2𝑛= −2𝑛−1𝜃𝑙\n2,\n(19)\nAs shown in Figure 3, the dynamic of MT neurons is described by:\n𝐼𝑙\n𝑗(𝑡) = 𝐹𝑙\n𝑗(𝒔𝑙−1\n,1\n(𝑡), ..., 𝒔𝑙−1\n,2𝑛(𝑡)),\n(20)\n𝑚𝑙\n𝑗(𝑡) = 𝑣𝑙\n𝑗(𝑡−1) + 𝐼𝑙\n𝑗(𝑡),\n(21)\n𝑠𝑙\n𝑗,𝑝(𝑡) = 𝑀𝑇𝐻𝜃1,𝜃2,𝑛(𝑚𝑙\n𝑗(𝑡))\n(22)\n𝑥𝑙\n𝑗(𝑡) =\n∑︁\n𝑝\n𝑠𝑙\n𝑗,𝑝(𝑡)𝜆𝑙\n𝑝,\n(23)\n𝑣𝑙\n𝑗(𝑡) = 𝑚𝑙\n𝑗(𝑡) −𝑥𝑙\n𝑗(𝑡).\n(24)\nThe variables 𝐼𝑙\n𝑗(𝑡),𝑠𝑙\n𝑗(𝑡),𝑥𝑙\n𝑗(𝑡), 𝑚𝑙\n𝑗(𝑡) and 𝑣𝑙\n𝑗(𝑡) respectively repre-\nsent the input, output, postsynaptic potential, and the membrane\npotential before and after spikes of the 𝑗-th neuron in the 𝑙-th layer\nat time 𝑡. Meanwhile, 𝐹is a linear or nonlinear function of this layer.\nThe function 𝑀𝑇𝐻𝜃1,𝜃2,𝑛(𝑥) can be described using the following\npiecewise function:\n𝑀𝑇𝐻𝜃1,𝜃2,𝑛(𝑥) :\n\n\n𝜆𝑙𝑛−𝜆𝑙\n1\n2 ≤𝑥:\n𝑠𝑙\n𝑗,𝑛(𝑡) = 1,\n𝜆𝑙\n𝑛−1 −𝜆𝑙\n1\n2 ≤𝑥< 𝜆𝑙𝑛−𝜆𝑙\n1\n2 :\n𝑠𝑙\n𝑗,𝑛−1(𝑡) = 1,\n...\n...\n𝜆𝑙\n1\n2 ≤𝑥< 𝜆𝑙\n2 −𝜆𝑙\n1\n2 :\n𝑠𝑙\n𝑗,1(𝑡) = 1,\n𝜆𝑙\n𝑛+1\n2\n≤𝑥< 𝜆𝑙\n1\n2 :\n𝑎𝑙𝑙𝑡ℎ𝑒𝑠𝑙\n𝑗,𝑝(𝑡) = 0,\n𝜆𝑙\n𝑛+2 −𝜆𝑙\n𝑛+1\n2\n≤𝑥< 𝜆𝑙\n𝑛+1\n2\n:\n𝑠𝑙\n𝑗,𝑛+1(𝑡) = 1,\n...\n...\n𝜆𝑙\n2𝑛−𝜆𝑙\n𝑛+1\n2\n≤𝑥< 𝜆𝑙\n2𝑛−1 −𝜆𝑙\n𝑛+1\n2\n:𝑠𝑙\n𝑗,2𝑛−1(𝑡) = 1,\n𝑥< 𝜆𝑙\n2𝑛−𝜆𝑙\n𝑛+1\n2\n:\n𝑠𝑙\n𝑗,2𝑛(𝑡) = 1.\n(25)\nThe results of experiments presented in Section 5.4 indicate that\nalthough this neuron has multiple thresholds, most of the spikes it\ngenerated are concentrated in 𝜃1 and −𝜃2. The spikes generated by\nother thresholds are minimal, which reduces energy consumption\nand inference latency.\n4.3.3\nParallel Parameter normalization for MT Neuron.\nSpike neurons communicate with each other by producing an\noutput spike of either 0 or 1. As for function 𝐹in Figure 3.\nIf 𝐹is a Matrix Product-EC function, we only need to send spikes\n𝑠𝑙(𝑡) to 𝐹as 𝑨𝑣𝑎(𝑡) or 𝑩𝑣𝑏(𝑡).\n\n\nMM ’24, October 28-November 1, 2024, Melbourne, VIC, Australia\nZihan Huang et al.\nNeuron \nNeuron \n...\n...\nLayer \nMT Neuron \nLayer \nLayer \nLayer \nMT Neuron \nFigure 4: Left: Original connection in ANN. Right: Parallel\nParameter normalization of MT neuron in SNN. The MT\nNeuron extends one connection to 2𝑛channels. At each time,\nonly one of the 2𝑛channels can emit a spike.\nIf 𝐹is a general nonlinear EC function, we will integrate spike\noutput by 𝐼𝑙\n𝑗(𝑡) = 𝐹𝑙\n𝑗(Í\n𝑝𝒔𝑙−1\n,𝑝(𝑡)𝜆𝑙−1\n𝑝\n).\nIf 𝐹is a linear function, 𝐼𝑙\n𝑗(𝑡) can be expressed by\n𝐼𝑙\n𝑗(𝑡) =\n∑︁\n𝑖\n𝑤𝑙\n𝑖𝑗ANN𝑥𝑙−1\n𝑖\n(𝑡) =\n∑︁\n𝑖\n𝑤𝑙\n𝑖𝑗ANN\n∑︁\n𝑝\n𝑠𝑙−1\n𝑖,𝑝(𝑡)𝜆𝑙−1\n𝑝\n(26)\nA parallel parameter normalization method is proposed to support\nspike communication between MT neurons in a linear layer. This\nmethod extends the ANN weight to 2n weights in the SNN corre-\nsponding to 2n thresholds of MT neurons, as shown in Figure 4.\nWe update these weights using the following formula:\n𝑊𝑙\nSNN,𝑝= 𝑊𝑙\nANN\n𝜆𝑙−1\n𝑝\n𝜆𝑙\n1\n(27)\nHere, we divide an extra variable 𝜆𝑙\n1 to equilibrate parameter size.\nLet’s set 𝜂𝑙= 𝜃𝑙\n2\n𝜃𝑙\n1 . This brings the neuron to an equivalent form,\nwhich is as follows:\n𝐼𝑙\n𝑗(𝑡) =\n∑︁\n𝑖,𝑝\n𝑤𝑙\n𝑖𝑗SNN,p𝑠𝑙−1\n𝑖,𝑝(𝑡)\n(28)\n𝜃1,𝑛𝑒𝑤= 1,𝜃2,𝑛𝑒𝑤= 𝜂\n(29)\nBased on the above discussion, we name this method: Expecta-\ntion Compensation and Multi-Threshold(ECMT). The overall con-\nversion algorithm can be summarized in Algorithm 1. The con-\nversion is a one-time process, allowing the converted model to be\nreused without other computations before use.\n5\nExperimental results\nIn this section, we first evaluate the proposed method’s perfor-\nmance on the ImageNet dataset. Then, we compare our method with\nstate-of-the-art SNN training and ANN-SNN conversion methods.\nAdditionally, we perform ablation experiments on Multi-Threshold\nNeurons. Finally, we analyze the power consumption of the SNNs\nconverted by our method.\n5.1\nExperimental Setup\nWe convert pre-trained Vision Transformer including the ViT-S/16,\nViT-B/16, ViT-L/16 with 224 resolution [47], and the EVA model\n[17] on Imagenet1k dataset [8]. For all Multi-Threshold Neurons,\nwe set 𝑛to 8 for ViT-S/16, ViT-B/16, ViT-L/16 and 6 for EVA. And\nwe set threshold percent 𝑝to 99. A more detailed setup can be found\nin supplementary materials.\nAlgorithm 1 The conversion method using Expectation Compen-\nsation Module and Multi-Threshold Neuron(ECMT)\nInput: Pre-trained Transformer ANN model 𝑓ANN(𝑾); Dataset 𝐷;\nTime-step 𝑇to test dataset; Threshold percent 𝑝.\nOutput: SNN model 𝑓SNN(𝑾, 𝜽1, 𝜽2, 𝒗)\n1: step1: Obtain the base thresholds 𝜽1 and 𝜽2\n2: for length of Dataset 𝑫do\n3:\nSample minibatch data from 𝑫\n4:\nRun the data on 𝑓ANN and static the activation values before\nlinear and matrix product module at 𝑝% and (1−𝑝%), setting\nthem as 𝜽1 and −𝜽2 respectively.\n5: end for\n6: step2: Converted to SNN model\n7: for module 𝑚in 𝑓ANN.Module do\n8:\nif 𝑚is Linear Module then\n9:\nAdd a Multi-Threshold Neuron before 𝑚\n10:\nelse if 𝑚is Matrix Product then\n11:\nreplace 𝑚by two Multi-Threshold Neurons followed by a\nMatrix Product EC Module\n12:\nelse if 𝑚is Other Nonlinear Module then\n13:\nreplace 𝑚by an EC Module\n14:\nend if\n15: end for\n16: Set the base thresholds of MT neurons to corresponding 𝜽1,−𝜽2\nand set the initial membrane potential 𝒗to 0.\n17: 𝑓SNN = Parallel Parameter normalization(𝑓ANN)\n18: return 𝑓SNN\n5.2\nExperimental results on different model\nBased on the provided data, Table 1 compares performance met-\nrics for various architectures. The analysis shows that our SNN\napproach can achieve comparable accuracies to traditional ANNs\nwith few time steps. Notably, there is only a 1% drop in accuracy ob-\nserved relative to their ANN counterparts at T=10 for ViT-S/16, T=8\nfor ViT-B/16, T=6 for ViT-L/16, and as early as T=4 for EVA. This\ntrend highlights the efficiency of our conversion strategy, especially\nwithin the larger models.\nTaking a closer look at the EVA model, our method achieves an\nimpressive 88.60% accuracy at just T=4, with a negligible 1% accu-\nracy degradation while using only 35% of the energy required by the\nequivalent ANN model. These results demonstrate our approach’s\neffectiveness and suggest its potential for significant energy sav-\nings without substantially compromising accuracy, particularly in\ncomplex and larger-scale model architectures.\n5.3\nComparison with the State-of-the-art\nOur experiments on the ImageNet1k dataset have pushed the fron-\ntiers of neural network efficiency and accuracy. Table 2 provides\na compelling narrative of our progress. Our method is unique in\nthat it facilitates the conversion of Transformer models into SNNs,\nand it stands out for its computational frugality and high accuracy\nyield. This marks a significant stride over previous state-of-the-art\nmethodologies.\nFirstly, our method is designed to be more efficient than direct\ntraining approaches. Instead of starting from scratch, we leverage\n\n\nTowards High-performance Spiking Transformers from ANN to SNN Conversion\nMM ’24, October 28-November 1, 2024, Melbourne, VIC, Australia\nTable 1: Accuracy and energy consumption ratio of ECMT(Ours) on ImageNet1k dataset\nArch.\nAccuracy/Energy\nOriginal (ANN)\nOurs (SNN)\nT=1\nT=2\nT=4\nT=6\nT=8\nT=10\nT=12\nViT-S/16\nAcc. (%)\n78.04\n0.17\n10.66\n62.85\n73.22\n76.03\n77.07\n77.41\nEnergy ratio\n1\n0.06\n0.15\n0.37\n0.59\n0.82\n1.03\n1.25\nViT-B/16\nAcc. (%)\n80.77\n0.24\n20.89\n69.98\n77.81\n79.40\n80.12\n80.38\nEnergy ratio\n1\n0.04\n0.12\n0.30\n0.48\n0.66\n0.84\n1.01\nViT-L/16\nAcc. (%)\n84.88\n3.62\n75.38\n83.20\n84.32\n84.60\n84.68\n84.71\nEnergy ratio\n1\n0.04\n0.12\n0.27\n0.43\n0.58\n0.74\n0.89\nEVA\nAcc. (%)\n89.62\n2.49\n84.08\n88.60\n89.23\n89.40\n89.45\n89.51\nEnergy ratio\n1\n0.06\n0.15\n0.35\n0.55\n0.74\n0.93\n1.13\nTable 2: Comparison between the proposed method and previous works on ImageNet1k dataset\nMethod\nType\nArch.\nParam. (M)\nT\nAccuracy (%)\nSpikingformer[63]\nDirect Training\nSpikingformer-4-384-400E\n66.34\n4\n75.85\nSpike-driven Transformer[55]\nDirect Training\nSpiking Transformer-8-768*\n66.34\n4\n77.07\nSpikeformer[31]\nDirect Training\nSpikeformer-7L/3×2×4\n38.75\n4\n78.31\nRMP[19]\nCNN-to-SNN\nVGG-16\n138\n4096\n73.09\nSNM[50]\nCNN-to-SNN\nVGG-16\n138\n64\n71.50\nTS[9]\nCNN-to-SNN\nVGG-16\n138\n64\n70.97\nQFFS[29]\nCNN-to-SNN\nVGG-16\n138\n4(8)\n72.10(74.36)\nQCFS[3]\nCNN-to-SNN\nResNet-34\n21.8\n64\n72.35\nVGG-16\n138\n64\n72.85\nSRP[21]\nCNN-to-SNN\nResNet-34\n21.8\n4(64)\n66.71(68.61)\nVGG-16\n138\n4(64)\n66.46(69.43)\nMST[51]\nTransformer-to-SNN\nSwin-T(BN)\n28.5\n128(512)\n77.88(78.51)\nSTA[26]\nTransformer-to-SNN\nViT-B/32\n86\n32(256)\n78.72(82.79)\nECMT(Ours)\nTransformer-to-SNN\nViT-S/16\n22\n8(10)\n76.03(77.07)\nViT-B/16\n86\n8(10)\n79.40(80.12)\nViT-L/16\n307\n4(8)\n83.20(84.60)\nEVA\n1074\n4(8)\n88.60(89.40)\nlarge pre-trained models to economize on computational efforts\nand achieve higher accuracy levels than traditional methods. This\napproach demonstrates our ability to capitalize on the intrinsic\nefficiencies of pre-trained networks and apply them successfully to\nSNNs.\nSecondly, our technique surpasses the CNN-to-SNN conversion\nmethods in every aspect. Remarkably, even with the ViT-S/16 model\nat just 8 time steps, we have achieved an accuracy of 76.0%, which\noutperforms the highest accuracy metrics achieved in previously\npublished CNN-to-SNN works. This highlights the effectiveness of\nour conversion protocol and confirms its superiority in translating\nCNN architectures into their spiking counterparts.\nFinally, compared to the Swin-T(BN) transformer-to-SNN con-\nversion method mentioned in [51], our approach does not require\nspecific transformer structures for SNN training. Instead, it enables\nthe direct conversion of mainstream ViT models. When compared\nto the transformer-to-SNN conversion method in [26], our method\ncan decrease overall energy consumption while requiring extremely\nlower latency. Based on the above discussion, our process ensures\nquick turnaround and achieves accuracy within 10 temporal steps.\nWe conducted experiments using four different models, ViT-S/16,\nViT-B/16, ViT-L/16, and EVA, and found that the accuracies achieved\nat time steps 8, 8, 4, and 4, respectively, were as follows: 76.03%,\n79.4%, 83.2%, and 88.6%. The EVA model, in particular, performed\nexceptionally well at reduced time steps, indicating the robustness\nof our method and its potential to set new benchmarks in SNN\nperformance.\n5.4\nThe Effect of Multi-Threshold Neuron\nTo verify the effectiveness of the Multi-Threshold Neuron, we con-\nducted an experiment to explore the model by varying the number\nof thresholds in the neurons. We denoted the number of thresholds\n\n\nMM ’24, October 28-November 1, 2024, Melbourne, VIC, Australia\nZihan Huang et al.\n0\n2\n4\n6\n8\n10\nT\n0\n10\n20\n30\n40\n50\n60\n70\n80\nAcc(%)\nn=4,threshold*1\nn=6,threshold*1\nn=8,threshold*1\nn=8,threshold*2\nn=8,threshold*3\nFigure 5: Accuracy under different number and size of thresh-\nolds on ViT-S/16, 2𝑛denotes the number of thresholds.\n18.257\n3.808\n5.325\n1.441\n0.517\n0.172\n0.042\n0.555 0.208 0.082 0.032\nthreshold\nfiring rate/percent\n17.994\n0.012\n0.005\n0.012 0.002\nFigure 6: Firing rate at different thresholds\nas 2𝑛and experimented with 𝑛= 4, 𝑛= 6, and 𝑛= 8. Our re-\nsults, depicted in Figure 5, illustrate that as the value of 𝑛increases,\nmore large thresholds are included. This suggests that having large\nthresholds is crucial for enhancing performance.\nWe also increased the base threshold to investigate further while\nkeeping 𝑛= 8. This allowed us to study the effect of smaller thresh-\nolds by their omission. The results were precise: models without\nsmall thresholds performed worse than those with both large and\nsmall thresholds. Our results showed that both large and small\nthresholds are crucial for the model. This emphasizes the need for\na larger 𝑛to achieve low-latency and high-accuracy conversion.\nAdditionally, we measured the firing rates of spikes associated\nwith each threshold when 𝑛was set to 8. The outcomes are pre-\nsented in Figure 6, which shows that the majority of spikes cluster\naround the base thresholds, while the spikes generated by other\nthresholds are minimal. This indicates that adding thresholds con-\nsumes less energy but significantly reduces the inference latency.\n5.5\nEnergy Estimation\nIn order to determine the energy consumption of the SNNs, we\nbegin by calculating the theoretical computational complexity for\neach module presented in the EVA model, as detailed in Table 3.\nWe then employ the formula presented in [39] to estimate the\nenergy consumption of SNNs, as detailed in Equation (30):\n𝐸SNN\n𝐸ANN\n= 𝑀𝐴𝐶𝑠SNN ∗𝐸MAC + 𝐴𝐶𝑠SNN ∗𝐸AC\n𝑀𝐴𝐶𝑠ANN ∗𝐸MAC\n.\n(30)\nTable 3: Theoretical calculation dimensions and actual nu-\nmerical results of different modules, with image patches\n𝑁= 577, channels 𝐶= 1408, self-attention heads 𝑁ℎ= 16, and\nMLP hidden layer channels 𝐶ℎ= 6144.\nModule\nComputation\nComplexity\nResults (M)\nLayerNorm 1\n𝑁∗𝐶\n0.81\nLinear 𝑞𝑘𝑣\n𝑁∗𝐶∗3𝐶\n3431.65\nMatrix Product 𝑞,𝑘\n𝑁ℎ∗𝑁∗(𝐶/𝑁ℎ)2\n71.49\nSoftmax\n𝑁ℎ∗𝑁∗𝑁\n5.33\nMatrix Product 𝑠, 𝑣\n𝑁ℎ∗𝑁∗𝑁∗(𝐶/𝑁ℎ)\n468.76\nLinear out\n𝑁∗𝐶∗𝐶\n1143.88\nLayerNorm 2\n𝑁∗𝐶\n0.81\nMLP Linear 1\n𝑁∗𝐶∗𝐶ℎ\n4991.48\nGELU\n𝑁∗𝐶ℎ\n3.54\nMLP Linear 2\n𝑁∗𝐶ℎ∗𝐶\n4991.48\nHere we set 𝐸MAC = 4.6𝑝𝐽and 𝐸AC = 0.9𝑝𝐽according to [23].\nThe original network performs most of its computation in linear\nand matrix product layers. Our method enables us to implement\nlinear transformations of spikes entirely using accumulations and\nmatrix products primarily using accumulations. As a result, we can\nestimate the number of multiply operations (𝑀𝐴𝐶𝑠SNN) to be zero.\nWe evaluated the total energy consumption ratio of our method\ncompared to the original ANNs, and the results are summarized in\nTable 1. Our method reaches a high accuracy of 88.60% using only\n4 time steps, with a marginal loss of 1% compared to the original\nANNs, while consuming only 35% of the energy.\n6\nConclusion and Discussion\nIn this paper, we propose a novel method for converting pretrained\nVision Transformers to SNNs with reduced latency. This approach\ndiverges from previous approaches focusing on converting CNNs\nto SNNs or directly training SNNs, our method converts pre-trained\nViTs to SNNs in a low latency. It replaces various modules with\na combination of Expectation Compensation Modules and Multi-\nThreshold Neurons, achieving significantly higher accuracy on the\nImageNet dataset with very low latency compared to previous\nconversion methods. Moreover, the converted models exhibit sub-\nstantially less energy consumption than the original ANN ViTs. Our\nmethod bridges the performance gap between SNNs and ANNs,\npaving the way for ultra-high-performance SNNs.\nOur research has made significant progress in converting Trans-\nformers into SNNs with better performance. However, our current\nmethod still requires a small amount of multiplication and cannot\nuse accumulations for implementation alone. Although, this issue\ncan be addressed by utilizing hybrid neural networks such as Zhao\net al [62], which is based on neuromorphic Tianjic chips [37]. Fu-\nture work may focus on finding alternative solutions for non-linear\nmodules to eliminate the remaining multiplications. This will make\nthem more suitable for conversion and pave the way for further\nexploration of the conversion from Transformers to SNNs.\n\n\nTowards High-performance Spiking Transformers from ANN to SNN Conversion\nMM ’24, October 28-November 1, 2024, Melbourne, VIC, Australia\nAcknowledgments\nThis work was supported by the National Natural Science Founda-\ntion of China (62176003, 62088102) and the Beijing Nova Program\n(20230484362).\nReferences\n[1] Sander M Bohte, Joost N Kok, and Johannes A La Poutré. 2000. SpikeProp:\nBackpropagation for Networks of Spiking Neurons.. In The European Symposium\non Artificial Neural Networks, Vol. 48. 419–424.\n[2] Tong Bu, Jianhao Ding, Zhaofei Yu, and Tiejun Huang. 2022. Optimized Potential\nInitialization for Low-Latency Spiking Neural Networks. Proceedings of the AAAI\nConference on Artificial Intelligence 36, 1 (2022), 11–20.\n[3] Tong Bu, Wei Fang, Jianhao Ding, PENGLIN DAI, Zhaofei Yu, and Tiejun Huang.\n2022. Optimal ANN-SNN Conversion for High-accuracy and Ultra-low-latency\nSpiking Neural Networks. In International Conference on Learning Representations.\n[4] Yongqiang Cao, Yang Chen, and Deepak Khosla. 2015. Spiking Deep Convolu-\ntional Neural Networks for Energy-Efficient Object Recognition. International\nJournal of Computer Vision 113 (2015), 54–66.\n[5] Peng Chen, Yingying ZHANG, Yunyao Cheng, Yang Shu, Yihang Wang, Qing-\nsong Wen, Bin Yang, and Chenjuan Guo. 2024. Multi-scale Transformers with\nAdaptive Pathways for Time Series Forecasting. In Proceedings of the International\nConference on Learning Representations.\n[6] Mike Davies, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang\nCao, Sri Harsha Choday, Georgios Dimou, Prasad Joshi, Nabil Imam, Shweta Jain,\nYuyun Liao, Chit-Kwan Lin, Andrew Lines, Ruokun Liu, Deepak Mathaikutty,\nSteven McCoy, Arnab Paul, Jonathan Tse, Guruguhanathan Venkataramanan,\nYi-Hsin Weng, Andreas Wild, Yoonseok Yang, and Hong Wang. 2018. Loihi: A\nNeuromorphic Manycore Processor with On-Chip Learning. IEEE Micro 38, 1\n(2018), 82–99.\n[7] Michael V. DeBole, Brian Taba, Arnon Amir, Filipp Akopyan, Alexander An-\ndreopoulos, William P. Risk, Jeff Kusnitz, Carlos Ortega Otero, Tapan K. Nayak,\nRathinakumar Appuswamy, Peter J. Carlson, Andrew S. Cassidy, Pallab Datta,\nSteven K. Esser, Guillaume J. Garreau, Kevin L. Holland, Scott Lekuch, Michael\nMastro, Jeff McKinstry, Carmelo di Nolfo, Brent Paulovicks, Jun Sawada, Kai\nSchleupen, Benjamin G. Shaw, Jennifer L. Klamo, Myron D. Flickner, John V.\nArthur, and Dharmendra S. Modha. 2019. TrueNorth: Accelerating From Zero to\n64 Million Neurons in 10 Years. Computer 52, 5 (2019), 20–29.\n[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet:\nA Large-Scale Hierarchical Image Database. In IEEE Conference on Computer\nVision and Pattern Recognition. 248–255.\n[9] Shikuang Deng and Shi Gu. 2021. Optimal Conversion of Conventional Artificial\nNeural Networks to Spiking Neural Networks. In International Conference on\nLearning Representations.\n[10] Peter U. Diehl, Daniel Neil, Jonathan Binas, Matthew Cook, Shih-Chii Liu, and\nMichael Pfeiffer. 2015. Fast-classifying, High-accuracy Spiking Deep Networks\nThrough Weight and Threshold Balancing. In Proceedings of International Joint\nConference on Neural Networks. 1–8.\n[11] Jianhao Ding, Zhaofei Yu, Yonghong Tian, and Tiejun Huang. 2021. Optimal\nANN-SNN Conversion for Fast and Accurate Inference in Deep Spiking Neu-\nral Networks. In Proceedings of the International Joint Conference on Artificial\nIntelligence. 2328–2336.\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\nHeigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is\nWorth 16x16 Words: Transformers for Image Recognition at Scale. In International\nConference on Learning Representations.\n[13] Chaoteng Duan, Jianhao Ding, Shiyan Chen, Zhaofei Yu, and Tiejun Huang. 2022.\nTemporal Effective Batch Normalization in Spiking Neural Networks. Advances\nin Neural Information Processing Systems 35 (2022), 34377–34390.\n[14] Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy,\nand Dharmendra S Modha. 2019. Learned Step Size Quantization. arXiv preprint\narXiv:1902.08153 (2019).\n[15] Wei Fang, Zhaofei Yu, Yanqi Chen, Tiejun Huang, Timothée Masquelier, and\nYonghong Tian. 2021. Deep Residual Learning in Spiking Neural Networks. In\nAdvances in Neural Information Processing Systems, Vol. 34. 21056–21069.\n[16] Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue\nCao. 2023. EVA-02: A Visual Representation for Neon Genesis. arXiv preprint\narXiv:2303.11331 (2023).\n[17] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang,\nTiejun Huang, Xinlong Wang, and Yue Cao. 2023. EVA: Exploring the Limits of\nMasked Visual Representation Learning at Scale. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. 19358–19369.\n[18] Wulfram Gerstner, Werner M Kistler, Richard Naud, and Liam Paninski. 2014.\nNeuronal Dynamics: From Single Neurons to Networks and Models of Cognition.\nCambridge University Press.\n[19] Bing Han, Gopalakrishnan Srinivasan, and Kaushik Roy. 2020. RMP-SNN: Resid-\nual Membrane Potential Neuron for Enabling Deeper High-Accuracy and Low-\nLatency Spiking Neural Network. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. 13558–13567.\n[20] Jianing Han, Ziming Wang, Jiangrong Shen, and Huajin Tang. 2023. Symmetric-\nthreshold ReLU for Fast and Nearly Lossless ANN-SNN Conversion. Machine\nIntelligence Research 20, 3 (2023), 435–446.\n[21] Zecheng Hao, Tong Bu, Jianhao Ding, Tiejun Huang, and Zhaofei Yu. 2023.\nReducing ANN-SNN Conversion Error through Residual Membrane Potential.\nProceedings of the AAAI Conference on Artificial Intelligence 37, 1 (2023), 11–21.\n[22] Nguyen-Dong Ho and Ik-Joon Chang. 2021. TCL: an ANN-to-SNN Conver-\nsion with Trainable Clipping Layers. In 2021 58th ACM/IEEE Design Automation\nConference (DAC). 793–798.\n[23] Mark Horowitz. 2014. 1.1 Computing’s energy problem (and what we can do\nabout it). In Proceedings of IEEE International Solid-State Circuits Conference Digest\nof Technical Papers. 10–14.\n[24] Jitesh Jain, Jiachen Li, Mang Tik Chiu, Ali Hassani, Nikita Orlov, and Humphrey\nShi. 2023. OneFormer: One Transformer To Rule Universal Image Segmenta-\ntion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition. 2989–2998.\n[25] Haiyan Jiang, Srinivas Anumasa, Giulia De Masi, Huan Xiong, and Bin Gu.\n2023. A Unified Optimization Framework of ANN-SNN Conversion: Towards\nOptimal Mapping from Activation Values to Firing Rates. In Proceedings of the\n40th International Conference on Machine Learning, Vol. 202. 14945–14974.\n[26] Yizhou Jiang, Kunlin Hu, Tianren Zhang, Haichuan Gao, Yuqian Liu, Ying Fang,\nand Feng Chen. 2024. Spatio-Temporal Approximation: A Training-Free SNN\nConversion for Transformers. In Proceedings of the International Conference on\nLearning Representations.\n[27] A Krizhevsky. 2009. Learning Multiple Layers of Features from Tiny Images.\nMaster’s thesis, University of Tront (2009).\n[28] Jun Haeng Lee, Tobi Delbruck, and Michael Pfeiffer. 2016. Training Deep Spiking\nNeural Networks Using Backpropagation. Frontiers in Neuroscience 10 (2016),\n228000.\n[29] Chen Li, Lei Ma, and Steve Furber. 2022. Quantization Framework for Fast Spiking\nNeural Networks. Frontiers in Neuroscience 16 (2022), 918793.\n[30] Yuhang Li, Shikuang Deng, Xin Dong, Ruihao Gong, and Shi Gu. 2021. A Free\nLunch From ANN: Towards Efficient, Accurate Spiking Neural Networks Cali-\nbration. In Proceedings of the 38th International Conference on Machine Learning,\nVol. 139. 6316–6325.\n[31] Yudong Li, Yunlin Lei, and Xu Yang. 2022. Spikeformer: A Novel Architecture\nfor Training High-Performance Low-Latency Spiking Neural Network. arXiv\npreprint arXiv:2211.10686 (2022).\n[32] Yang Li and Yi Zeng. 2022. Efficient and Accurate Conversion of Spiking Neural\nNetwork with Burst Spikes. In Proceedings of the International Joint Conference\non Artificial Intelligence. 2485–2491.\n[33] Xinyu Liu, Houwen Peng, Ningxin Zheng, Yuqing Yang, Han Hu, and Yixuan\nYuan. 2023. EfficientViT: Memory Efficient Vision Transformer With Cascaded\nGroup Attention. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition. 14420–14430.\n[34] Wolfgang Maass. 1997. Networks of Spiking Neurons: The Third Generation of\nNeural Network Models. Neural Networks 10, 9 (1997), 1659–1671.\n[35] Paul A. Merolla, John V. Arthur, Rodrigo Alvarez-Icaza, Andrew S. Cassidy,\nJun Sawada, Filipp Akopyan, Bryan L. Jackson, Nabil Imam, Chen Guo, Yutaka\nNakamura, Bernard Brezzo, Ivan Vo, Steven K. Esser, Rathinakumar Appuswamy,\nBrian Taba, Arnon Amir, Myron D. Flickner, William P. Risk, Rajit Manohar,\nand Dharmendra S. Modha. 2014. A Million Spiking-neuron Integrated Circuit\nwith a Scalable Communication Network and Interface. Science 345, 6197 (2014),\n668–673.\n[36] Emre O. Neftci, Hesham Mostafa, and Friedemann Zenke. 2019. Surrogate Gradi-\nent Learning in Spiking Neural Networks: Bringing the Power of Gradient-Based\nOptimization to Spiking Neural Networks. IEEE Signal Processing Magazine 36, 6\n(2019), 51–63.\n[37] Jing Pei, Lei Deng, Sen Song, Mingguo Zhao, Youhui Zhang, Shuang Wu, Guanrui\nWang, Zhe Zou, Zhenzhi Wu, Wei He, et al. 2019. Towards Artificial General\nIntelligence with Hybrid Tianjic Chip Architecture. Nature 572, 7767 (2019),\n106–111.\n[38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,\nSandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models\nFrom Natural Language Supervision. In Proceedings of the International Conference\non Machine Learning, Vol. 139. 8748–8763.\n[39] Nitin Rathi and Kaushik Roy. 2020. Diet-snn: Direct Input Encoding with Leakage\nand Threshold Optimization in Deep Spiking Neural Networks. arXiv preprint\narXiv:2008.03658 (2020).\n[40] Kaushik Roy, Akhilesh Jaiswal, and Priyadarshini Panda. 2019. Towards Spike-\nbased Machine Intelligence with Neuromorphic Computing. Nature 575, 7784\n(2019), 607–617.\n[41] Bodo Rueckauer, Iulia-Alexandra Lungu, Yuhuang Hu, Michael Pfeiffer, and Shih-\nChii Liu. 2017. Conversion of Continuous-Valued Deep Networks to Efficient\n\n\nMM ’24, October 28-November 1, 2024, Melbourne, VIC, Australia\nZihan Huang et al.\nEvent-Driven Networks for Image Classification. Frontiers in Neuroscience 11\n(2017), 294078.\n[42] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean\nMa, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al.\n2015. ImageNet Large Scale Visual Recognition Challenge. International Journal\nof Computer Vision 115 (2015), 211–252.\n[43] Abhronil Sengupta, Yuting Ye, Robert Wang, Chiao Liu, and Kaushik Roy. 2019.\nGoing Deeper in Spiking Neural Networks: VGG and Residual Architectures.\nFrontiers in Neuroscience 13 (2019), 95.\n[44] Xinyu Shi, Zecheng Hao, and Zhaofei Yu. 2024. SpikingResformer: Bridging\nResNet and Vision Transformer in Spiking Neural Networks. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\n5610–5619.\n[45] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. 2023. EVA-CLIP:\nImproved Training Techniques for Clip at Scale. arXiv preprint arXiv:2303.15389\n(2023).\n[46] Amirhossein Tavanaei, Masoud Ghodrati, Saeed Reza Kheradpisheh, Timothée\nMasquelier, and Anthony Maida. 2019. Deep Learning in Spiking Neural Net-\nworks. Neural Networks 111 (2019), 47–63.\n[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In Advances in Neural Information Processing Systems, Vol. 30.\n[48] Bingsen Wang, Jian Cao, Jue Chen, Shuo Feng, and Yuan Wang. 2023. A New ANN-\nSNN Conversion Method with High Accuracy, Low Latency and Good Robustness.\nIn Proceedings of the International Joint Conference on Artificial Intelligence. 3067–\n3075.\n[49] Qingyu Wang, Tielin Zhang, Minglun Han, Yi Wang, Duzhen Zhang, and Bo Xu.\n2023. Complex Dynamic Neurons Improved Spiking Transformer Network for\nEfficient Automatic Speech Recognition. In Proceedings of the AAAI Conference\non Artificial Intelligence, Vol. 37. 102–109.\n[50] Yuchen Wang, Malu Zhang, Yi Chen, and Hong Qu. 2022. Signed Neuron with\nMemory: Towards Simple, Accurate and High-Efficient ANN-SNN Conversion.\nIn Proceedings of the International Joint Conference on Artificial Intelligence. 2501–\n2508.\n[51] Ziqing Wang, Yuetong Fang, Jiahang Cao, Qiang Zhang, Zhongrui Wang, and\nRenjing Xu. 2023. Masked Spiking Transformer. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision. 1761–1771.\n[52] Yujie Wu, Lei Deng, Guoqi Li, Jun Zhu, and Luping Shi. 2018. Spatio-Temporal\nBackpropagation for Training High-Performance Spiking Neural Networks. Fron-\ntiers in Neuroscience 12 (2018), 323875.\n[53] Yujie Wu, Rong Zhao, Jun Zhu, Feng Chen, Mingkun Xu, Guoqi Li, Sen Song,\nLei Deng, Guanrui Wang, Hao Zheng, et al. 2022. Brain-inspired Global-local\nLearning Incorporated with Neuromorphic Computing. Nature Communications\n13, 1 (2022), 65.\n[54] Mingkun Xu, Faqiang Liu, Yifan Hu, Hongyi Li, Yuanyuan Wei, Shuai Zhong,\nJing Pei, and Lei Deng. 2024. Adaptive Synaptic Scaling in Spiking Networks\nfor Continual Learning and Enhanced Robustness. IEEE Transactions on Neural\nNetworks and Learning Systems (2024), 1–15.\n[55] Man Yao, JiaKui Hu, Zhaokun Zhou, Li Yuan, Yonghong Tian, Bo Xu, and Guoqi\nLi. 2023. Spike-driven Transformer. In Advances in Neural Information Processing\nSystems, Vol. 36. 64043–64058.\n[56] Huifeng Yin, Hanle Zheng, Jiayi Mao, Siyuan Ding, Xing Liu, Mingkun Xu, Yifan\nHu, Jing Pei, and Lei Deng. 2024. Understanding the Functional Roles of Modelling\nComponents in Spiking Neural Networks. arXiv preprint arXiv:2403.16674 (2024).\n[57] Friedemann Zenke, Sander M. Bohté, Claudia Clopath, Iulia M. Comşa, Julian\nGöltz, Wolfgang Maass, Timothée Masquelier, Richard Naud, Emre O. Neftci,\nMihai A. Petrovici, Franz Scherr, and Dan F.M. Goodman. 2021. Visualizing a\nJoint Future of Neuroscience and Neuromorphic Engineering. Neuron 109, 4\n(2021), 571–575.\n[58] Anguo Zhang, Jieming Shi, Junyi Wu, Yongcheng Zhou, and Wei Yu. 2023. Low\nLatency and Sparse Computing Spiking Neural Networks With Self-Driven Adap-\ntive Threshold Plasticity. IEEE Transactions on Neural Networks and Learning\nSystems (2023), 1–12.\n[59] Jiqing Zhang, Bo Dong, Haiwei Zhang, Jianchuan Ding, Felix Heide, Baocai\nYin, and Xin Yang. 2022. Spiking Transformers for Event-Based Single Object\nTracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition. 8801–8810.\n[60] Jiyuan Zhang, Lulu Tang, Zhaofei Yu, Jiwen Lu, and Tiejun Huang. 2022. Spike\nTransformer: Monocular Depth Estimation for Spiking Camera. In European\nConference on Computer Vision. 34–52.\n[61] Wenrui Zhang and Peng Li. 2020. Temporal Spike Sequence Learning via Back-\npropagation for Deep Spiking Neural Networks. In Advances in Neural Information\nProcessing Systems, Vol. 33. 12022–12033.\n[62] Rong Zhao, Zheyu Yang, Hao Zheng, Yujie Wu, Faqiang Liu, Zhenzhi Wu, Lukai\nLi, Feng Chen, Seng Song, Jun Zhu, et al. 2022. A Framework for the General\nDesign and Computation of Hybrid Neural Networks. Nature communications\n13, 1 (2022), 3427.\n[63] Chenlin Zhou, Liutao Yu, Zhaokun Zhou, Han Zhang, Zhengyu Ma, Huihui Zhou,\nand Yonghong Tian. 2023. Spikingformer: Spike-driven Residual Learning for\nTransformer-based Spiking Neural Network. arXiv preprint arXiv:2304.11954\n(2023).\n[64] Zhaokun Zhou, Yuesheng Zhu, Chao He, Yaowei Wang, Shuicheng YAN,\nYonghong Tian, and Li Yuan. 2023. Spikformer: When Spiking Neural Network\nMeets Transformer. In Proceedings of the International Conference on Learning\nRepresentations.\n[65] Xiaolei Zhu, Baixin Zhao, De Ma, and Huajin Tang. 2022. An Efficient Learning\nAlgorithm for Direct Training Deep Spiking Neural Networks. IEEE Transactions\non Cognitive and Developmental Systems 14, 3 (2022), 847–856.\n\n\nTowards High-performance Spiking Transformers from ANN to SNN Conversion\nMM ’24, October 28-November 1, 2024, Melbourne, VIC, Australia\n7\nProof of Theorem 1\nTheorem 7.1. Consider a non-linear layer 𝑙with a function 𝐹.\nIn SNNs, the output of this layer at time 𝑡is denoted as 𝑶𝑙(𝑡). Let\n𝑺𝑙(𝑇) be the cumulative sum of layer 𝑙outputs up to time 𝑇, given\nby 𝑺𝑙(𝑇) = Í𝑇\n𝑡=1 𝑶𝑙(𝑡). The expected output of the SNNs at time 𝑇is\ngiven by:\n𝑶𝑙(𝑇) = 𝑇𝐹\n \n𝑺𝑙−1(𝑇)\n𝑇\n!\n−(𝑇−1)𝐹\n \n𝑺𝑙−1(𝑇−1)\n𝑇−1\n!\n.\n(31)\nProof. According to Section 3.2, we denote 𝒙𝑙(𝑡) as 𝑶𝑙(𝑡), which\nhas the same meaning, and we can approximate the output value\nof ANNs using the mean value of the output for the first T time\nsteps in SNNs:\n𝒂𝑙\n𝑇= Φ𝑙(𝑇) =\nÍ𝑇\n𝑡=1 𝑶𝑙(𝑡)\n𝑇\n(32)\nwhere 𝒂𝑙\n𝑇represents the estimated values of neurons in layer 𝑙at\ntime𝑇in ANNs. It will change as the corresponding spikes in SNNs\naccumulate over time.\nMeanwhile, in the case of ANNs, 𝒂𝑙\n𝑇can be formulated as:\n𝒂𝑙\n𝑇= 𝐹(𝒂𝑙−1\n𝑇\n).\n(33)\nFurthermore, we can deduce the output by subtracting the total\noutput of the previous T and T-1 time steps from the formula 32\nand the formula 33.\n𝑶𝑙(𝑇) =\n𝑇\n∑︁\n𝑡=1\n𝑶𝑙(𝑡) −\n𝑇−1\n∑︁\n𝑡=1\n𝑶𝑙(𝑡)\n= 𝑇𝒂𝑙\n𝑇−(𝑇−1)𝒂𝑙\n𝑇−1\n= 𝑇𝐹(𝒂𝑙−1\n𝑇\n) −(𝑇−1)𝐹(𝒂𝑙−1\n𝑇−1)\n= 𝑇𝐹\n Í𝑇\n𝑡=1 𝑂𝑙−1(𝑡)\n𝑇\n!\n−(𝑇−1)𝐹\n Í𝑇−1\n𝑡=1 𝑂𝑙−1(𝑡)\n𝑇−1\n!\n= 𝑇𝐹\n \n𝑺𝑙−1(𝑇)\n𝑇\n!\n−(𝑇−1)𝐹\n \n𝑺𝑙−1(𝑇−1)\n𝑇−1\n!\n.\n(34)\n□\n8\nProof of Theorem 2\nTheorem 8.1. Consider a module for matrix product that receives\ntwo sets of spike inputs, denoted by 𝑨𝑣𝑎(𝑡) and 𝑩𝑣𝑏(𝑡). These inputs\nare generated by neurons 𝐴and 𝐵, respectively, and are characterized\nby multiple thresholds 𝑣𝑎and 𝑣𝑏, as described in Section 4.3.\nWe can integrate the input by 𝑨(𝑡) = Í\n𝑣𝑎𝑣𝑎𝑨𝑣𝑎(𝑡) and 𝑩(𝑡) =\nÍ\n𝑣𝑏𝑣𝑏𝑩𝑣𝑏(𝑡). Here, 𝑨(𝑡) and 𝑩(𝑡) are the sum matrices weighted\nby multiple thresholds 𝑣𝑎and 𝑣𝑏, respectively.\nLet 𝑺𝐴(𝑇) = Í𝑇\n𝑡=1 𝐴(𝑡) and 𝑺𝐵(𝑇) = Í𝑇\n𝑡=1 𝐵(𝑡) represent the cu-\nmulative sum of inputs up to time𝑇. We define 𝑺𝐾(𝑇) = 𝑺𝐴(𝑇)𝑺𝐵(𝑇).\nThen, the expected output at time T can be formulated as:\n𝑶(𝑇) = 1\n𝑇𝑺𝐾(𝑇) −\n1\n𝑇−1𝑺𝐾(𝑇−1),\n(35)\nwhere 𝑺𝐾(𝑇) can be calculated mainly using addition, as described\nby the following equation:\n𝑺𝐾(𝑇) = 𝑺𝐾(𝑇−1) + 𝑲(𝑇)\n(36)\n𝐾(𝑇) =\n∑︁\n𝑣𝑎,𝑣𝑏\n𝑣𝑎𝑣𝑏𝑨𝑣𝑎(𝑇)𝑩𝑣𝑏(𝑇) +\n∑︁\n𝑣𝑎\n𝑣𝑎𝑨𝑣𝑎(𝑇)𝑺𝐵(𝑇−1)\n+\n∑︁\n𝑣𝑏\n𝑣𝑏𝑺𝐴(𝑇−1)𝑩𝑣𝑏(𝑇).\n(37)\nProof. Since we approximate the value of ANNs using the mean\nvalue for the first T times in SNNs, let the expected input matrices\n𝑨𝑇, 𝑩𝑇, and 𝑶𝑇= 𝑨𝑇𝑩𝑇in ANNs be calculated based on the input\nspikes during the first 𝑇time steps in SNNs, denoted as:\n𝑨𝑇=\nÍ𝑇\n𝑡=1 𝑨(𝑡)\n𝑇\n(38)\n𝑩𝑇=\nÍ𝑇\n𝑡=1 𝑩(𝑡)\n𝑇\n(39)\n𝑶𝑇=\nÍ𝑇\n𝑡=1 𝑶(𝑡)\n𝑇\n(40)\nSo, the expected output matrix 𝑶(𝑇) at time 𝑇can be calculated\nby:\n𝑶(𝑇) =\n𝑇\n∑︁\n𝑡=1\n𝑶(𝑡) −\n𝑇−1\n∑︁\n𝑖=𝑡\n𝑶(𝑡)\n= 𝑇𝑶𝑇−(𝑇−1)𝑶𝑇−1\n= 𝑇𝑨𝑇𝑩𝑇−(𝑇−1)𝑨𝑇−1𝑩𝑇−1\n= 𝑇\nÍ𝑇\n𝑡=1 𝑨(𝑡)\n𝑇\nÍ𝑇\n𝑡=1 𝑩(𝑡)\n𝑇\n−(𝑇−1)\nÍ𝑇−1\n𝑡=1 𝑨(𝑡)\n𝑇−1\nÍ𝑇−1\n𝑡=1 𝑩(𝑡)\n𝑇−1\n= 1\n𝑇\n𝑇\n∑︁\n𝑡=1\n𝑨(𝑡)\n𝑇\n∑︁\n𝑡=1\n𝑩(𝑡) −\n1\n(𝑇−1)\n𝑇−1\n∑︁\n𝑡=1\n𝑨(𝑡)\n𝑇−1\n∑︁\n𝑡=1\n𝑩(𝑡)\n= 1\n𝑇𝑺𝐴(𝑇)𝑺𝐵(𝑇) −\n1\n𝑇−1𝑺𝐴(𝑇−1)𝑺𝐵(𝑇−1)\n= 1\n𝑇𝑺𝐾(𝑇) −\n1\n𝑇−1𝑺𝐾(𝑇−1)\n(41)\nAnd 𝑺𝐾(𝑇) can be calculated by:\n𝑺𝐾(𝑇) = 𝑺𝐴(𝑇)𝑺𝐵(𝑇)\n= (𝑺𝐴(𝑇−1) + 𝑨(𝑇))(𝑺𝐵(𝑇−1) + 𝑩(𝑇))\n= 𝑺𝐴(𝑇−1)𝑺𝐵(𝑇−1) + 𝑨(𝑇)𝑩(𝑇)\n+ 𝑨(𝑇)𝑺𝐵(𝑇−1) + 𝑺𝐴(𝑇−1)𝑩(𝑇)\n= 𝑺𝐾(𝑇−1) +\n∑︁\n𝑣𝑎,𝑣𝑏\n𝑣𝑎𝑣𝑏𝑨𝑣𝑎(𝑇)𝑩𝑣𝑏(𝑇)\n+\n∑︁\n𝑣𝑎\n𝑣𝑎𝑨𝑣𝑎(𝑇)𝑺𝐵(𝑇−1) +\n∑︁\n𝑣𝑏\n𝑣𝑏𝑺𝐴(𝑇−1)𝑩𝑣𝑏(𝑇)\n= 𝑺𝐾(𝑇−1) + 𝑲(𝑇).\n(42)\nAssuming the dimension of 𝑺𝐾(𝑇), 𝑺𝐴(𝑇) and 𝑺𝐵(𝑇) are 𝑛×𝑚, 𝑛×𝑝\nand 𝑝× 𝑚, respectively. And suppose the firing rate of 𝐴(𝑇) and\n𝐵(𝑇) are 𝜂1 and 𝜂2.\nIn order to determine the number of different operations required\nto update 𝑺𝐾(𝑇), we conduct a brief analysis: Multiplications occur\nwhen the threshold is multiplied by the results of various matrix\nmultiplications; Additions occur during the calculation of individual\nmatrix multiplications, as well as the accumulation of the results of\nthe four parts.\n\n\nMM ’24, October 28-November 1, 2024, Melbourne, VIC, Australia\nZihan Huang et al.\nAs each position of the input matrix has only one effective thresh-\nold at each time, it restricts the total number of input spikes, thus\nlimiting the total number of operations.\nThe maximum addition operation number is\n𝐴𝐶𝑠𝑚𝑎𝑥\nSNN = 𝜂1𝜂2𝑛𝑝𝑚+ 𝜂1𝑛𝑝𝑚+ 𝜂2𝑛𝑝𝑚+ 3𝑛𝑚\n(43)\nwhere 𝜂1𝜂2𝑛𝑝𝑚, 𝜂1𝑛𝑝𝑚and 𝜂2𝑛𝑝𝑚are the maximum addition oper-\nations in calculating Í\n𝑣𝑎,𝑣𝑏𝑣𝑎𝑣𝑏𝑨𝑣𝑎(𝑇)𝑩𝑣𝑏(𝑇) , Í\n𝑣𝑎𝑣𝑎𝑨𝑣𝑎(𝑇)𝑺𝐵(𝑇−\n1) and Í\n𝑣𝑏𝑣𝑏𝑺𝐴(𝑇−1)𝑩𝑣𝑏(𝑇), respectively. 3𝑛𝑚is the maximum\noperation in accumulating four parts in Equation (36).\nThe maximum multiplication operation number is\n𝑀𝐴𝐶𝑠𝑚𝑎𝑥\nSNN = 𝑚𝑖𝑛(𝜂1,𝜂2)𝑛𝑚+ 𝜂1𝑛𝑚+ 𝜂2𝑛𝑚\n(44)\nwhere𝑚𝑖𝑛(𝜂1,𝜂2)𝑛𝑚, 𝜂1𝑛𝑚and 𝜂2𝑛𝑚are the maximum multiplica-\ntion operations in calculating Í\n𝑣𝑎,𝑣𝑏𝑣𝑎𝑣𝑏𝑨𝑣𝑎(𝑇)𝑩𝑣𝑏(𝑇), Í\n𝑣𝑎𝑣𝑎𝑨𝑣𝑎(𝑇)𝑺𝐵(𝑇−\n1) and Í\n𝑣𝑏𝑣𝑏𝑺𝐴(𝑇−1)𝑩𝑣𝑏(𝑇), respectively.\nIt can be seen that 𝐴𝐶𝑠𝑚𝑎𝑥\nSNN ≫𝑀𝐴𝐶𝑠𝑚𝑎𝑥\nSNN, so 𝑆𝐾(𝑇) can be cal-\nculated mainly using addition.\n□\n9\nExperiment Details\n9.1\nDatasets\nCIFAR-10. The CIFAR-10 dataset [27] consists of 60000 32 × 32\nimages in 10 classes. There are 50000 training images and 10000\ntest images.\nCIFAR-100. The CIFAR-100 dataset [27] consists of 60000 32\n× 32 images in 100 classes. There are 50000 training images and\n10000 test images.\nImageNet1k. We use the ILSVRC 2012 dataset [42], which con-\nsists of 1,281,167 training images and 50000 testing images.\n9.2\nData Preprocessing\nTo process our image data, we followed a series of steps. First, we\nresized the image to the desired size and then cropped it to match the\ninput size. After that, we converted the image into a PyTorch tensor.\nNext, we normalized the pixel values using the provided mean\nand standard deviation values. The mean and standard deviation\nvalues were specified as (0.48145466, 0.4578275, 0.40821073) and\n(0.26862954, 0.26130258, 0.27577711). Finally, we normalized the\npixel values of the three-channel images based on the provided\nmean and standard deviation.\n9.3\nExperimental Setup\nThe conversion in this paper is based on pre-trained Vision Trans-\nformer including the ViT-S/16, ViT-B/16, ViT-L/16 with 224 resolu-\ntion [47], and the EVA model eva_g_patch14 in [17].\nFor all Multi-Threshold Neurons, we set 𝑛to 8 for ViT-S/16,\nViT-B/16, ViT-L/16 and 6 for EVA. We set threshold percent 𝑝to 99\nto get thresholds for each neuron. In particular, due to huge differ-\nences in GELU and softmax layers’ output values, we configure the\npositive and negative base thresholds to 0.5 and 0.08, respectively,\nfor neurons following the GELU module in 𝑉𝑖𝑇models, and to\n0.0125 for neurons following the softmax module to prevent too\nfew spikes.\nBesides, the precision of the network is highly sensitive to the\nprecision of the classification layer, as mentioned in [29]. Since\nthe classification layer has minimal energy consumption during\nruntime, we retained analog input in the classification layer.\n10\nAdditional Experimental Details\n10.1\nDetailed results on other datasets\nTables 4 and 5 present a comparison of the accuracy and energy\nconsumption of different neural network architectures - ANNs and\nSNNs - on CIFAR10 and CIFAR100 datasets.\nTable 4 compares the accuracy of ANN and SNN architectures for\nthe CIFAR10 dataset across three model scales: ViT-S/16, ViT-B/16,\nand ViT-L/16. It can be seen that the SNN model can reach a com-\nparable accuracy while significantly reducing the consumption.\nFor example, when the SNN model is run for 6 time steps, models\nsuch as ViT-S/16, ViT-B/16, and ViT-L/16 achieve accuracy levels\nof 97.37%, 98.24%, and 99.1%, respectively. The remarkable fact is\nthat they only consume 0.6, 0.48, and 0.4 energy, respectively when\ncompared to the original ANN (Artificial Neural Network) models.\nTable 5 presents a similar comparison for the more complex\nCIFAR100 dataset. For instance, at 6 timesteps, ViT-S/16, ViT-B/16,\nand ViT-L/16 achieve accuracies of 84.75%, 90.22%, and 93.04%, re-\nspectively, while using only 0.61, 0.48, and 0.43 energy compared\nto original ANN models. It shows the potential of our method to re-\nduce energy consumption while maintaining accuracy. The results\ndemonstrate our method’s potential to reduce energy consumption\nwhile maintaining accuracy.\n10.2\nComparison with the State-of-the-art on\nCIFAR10 and CIFAR100 datasets\nWe compare the experimental results using the ViT-S/16, ViT-B/16,\nViT-L/16 model on the CIFAR10 and CIFAR100 datasets with previ-\nous state-of-the-art methods, as shown in Table 6 and 7.\nIn the evaluation of the CIFAR10 dataset, the ECMT model\nachieved an impressive accuracy rate of 97.37%, 98.24%, and 99.1%\nrespectively, using the architecture of ViT-S/16, ViT-B/16, ViT-L/16\nover just six timesteps. This level of precision is highly competitive,\nespecially compared to similarly-sized models. In evaluating the\nCIFAR100 dataset, considered more complex, the ECMT method\nagain displays its strength. The results demonstrate that the ECMT\nmethod achieves a similar high accuracy.\nThe ECMT model uses the Transformer-to-SNN approach and\nhas performed exceptionally well on the CIFAR10 and CIFAR100\ndatasets. Its ViT-B/16 variant stands out by achieving high accuracy\nwith a moderate number of parameters, indicating the potential\nof SNNs in achieving state-of-the-art results with a significant\nreduction in computational resources. This balance of efficiency and\naccuracy makes the ECMT a promising model for energy-efficient\nand fast processing tasks.\n\n\nTowards High-performance Spiking Transformers from ANN to SNN Conversion\nMM ’24, October 28-November 1, 2024, Melbourne, VIC, Australia\nTable 4: Accuracy and energy consumption ratio of ECMT(Ours) on CIFAR10 dataset\nArch.\nAccuracy/Energy\nOriginal (ANN)\nOurs (SNN)\nT=1\nT=2\nT=4\nT=6\nT=8\nT=10\nViT-S/16\nAcc. (%)\n98.33\n8.53\n31.32\n93.82\n97.37\n98.01\n98.21\nEnergy ratio\n1\n0.06\n0.15\n0.37\n0.60\n0.82\n1.03\nViT-B/16\nAcc. (%)\n98.75\n9.17\n32.25\n95.17\n98.24\n98.55\n98.69\nEnergy ratio\n1\n0.04\n0.12\n0.30\n0.48\n0.66\n0.83\nViT-L/16\nAcc. (%)\n99.07\n10.55\n95.14\n98.89\n99.1\n99.03\n99.08\nEnergy ratio\n1\n0.03\n0.11\n0.27\n0.42\n0.57\n0.72\nTable 5: Accuracy and energy consumption ratio of ECMT(Ours) on CIFAR100 dataset\nArch.\nAccuracy/Energy\nOriginal (ANN)\nOurs (SNN)\nT=1\nT=2\nT=4\nT=6\nT=8\nT=10\nViT-S/16\nAcc. (%)\n89.28\n0.95\n4.9\n69.49\n84.75\n87.83\n88.93\nEnergy ratio\n1\n0.06\n0.16\n0.38\n0.61\n0.84\n1.07\nViT-B/16\nAcc. (%)\n92.26\n0.87\n17.07\n82.86\n90.22\n91.5\n91.91\nEnergy ratio\n1\n0.04\n0.12\n0.30\n0.48\n0.66\n0.84\nViT-L/16\nAcc. (%)\n93.84\n1.61\n69.08\n91.82\n93.04\n93.34\n93.56\nEnergy ratio\n1\n0.04\n0.12\n0.27\n0.43\n0.58\n0.73\nTable 6: Comparison between the proposed method and previous works on CIFAR10 dataset\nMethod\nType\nArch.\nParam. (M)\nT\nAccuracy (%)\nSpikingformer[63]\nDirect Training\nSpikingformer-4-384-400E\n9.32\n4\n95.81\nSpike-driven Transformer[55]\nDirect Training\nSpikingformer-4-384-400E\n9.32\n4\n95.6\nRMP[19]\nCNN-to-SNN\nVGG-16\n138\n64(2048)\n90.35(93.63)\nSNM[50]\nCNN-to-SNN\nVGG-16\n138\n32(128)\n93.43(94.07)\nTS[9]\nCNN-to-SNN\nVGG-16\n138\n16(32)\n92.29(92.29)\nQFFS[29]\nCNN-to-SNN\nVGG-16\n138\n4\n92.64\nQCFS[3]\nCNN-to-SNN\nResNet-18\n11.8\n8(64)\n94.82(96.06)\nVGG-16\n138\n8(64)\n94.95(95.55)\nSRP[21]\nCNN-to-SNN\nResNet-18\n11.8\n4(16)\n95.25(95.55)\nVGG-16\n138\n4(16)\n95.32(95.42)\nMST[51]\nTransformer-to-SNN\nSwin-T(BN)\n27.6\n64(256)\n96.32(97.27)\nSTA[26]\nTransformer-to-SNN\nViT-B/32\n86\n32(256)\n95.49(95.82)\nECMT(Ours)\nTransformer-to-SNN\nViT-S/16\n22\n6(8)\n97.37(98.01)\nViT-B/16\n86\n6(8)\n98.24(98.55)\nViT-L/16\n307\n6(8)\n99.1(99.03)\n\n\nMM ’24, October 28-November 1, 2024, Melbourne, VIC, Australia\nZihan Huang et al.\nTable 7: Comparison between the proposed method and previous works on CIFAR100 dataset\nMethod\nType\nArch.\nParam. (M)\nT\nAccuracy (%)\nSpikingformer[63]\nDirect Training\nSpikingformer-4-384-400E\n9.32\n4\n79.21\nSpike-driven Transformer[55]\nDirect Training\nSpikingformer-4-384-400E\n9.32\n4\n78.4\nRMP[19]\nCNN-to-SNN\nVGG-16\n138\n128(2048)\n63.76(70.93)\nSNM[50]\nCNN-to-SNN\nVGG-16\n138\n32(128)\n71.8(73.95)\nTS[9]\nCNN-to-SNN\nVGG-16\n138\n16(64)\n63.73(69.27)\nQCFS[3]\nCNN-to-SNN\nResNet-18\n11.8\n8(64)\n78.48(79.54)\nVGG-16\n138\n8(64)\n73.96(77.10)\nSRP[21]\nCNN-to-SNN\nResNet-20\n0.27\n4(32)\n59.34(65.50)\nVGG-16\n138\n4(32)\n75.42(76.45)\nMST[51]\nTransformer-to-SNN\nSwin-T(BN)\n27.6\n64(256)\n85.4(86.91)\nSTA[26]\nTransformer-to-SNN\nViT-B/32\n86\n32(256)\n84.15(85.98)\nECMT(Ours)\nTransformer-to-SNN\nViT-S/16\n22\n6(8)\n84.75(87.83)\nViT-B/16\n86\n6(8)\n90.22(91.5)\nViT-L/16\n307\n6(8)\n93.04(93.34)\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21193v1.pdf",
    "total_pages": 14,
    "title": "Towards High-performance Spiking Transformers from ANN to SNN Conversion",
    "authors": [
      "Zihan Huang",
      "Xinyu Shi",
      "Zecheng Hao",
      "Tong Bu",
      "Jianhao Ding",
      "Zhaofei Yu",
      "Tiejun Huang"
    ],
    "abstract": "Spiking neural networks (SNNs) show great potential due to their energy\nefficiency, fast processing capabilities, and robustness. There are two main\napproaches to constructing SNNs. Direct training methods require much memory,\nwhile conversion methods offer a simpler and more efficient option. However,\ncurrent conversion methods mainly focus on converting convolutional neural\nnetworks (CNNs) to SNNs. Converting Transformers to SNN is challenging because\nof the presence of non-linear modules. In this paper, we propose an Expectation\nCompensation Module to preserve the accuracy of the conversion. The core idea\nis to use information from the previous T time-steps to calculate the expected\noutput at time-step T. We also propose a Multi-Threshold Neuron and the\ncorresponding Parallel Parameter normalization to address the challenge of\nlarge time steps needed for high accuracy, aiming to reduce network latency and\npower consumption. Our experimental results demonstrate that our approach\nachieves state-of-the-art performance. For example, we achieve a top-1 accuracy\nof 88.60\\% with only a 1\\% loss in accuracy using 4 time steps while consuming\nonly 35\\% of the original power of the Transformer. To our knowledge, this is\nthe first successful Artificial Neural Network (ANN) to SNN conversion for\nSpiking Transformers that achieves high accuracy, low latency, and low power\nconsumption on complex datasets. The source codes of the proposed method are\navailable at https://github.com/h-z-h-cell/Transformer-to-SNN-ECMT.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}