{
  "id": "arxiv_2502.20655v1",
  "text": "WAVELET-BASED DENSITY SKETCHING WITH FUNCTIONAL\nHIERARCHICAL TENSOR\nXUN TANG∗AND LEXING YING†\nAbstract.\nWe introduce the functional hierarchical tensor under a wavelet basis (FHT-W)\nansatz for high-dimensional density estimation in lattice models.\nRecently, the functional tensor\nnetwork has emerged as a suitable candidate for density estimation due to its ability to calculate\nthe normalization constant exactly, a defining feature not enjoyed by neural network alternatives\nsuch as energy-based models or diffusion models. While current functional tensor network models\nshow good performance for lattice models with weak or moderate couplings, we show that they\nface significant model capacity constraints when applied to lattice models with strong coupling. To\naddress this issue, this work proposes to perform density estimation on the lattice model under a\nwavelet transformation. Motivated by the literature on scale separation, we perform iterative wavelet\ncoarsening to separate the lattice model into different scales. Based on this multiscale structure,\nwe design a new functional hierarchical tensor ansatz using a hierarchical tree topology, whereby\ninformation on the finer scale is further away from the root node of the tree. Our experiments show\nthat the numerical rank of typical lattice models is significantly lower under appropriate wavelet\ntransformation. Furthermore, we show that our proposed model allows one to model challenging\nGaussian field models and Ginzburg-Landau models.\nKey word. High-dimensional density estimation; Functional tensor network; Wavelet transform.\nMSC codes. 15A69, 65F99, 42C40\n1. Introduction. Estimating high-dimensional density is one of the most im-\nportant tasks in applied sciences.\nIt is used in various fields of data science and\nscientific computing. During density estimation, one is given a collection of samples\nfrom a target distribution p: Rd →R, and the goal is to use the samples to obtain an\naccurate approximation of p from a chosen function class.\nThe functional tensor network (FTN) has recently emerged as an appealing func-\ntion class to model probability distributions. The key feature of an FTN model is that\nits normalization constant calculation can be done via an efficient tensor contraction\ndiagram, which is especially important for obtaining normalized density functions. For\nFTN models based on line graph [49, 6] and the binary tree graph [27, 66], calculating\nthe normalization constant has an O(d) scaling, where d stands for the dimension of\nthe system. In contrast, performing an accurate numerical integration of parametric\nfunctions typically suffers from an exponential scaling in d.\nThis work focuses on distributions coming from lattice models in 1D and 2D, for\nwhich FTN models are known to be suitable numerical candidates for density esti-\nmation tasks [10, 66]. Lattice models in 1D and 2D often come from discretizing a\nstochastic process. For example, let {Xt : t ∈[0, 1]} be a Gaussian process, e.g., the\nOrnstein–Uhlenbeck process [70], and then x = (Xi/d)d\ni=1 constitutes a 1D lattice\nmodel. Similar constructions can be done in 2D or higher physical dimensions. Over-\nall, discretization of smaller domains leads to lattice models with stronger site-wise\ncoupling.\nThis work is primarily motivated by difficulties in using existing tensor network\nmodels to tackle lattice representations with strong coupling. In practice, a 1D lattice\n∗Corresponding author.\nInstitute for Computational and Mathematical Engineering (ICME),\nStanford University, Stanford, CA 94305, USA. (xuntang@stanford.edu)\n†Department of Mathematics and Institute for Computational and Mathematical Engineering\n(ICME), Stanford University, Stanford, CA 94305, USA. (lexing@stanford.edu)\nFunding: X.T. and L.Y. are supported by AFOSR MURI award FA9550-24-1-0254.\n1\narXiv:2502.20655v1  [math.NA]  28 Feb 2025\n\n\n2\nX. TANG, L. YING\nmodel is often of the form\np(x1, . . . , xd) = exp\n\n−α\n2\nX\n|i−j|=1\n(xi −xj)2 −\nd\nX\ni=1\nVi(xi)\n\n,\nand the case where α is large leads to situations where one needs to increase the\nrank of the FTN models to represent p faithfully. To give p a more efficient low-rank\napproximation, we propose to use the wavelet transformation, which has found great\nuses in image analysis and signal processing [13, 45]. A wavelet transformation can\nlocalize information both in frequency and in space, which allows one to model p with\na smaller rank requirement. The wavelet transformation is orthogonal, and therefore,\nit is an invertible and measure-preserving transformation.\nFor a 1D lattice model with d = 2L, successive wavelet coarse-graining transforms\nthe physical x space into the wavelet coordinate (ck,l)k,l. The l = −1, . . . , L−1 index\nis the resolution scale index, with larger l standing for information on a finer scale.\nThe k = 1, . . . , 2max(l,0) index is the location index at scale l. The recent work in [46]\nshows that a multiscale representation with successive wavelet-based coarse-graining\ncan well approximate 1D and 2D lattice models. Moreover, for practical lattice models,\nthe analysis in [46] shows that the interaction between the variables ck,l and ck′,l′ is\nonly significant when (k, l) is close to (k′, l′) in both indices. The localized interaction\nis an instance of the well-celebrated scale separation phenomenon in renormalization\ngroup (RG) theory [71, 72, 38].\nThe proposed iterated wavelet transform, commonly referred to as wavelet mul-\ntiresolution approximation [45], calls for a novel design of FTN topology. The trans-\nformed variable c = (ck,l)k,l has a natural hierarchical structure suitable for a binary\ntree graph reminiscent of the functional hierarchical tensor model [66]. However, the\ncurrent FHT model places the external bond on the leaves nodes of a binary tree,\nwhich is less suitable. Therefore, we introduce a new functional tensor network struc-\nture where the variables correspond to nodes on a modified binary tree graph. We\nrefer to the proposed function class as the functional hierarchical tensor under a wave-\nlet basis (FHT-W). We introduce the density algorithm for FHT-W, and for future\nreference, we go through the subroutine of density estimation for FTN models under\narbitrary tree structures. Our numerical experiments show that the FHT-W ansatz\ncan successfully model high-dimensional 1D and 2D lattice models.\n1.1. Motivating example. For FTN models, the most important model capac-\nity parameter is the maximal internal bond dimension r. For a d-dimensional FHT\nmodel, the parameter size is O(dr3), and for functional tensor train (FTT) [6] the\nparameter size is O(dr2). A coordinate system with a better low-rank structure leads\nto a significant practical speedup both in storage complexity and computational com-\nplexity. In this light, an FTN ansatz’s ability to capture a 1D or 2D lattice model\nlargely depends on whether it can represent the model with a reasonably small rank\nr.\nWe illustrate how coupling strength influences rank through a simple 2D distri-\nbution function p(x1, x2) = e−\nx2\n1\n2 −\nx2\n2\n2 −α\n2 (x1−x2)2 with α > 0. In this setting, using\nan FTT or FHT approximation for p is equivalent to the task of finding functions\nhi(x1), gi(x2) for i = 1, . . . , r so that p(x1, x2) ≈Pr\ni=1 hi(x1)gi(x2). When α is small,\nthe system has weak coupling between x1 and x2, and it is standard to show that\np admits a low-rank approximation, e.g. by a Chebyshev approximation of the ex-\nponential function [15]. However, as α increases, it becomes increasingly difficult to\n\n\nFHT UNDER WAVELET BASIS\n3\napproximate p in a low-rank representation unless one increases r with α.\nWrite\nX = (X1, X2) ∼p. One sees that X1 ∼N(0, 1+α\n1+2α) and X2|X1 = c ∼N(c,\n1\n1+2α).\nTherefore, as α →∞, one has X converging in distribution to W = (W1, W2), where\nW1 ∼N(0, 1/2) and W2 = W1. One can see that the distribution of W is singular\nand does not admit a low-rank approximation.\nWhile the large rank issue occurring for p in the limiting case might seem dis-\ncouraging for tensor network models, one can address the issue in this case by a co-\nordinate transformation. Writing (ξ1, ξ2) = ( x1−x2\n√\n2 , x1+x2\n√\n2 ), one sees that p(ξ1, ξ2) =\nexp(−(α + 1\n2)ξ2\n1) exp(−ξ2\n2\n2 ) is exactly of rank r = 1. Therefore, p in the (ξ1, ξ2) co-\nordinate always admits a low-rank representation regardless of what values of α one\nchooses.\nAn extension of the two-dimensional example is the Ornstein–Uhlenbeck (O-U)\nmodel, for which one has p(x1, . . . , xd) = exp\n\u0010\n−α\n2\nP\ni∼j(xi −xj)2 −β\n2\nPd\ni=1 x2\ni\n\u0011\n,\nwhere i ∼j if i −j = 1 mod d. One sees that the precision matrix of p is a cir-\nculant matrix, and so the model p(ξ) is separable if one takes ξ to be the discrete\nFourier transform (DFT) of x.\nBy the illustrated examples, one can see that performing an FTN density estima-\ntion of O-U models is practically much more efficient after a Fourier transformation.\nThe Fourier transform is generally not sufficient for a low-rank parameterization, as\nDFT transforms local nonlinear terms into global interaction terms in the frequency\nspace.\nIn contrast, the wavelet transformation is known to give lattice models a\nbetter low-rank structure [46]. In Section 4, we exhibit strong numerical evidence\nthat the probability distribution under a wavelet-transformed coordinate leads to a\nsignificantly smaller numerical rank to capture the target distribution. In fact, our\nexperiments in Section 4 show that the numerical rank for practical 2D lattice models\ncan be reduced by up to a factor of five if one uses the wavelet basis.\n1.2. Discussion on high-dimensional density estimation. In addition to\nthe key role of density estimation in generative modeling [22], the density estimation\nsubroutine can often aid one in solving high-dimensional partial differential equations\n(PDE). For the Fokker-Planck equation, which models the evolution of probability\ndensities of a particle undergoing advection and diffusion, one can obtain an approx-\nimate PDE solution by using density estimation in a particle-based framework [66].\nIn the Kolmogorov backward equation, which models the evolution of the conditional\nexpectation function, one can use density estimation subroutines to construct the so-\nlution operator, which gives the PDE solution for arbitrary terminal conditions [65].\nDensity estimation is a difficult task. Given samples from a high-dimensional\ndistribution p, it is well-known that nonparametric density estimation requires an ex-\nponential number of samples in the dimension d [69]. Traditional parametric function\nclasses such as tree-based graphical models [37] do admit efficient density estima-\ntion through maximum likelihood estimation, but its representation power is limited.\nWhile neural networks provide several new function classes for density estimation, a\nneural network often represents an unnormalized likelihood function, as is true for\nenergy-based models [31, 42] and diffusion models [58].\nTherefore, a key element\nin density estimation research lies in designing new parametric function classes that\npossess the ability to produce a normalized likelihood function.\nWhen one applies density estimation to lattice models, the goal is to model a prob-\nability distribution under moderate computational resources. Gaussian field models\ncan be well-approximated by fitting a multivariate normal distribution with maxi-\n\n\n4\nX. TANG, L. YING\nmum likelihood. However, it is often true that the lattice model is multi-modal and\nis hence ill-suited to be modeled by normal distributions. An example of a lattice\nmodel with multimodality is the Ginzburg-Landau (G-L) model used for studying\nthe phenomenological theory of superconductivity [20, 34, 35, 17]. In particular, the\nGinzburg-Landau model contains a double-well term, which makes a multivariate\nnormal distribution unsuitable to fit such models.\nIn the 2D G-L model, one can intuitively think of a particle taking the form of a\nfield x(a): [0, 1]2 →R. To obtain a lattice model, one discretizes the unit square [0, 1]2\ninto a grid of d = m2 points {(ih, jh)} for h =\n1\nm+1 and 1 ≤i, j ≤m. The discretiza-\ntion of the field at this grid is the d-dimensional vector x = (x(i,j))1≤i,j≤m, where\nx(i,j) = x(ih, jh). The field dynamics under this discretization is the overdamped\nLangevin dynamics under the following potential function\n(1.1)\nV (x) = V (x(1,1), . . . , x(m,m)) := λ\n2\nX\nv∼w\n\u0012xv −xw\nh\n\u00132\n+ 1\n4λ\nX\nv\n\u00001 −x2\nv\n\u00012 ,\nwhere v and w are Cartesian grid points and v ∼w if and only if they are adjacent.\nOne can see that the term (1 −xv)2 has two favorable configurations of xv = 1 and\nxv = −1, which is the double-well term that makes multivariate normal distributions\nill-suited to approximate G-L models. Moreover, a finer spatial resolution increases\nthe 1/h term in Equation (1.1), which leads to stronger site-wise coupling.\nThe work in [66] shows that FHT works well for 1D-3D Ginzburg-Landau models\nwith moderate coupling. However, when the G-L model has a strong coupling term\nand a weak double-well term, one can see that the necessary rank to approximate the\nG-L model can be quite large. Therefore, the proposed FHT-W ansatz is a reasonable\nway to utilize the efficiency of the FTN models while successfully dealing with strong\ncoupling situations in the G-L model.\nFor general distributions, it is less clear how one can apply a coordinate transfor-\nmation to provide a good low-rank structure. However, given the inherent difficulty\nof density estimation tasks, FTN models remain one of the most expressive existing\nfunction classes to generate un-normalized densities, and combining FTN with coor-\ndinate transformation strategies can strongly improve the expressive power of existing\nFTN models [14, 52].\n1.3. Related work.\nNeural network models in density estimation. Neural networks have been used\nin density estimation-related tasks within generative modeling and variational infer-\nence. Examples include restricted Boltzmann machines (RBMs) [30, 55], energy-based\nmodels (EBMs) [31, 42, 26], variational auto-encoders (VAEs) [16, 41], generative\nadversarial networks (GANs) [22], normalizing flows [62, 61, 53, 50], diffusion and\nflow-based models [57, 74, 59, 60, 58, 32, 2, 44, 43, 1]. Among these methods, only\nnormalizing flow can produce a normalized probability density. We remark that nor-\nmalizing flow uses a neural network to parameterize a reversible flow map, which\ntypically leads to reduced expressivity compared to other approaches. Moreover, the\nback-propagation of the Jacobian term during the maximum likelihood training is\nquite difficult compared to diffusion models.\nFunctional tensor network methods for density estimation. The functional tensor\nnetwork has been extensively explored as an ansatz for density estimation. Density\nestimation with functional tensor networks has been done in functional tensor trains\n[48, 36, 10] and functional hierarchical tensors [66, 65, 56]. The aforementioned de-\nvelopment of density estimation subroutines in the continuous case is heavily inspired\n\n\nFHT UNDER WAVELET BASIS\n5\nby the discrete counterpart in the case of tensor train models and hierarchical tensor\nmodels [29, 11, 21, 7, 24, 64, 52, 40, 51].\nWavelet transformation in lattice models. Wavelet analysis has proven to be a\ncritical tool in understanding lattice models through its close connection with renor-\nmalization groups [4, 3]. In [18], a multiscale entanglement renormalization ansatz is\nconstructed for the 1D lattice system of fermions through the Daubechies D4 wave-\nlets, which results in a remarkably accurate approximation of the ground state of\nthe critical Ising model, and a subsequent work in [28] considers the case of wavelet-\nbased ground state approximation in 2D lattice fermionic systems. For continuous\nlattice models considered in this work, one of the main theoretical foundations for\nusing wavelets for the Ginzburg-Landau model is that the wavelet transformation\ncan nearly diagonalize the Laplacian term [5, 47]. In [46], it is further shown that\nsufficiently high-order wavelet methods can localize the double well term in the G-L\nmodel.\nWavelet transformation in generative modeling. Wavelet has been instrumental\nin generative modeling, especially in the setting of image synthesis.\nWavelet has\nbeen used in normalizing flow [73, 12], diffusion models [25, 39], energy-based models\n[46], and auto-encoders [8].\nOther related works include [75, 19, 33, 54].\nAmong\nthese, this work bears the most resemblance to the work in [46], which performs\ngenerative modeling under a wavelet representation using a multiscale energy-based\nmodel. Compared to [46], while this work likewise performs a wavelet transformation\non the input data, this work uses a tensor network model so that one can perform\ndensity estimation under a wavelet representation.\n1.4. Outline. This work is organized as follows. Section 2 details the density\nestimation subroutine for FTN models under general tree topology. Section 3 cov-\ners the proposed functional hierarchical tensor used for wavelet-transformed samples.\nSection 4 presents numerical experiments that show wavelet transformation decreases\nthe numerical rank of lattice models. Section 5 shows the performance of the proposed\nansatz on practical 1D and 2D lattice models with strong coupling.\n2. Density estimation under general tree topology. This section details\nthe sketching-based density estimation subroutine for functional tensor network mod-\nels under general tree topology. The FHT-W ansatz to be introduced in Section 3 is a\nspecial case that takes on a specific tree topology. This generalized setting allows for\na cleaner presentation and accommodates other possible tree topologies in potential\nfuture works. The introduced subroutine is also a generalized version of the density\nestimation routine considered in [36, 10, 66].\nNotation. For notational compactness, we introduce several shorthand notations\nfor simple derivation. For n ∈N, let [n] := {1, . . . , n}. For an index set S ⊂[d], we\nlet xS stand for the subvector with entries from index set S. The symbol ¯S stands\nfor the set-theoretic complement ¯S = [d] −S. For a d-tensor D: Qd\nj=1[nj] →R, we\nuse D(iS; i ¯S): [Q\nj∈S nj] × [Q\nj∈¯S nj] →R to denote the unfolding matrix by merging\nthe indices in iS to the row index and merging the indices in i ¯S to the column index.\nFunctional tensor network. We first introduce the general functional tensor net-\nwork (FTN) ansatz. Let {ψi,j}nj\ni=1 denote a collection of orthonormal function basis\nover a single variable for the j-th coordinate, and let D: Qd\nj=1[nj] →R be a d-tensor.\nThe functional tensor network is the d-dimensional function defined by the following\n\n\n6\nX. TANG, L. YING\nequation:\n(2.1) p(x1, . . . , xd) =\nX\nik∈[nk],k∈[d]\nDi1,...,idψi1,1(x1) · · · ψid,d(xd) =\n*\nD,\nd\nO\nj=1\n⃗Ψj(xj)\n+\n,\nwhere ⃗Ψj(xj) = [ψ1,j(xj), . . . , ψn,j(xj)] is an n-vector encoding the evaluation for the\nxj variable over the entire j-th functional basis. The definition is general, and one\ntypically asserts special structures in D to ensure that calculations in p are efficient.\nTree structure notation. We provide notations for a tree graph. A tree graph\nT = (V, E) is a connected undirected graph without cycles.\nAn undirected edge\n{v, v′} ∈E is written interchangeably as (v, v′) or (v′, v). For any v ∈V , define N(v)\nto be the neighbors of v. Moreover, define E(v) as the set of edges incident to v. For\nan edge e = (k, w), removing the edge e in E results in two connected components\nI1, I2 with I1 ∪I2 = V . For any (w, k) ∈E, we use k →w to denote the unique\nconnected component among I1, I2 which contains k as a node.\nTree-based FTN. We introduce the tree-based functional tensor network ansatz\nto be used for density estimation. Let T = (V, E) be a tree graph and let Vext be the\nsubset of vertices in T whose associated tensor component admits an external bond.\nTo adhere to the setting of d-dimensional functions, let |Vext| = d. The total vertex\nsize ˜d = |V | is equal to d only when the tensor network has no internal nodes. We\ndefine the tree-based functional tensor network in Definition 2.1. The tensor network\nis illustrated in Figure 1\nDefinition 2.1. (Tree-based functional tensor network) Suppose one has a tree\nstructure T = (V, E) and the corresponding ranks {re : e ∈E}. The symbol Vext is\nthe subset of V which contains external bonds. For simplicity, we apply a labeling of\nnodes so that V = [ ˜d] and Vext = [d].\nThe tensor component at k ∈V is denoted by Gk. Let deg(k) stand for the degree\nof k in T. When k ∈Vext, Gk is defined as a (deg(k) + 1)-tensor of the following\nshape:\nGk : [nk] ×\nY\ne∈E(k)\n[re] →R.\nWhen k ̸∈Vext, Gk is defined as a deg(k)-tensor of the following shape:\nGk :\nY\ne∈E(k)\n[re] →R.\nA d-tensor D is said to be a tree tensor network defined over the tree T and\ntensor components {Gk} ˜d\nk=1 if\n(2.2)\nD(i1, . . . , id) =\nX\nαE\nd\nY\nk=1\nGk\n\u0000ik, αE(k)\n\u0001\n˜d\nY\nk=d+1\nGk\n\u0000αE(k)\n\u0001\n.\nMoreover, for j = 1, . . . , d, let {ψi,j}nj\ni=1 denote a collection of orthonormal func-\ntion basis over a single variable for the j-th coordinate.\nA tree-based FTN is the\nd-dimensional function defined by Equation (2.1) with D as the coefficient tensor.\nSpecifically, one has\n(2.3)\np(x1, . . . , xd) =\nX\nαE\nd\nY\nk=1\n nk\nX\nik=1\nGk\n\u0000ik, αE(k)\n\u0001\nψik,k(xk)\n!\n˜d\nY\nk=d+1\nGk\n\u0000αE(k)\n\u0001\n.\n\n\nFHT UNDER WAVELET BASIS\n7\n(a)\n(b)\nFig. 1: (A) A tree structure T = (V, E) with V = Vext = {1, . . . , 10}. (B) Tensor\nDiagram representation of a tree tensor network over T.\nSketched linear equation. We cover the procedure to perform density estimation\nover a tree-based FTN ansatz. The input is a collection of independent empirical sam-\nples\nn\nx(j) :=\n\u0010\nx(j)\n1 , . . . , x(j)\nd\n\u0011oN\nj=1 sampled according to an underlying distribution p.\nIn this work, the input would be samples from lattice models after the iterative wave-\nlet transformation. We assume that p: Rd →R is a tree-based FTN defined over D,\nand D is defined over tensor components {Gk} ˜d\nk=1 as in Definition 2.1.\nWe shall derive the linear equation for each Gk. Let e1, . . . , edeg(k) be all edges\nincident to k, and let ej = (vj, k) for j = 1, . . . , deg(k). For each vj, we construct\nDvj→k by contracting all tensor components Gw for w ∈vj →k. When k ∈Vext = [d],\none has the following linear equation:\n(2.4)\nD(i1, . . . , id) =\nX\naE(k)\nGk(ik, aE(k))\ndeg(k)\nY\nj=1\nDvj→k(αej, ivj→k∩[d]),\nand the equation for k = d + 1, . . . , ˜d can be obtained from Equation (2.4) by simply\nomitting the ik index in the right hand side of the equation.\nWe first consider the case for k ∈Vext = [d].\nWe see that Equation (2.4) is\nexponential-size. Thus, we apply sketching to obtain a linear system of reasonable\nsize. Let E(k) = {ej = (vj, k)}deg(k)\nj=1\n. For j = 1, . . . , deg(k), we let ˜rej > rej be\nsome integer, and we introduce sketch tensors Svj→k : Q\nl∈vj→k∩[d][nl] × [˜rej].\nBy\ncontracting Equation (2.4) with Ndeg(k)\nj=1\nSvj→k, one obtains the following sketched\nlinear equation for Gk:\n(2.5)\nBk(ik, βE(k)) =\nX\nαE(k)\ndeg(k)\nY\nj=1\nAvj→k(βej, αej)Gk(ik, αE(k)),\nwhere Bk is the contraction of D with Ndeg(k)\nj=1\nSvj→k, and each Avj→k is the con-\ntraction of Dvj→k with Svj→k. Specifically, the contraction with each Svj→k is done\nby contracting the ivj→k∩[d] multi-index. Writing Ak = Ndeg(k)\nj=1\nAvj→k, one sees that\nEquation (2.5) can be simplified to the linear equation\nX\nαE(k)\nAk(βE(k), αE(k))Gk(αE(k), ik) = Bk(βE(k), ik).\n\n\n8\nX. TANG, L. YING\nThe case for k = d + 1, . . . , ˜d is similar. We have\nD(i1, . . . , id) =\nX\naE(k)\nGk(aE(k))\ndeg(k)\nY\nj=1\nDvj→k(αej, ivj→k∩[d]),\nfor which we apply the contraction with Ndeg(k)\nj=1\nSvj→k. The formula for Avj→k does\nnot change, and the formula for Bk is\n(2.6)\nBk(βE(k)) =\nX\nαE(k)\ndeg(k)\nY\nj=1\nAvj→k(βej, αej)Gk(ik, αE(k)).\nThus, for k = d + 1, . . . , ˜d, the sketched linear equation is\nX\nαE(k)\nAk(βE(k), αE(k))Gk(αE(k)) = Bk(βE(k)).\nObtain Bk by samples. We shall solve Gk by the aforementioned linear equation\nAkGk = Bk. One sees that in fact Bk is fully specified. We show how to approximate\nBk by the samples\nn\nx(j) :=\n\u0010\nx(j)\n1 , . . . , x(j)\nd\n\u0011oN\nj=1.\nTo perform the sketching with samples as input, we construct each sketch tensor\nSv→k implicitly with a continuous sketch function sv→k : R|v→k| × [˜r(v,k)] →R. The\nfunction sv→k is required to be easy to evaluate, and sv→k is related to Sv→k through\nthe following construction:\nsv→k(xv→k∩[d], β) =\nX\nij∈[nj],j∈v→k∩[d]\nSv→k(iv→k∩[d], β)\nY\nj∈v→k∩[d]\nψij,j(xj).\nWe assume that one is given sv→v′ over all v, v′. For k = 1, . . . , d, one has\nBk(ik, βE(k)) =\nZ\nRd p(x1, . . . , xd)ψik,k(xk)\nY\nv∈N(k)\nsv→k(xv→k∩[d], β(v,k)) d w\n=EX∼p\n\nY\nv∈N(k)\nsv→k(Xv→k∩[d], β(v,k))ψik,k(Xk)\n\n,\nwhere the first equality uses the orthonormality of the function basis ⃗Ψj.\nTherefore one can approximate Bk with samples by\n(2.7)\nBk(ik, βE(k)) ≈1\nN\nN\nX\nj=1\n\nY\nv∈N(k)\nsv→k(x(j)\nv→k∩[d], β(w,k))ψik,k(x(j)\nk )\n\n.\nWhen k = d + 1, . . . , ˜d, one likewise obtains the sample-wise approximation\n(2.8)\nBk(βE(k)) ≈1\nN\nN\nX\nj=1\n\nY\nv∈N(k)\nsv→k(x(j)\nv→k∩[d], β(v,k))\n\n.\n\n\nFHT UNDER WAVELET BASIS\n9\nObtain Ak. Equation (2.5) is the linear equation we shall use for Gk, and so one\nneeds to obtain Bk as well as each Avj→k. One sees that each Avj→k depends on the\ngauge degree of freedom for Dvj→k. Let v ∈N(k) be one of the vj. To fully specify\nAv→k, we calculate the contraction of D with Sk→v\nN Sv→k, thereby obtaining\n(2.9)\nZ(k,v)(β, γ) =\nX\ni[d]\nD(i1, . . . , id)Sk→v(ik→v, β)Sv→k(iv→k, γ).\nOne performs the singular value decomposition (SVD) on Z(k,v), resulting in\nZ(k,v)(β, γ) = P\nµ U(β, µ)W(µ, γ), and the choice of Ak→v = U and Av→k = W\nforms a unique and consistent choice of gauge between the pair (Dv→k, Dk→v). As\nobtaining U, W requires merging the middle factor of the SVD to either the left or\nright factor, one can use the root information to specify U, W fully. In particular, if\nv is the parent to k, then one takes U to be the left orthogonal factor in the SVD. If\nk is the parent to v, then one takes W to be the right orthogonal factor in the SVD.\nFinally, Z(k,v) requires a contraction with D, and it can be obtained by samples\nby\n(2.10)\nZ(k,v)(β, γ) ≈1\nN\nN\nX\nj=1\n\u0010\nsk→v(x(j)\nk→v∩[d], β)sv→k(x(j)\nv→k∩[d], γ)\n\u0011\n.\nSummary. We summarize the density estimation algorithm in Algorithm 2.1.\nAlgorithm 2.1 Tree-based FTN density estimation.\nRequire: Sample {x(j)}N\nj=1.\nRequire: Tree T = (V, E) with V = [ ˜d] and Vext = [d].\nRequire: Chosen variable-dependent function basis {⃗Ψj}j∈[d].\nRequire: Collection of sketch tensors {Sv→k, Sk→v} and target internal ranks {r(k,v)}\nfor each edge (k, v) ∈E.\n1: for each edge (k, v) in T do\n2:\nObtain Z(k,v) by Equation (2.10).\n3:\nObtain Ak→v as the left factor of the best rank r(k,v) factorization of Z(k,v)\n4:\nObtain Av→k as the right factor of the best rank r(k,v) factorization of Z(k,v).\n5: end for\n6: for each node k in T do\n7:\nif node k contains an external bond then\n8:\nObtain Bk by Equation (2.7).\n9:\nelse\n10:\nObtain Bk by Equation (2.8).\n11:\nend if\n12:\nWith E(k) = {(vj, k)}deg(k)\nj=1\n, collect each Avj→k.\n13:\nObtain Gk by solving the over-determined linear system (Ndeg(k)\nj=1\nAvj→k)Gk =\nBk.\n14: end for\n3. Main formulation. This section details the functional hierarchical tensor\nunder a wavelet basis (FHT-W) for 1D and 2D lattice models. Subsection 3.1 covers\nthe preliminary information on 1D wavelet transformation. Subsection 3.2 details the\n\n\n10\nX. TANG, L. YING\nmodel architecture for FHT-W and how one applies the ansatz to 1D lattice models.\nSubsection 3.3 details how one applies the ansatz to 2D lattice models. Subsection 3.4\ndiscusses extensions of the FHT-W ansatz to other settings.\n3.1. Background on 1D wavelet transformation. By possibly performing\ndimension padding, we assume without loss of generality that the dimension d satisfies\nd = 2L. A 1D lattice model has a natural ordering, and we denote the variables by\nx = (x1, . . . , x2L). Typically, after appropriate normalization, one can think of a 1D\nlattice model as the discretization of a model on the 1D field x(a): [0, 1] →R. Thus,\none can think of each xj for j = 1, . . . , 2L as approximating the field x at length scale\n2−L.\nThe wavelet approximation proposes to perform an iterative coarse-graining of the\nlattice model in (x1, . . . , x2L) into orthogonal signals at different length scales. Thus,\nthe input to the transform is data at scale 2−L, and the output to the transform is the\ndata at scale 2−l for l = L −1, . . . , 0. This iterative coarse-graining approach gives\nrise to the multiscale nature of this work.\nWe explain the multiresolution wavelet approximation procedure, and more de-\ntails can be found in [45].\nFor concreteness, we go through the concept with the\nsimple Haar wavelet. In the first step of the coarse-graining process, the wavelet filter\ntransforms the x variable into two 2L−1-dimensional variables yL−1 = (yj,L−1)j∈[2L−1]\nand cL−1 = (cj,L−1)j∈[2L−1]. which are defined by the following equation\nyj,L−1 = x2j−1 + x2j\n√\n2\n,\ncj,L−1 = x2j−1 −x2j\n√\n2\n.\nThe variable yL−1 is the scaling coefficient of x at length scale 2−(L−1), and cL−1\nis the detail coefficient of x at length scale 2−(L−1). One can see that the mapping\nx →(yL−1, cL−1) is an invertible and orthogonal transformation.\nRepeating the same approach, for l = L−1, . . . , 1, one performs wavelet transfor-\nmation on yl 2l−1-dimensional variables yl−1 and cl−1 given by the following equation\nyj,l−1 = y2j−1,l + y2j,l\n√\n2\n,\ncj,l−1 = y2j−1,l −y2j,l\n√\n2\n.\nFinally, we note that y1 is 2-dimensional the procedure ends at y1 →(y0, c0). For\nnotational compactness, we denote c−1 := y0.\nThe wavelet multiresolution approximation transforms the x = (x1, . . . , x2L) vari-\nable into the variables c = (cL−1, . . . , c1, c0, c−1), where each cl is a 2l-dimensional\ncontaining the detail of x at scale 2−l, and c−1 = y0 is the coarse-grained approxima-\ntion of x at scale 1. The mapping x →(ck,l)l=L−1,...,−1,k=1,...,2max(l,0) is an invertible\nand orthogonal transformation.\nOne can also consider other wavelets. For numerical experiments used in this\nwork, we use the Daubechies D4 wavelet [13], which performs the transformation yl →\n(yl−1, cl−1) with a different filter. In the discrete wavelet transform procedure, we use\nperiodic signal extension mode so that any 2l-dimensional variable is transformed to\ntwo 2l−1-dimensional variables [45]. Extension of this model to pther discrete wavelet\ntransform methods are discussed at the end of this section.\n3.2. Model architecture. For d = 2L, an FHT-W ansatz is a functional tensor\nnetwork f taking the variable c = (ck,l)l=L−1,...,−1,k=1,...,2max(l,0). We propose FHT-W\nas a tree-based FTN based on a tree structure T = (V, E). We illustrate the ansatz\nin Figure 2.\n\n\nFHT UNDER WAVELET BASIS\n11\nWe first specify the vertex of T.\nFor the vertex set, we define the external\nnodes to be Vext = {vk,l}l∈[L−1]∪{0,−1},k∈[2max(l,0)].\nThe total vertex set is V\n=\nVext ∪{wk,l}l∈[L−2]∪{0},k∈[2l]. The vertex vk,l corresponds to the variable ck,l, and\nthe variables wk,l is an internal node inserted at each level l = 0, . . . , L −2.\nThe edge set of T is E = E1 ∪E2 ∪{(v1,0, v1,−1)}, where E1 = {(wk,l, vk,l)}wk,l∈V\nand E2 = {(wk,l, v2k−1,l+1), (wk,l, v2k,l+1)}wk,l∈V . The definition of E shows that each\nvariable c2k−1,l+1 and c2k,l+1 is connected to the variable ck,l through an internal node\nwk,l. This construction for T ensures that ck,l is placed closed to ck′,l′ when (k, l) is\nclose to (k′, l′).\nThe above description fully characterizes the tree T = (V, E). The FHT-W ansatz\nf is a tree-based FTN based on T. The coefficient tensor is D, and Section 2 specifies\nthe procedure to obtain f when one f is a probability density function and one has\naccess to samples of f.\nFig. 2: Illustration of the FHT-W ansatz for L = 4. Each vk,l represents the tensor\ncomponent Gvk,l at the external node vk,l, and the vk,l corresponds to the variable\nck,l in the wavelet transformation. Each wk,l represents the tensor component Gwk,l\nat the internal node wk,l.\nWe explain some of the intuitions behind the design of the FHT-W ansatz. The\ndesign of tensor networks for applied sciences is primarily tailored to the problem\nstructure. From the analysis of the O-U model and the ϕ4 model in [46], one sees\nthat a large class of lattice models in practice has the property that ck,l has limited\ninteraction with ck′,l′ unless (k, l) is close to (k′, l′) in both indices. The goal to design\nT is to place strongly coupled variables at closer locations on T. Therefore, a binary\ntree structure in FHT is the optimal shape to place the variables c = (ck,l)k,l. To\nensure maximal efficiency, we insert internal nodes wk,l, which is done so as to prevent\nany tensor component in FHT-W from having more than three indices. In practice,\n\n\n12\nX. TANG, L. YING\nchl\n2,(1,1)\nchh\n2,(1,1)\nchl\n2,(1,2)\nchh\n2,(1,2)\nchl\n2,(2,1)\nchh\n2,(2,1)\nchl\n2,(2,2)\nchh\n2,(2,2)\nchl\n2,(1,3)\nchh\n2,(1,3)\nchl\n2,(1,4)\nchh\n2,(1,4)\nchl\n2,(2,3)\nchh\n2,(2,3)\nchl\n2,(2,4)\nchh\n2,(2,4)\nchl\n2,(3,1)\nchh\n2,(3,1)\nchl\n2,(3,2)\nchh\n2,(3,2)\nchl\n2,(4,1)\nchh\n2,(4,1)\nchl\n2,(4,2)\nchh\n2,(4,2)\nchl\n2,(3,3)\nchh\n2,(3,3)\nchl\n2,(3,4)\nchh\n2,(3,4)\nchl\n2,(4,3)\nchh\n2,(4,3)\nchl\n2,(4,4)\nchh\n2,(4,4)\nclh\n2,(1,1)\nclh\n2,(1,2)\nclh\n2,(2,1)\nclh\n2,(2,2)\nclh\n2,(1,3)\nclh\n2,(1,4)\nclh\n2,(2,3)\nclh\n2,(2,4)\nclh\n2,(3,1)\nclh\n2,(3,2)\nclh\n2,(4,1)\nclh\n2,(4,2)\nclh\n2,(3,3)\nclh\n2,(3,4)\nclh\n2,(4,3)\nclh\n2,(4,4)\nchl\n1,(1,1)\nchh\n1,(1,1)\nchl\n1,(1,2)\nchh\n1,(1,2)\nchl\n1,(2,1)\nchh\n1,(2,1)\nchl\n1,(2,2)\nchh\n1,(2,2)\nclh\n1,(1,1)\nclh\n1,(1,2)\nclh\n1,(2,1)\nclh\n1,(2,2)\nchl\n0,(1,1)\nchh\n0,(1,1)\nclh\n1,(1,1)\ny0\nFig. 3: Illustration of the chosen tree structure for 2D wavelet iterative coarsening\nfor a d = 82 lattice system. We use circles to illustrate nodes at q = 2, 0, and we\nuse squares to illustrate nodes at q = 1. Internal nodes on the tree T are omitted for\nsimplicity.\na tensor component with four indices is less efficient.\n3.3. Extension to 2D lattice. We now proceed to the main formulation for\n2D lattice models. In this case, we denote the variables by x = (x(i,j))1≤i,j≤m, where\nd = m2.\nWithout loss of generality, we assume that m = 2L so that d satisfies\nd = 22L. In this case, one can think of the 2D lattice model as the discretization of a\nmodel on the function field x(a): [0, 1]2 →R. Thus, one can think of each x(i,j) for\ni, j = 1, . . . , 2L as approximating the field x at length scale 2−L.\n2D wavelet coarse-graining. In the multidimensional case, we construct the asso-\nciated filters by separable products of 1D wavelet filters. To apply the iterative wavelet\ncoarse-graining, we perform 1D wavelet filter transformation on x = (x(i,j))1≤i,j≤m\nfirst along the horizontal i direction followed and then along the vertical j direction.\nAs in the 1D case, we consider the periodic signal extension mode, and examples of the\n1D wavelet filter include the Haar filter and the D4 filter. This procedure transforms\nthe x variable into four 22L−2-dimensional variables yL−1, clh\nL−1, chl\nL−1, chh\nL−1, which are\nrespectively obtained by applying (low, low), (low, high), (high, low), (high, high)\nfilters on the (i, j) directions.\nSubsequently, at each q = L −1, . . . , 1, the 2D wavelet filter transforms yq into\nfour 22q−2-dimensional variables yq−1, clh\nq−1, chl\nq−1, chh\nq−1 by applying (low, low), (low,\nhigh), (high, low), (high, high) filters on the (i, j) directions. At q = 1, we have y1\ntransforming into y0, clh\n0 , chl\n0 , chh\n0 .\nTree architecture. In this case, we use the tree T defined in Subsection 3.2 with\n2L levels. We give the 2D wavelet-transformed variables a hierarchical structure c =\n(c2L−1, . . . , c0, c−1) with cl = (c1,l, . . . , c2l,l), which would allow one to use the FHT-W\nansatz in Subsection 3.2 and the density estimation subroutine in Algorithm 2.1.\nWe see that each clh\nq , chl\nq , chh\nq\nvariable is naturally indexed by (clh\nq )i,j, (chl\nq )i,j,\n(chh\nq )i,j where 1 ≤i, j ≤2q. At the coarsest level l = −1, 0, 1, we define c−1 := y0,\n\n\nFHT UNDER WAVELET BASIS\n13\nc0 := clh\n1,0 and c1 := (chl\n0 , chh\n0 ). On level l = 2q, 2q + 1, for each index i, j ∈[2q], one\nperforms a length q binary expansion i = a1 . . . aq and j = b1 . . . bq. We take k to be\nthe integer taking the length 2q binary expansion k = a1b1 . . . aqbq. We perform the\nvariable identification in clh\nq , chl\nq , chh\nq\nby defining ck,2q := (clh\nq )i,j, c2k−1,2q+1 = (chl\nq )i,j,\nand c2k,2q+1 := (chh\nq )i,j. The corresponding tree structure is illustrated in Figure 3.\nWe give some intuition on how this hierarchical structure is designed. The con-\nstructed structure on ((clh\nq , chl\nq , chh\nq )L−1\nq=0 ) identifies c2q with clh\nq and identifies c2q+1 with\n(chl\nq , chh\nq ). Moreover, an appropriate flattening is applied so that the aforementioned\nvariable identification remains consistent. Following the variable identification, the\nillustration in Figure 3 shows that (clh\nq )i,j, (chl\nq )i,j and (chh\nq )i,j are placed on nodes\nclose to each other on T.\nFurthermore, this flattening scheme allows (clh\nq )i,j and\n(clh\nq )i′,j′ to be closer on the binary tree T if (i, j) is close to (i′, j′), and the same is\ntrue for chl\nq and chh\nq . Therefore, the variable identification procedure specified by the\nflattening procedure gives each variable in ((clh\nq , chl\nq , chh\nq )L−1\nq=0 ) an appropriate location\non T. While the geometric structure of T cannot reflect all strongly coupled variable\npairs, our construction retains the global 2D structure quite well.\n3.4. Extensions.\nExtension to other wavelet filters. When one calculates the discrete wavelet trans-\nform a variable yl into two variables (yl−1, cl−1), it is in general unclear if the dimen-\nsion of (yl−1, cl−1) matches with that of yl, as the dimension of (yl−1, cl−1) might\nexceed that of yl. For example, if one uses the Daubechies D4 wavelet with the zero\npadding as the signal extension mode, then (yl−1, cl−1) exceeds the dimension of yl by\ntwo due to boundary effects. In this case, one can incorporate these extra variables\nby inserting more nodes to T in Figure 2. In addition, for general discrete wavelet\ntransform, one does not require the variable set x = (x1, . . . , xd) to satisfy d = 2L.\nIn such cases, one can use an incomplete binary tree by pruning certain branches in\nFigure 2. For both cases, one can perform density estimation based on Section 2.\nExtensions to the Kolmogorov backward equation. In the case of FHT, by exclu-\nsively using a high-dimensional density estimation subroutine, one can construct the\nsolution operator for the Kolmogorov backward equation (KBE). The approximation\nof the operator is in an FHT format, and it is termed the operator-valued functional\nhierarchical tensor [65]. In particular, when the terminal condition f in the KBE is\nin an FHT format, one can use a simple tensor contraction between the FHT ansatz\nof f and the FHT ansatz of the approximated solution operator. The same procedure\napplies in this case. By first applying a 1D or 2D iterative wavelet coarse-graining,\none can use the FHT-W ansatz to approximate the solution operator. By constructing\nthe terminal condition f in an FHT-W format, one can perform an efficient tensor\ncontraction to obtain the approximated PDE solution.\nExtension to stochastic optimal control. The work in [63] provides a framework for\nsolving a time-dependent Hamilton-Jacobi-Bellman equation through solving a series\nof stochastic optimal control problems. When one considers optimal control problems\nin 1D or 2D lattice models, it might be beneficial to use the FHT-W ansatz, and the\nsame numerical treatment in [63] can be performed in the FHT-W formulation.\n4. Experiments on numerical rank. The key motivation for the FHT-W\nansatz is that the wavelet transformation gives a better low-rank structure to the\ntarget probability distribution p. In this section, we demonstrate through numerical\nexperiments that the probability distribution p has a substantially lower numerical\nrank under a wavelet transformation parameterization. For simplicity, we use p(x)\n\n\n14\nX. TANG, L. YING\nto denote the probability distribution as a function of the original variable x, and we\nuse p(c) to denote the probability distribution over the wavelet-transformed variable\nc. Subsection 4.1 and Subsection 4.2 go through the background and methodology of\napproximating the numerical rank of p(x) and p(c). Subsection 4.3 and Subsection 4.4\nrespectively study the numerical rank of p(x) and p(c) in 1D and 2D lattice models.\nSubsection 4.5 summarizes the numerical finding.\n4.1. Background on numerical rank. For both 1D and 2D lattice models,\nwe consider a d-dimensional probability distribution function p: Rd →R.\nWhen\np is analytic, one can approximate p under multivariate Chebyshev approximation,\nand the associated convergence is exponential in the number of basis functions [67].\nSimilarly, when p is smooth and periodic, the multivariate Fourier approximation\nadmits a convergence rate faster than any polynomial decay rate [23]. Thus, up to\nnegligible error, it is mild to assume that p admits an FTN ansatz under the equation\np(x1, . . . , xd) =\n*\nD,\nd\nO\nj=1\n⃗Ψj(xj)\n+\n,\nwhere D: Q\nj=1[nj] →R is the coefficient tensor under some appropriate and suffi-\nciently large function basis {ψi,j}nj\ni=1.\nThe concept of the coefficient tensor D allows one to consider the rank of the\nassociated function p(x). The function p(x) is said to be of rank r along the bipartition\n[d] = I ∪J if there exist functions hα : R|I| →R, tα : R|J| →R for α = 1, . . . , r so that\nthe following holds\np(x) = p(x1, . . . , xd) =\nr\nX\nα=1\nhα(xI)tα(xJ),\nwhich translates to the equivalent condition that D(iI; iJ) is of rank r.\nNumerical rank. Exact low-rankness is typically unattainable for lattice models.\nInstead, we use the concept of a numerical rank to study the approximate low-rankness\nof probability distribution functions on lattice models. The numerical rank of p is\nnaturally associated with the numerical rank of the unfolding matrix D(iI; iJ). We\ndefine the numerical rank of p along [d] = I ∪J as the smallest integer r so that there\nexists a rank-r factorization D(iI; iJ) ≈UV with ∥D(iI; iJ) −UV ∥≤ε∥D∥, and we\ntake ∥·∥to be the matrix 2-norm.\nLikewise, under the wavelet-transformed variable c = (ck,l)k,l, the function p(c)\nadmits an FTN ansatz of the form\np(c) =\n*\nF,\nO\nk,l\n⃗Ψk,l(ck,l)\n+\n,\nwhere F : Q\nk,l[nk,l] →R is the coefficient tensor.\nThe numerical rank of p(c) is\ndefined along an unfolding matrix of F.\nCase study 1: Numerical rank of a d = 2 model. We first test the numerical\nrank on the bivariate model p(x1, x2) = exp\n\u0010\n−x2\n1\n2 −x2\n2\n2 −α\n2 (x1 −x2)2\u0011\nwith α ≥0\ndetermining the coupling strength, with larger values of α leading to stronger coupling.\nThe function p is the example explored in Subsection 1.1. Under the Haar wavelet\ntransform as defined in Subsection 3.2, one has one has (c0, c−1) = ( x1−x2\n√\n2 , x1+x2\n√\n2 ),\n\n\nFHT UNDER WAVELET BASIS\n15\none sees that p(c0, c−1) = exp(−(α + 1\n2)c2\n0) exp(−\nc2\n−1\n2 ). Thus in this simple case p(c)\nis exactly of rank 1, which is illustrated by Figure 4(b). Therefore, we only need to\nstudy the numerical rank of p(x).\nThe function p(x) is effectively supported in the square Ω= [−3, 3]2. We let\n⃗Ψ1, ⃗Ψ2 be the orthogonal Legendre polynomial basis on L2([−3, 3]). We take the first\nn1 = n2 = 60 Legendre polynomials for each variable, and we obtain the coefficient\ntensor D ∈R60×60 of p by Chebyshev interpolation.\nFigure 4(c) shows that the\nnumerical rank of D increases as α increases.\n−3\n−2\n−1\n0\n1\n2\n3\nx1\n−3\n−2\n−1\n0\n1\n2\n3\nx2\n(a)\np(x)\n−3\n−2\n−1\n0\n1\n2\n3\nc0\n−3\n−2\n−1\n0\n1\n2\n3\nc−1\n(b)\np(c)\n0\n1\n0\n5\n10\n15\nα\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\nRank\n(c) Numerical rank w/z correlation strength\nε = 0.01\nε = 0.002\nFig. 4: Results of the d = 2 model p(x1, x2) = exp\n\u0000−x2\n1/2 −x2\n2/2 −α\n2 (x1 −x2)2\u0001\n.\nFigure 4(a)-(b) shows the contour plot of p(x) and p(c). Figure 4(c) shows that the\nnumerical rank of p(x) increases with the coupling strength parameter α.\n4.2. Approximating the numerical rank. For high-dimensional models, it\nis no longer feasible to perform direct estimation on the numerical rank of the un-\nfolding matrix of D, F, as the rows and columns of the unfolding matrix are typically\nexponential-sized. In practice, one can approximate the rank via repeated moment es-\ntimations. For β = 1, . . . , B1, γ = 1, . . . , B2, we let sI,β : R|I| →R and sJ,γ : R|J| →R\ndenote functions respectively taking xI and xJ as variable.\nSimilar to the proce-\ndure in Section 2, one can perform sketching on D(iI; iJ) by integrating p(x) with\nsI,β(xI)sJ,γ(xJ). We define Z ∈RB1×B2 as the matrix recording all possible integra-\ntions of p, i.e.\nZI,J(β, γ) =\nZ\np(x1, . . . , xd)sI,β(xI)sJ,γ(xJ) dx,\nand one can see that ZI,J has at most rank r if p(x) is of rank r. We take the numerical\nrank of the approximated ZI,J matrix as an approximation of the numerical rank of\np(x) along [d] = I ∪J. By the same procedure, one can approximate the numerical\nrank of F by performing repeated moment estimation on p(c). In other words, for both\np(x) and p(c), we form ZI,J and we define the approximate numerical rank of p(x) and\np(c) as the minimal rank for a factorization U, V to achieve ∥ZI,J −UV ∥≤ε∥ZI,J∥.\nTo make sure that the numerical rank of D, F is accurately estimated by the\nnumerical rank of ZI,J, one needs to make sure that the sketching procedure on D, F\nis reasonable. For example, one necessary condition is the assumption that the left\nand right factors of the SVD factorization of D, F satisfy incoherence conditions. By\ncarrying out the analysis in [68], under incoherence condition, one can show that\ntaking ZI,J to be a submatrix of D, F by random column and row selection ensures\nthat the numerical rank of ZI,J matches that of D, F. Inspired by this theoretical\nanalysis, in our experiments, we choose sI,β, sJ,γ so that ZI,J forms a submatrix of\nD, F. As the incoherence condition is hard to verify numerically, we choose a large\n\n\n16\nX. TANG, L. YING\nnumber of functions sI,β, sJ,γ, and we increase the number of functions until the rank\nestimation saturates.\nAdditionally, there is another source of approximation error from obtaining ZI,J\nby moment estimation from samples of p.\nWhen one performs SVD on ZI,J, the\nsample estimation error is quite mild, and the estimated spectra follow the Monte-\nCarlo rate [9]. To address this type of error, we take a large sample size N and a\nrelatively large rank truncation parameter ε. In addition, we choose the sample size\nso that the rank estimation saturates.\n4.3. 1D lattice models.\nCase study 2: Approximate numerical rank of 1D O–U model. In this case, we\nconsider the approximate numerical rank of the 1D Ornstein–Uhlenbeck model\np(x1, . . . , xd) = exp\n\n−α\n2\nX\ni∼j\n(xi −xj)2 −1\n2\nd\nX\ni=1\nx2\ni\n\n,\nwhere α controls the correlation strength, and i ∼j if i −j ≡1 mod d. We take\nd = 128 and α = 1000. The parameter α is chosen so that neighbor points have strong\ncoupling with corr(Xi, Xi+1) ≈1 while far away points have moderate coupling with\ncorr(Xi, Xi+⌈d/2⌉) ≈1/4. For both p(x) and p(c), we form a ZI,J matrix by moment\nestimation. For numerical rank of ZI,J, we take ε = 10−2, and we choose the threshold\nparameter ε so that ZI,J is of numerical rank r = 1 if α = 1.\nIn summary, our\nexperiment reports a numerical rank of r = 28 for p(x) and a numerical rank of r = 7\nfor p(c). We detail the methodology in subsequent paragraphs.\nWe first test the approximate numerical rank of p(x). When one uses the FHT\nand FTT ansatz in modeling the 1D O-U model, one implicitly assumes that the\nnumerical rank of p(x) is small along the variable bipartition with I = {1, . . . , d/2}\nand J = {d/2 + 1, . . . , d}. By the choice of parameter, the distribution X ∼p(x)\nis effectively supported on Ω= [−0.8, 0.8]d. For k = 1, . . . , d, we take each ⃗Ψk =\n{ψi,k}n\ni=0 to be the collection of orthonormal Legendre polynomial basis functions on\nL2([−0.8, 0.8]). We take ψi,k(xk) to be the unique basis function of maximal degree i.\nThe distribution X ∼p(x) is such that the variable pairs (X1, Xd) and (Xd/2, Xd/2+1)\nare strongly coupled. For sI,β, we take q = 50 and we construct sI,β to consist of\n{ψi,k} for i = 0, . . . , q and k ∈{1, d/2}. Similarly, we take sJ,γ to consist of {ψi,k}\nfor i = 0, . . . , q and k ∈{d/2 + 1, d}. We note that Ψ0,k is a constant function for\neach k, and so there are in total B = 2q + 1 non-duplicate functions. The matrix\nZI,J is estimated by N = 106 samples of p. The singular values of ZI,J are plotted in\nFigure 5(a). In this case, the numerical rank of ZI,J is r = 28. We remark that ZI,J\nis a submatrix of D, and the selection of sI,β and sJ,γ is chosen such that the rank\nsaturates.\nWe then test the approximate numerical rank of p(c).\nFor the wavelet filter,\nwe use the Daubechies D4 filter with periodic signal extension mode.\nUnder the\nwavelet iterative coarsening, p(c) takes c = (cL−1, . . . , c0, c−1) as the variable with\ncl = (c1,l, . . . , c2l,l) for l = 0, . . . , L −1. The FHT-W ansatz is motivated by two\nempirical claims in the renormalization group theory community (c.f. [46]): (a) p(c)\nis low-rank across scales, and (b) p(c) is low-rank across physical sites.\nWe test\nboth claims simultaneously.\nWe choose I to correspond to all variables ck,l with\nk = 1, . . . , 2l−1 and l = 1, . . . , L −1, and we choose J to correspond to c0, c−1 and all\nvariables ck,l with k = 2l−1 + 1, . . . , 2l and l = 1, . . . , L −1. This construction makes\nsure that I and J each contains half of the variables for cl at each scale 2−l.\n\n\nFHT UNDER WAVELET BASIS\n17\nAs the goal of this work is to justify the use of wavelet transformations, we\nconstruct a larger matrix ZI,J for testing the approximate numerical rank. Similar to\nthe case of p(x), the distribution of C ∼p(c) is effectively supported on a hypercube\nΩ= Q\nk,l[−ak,l, ak,l], where ak,l captures the range of the distribution C ∼p(c) on\nck,l. For each ck,l variable, we let ⃗Ψk,l = (ψi,(k,l))n\ni=0 be the collection of orthonormal\nLegendre polynomial basis functions on L2([−ak,l, ak,l]), where ψi,(k,l) corresponds to\nthe basis function of maximal degree i. We take q = 30. For each level l = L−1, . . . , 1,\nwe let sI,β include all ψi,(k,l) for i = 0, . . . , q and k ∈{1, 2l−1}, and we let sJ,γ include\nall ψi,(k,l) for i = 0, . . . , q and k ∈{2l−1 + 1, 2l}. This choice corresponds to taking\nthe boundary variables in cl at each level l. The singular values of ZI,J are plotted\nin Figure 5(b). In this case, the numerical rank of ZI,J is r = 7.\n0\n25\n50\n75\n100\nIndex i\n0.00\n0.05\n0.10\n0.15\n0.20\nSingular value σi\n(a)\nSingular value decay for p(x)\n0\n25\n50\n75\n100\nIndex i\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSingular value σi\n(b)\nSingular value decay for p(c)\n10\n20\n30\n40\n50\n5\nRank\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nε\n(c)\nNumerical rank comparison\np(x)\np(c)\nFig. 5: Singular value of ZI,J for the 1D Ornstein–Uhlenbeck model under p(x) and\np(c), where x is the original variable and c is the variable after wavelet transformation.\nBoth matrices are scaled so that the singular values sum to one, and only the first\n100 singular values are plotted. Wavelet transformation leads to more rapid singular\nvalue decay in ZI,J.\nIn both p(x) and p(c), we remark that ZI,J is constructed by forming a submatrix\nof the unfolding matrix of D and F. The choice of ZI,J for p(x) is less extensive and\nmight lead to underreporting of the approximate numerical rank of D. However, one\ncan see that the approximate numerical rank of p(x) is more than double that of\np(c). If one models p(x) with the FHT ansatz and p(c) with the FHT-W ansatz, the\ndifference in the rank r leads to an order of magnitude difference in the parameter size\ndue to the O(r3) scaling for both models. Moreover, we see from Figure 5 that the\nsingular value decay is more rapid for p(c), which suggests that the sketching algorithm\nfor the FHT-W ansatz is more robust to a small target internal rank parameter\n{re}e∈E.\nCase study 3: Approximate numerical rank of 1D Ginzburg-Landau model. In this\ncase, we consider the approximate numerical rank of the 1D Ginzburg–Landau model\np(x1, . . . , xd) = exp\n\n−α\n2\nX\ni∼j\n(xi −xj)2 −λ\n2\nd\nX\ni=1\n(1 −x2\ni )2\n\n,\nwhere α controls the correlation strength, λ controls the strength of the double-well\nterm, and i ∼j if i −j ≡1 mod d. We test an interesting case with a strong site-\nwise interaction and a non-trivial double-well phenomenon, and we choose d = 128,\nα = 250 and λ = 5. We compare the numerical rank between p(x) and p(c).\n\n\n18\nX. TANG, L. YING\nIn this case, we can similarly identify a hypercube support for X ∼p(x) and\nC ∼p(c), and subsequently we can give p(x) and p(c) an FTN structure by a Legendre\npolynomial basis. Thus, by changing the definition of each ψi,k and ψi,(k,l) to be based\non the data support in the G-L model, we can use the same choice of sI,β and sJ,γ\nas in the O-U model.\nIn this case, we also obtain ZI,J with moment estimation,\nand we use N = 5×105 samples generated from Markov-chain Monte-Carlo (MCMC)\nsimulations based on p. The result is shown in Figure 6. With ε = 0.01, the numerical\nrank for ZI,J is r = 45 for p(x), and the numerical rank for ZI,J is r = 14 for p(c).\n0\n25\n50\n75\n100\nIndex i\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\nSingular value σi\n(a)\nSingular value decay for p(x)\n0\n25\n50\n75\n100\nIndex i\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nSingular value σi\n(b)\nSingular value decay for p(c)\n10\n20\n30\n40\n50\n5\nRank\n0.0\n0.1\n0.2\n0.3\n0.4\nε\n(c)\nNumerical rank comparison\np(x)\np(c)\nFig. 6: Singular value of ZI,J for the 1D Ginzburg-Landau model under p(x) and p(c).\nBoth matrices are scaled so that the singular values sum to one, and only the first\n100 singular values are plotted. Wavelet transformation leads to more rapid singular\nvalue decay in ZI,J.\n4.4. 2D lattice models.\nCase study 4: Approximate numerical rank of 2D O-U model. In this case, we\nconsider the approximate numerical rank of the 2D Ornstein–Uhlenbeck model,\np(x(1,1), . . . , x(m,m))\n= exp\n\n−α1\n2\nX\nj\nX\ni∼i′\n(x(i,j) −x(i′,j))2 −α2\n2\nX\ni\nX\nj∼j′\n(x(i,j) −x(i,j′))2 −1\n2\nm\nX\ni,j=1\nx2\n(i,j)\n\n,\nwhere α1, α2 control the correlation strength, and i ∼i′ if i −i′ ≡1 mod m. We\ntake m = 8 with d = m2 = 64. When both α1 and α2 are large, the rank of p(x) is\nnot significantly larger than 1D O-U models. Thus, we consider an interesting case\nwhere α1 = 200 and α2 = 10, which corresponds to the case where the interaction\nat the first spatial index is considerably stronger than the second spatial index. In\nsummary, the approximate numerical rank of p(x) is r = 73 and the approximate\nnumerical rank for p(c) is r = 17.\nWe first choose the unfolding matrix for p(x) and p(c). For p(x), we take I to be\nthe collection of x(i,j) with i ∈{1, . . . , m/2} and j ∈[m], and we take J to be the\ncollection of x(i,j) with i ∈{m/2 + 1, . . . , m} and j ∈[m]. For p(c), we perform a 2D\nwavelet transformation to get the c variable, and we take the separable Daubechies D4\nfilter with periodic signal extension mode. The recursive binary expansion detailed\nin Subsection 3.3 leads to the variable structure that is similar to the 1D case, and\none has c = (c2L−1, . . . , c0, c−1) as the variable with cl = (c1,l, . . . , c2l,l) for l =\n1, . . . , 2L −1. We choose I to correspond to all variables ck,l with k = 1, . . . , 2l−1 and\n\n\nFHT UNDER WAVELET BASIS\n19\nl = 1, . . . , 2L −1, and we choose J to correspond to c0, c−1 and all variables ck,l with\nk = 2l−1 + 1, . . . , 2l and l = 1, . . . , 2L −1.\nSimilar to the case of the 1D O-U model, both p(x) and p(c) are respectively\nsupported on a hypercube, and so one can use a Legendre polynomial basis function\non each variable. Thus, to form ZI,J, we choose sI,β and sJ,γ to be univariate Legendre\nbasis functions. For p(x), we take Legendre polynomial basis functions up to degree\nq = 50. We let sI,β include the basis functions for x(i,j) for i ∈{1, m/2} and j ∈[m].\nWe let sJ,γ include the basis functions for x(i,j) for i ∈{m/2 + 1, m} and j ∈[m].\nFor p(c), picking the variables of interest is difficult. Therefore, we construct sI,β and\nsJ,γ to include all variables in I and J for Legendre polynomial basis functions up to\ndegree q = 20.\nWe use N = 5 × 105 samples to estimate ZI,J and we take ε = 0.01.\nThe\napproximate numerical rank of p(x) is r = 73 and the approximate numerical rank\nfor p(c) is r = 17. The results are summarized in Figure 7.\n0\n25\n50\n75\n100\nIndex i\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nSingular value σi\n(a)\nSingular value decay for p(x)\n0\n25\n50\n75\n100\nIndex i\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nSingular value σi\n(b)\nSingular value decay for p(c)\n20\n40\n60\n80\n100\n5\nRank\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\n0.10\nε\n(c)\nNumerical rank comparison\np(x)\np(c)\nFig. 7: Singular value of ZI,J for the 2D Ornstein–Uhlenbeck model under p(x) and\np(c). Both matrices are scaled so that the singular values sum to one, and only the\nfirst 100 singular values are plotted.\nWavelet transformation leads to more rapid\nsingular value decay in ZI,J.\n0\n25\n50\n75\n100\nIndex i\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nSingular value σi\n(a)\nSingular value decay for p(x)\n0\n25\n50\n75\n100\nIndex i\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nSingular value σi\n(b)\nSingular value decay for p(c)\n20\n40\n60\n80\n100\n5\nRank\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\nε\n(c)\nNumerical rank comparison\np(x)\np(c)\nFig. 8: Singular value of ZI,J for the 2D Ginzburg-Landau model under p(x) and p(c).\nBoth matrices are scaled so that the singular values sum to one, and only the first\n100 singular values are plotted. Wavelet transformation leads to more rapid singular\nvalue decay in ZI,J.\n\n\n20\nX. TANG, L. YING\nCase study 5: Approximate numerical rank of 2D Ginzburg-Landau model. In this\ncase, we consider the approximate numerical rank of the 2D Ginzburg–Landau model,\np(x(1,1), . . . , x(m,m)) = exp\n\u0010\n−α1\n2\nX\nj\nX\ni∼i′\n(x(i,j) −x(i′,j))2\n−α2\n2\nX\ni\nX\nj∼j′\n(x(i,j) −x(i,j′))2 −λ\n2\nm\nX\ni,j=1\n\u0010\n1 −x2\n(i,j)\n\u00112 \u0011\n,\nwhere α1, α2 control the correlation strength, λ controls the strength of the double-\nwell term, and i ∼i′ if i −i′ ≡1 mod m. We take m = 8 with d = m2 = 64. For the\nmodel parameter, we take α1 = 20, α2 = 0.6 and λ = 1. By adjusting the support for\neach Legendre polynomial basis, we can use the same choice of sI,β and sJ,γ as in the\n2D O-U model. We obtain ZI,J with moment estimation, and we use N = 5 × 105\nsamples generated from Markov-chain Monte-Carlo (MCMC) simulations based on p.\nThe result is shown in Figure 8. With ε = 0.01, the numerical rank for ZI,J is r = 84\nfor p(x), and the numerical rank for ZI,J is r = 17 for p(c).\n4.5. Discussion of results. From the experiments carried out in 1D and 2D\nsystems, we show that p(c) has a more favorable numerical rank than p(x) across a\nlarge collection of lattice models with strong interactions. We make several remarks\nbased on the findings.\nThe experiment finding indicates when it is appropriate to use the FHT-W ansatz.\nIn our case studies, the problem parameters are chosen to be numerically challeng-\ning instances so that one can draw empirical observations among relatively difficult\nprobability distributions among each category. Overall, the case studies lead to three\nempirical observations for p(x): (a) Numerical rank of ZI,J tend to increase with\ncoupling strength, (b) Numerical rank of ZI,J is typically larger for 2D lattice models\nthan for 1D systems, and (c) Numerical rank tend to increase under the presence\nof double-well terms. Thus, when one encounters a strongly coupled 2D Ginzburg-\nLandau model or a ϕ4 model [76], it might be preferable to use the FHT-W ansatz\nover the FHT ansatz defined in [66].\nMoreover, our case study shows that the FHT-W density estimation is more ef-\nficient than the FHT ansatz in modeling lattice models. In the 2D G-L model, the\nrank for p(c) is smaller by a factor of five, which leads to a difference of two or-\nders of magnitude in the model complexity. For the density estimation subroutine in\nAlgorithm 2.1, the main computational bottleneck lies in performing the moment esti-\nmation subroutine. When one has N samples, obtaining Bk by density estimation has\na cost of O(N ˜r3), where ˜r := maxe ˜re, and we take ˜r to be larger than the numerical\nrank to ensure the sketched linear system is well-conditioned. Thus, as the numerical\nrank is as large as r ≈80 in the 2D G-L model, we can see that it is numerically\nchallenging to model p(x) with the FHT density estimation subroutine. In contrast,\nwe see that p(c) is consistently low-rank with r < 20 in all of the experiments, which\nshows that the FHT-W ansatz admits a more efficient density estimation subroutine\nfor the studied lattice models.\n5. Numerical experiment. This section demonstrates the numerical perfor-\nmance of the proposed density estimation algorithm. In Subsection 5.1, we test the\nFHT-W ansatz in density estimation of 1D lattice models. In Subsection 5.2, we test\nthe FHT-W ansatz in density estimation of 2D lattice models.\n\n\nFHT UNDER WAVELET BASIS\n21\n−1.0\n−0.5\n0.0\n0.5\n1.0\nc at index (15, 5)\n−1.00\n−0.75\n−0.50\n−0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nc at index (8, 4)\n1D G-L - Predicted particle density\n−1.0\n−0.5\n0.0\n0.5\n1.0\nc at index (15, 5)\n−1.00\n−0.75\n−0.50\n−0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nc at index (8, 4)\n1D G-L - Sample particle density\n0\n1\n2\n3\n4\n5\n6\n0\n1\n2\n3\n4\n5\n6\n−1.0\n−0.5\n0.0\n0.5\n1.0\nc at index (15, 5)\n−1.00\n−0.75\n−0.50\n−0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nc at index (9, 4)\n1D G-L - Predicted particle density\n−1.0\n−0.5\n0.0\n0.5\n1.0\nc at index (15, 5)\n−1.00\n−0.75\n−0.50\n−0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nc at index (9, 4)\n1D G-L - Sample particle density\n0\n1\n2\n3\n4\n5\n6\n7\n0\n1\n2\n3\n4\n5\n6\n7\nFig. 9: 1D Ginzburg-Landau model. Plots of the marginal distribution of C ∼p(c) at\n(c15,5, c8,4) and (c15,5, c9,4). For illustration purposes, a scaling is performed so that\nthe marginal distribution lies in [−1, 1]2.\n5.1. 1D lattice models. This subsection tests Algorithm 2.1 on the density\nestimation of 1D lattice models. We first consider a strongly coupled 1D Ginzburg-\nLandau model. The distribution is of the form\np(x1, . . . , xd) = exp\n\n−α\n2\nX\ni∼j\n(xi −xj)2 −λ\n2\nd\nX\ni=1\n(1 −x2\ni )2\n\n,\nwhere i ∼j if i−j ≡1 mod d. We take the parameter as specified in Subsection 4.3 so\nthat d = 128, α = 250 and λ = 5. We perform Algorithm 2.1 on N = 12000 samples\nof p after wavelet transformation. We take the maximal internal dimension to be\nr = 12. On each variable, we use a Legendre polynomial basis with a maximal degree\nof q = 25. Figure 9 shows that the FHT-W ansatz accurately captures the marginal\ndistribution of C ∼p(c) at (c15,5, c8,4) and (c15,5, c9,4). It is noteworthy from the\n2-marginal distribution that (c15,5, c9,4) is already effectively decoupled. Even for the\nmildly correlated 2-marginal on (c15,5, c8,4), we see that the distribution is unimodal.\nThe relatively simple behavior of p(c) at finer scales is one key reason underlying the\nsmall numerical rank observed in Subsection 4.3.\nTo give a challenging numerical test of the FHT-W ansatz, we use the FHT-W\n\n\n22\nX. TANG, L. YING\n0\n50\n100\nIndex i\n0\n20\n40\n60\n80\n100\n120\nIndex j\n1D G-L - Predicted Correlation\n0\n50\n100\nIndex i\n0\n20\n40\n60\n80\n100\n120\nIndex j\n1D G-L - True Correlation\n0\n50\n100\nIndex i\n0\n20\n40\n60\n80\n100\n120\nIndex j\nPrediction error\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nFig. 10: 1D Ginzburg-Landau model. Plot of the correlation matrix predicted by the\nFHT-W ansatz.\n0\n50\n100\nIndex i\n0\n20\n40\n60\n80\n100\n120\nIndex j\n1D O-U - Predicted Correlation\n0\n50\n100\nIndex i\n0\n20\n40\n60\n80\n100\n120\nIndex j\n1D O-U - True Correlation\n0\n50\n100\nIndex i\n0\n20\n40\n60\n80\n100\n120\nIndex j\nPrediction error\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nFig. 11: 1D Ornstein–Uhlenbeck model. Plot of the correlation matrix predicted by\nthe FHT-W ansatz.\nansatz to predict the correlation matrix,\nM(i, j) = CorrX∼p(x) (Xi, Xj) ,\nwhich one can see can be carried out by performing O(d2) observable estimation\ntasks from the FHT-W ansatz obtained from Algorithm 2.1. Figure 10 shows that\nthe predicted correlation closely matches the ground truth obtained from sample\nestimation.\nWe repeat the same numerical experiment on the 1D Ornstein–Uhlenbeck model,\nwhere the model parameter likewise follows from that of Subsection 4.3. The proba-\nbility density function is\np(x1, . . . , xd) = exp\n\n−α\n2\nX\ni∼j\n(xi −xj)2 −1\n2\nd\nX\ni=1\nx2\ni\n\n,\nwhere d = 128 and α = 1000. We perform Algorithm 2.1 on N = 12000 samples of p.\nIn Figure 11, we see that the FHT-W ansatz can likewise accurately model the global\ncorrelation structure of the 1D O-U model.\n5.2. 2D lattice models. This subsection tests Algorithm 2.1 on strongly cou-\npled 2D lattice models. We first consider the strongly coupled 2D Ginzburg-Landau\nmodel. We use the parameter setting as in Subsection 4.4. The probability distribu-\n\n\nFHT UNDER WAVELET BASIS\n23\n−1.0\n−0.5\n0.0\n0.5\n1.0\nc at index (15, 5)\n−1.00\n−0.75\n−0.50\n−0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nc at index (8, 4)\n2D G-L - Predicted particle density\n−1.0\n−0.5\n0.0\n0.5\n1.0\nc at index (15, 5)\n−1.00\n−0.75\n−0.50\n−0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nc at index (8, 4)\n2D G-L - Sample particle density\n0\n1\n2\n3\n4\n5\n6\n0\n1\n2\n3\n4\n5\n6\n−1.0\n−0.5\n0.0\n0.5\n1.0\nc at index (2, 2)\n−1.00\n−0.75\n−0.50\n−0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nc at index (1, 1)\n2D G-L - Predicted particle density\n−1.0\n−0.5\n0.0\n0.5\n1.0\nc at index (2, 2)\n−1.00\n−0.75\n−0.50\n−0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nc at index (1, 1)\n2D G-L - Sample particle density\n0\n1\n2\n3\n4\n5\n6\n7\n0\n1\n2\n3\n4\n5\n6\n7\n−1.0\n−0.5\n0.0\n0.5\n1.0\nc at index (1, 0)\n−1.00\n−0.75\n−0.50\n−0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nc at index (1, -1)\n2D G-L - Predicted particle density\n−1.0\n−0.5\n0.0\n0.5\n1.0\nc at index (1, 0)\n−1.00\n−0.75\n−0.50\n−0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nc at index (1, -1)\n2D G-L - Sample particle density\n0\n1\n2\n3\n4\n5\n0\n1\n2\n3\n4\n5\nFig. 12: 2D Ginzburg-Landau model.\nPlots of the marginal distribution of C ∼\np(c) at (c15,5, c8,4), (c2,2, c1,1) and (c1,0, c1,−1). For illustration purposes, a scaling is\nperformed so that the marginal distribution lies in [−1, 1]2.\ntion function is\np(x(1,1), . . . , x(m,m)) = exp\n\u0010\n−α1\n2\nX\nj\nX\ni∼i′\n(x(i,j) −x(i′,j))2\n−α2\n2\nX\ni\nX\nj∼j′\n(x(i,j) −x(i,j′))2 −λ\n2\nm\nX\ni,j=1\n\u0010\n1 −x2\n(i,j)\n\u00112 \u0011\n,\n\n\n24\nX. TANG, L. YING\nwhere m = 8 with d = m2 = 64, α1 = 20, α2 = 0.6 and λ = 1. From the discussion in\nSubsection 4.4, we see that the numerical rank of this model makes it challenging for\nan FHT ansatz to perform density estimation. However, the numerical rank under\np(c) is relatively small, which shows that the FHT-W ansatz is a suitable numerical\ncandidate.\nAxis j\nAxis i\n2D Ginzburg-Landau:\nPredicted 2-pt Correlation\nAxis j\nAxis i\n2D Ginzburg-Landau:\nTrue 2-pt Correlation\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFig. 13: 2D Ginzburg-Landau model.\nPlot of the two-point correlation function\npredicted by the FHT-W ansatz.\nAxis j\nAxis i\n2D Ornstein–Uhlenbeck:\nPredicted 2-pt Correlation\nAxis j\nAxis i\n2D Ornstein–Uhlenbeck:\nTrue 2-pt Correlation\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFig. 14: 2D Ornstein–Uhlenbeck model. Plot of the two-point correlation function\npredicted by the FHT-W ansatz.\nThe procedure to perform density estimation of 2D G-L models follows that of the\n1D case. We first perform iterative wavelet transformations on N = 12000 samples.\nWe then perform density estimation on the wavelet transformed samples using the\nFHT-W ansatz architecture as specified in Subsection 3.3. Following the numerical\nexperiment in Subsection 4.4, we choose the maximal internal dimension to be r = 20.\nOn each variable, we use a Legendre polynomial basis with a maximal degree of q = 25.\nBy the construction in Subsection 3.3, we see that the 2D case also has a variable\nstructure c = (c2L−1, . . . , c0, c−1) with cl = (c1,l, . . . , c2l,l) for l = 1, . . . , 2L −1.\nFigure 12 shows that the FHT-W ansatz accurately captures the marginal distri-\nbution of C ∼p(c) at (c15,5, c8,4), (c2,2, c1,1) and (c1,0, c1,−1), which shows that the\nFHT-W ansatz is capable of capturing the interaction on fine scales and coarse scales.\nTo give a challenging test to the proposed method, we use the obtained FHT-W\nansatz to predict the value of the two-point correlation function,\nf(i, j) = CorrX∼p(x)\n\u0000X(i,j), X(4,4)\n\u0001\n,\n\n\nFHT UNDER WAVELET BASIS\n25\nwhich is done by using the obtained ansatz to perform repeated observable estimation\ntasks. Figure 13 shows that the proposed FHT-W ansatz closely matches the ground\ntruth. From the plot, we see that the distribution X = (X(i,j)) ∼p(x) has a peculiar\ncorrelation structure of having a strong coupling on the i index but a weak coupling\nat the j index, which is one of the underlying reasons for the large numerical rank in\nSubsection 4.4.\nWe repeat the experiment on the 2D Ornstein–Uhlenbeck model, where the model\nparameter likewise follows from that of Subsection 4.4. We perform Algorithm 2.1 on\nN = 12000 samples of p. In Figure 14, we see that the FHT-W ansatz can likewise\naccurately model the two-point correlation function of the 2D O-U model.\n6. Conclusion. This paper introduces a functional hierarchical tensor network\nunder a wavelet basis for density estimation on lattice models. The approach combines\na specific tree-based functional tensor network with wavelet transformation.\nThe\nalgorithm is applied successfully to the discretized Ornstein–Uhlenbeck model and\nGinzburg-Landau model in 1D and 2D. With appropriate numerical treatment, this\napproach has the potential for modeling high-dimensional lattice models with complex\ncoupling structures. An open question is whether one can use the FHT-W ansatz to\nefficiently solve the backward Kolmogorov equation and the Hamilton-Jacobi equation\non lattice models.\nREFERENCES\n[1] M. S. Albergo, N. M. Boffi, and E. Vanden-Eijnden, Stochastic interpolants: A unifying\nframework for flows and diffusions, arXiv preprint arXiv:2303.08797, (2023).\n[2] M. S. Albergo and E. Vanden-Eijnden, Building normalizing flows with stochastic inter-\npolants, arXiv preprint arXiv:2209.15571, (2022).\n[3] M. Altaisky, Unifying renormalization group and the continuous wavelet transform, Physical\nReview D, 93 (2016), p. 105043.\n[4] G. Battle, Wavelets and renormalization, vol. 10, World Scientific, 1999.\n[5] G. Beylkin, On the representation of operators in bases of compactly supported wavelets, SIAM\nJournal on Numerical Analysis, 29 (1992), pp. 1716–1740.\n[6] D. Bigoni, A. P. Engsig-Karup, and Y. M. Marzouk, Spectral tensor-train decomposition,\nSIAM Journal on Scientific Computing, 38 (2016), pp. A2405–A2439.\n[7] T.-D. Bradley, E. M. Stoudenmire, and J. Terilla, Modeling sequences with quantum\nstates:\na look under the hood, Machine Learning:\nScience and Technology, 1 (2020),\np. 035008.\n[8] T. Chen, L. Lin, W. Zuo, X. Luo, and L. Zhang, Learning a wavelet-like auto-encoder to\naccelerate deep neural networks, in Proceedings of the Thirty-Second AAAI Conference on\nArtificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Con-\nference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence,\nAAAI’18/IAAI’18/EAAI’18, AAAI Press, 2018.\n[9] Y. Chen, Y. Chi, J. Fan, C. Ma, et al., Spectral methods for data science: A statistical\nperspective, Foundations and Trends® in Machine Learning, 14 (2021), pp. 566–806.\n[10] Y. Chen and Y. Khoo, Combining particle and tensor-network methods for partial differential\nequations via sketching, arXiv preprint arXiv:2305.17884, (2023).\n[11] S. Cheng, L. Wang, T. Xiang, and P. Zhang, Tree tensor networks for generative modeling,\nPhysical Review B, 99 (2019), p. 155131.\n[12] B. Dai and U. Seljak, Multiscale flow for robust and optimal cosmological analysis, Proceed-\nings of the National Academy of Sciences, 121 (2024), p. e2309624121.\n[13] I. Daubechies, Ten lectures on wavelets, Society for industrial and applied mathematics,\n(1992).\n[14] A. Dektor and D. Venturi, Tensor rank reduction via coordinate flows, Journal of Compu-\ntational Physics, 491 (2023), p. 112378.\n[15] L. Demanet and L. Ying, On chebyshev interpolation of analytic functions, preprint, (2010).\n[16] C. Doersch, Tutorial on variational autoencoders, arXiv preprint arXiv:1606.05908, (2016).\n[17] W. E, W. Ren, and E. Vanden-Eijnden, Minimum action method for the study of rare events,\n\n\n26\nX. TANG, L. YING\nCommunications on pure and applied mathematics, 57 (2004), pp. 637–656.\n[18] G. Evenbly and S. R. White, Entanglement renormalization and wavelets, Physical review\nletters, 116 (2016), p. 140403.\n[19] R. Gal, D. C. Hochberg, A. Bermano, and D. Cohen-Or, Swagan: A style-based wavelet-\ndriven generative model, ACM Transactions on Graphics (TOG), 40 (2021), pp. 1–11.\n[20] V. L. Ginzburg, V. L. Ginzburg, and L. Landau, On the theory of superconductivity,\nSpringer, 2009.\n[21] I. Glasser, R. Sweke, N. Pancotti, J. Eisert, and I. Cirac, Expressive power of tensor-\nnetwork factorizations for probabilistic modeling, Advances in neural information process-\ning systems, 32 (2019).\n[22] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,\nA. Courville, and Y. Bengio, Generative adversarial nets, Advances in neural infor-\nmation processing systems, 27 (2014).\n[23] L. Grafakos, Classical Fourier Analysis, Springer, 2008.\n[24] E. Grelier, A. Nouy, and R. Lebrun, Learning high-dimensional probability distributions\nusing tree tensor networks, International Journal for Uncertainty Quantification, 12 (2022).\n[25] F. Guth, E. Lempereur, J. Bruna, and S. Mallat, Conditionally strongly log-concave gen-\nerative models, in International Conference on Machine Learning, PMLR, 2023, pp. 12224–\n12251.\n[26] M. Gutmann and A. Hyv¨arinen, Noise-contrastive estimation: A new estimation principle\nfor unnormalized statistical models, in Proceedings of the thirteenth international confer-\nence on artificial intelligence and statistics, JMLR Workshop and Conference Proceedings,\n2010, pp. 297–304.\n[27] W. Hackbusch and S. K¨uhn, A new scheme for the tensor representation, Journal of Fourier\nanalysis and applications, 15 (2009), pp. 706–722.\n[28] J. Haegeman, B. Swingle, M. Walter, J. Cotler, G. Evenbly, and V. B. Scholz, Rigor-\nous free-fermion entanglement renormalization from wavelet theory, Physical Review X, 8\n(2018), p. 011003.\n[29] Z.-Y. Han, J. Wang, H. Fan, L. Wang, and P. Zhang, Unsupervised generative modeling\nusing matrix product states, Physical Review X, 8 (2018), p. 031012.\n[30] G. Hinton, A practical guide to training restricted boltzmann machines, Momentum, 9 (2010),\np. 926.\n[31] G. E. Hinton, Training products of experts by minimizing contrastive divergence, Neural com-\nputation, 14 (2002), pp. 1771–1800.\n[32] J. Ho, A. Jain, and P. Abbeel, Denoising diffusion probabilistic models, Advances in neural\ninformation processing systems, 33 (2020), pp. 6840–6851.\n[33] J. Ho, C. Saharia, W. Chan, D. J. Fleet, M. Norouzi, and T. Salimans, Cascaded diffusion\nmodels for high fidelity image generation, Journal of Machine Learning Research, 23 (2022),\npp. 1–33.\n[34] K.-H. Hoffmann and Q. Tang, Ginzburg-Landau phase transition theory and superconduc-\ntivity, vol. 134, Birkh¨auser, 2012.\n[35] P. C. Hohenberg and A. P. Krekhov, An introduction to the ginzburg–landau theory of\nphase transitions and nonequilibrium patterns, Physics Reports, 572 (2015), pp. 1–42.\n[36] Y. Hur, J. G. Hoskins, M. Lindsey, E. M. Stoudenmire, and Y. Khoo, Generative modeling\nvia tensor train sketching, Applied and Computational Harmonic Analysis, 67 (2023),\np. 101575.\n[37] M. I. Jordan, Learning in graphical models, MIT press, 1999.\n[38] L. P. Kadanoff, A. Houghton, and M. C. Yalabik, Variational approximations for renor-\nmalization group transformations, Journal of Statistical Physics, 14 (1976), pp. 171–203.\n[39] Z. Kadkhodaie, F. Guth, S. Mallat, and E. P. Simoncelli, Learning multi-scale local\nconditional probability models of images, arXiv preprint arXiv:2303.02984, (2023).\n[40] Y. Khoo, M. Lindsey, and H. Zhao, Tensorizing flows: a tool for variational inference, arXiv\npreprint arXiv:2305.02460, (2023).\n[41] D. P. Kingma, M. Welling, et al., An introduction to variational autoencoders, Foundations\nand Trends® in Machine Learning, 12 (2019), pp. 307–392.\n[42] Y. LeCun, S. Chopra, R. Hadsell, M. Ranzato, and F. Huang, A tutorial on energy-based\nlearning, Predicting structured data, 1 (2006).\n[43] Y. Lipman, R. T. Chen, H. Ben-Hamu, M. Nickel, and M. Le, Flow matching for generative\nmodeling, arXiv preprint arXiv:2210.02747, (2022).\n[44] X. Liu, C. Gong, and Q. Liu, Flow straight and fast: Learning to generate and transfer data\nwith rectified flow, arXiv preprint arXiv:2209.03003, (2022).\n[45] S. Mallat, A wavelet tour of signal processing, 1999.\n\n\nFHT UNDER WAVELET BASIS\n27\n[46] T. Marchand, M. Ozawa, G. Biroli, and S. Mallat, Multiscale data-driven energy estima-\ntion and generation, Physical Review X, 13 (2023), p. 041038.\n[47] Y. Meyer, Wavelets and operators: volume 1, no. 37 in Cambridge Studies in Advanced\nMathematics, Cambridge university press, 1992.\n[48] G. S. Novikov, M. E. Panov, and I. V. Oseledets, Tensor-train density estimation, in\nUncertainty in artificial intelligence, PMLR, 2021, pp. 1321–1331.\n[49] I. Oseledets and E. Tyrtyshnikov, Tt-cross approximation for multidimensional arrays,\nLinear Algebra and its Applications, 432 (2010), pp. 70–88.\n[50] G. Papamakarios, E. Nalisnick, D. J. Rezende, S. Mohamed, and B. Lakshminarayanan,\nNormalizing flows for probabilistic modeling and inference, Journal of Machine Learning\nResearch, 22 (2021), pp. 1–64.\n[51] Y. Peng, Y. Chen, E. M. Stoudenmire, and Y. Khoo, Generative modeling via hierarchical\ntensor sketching, arXiv preprint arXiv:2304.05305, (2023).\n[52] Y. Ren, H. Zhao, Y. Khoo, and L. Ying, High-dimensional density estimation with tensoriz-\ning flow, Research in the Mathematical Sciences, 10 (2023), p. 30.\n[53] D. Rezende and S. Mohamed, Variational inference with normalizing flows, in International\nconference on machine learning, PMLR, 2015, pp. 1530–1538.\n[54] C. Saharia, J. Ho, W. Chan, T. Salimans, D. J. Fleet, and M. Norouzi, Image super-\nresolution via iterative refinement, IEEE transactions on pattern analysis and machine\nintelligence, 45 (2022), pp. 4713–4726.\n[55] R. Salakhutdinov and H. Larochelle, Efficient learning of deep boltzmann machines, in\nProceedings of the thirteenth international conference on artificial intelligence and statis-\ntics, JMLR Workshop and Conference Proceedings, 2010, pp. 693–700.\n[56] N. Sheng, X. Tang, H. Chen, and L. Ying, Approximation of high-dimensional gibbs distri-\nbutions using the functional hierarchical tensor, arXiv preprint arXiv:2501.17143, (2025).\n[57] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli, Deep unsupervised\nlearning using nonequilibrium thermodynamics, in International Conference on Machine\nLearning, PMLR, 2015, pp. 2256–2265.\n[58] Y. Song, C. Durkan, I. Murray, and S. Ermon, Maximum likelihood training of score-\nbased diffusion models, Advances in Neural Information Processing Systems, 34 (2021),\npp. 1415–1428.\n[59] Y. Song and S. Ermon, Generative modeling by estimating gradients of the data distribution,\nAdvances in Neural Information Processing Systems, 32 (2019).\n[60] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole,\nScore-based generative modeling through stochastic differential equations, arXiv preprint\narXiv:2011.13456, (2020).\n[61] E. G. Tabak and C. V. Turner, A family of nonparametric density estimation algorithms,\nCommunications on Pure and Applied Mathematics, 66 (2013), pp. 145–164.\n[62] E. G. Tabak and E. Vanden-Eijnden, Density estimation by dual ascent of the log-likelihood,\nCommunications in Mathematical Sciences, 8 (2010), pp. 217–233.\n[63] X. Tang, L. Collis, and L. Ying, Solving high-dimensional kolmogorov backward equations\nwith operator-valued functional hierarchical tensor for markov operators, arXiv preprint\narXiv:2404.08823, (2024).\n[64] X. Tang, Y. Hur, Y. Khoo, and L. Ying, Generative modeling via tree tensor network states,\nResearch in the Mathematical Sciences, 10 (2023), p. 19.\n[65] X. Tang, N. Sheng, and L. Ying, Solving high-dimensional hamilton-jacobi-bellman equation\nwith functional hierarchical tensor, arXiv preprint arXiv:2408.04209, (2024).\n[66] X. Tang and L. Ying, Solving high-dimensional fokker-planck equation with functional hier-\narchical tensor, Journal of Computational Physics, 511 (2024), p. 113110.\n[67] L. Trefethen, Multivariate polynomial approximation in the hypercube, Proceedings of the\nAmerican Mathematical Society, 145 (2017), pp. 4837–4844.\n[68] J. A. Tropp, Improved analysis of the subsampled randomized hadamard transform, Advances\nin Adaptive Data Analysis, 3 (2011), pp. 115–126.\n[69] L. Wasserman, All of nonparametric statistics, Springer Science & Business Media, 2006.\n[70] C. K. Williams and C. E. Rasmussen, Gaussian processes for machine learning, vol. 2, MIT\npress Cambridge, MA, 2006.\n[71] K. G. Wilson, Renormalization group and critical phenomena. ii. phase-space cell analysis of\ncritical behavior, Physical Review B, 4 (1971), p. 3184.\n[72] K. G. Wilson and M. E. Fisher, Critical exponents in 3.99 dimensions, Physical Review\nLetters, 28 (1972), p. 240.\n[73] J. J. Yu, K. G. Derpanis, and M. A. Brubaker, Wavelet flow: Fast training of high reso-\nlution normalizing flows, Advances in Neural Information Processing Systems, 33 (2020),\n\n\n28\nX. TANG, L. YING\npp. 6184–6196.\n[74] L. Zhang, W. E, and L. Wang, Monge-amp`ere flow for generative modeling, arXiv preprint\narXiv:1809.10188, (2018).\n[75] S. Zhang, P. Zhang, and T. Y. Hou, Multiscale invertible generative networks for high-\ndimensional bayesian inference, in International Conference on Machine Learning, PMLR,\n2021, pp. 12632–12641.\n[76] J. Zinn-Justin, Quantum field theory and critical phenomena, vol. 171, Oxford university\npress, 2021.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20655v1.pdf",
    "total_pages": 28,
    "title": "Wavelet-based density sketching with functional hierarchical tensor",
    "authors": [
      "Xun Tang",
      "Lexing Ying"
    ],
    "abstract": "We introduce the functional hierarchical tensor under a wavelet basis (FHT-W)\nansatz for high-dimensional density estimation in lattice models. Recently, the\nfunctional tensor network has emerged as a suitable candidate for density\nestimation due to its ability to calculate the normalization constant exactly,\na defining feature not enjoyed by neural network alternatives such as\nenergy-based models or diffusion models. While current functional tensor\nnetwork models show good performance for lattice models with weak or moderate\ncouplings, we show that they face significant model capacity constraints when\napplied to lattice models with strong coupling. To address this issue, this\nwork proposes to perform density estimation on the lattice model under a\nwavelet transformation. Motivated by the literature on scale separation, we\nperform iterative wavelet coarsening to separate the lattice model into\ndifferent scales. Based on this multiscale structure, we design a new\nfunctional hierarchical tensor ansatz using a hierarchical tree topology,\nwhereby information on the finer scale is further away from the root node of\nthe tree. Our experiments show that the numerical rank of typical lattice\nmodels is significantly lower under appropriate wavelet transformation.\nFurthermore, we show that our proposed model allows one to model challenging\nGaussian field models and Ginzburg-Landau models.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}