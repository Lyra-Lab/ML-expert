{
  "id": "arxiv_2502.21126v1",
  "text": "1\nA general partitioning strategy\nfor non-centralized control\nAlessandro Riccardi, Luca Laurenti Member, IEEE, and Bart De Schutter Fellow, IEEE\nAbstract—Partitioning is a fundamental challenge for non-\ncentralized control of large-scale systems, such as hierarchical,\ndecentralized, distributed, and coalitional strategies. The problem\nconsists of finding a decomposition of a network of dynamical\nsystems into system units for which local controllers can be\ndesigned. Unfortunately, despite its critical role, a generalized\napproach to partitioning applicable to every system is still missing\nfrom the literature. This paper introduces a novel partitioning\nframework that integrates an algorithmic selection of fundamen-\ntal system units (FSUs), considered indivisible entities, with an\naggregative procedure, either algorithmic or optimization-based,\nto select composite system units (CSUs) made of several FSUs. A\nkey contribution is the introduction of a global network metric,\nthe partition index, which quantitatively balances intra- and\ninter-CSU interactions, with a granularity parameter accounting\nfor the size of CSUs, allowing for their selection at different\nlevels of aggregation. The proposed method is validated through\ncase studies in distributed model predictive control (DMPC)\nfor linear and hybrid systems, showing significant reductions\nin computation time and cost while maintaining or improving\ncontrol performance w.r.t. conventional strategies.\nIndex Terms—Partitioning, Non-Centralized Control, Large-\nScale Systems, Multi-Agent Systems, Predictive Control.\nI. INTRODUCTION\nT\nHE technological and scientific progress of the last cen-\ntury allowed the realization of large-scale infrastructures\nin several domains. These networks of systems are commonly\nfound in the fields of transportation [1], [2], power generation\nand distribution [3]–[7], and telecommunications [8], among\nothers [9]–[11]. These are considered large-scale systems\n(LSSs) due to their geographical distribution, heterogeneity\nof components, and complexity of design and operation [12].\nWith such characteristics, large-scale networks represent a\nstimulating challenge for control.\nSignificant improvements in performance, safety, and user\nexperience can be achieved by applying control methodolo-\ngies. Of particular interest are non-centralized control tech-\nniques [13], [14], solutions developed for the application of\nreal-time control to large-scale networks. The more traditional\ncentralized approach to control is often unsuitable in this\ncase. Limitations arise from the computational complexity\nintroduced by the size of the problem, delays in data com-\nmunication, operational constraints, and privacy limitations.\nThis project has received funding from the European Research Council\n(ERC) under the European Union’s Horizon 2020 research and innovation pro-\ngram (Grant agreement No. 101018826) – Project CLariNet. (Corresponding\nauthor: Alessandro Riccardi)\nThe authors are with the Delft Center for Systems and Control (DCSC),\nDelft University of Technology, 2628 CD Delft, The Netherlands (email:\na.riccardi@tudelft.nl; l.laurenti@tudelft.nl; b.deschutter@tudelft.nl).\nWith non-centralized control, such issues can be mitigated or\ntranslated into a more tractable class of problems [15].\nA fundamental task at the core of every non-centralized\ncontrol strategy is the definition of the sub-networks of the sys-\ntem that will be considered as individual operational units, i.e.\nsuitable for the application of a local controller part of a non-\ncentralized architecture. The act of defining such sub-networks\nis defined partitioning [16]. From this abstract formulation, it\nimmediately follows that partitioning a network is a complex\ntask that often has to be tailored for the specific application\nconsidered. As a matter of fact, a generalized approach to\npartitioning is currently missing in the literature. On one side,\nthis issue is related to the fact that often when a novel control\nstrategy is being developed, a network partition is assumed to\nbe given a priori; on the other, partitioning might depend on\nrequirements non exclusively related to control objectives. The\npresent article addresses some of these problems, mainly in the\nformer class, laying the foundation for developing generalized\npartitioning strategies for control, and showing how it is a\ncentral problem to address when deploying a non-centralized\ncontrol strategy.\nIn the literature, the partitioning problem is usually ap-\nproached in one of the two following ways: i) using the top-\ndown aggregation, where we decompose a monolithic system\ninto components, which define the control units; ii) through\nthe bottom-up approach, where predefined subsystems are\nmerged into larger control units. However, an approach that\nworks for all categories of systems following a systematic\nprocedure, i.e. a generally valid technique, is not present yet\nin literature. Additionally, the problem of defining the number\nof control units in the partition is often solved by introducing\nassumptions or heuristics, whereas the role of the size of these\nunits and their hierarchy is not usually a point of investigation.\nIn this work, we propose a general partitioning strategy\nfor control, a novel approach expanding and improving on\nour preliminary results [16], [17] with stronger mathematical\nfoundations, refined algorithms, generalized metrics, and new\nstrategies to approach the problem. The main contributions we\npropose are: i) a formal definition of graph equivalence, which\nallows the definition of the concepts of fundamental system\nunits (FSUs), composite system units (CSUu), and control\npartition; ii) a novel algorithm for the selection of the FSUs\nfor any graph equivalent to a dynamical system; iii) a global\nmetric, the partition index, accounting for the interactions in\nthe network, and the size of CSUs through a granularity pa-\nrameter α; iv) optimization-based and algorithmic partitioning\napproaches, for which the partition index can be specifically\nThis work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.\narXiv:2502.21126v1  [eess.SY]  28 Feb 2025\n\n\n2\ncharacterized, and returning the control partition in terms of\ngroups of FSUs depending on the desired level of granularity.\nThe paper is organized as follows. In the remainder of\nthis section, we present the current partitioning approaches\nin literature, and we briefly recapitulate some graph theory\nconcepts and notation. In Sec. II we propose the concept\nof graph equivalent to a dynamical system. Then, the novel\ncore concepts of FSU, CSU, control partition, aggregation\noperation, and the proposed structure of the generalized par-\ntitioning strategy are presented in Sec. III. The algorithm we\npropose for the selection of the FSUs is detailed in Sec. IV;\nwhile the generalized partitioning strategy, including the new\nconcept of partition index, and the optimization-based and\nalgorithmic approaches are presented in Sec. V. We conclude\nwith some application examples for partitioning in Sec. VI,\nand a case study illustrating the role of partitioning for the\ndistributed model predictive control of networks of linear and\nhybrid systems in Sec. VII. All the experiments and software\ndeveloped for this article are available at the open-source long-\nterm access repository [18].\nA. Literature Survey\nDespite the absence of a generalized approach, partitioning\nis a relevant topic for the control [19]. Existing partitioning\ntechniques can be classified into the two fundamental ap-\nproaches presented below.\n1) Top-Down Approaches: These methods address the par-\ntitioning problem by decomposing a system into smaller, man-\nageable sub-networks based on network structure, interaction\nstrength, or physical properties. These approaches primarily\nleverage graph-theoretic metrics [20], [21]. In this context, the\nmost widely accepted partitioning metric is modularity [22],\nallowing the definition of partitioning techniques referred to\nas community detection strategies [23]–[25]. The modularity\nmetric can be used to capture both the information in the\nstate transition matrix of a linear dynamical system, and\nthe topological information about the network. The scope of\nmodularity-based techniques is to maximize the modularity.\nMaximization of modularity is known to be an NP-hard\nproblem [20], [26]; therefore, polynomial-time algorithmic\nprocedures have been developed for partitioning according to\nmodularity maximization.\nTop-down partitioning approaches have also been developed\nin specialized contexts for selected applications. For example,\nin [27] a wind farm is partitioned into control units according\nto the coupling given by the wake-effect that influences down-\nstream turbines. In [28], flow-based distribution systems are\nconsidered, focusing on water distribution networks. One\nrelevant feature of the approach proposed in that work is multi-\ncriteria optimization for partitioning.\n2) Bottom-Up Approaches: These methods are aggregative\nand start from the assumption that a group of autonomous\nand indivisible FSUs is given a priori. This assumption will\ngenerally hold for cooperative groups of mobile agents, such\nas coalitions of mobile robots, fleets of aerial vehicles, and\nplatoons of autonomous driving cars. However, bottom-up\napproaches are also used in transportation and power networks.\nOne recent distributed control setting is coalitional control\n[29], [30] where control theory and game theory are combined.\nIn coalitional control, FSUs aggregate into bigger groups,\ndenoted coalitions, to pursue a common control objective. For\neach coalition, i.e. collection of FSUs, a performance index\nis computed, which is a function of the aggregated state and\ninput values of the coalition. This index can be interpreted as\na transferable utility that can be reallocated among the agents.\nPartitioning is achieved through an algorithmic procedure,\nwhere agents are exchanged among coalitions to maximize\nthe differential gain in the coalitional cost until no further\nimprovement is possible.\nBottom-up approaches are generally more suited for situ-\nations where the definition of the sub-networks is non-static,\nwhich can occur in the case of non-stationary networks, where\na partitioning technique is required to produce a non-stationary\ndefinition of the groups. An example of this approach is in\n[31], [32], which considers linear switching systems and event-\ndriven systems. In these cases, conditions triggering the re-\npartition of the network are defined, together with ad-hoc\nstrategies to perform it. The effects of partitioning on the\nproperties of a control architecture are explored less frequently.\nAn exception is [33], where feasibility regions and robust\ninvariant sets are defined for distributed tube-based MPC.\n3) Mixed Strategies: In [34] coalitional control, a bottom-\nup approach, is coordinated by a supervisory layer introduc-\ning a clustering problem for the selection of the size and\ncomposition of the coalitions, which is a top-down decision.\nPartitioning is achieved through a binary quadratic program\nwhere the decision variables are the states of the links between\nagents, and the cost is a composition of the gradient vectors\nof the control objective. However, such approach inherits the\ncomputation complexity of the general clustering problem,\nthus requiring the solution of an NP-hard problem.\nB. Preliminary Concepts and Notation\nA graph [35] is an ordered pair of sets G = (V, E) where\nV = {1, . . . , n} is the set of n vertices, and E ⊆V × V is\nthe set of the edges. The edges are associated to the vertices\nthrough a binary adjacency matrix Aadj, where Aadj,(i,j) = 1\nif and only if an edge ϵij = (i, j) ∈E exists. Therefore, the\ntopology of the graph is specified by the adjacency matrix\nAadj, and the set of the edges can also be written as E =\n{(i, j) | i, j ∈V ∧Aadj,(i,j) = 1}. A subgraph of G is a graph\nSi = (Vi, Ei) representing a part of G. The set of vertices Vi\nis a subset of V, i.e. Vi ⊆V, and the set of the edges is Ei =\n{(i, j) | i, j ∈Vi ∧Aadj,(i,j) = 1}, where the topology is still\nspecified by the relevant entries of Aadj. For a directed graph\nG, an edge ϵij = (i, j) denotes an arrow starting from node i\nand ending in node j. A graph G is weighted if a weighting\nmatrix Wadj assigning to each edge a number is specified in\naddition to Aadj. For each vertex i ∈V we denote by di its\ndegree, i.e. the number of edges entering or exiting that vertex.\nIn directed graphs, we can specify an in-degree (di,in) and an\nout-degree (di,out), if the edge is respectively ending or starting\nin the vertex i. For a vertex i, the neighborhood of i is the\nset of all vertices connected to it, and we denoted it by Ni =\n\n\n3\n{j ∈V | (i, j)∨(j, i) ∈E}. For a subgraph Si = (Vi, Ei), the\nfrontier is its set of nodes that are connected to nodes outside\nthe subgraph, i.e. Fi = {i ∈Vi | (i, j)∨(j, i) ∈E, j ∈V\\Vi}.\nII. THE EQUIVALENT GRAPH OF A DYNAMICAL SYSTEM\nIn this section, we introduce the definition of the equivalent\ngraph of a dynamical system for a general class of nonlinear\ndiscrete-time systems. Then, we characterize the definition\nfor two important classes of dynamical systems: linear and\npiecewise affine (PWA) systems. For both classes, we highlight\nthe properties that can be established from their topology,\nwhich are relevant for defining a partitioning strategy.\nA. General Definition of Equivalent Graph\nConsider a discrete-time system of the form:\nx(k + 1) = f(x(k), u(k)) + g\n(1)\nwith x ∈X ⊆Rn, u ∈U ⊆Rp, with f differentiable over\nΩ= (X, U) almost everywhere (i.e. except for a Lebesgue set\nof measure zero), and g is a constant. Moreover, assume that\nf(0, 0) = 0. The equivalent graph representation of (1) is a\ntime-dependent graph Gk = (V, Ek, wk, ˜g), where V is the set\nof the vertices, Ek ⊆V × V is the set of the edges, wk is a\nweighting function for the edges that implicitly characterizes\nthe topology of the graph, and ˜g is a labeling function for\nthe vertices. To construct the equivalent graph, first, we define\nthe set of vertices: a new vertex is defined for each input\nand state variable of (1). These vertices belong respectively\nto two subsets of vertices, which are Vu = {u1, . . . , up} and\nVx = {x1, . . . , xn}. The set of vertices of the equivalent graph\nrepresentation is thus the union V = Vu\nS Vx. Then, we define\nthe weighting function wk, from which we can also extract the\nedges of the graph. This function wk assigns to each couple\nof vertices a function defined as:\nwk(i, j) =\n( ∂fj(k)\n∂i\nfor\ni ∈V, j ∈Vx\n0\nfor\ni ∈V, j ∈Vu\n(2)\nFrom wk, it is possible to distinguish between the sets of input-\nto-state and state-to-state edges, which are respectively:\nEux,k = {(i, j) | i ∈Vu, j ∈Vx, wk(i, j) ̸= 0}\n(3a)\nExx,k = {(i, j) | i, j ∈Vx, wk(i, j) ̸= 0}\n(3b)\nThe set of the edges of the equivalent graph is then obtained\nas the union Ek = Eux,k\nS Exx,k. We will use an analogous\nnotation for the weighting function referring to the above sets,\nusing wux,k and wxx,k respectively. Finally, the labeling ˜g of\nthe vertices is selected to coincide with the constants g of (1)\nfor state vertices, and zero for input vertices:\n˜g(i) =\n(\ngi\nfor\ni ∈Vx\n0\nfor\ni ∈Vu\n(4)\nThe equivalent graph G thus obtained is a variable structure\nthat reflects the nonlinear nature of system (1). Specifically,\nfor each pair (x(k), u(k)), the topology and the weighting of\nthe graph may change, while the set of the vertices is time-\nindependent. We now show how to build the equivalent graph\nfor linear and piecewise linear systems.\nB. Equivalent Graph Equivalent of a Linear System\nConsider the system:\nx(k + 1) = Ax(k) + Bu(k)\n(5)\nwhere A and B are matrices of appropriate dimensions. The\nstructure of the graph G is now time-independent and presents\nno labeling. Consequently, the sets of the edges are:\nEux =\n\b\n(i, j) | i ∈Vu, j ∈Vx, B(j,i) ̸= 0\n\t\n(6a)\nExx =\n\b\n(i, j) | i, j ∈Vx, A(j,i) ̸= 0\n\t\n(6b)\nC. Equivalent Graph of a Piecewise Affine Linear (PWA)\nSystem\nConsider a PWA system of the form:\nx(k +1) = Aqx(k)+Bqu(k)+gq\nfor\n\"\nx(k)\nu(k)\n#\n∈Cq, (7)\nfor operational modes q = 1, . . . , N, with C1, . . . , CN con-\nvex polyhedra in the input-state space with non-overlapping\ninteriors [36]. For such systems, the weighting function wk\n(2), the set of the edges E, and the labeling ˜g can change\naccording to the convex polyhedron Cq. Considering the set of\nthe edges E, depending on the transition, the change can occur\nin Eu, in Ex, in both simultaneously, or not occur. According\nto this description, the sets of the edges (3) for PWA systems\nassume the characterization depending on the current mode\nq = q(x(k), u(k)):\nEq\nux =\nn\n(i, j) | i ∈Vu, j ∈Vx, Bq\n(j,i) ̸= 0\no\n(8a)\nEq\nxx =\nn\n(i, j) | i, j ∈Vx, Aq\n(j,i) ̸= 0\no\n(8b)\nand the set of the edges of the equivalent graph is Eq =\nEq\nux\nS Eq\nxx. Therefore, we have a finite set of at most N\ntopologies that can vary according to the operational mode.\nD. Number of Distinct Topologies for a Dynamical System\nThe topology of the equivalent graph of a dynamical system\nis determined by the set of the edges E. As stressed above,\nfor linear systems this topology is static; thus a linear system\nonly has one possible configuration for the edges for a given\npair (A, B) of state space matrices. For PWA systems, the\npossible number of topologies is equal to the number N of\ndifferent operating modes at most. However, some operating\nmodes may share the same topology and only differ in the\nformulation of the dynamics. For a general dynamical system\nof the form (1), we can define an upper bound on the number\nof distinct topologies:\nLemma 1. The graph associated with system (1) has at most\n2n(n+p) distinct topologies. If self-edges are not considered,\nthe graph has at most 2n(n+p−1) distinct topologies.\nProof. This statement is shown by considering the definition\n(3), where for each pair of vertices (i, j), we are interested\nonly in the cases where the derivative is zero or non-zero.\nThus, we have two possible combinations for each of the\nn functions and p or n variables, which provides the result.\nIf self-edges are not considered, the statement holds without\naccounting for the edges (i, i) ∈E with i ∈Vx.\n\n\n4\nIII. THE CONCEPTS OF COMPOSITE SYSTEM UNIT AND\nCONTROL PARTITION, AND THE STRUCTURE OF THE\nGENERALIZED PARTITIONING STRATEGY\nA. Composite System Unit and Control Partition\nOne of the scopes of a partitioning strategy applied to a\ndynamical system is to select subsystems that can be con-\nsidered as individual control units for a specified application.\nIn this section, we first define a special type of subsystem,\nthe composite system unit, which allows the definition of a\ngeneralized partitioning strategy. Subsystems of this type can\nbe systematically represented as graphs. Then, we show how\nto aggregate these subsystems. Finally, we provide the general\ndefinition of partitioning based on the concepts above. First,\nwe consider the abstract definition of subsystem:\nDefinition 1 (Subsystem). For a dynamical system of the form\n(1), a subsystem is a group of state dynamics collected into\na vector x[i] and whose behavior is defined by x[i](k + 1) =\nf [i](x(k), u(k)) + g[i], with x[i] ∈X [i] ⊆Rni.\nThis definition is not sufficiently specialized to define a\ngeneralized partitioning strategy. To this end, we introduce the\ndefinition of composite system unit, which allows the system-\natic selection of input-state vector pairs (x[i], u[i]). The choice\nof these pairs is not unique; however, our characterization is\nmotivated by the scope of defining a generalized partitioning\nstrategy.\nDefinition 2 (Composite System Unit). A composite system\nunit (CSU) is a nonautonomous dynamical system whose\ninputs affect only the state dynamics of the CSU itself. All the\ndynamic relations the CSU has with other CSUs occur through\ndynamical coupling among the states, or direct input-output\nconnections. For the dynamics (1), a CSU is a subsystem\nof the form x[i](k + 1) = f [i](x(k), u[i](k)) + g[i] with\nx[i] ∈X [i] ⊆Rni, u[i] ∈U[i] ⊆Rpi, and such that\nw(k, l) = 0 for all l ∈Vi,x, k ∈Vj,u.\nFrom this definition, it follows immediately that a CSU is\na subsystem in the sense of Definition 1. We denote a CSU i\nassociated to the input-state pair (x[i], u[i]) with the symbol Si.\nThe graph equivalent to the CSU in Definition 2 is constituted\nby the set of vertices Vi = Vi,x ∪Vi,u ⊆V, by the weighting\nfunction w(i, j) with i, j ∈Vi, by the set of edges Ei,k =\n{(i, j) | i, j ∈Vi, wi,k(i, j) ̸= 0}, and by the labeling ˜gi. This\nis the graph Si,k = (Vi, Ei,k, wi,k, ˜gi).\nIf we consider linear discrete-time time-invariant systems of\nthe form (5), then from Definition 2 a CSU indexed by i has\nthe form:\nx[i](k + 1) = A[i]x[i](k) + B[i]u[i](k) + ω[i](k)\n(9a)\nω[i](k) =\nX\nj∈Ni\nA[ij]x[j](k)\n(9b)\nwhere x ∈X [i] ⊆Rni, u ∈U[i] ⊆Rpi, and A[i], B[i], A[ij]\nare matrices of appropriate dimensions. The signal ω[i]\nk is the\ncoupling effect that CSU i experiences with its neighboring\nstates j ∈Ni.\nIn the definition of a general partitioning strategy, a fun-\ndamental task is the selection of the smallest possible CSUs\nconstituting the network. We specify this concept through the\ndefinition of a fundamental system unit.\nDefinition 3 (Fundamental System Unit). Given a network of\ndynamical systems, a fundamental system unit (FSU) is the\nsmallest CSU definable through any network decomposition.\nFSUs represent the smallest individual components that can\nbe selected in a network without losing the nonautonomous\nproperty of the dynamics, i.e. the smallest subsystem that\nhas at least one control input and one state variable. Before\nproceeding with the general definition of partitioning, we\nintroduce the aggregation operation for two CSUs, which\nallows the merging of two distinct dynamics to form a bigger\nsubsystem.\nDefinition 4 (Aggregation Operation). Consider the equiv-\nalent graph representation Gk of the dynamical system (1),\nand two subsystems of the same dynamical system described\nby the equivalent graphs S1,k, S2,k. We define the aggrega-\ntion of the two subsystems over the graph G as S(1,2),k =\n(S1,k ⊎S2,k)|Gk:\nS(1,2),k =\n\u0000V1 ∪V2, E1,k ∪E2,k ∪E(1,2),k, w(1,2),k, (˜g1, ˜g2)\n\u0001\nwhere w(1,2),k = {wk(i, j) | i, j ∈V1 ∪V2}, E(1,2),k =\n{(i, j) | ((i ∈V1, j ∈V2) ∨(i ∈V2, j ∈V1)) ∧wk(i, j) ̸= 0}.\nThe following proposition allows the construction of CSUs\nas the result of the aggregation of other CSUs:\nProposition 1. The aggregation of multiple CSUs is a CSU.\nProof. The statement can be verified by construction.\nWe conclude this section by providing the general definition\nof partitioning for a dynamical system:\nDefinition 5 (Control Partition of a Dynamical System). For\na dynamical system of the form (1) with equivalent graph Gk,\na control partition Pk is defined as a collection of CSUs:\nPk = {S1,k, . . . , Sm,k}\ns.t.\n m\n]\ni=1\nSi,k\n!\f\f\f\f\f\nGk\n= Gk.\n(10)\nThis definition of control partition comprehends both non-\noverlapping and overlapping partitionings thanks to the defi-\nnition of aggregation operation.\nB. The Structure of the Generalized Partitioning Strategy\nIn this section, we present our novel generalized parti-\ntioning technique1. This technique applies to all types of\ndynamical systems that can be represented in form (1). Here,\nwe describe how the algorithms, metrics, and optimization\nproblem presented in the next sections merge to construct the\ngeneralized partitioning strategy. A generalized partitioning\ntechnique consists of two main steps:\n1) Selection of the FSUs, performed through an algorithmic\nprocedure.\n1The optimization problem and algorithms, as well as the examples and\ncase studies presented below, can be accessed through the repository [17].\n\n\n5\n2) Partitioning by aggregation, achieved either by solving\nan integer optimization problem, or through an algo-\nrithm.\nFor step 1), we describe the algorithmic approach in Section\nIV. This algorithm is designed to work over the equivalent\ngraph representation of the dynamical system.\nOnce the FSUs are defined, we are presented with a collec-\ntion of subsystems that can not be further divided without\nlosing the non-autonomicity characteristic. By aggregating\nthese subsystems, we obtain the desired partition of the system.\nTo this end, first, we introduce a novel metric in Section V-A,\nwhich we call the partition index. This is a generalized global\nmetric that needs information about the entire structure of the\nnetwork and can be characterized according to the partitioning\nprocedure that one wants to deploy. Then, we formulate the\ninteger optimization problem to obtain the best value of the\npartition index in Sec. V-D. Alternatively, two algorithmic\napproaches to solve the same problem with a different charac-\nterization of the partition index are presented in Sec. V-B and\nV-C. All strategies have advantages and disadvantages, which\nwill be discussed in the respective sections.\nIV. ALGORITHM FOR THE SELECTION OF FSUS\nHere, we present the algorithm for the selection of the\nFSUs. This algorithm is general and can be applied to any\nnetwork with structure (1). Given the time-dependence of (1),\nthis algorithm is potentially applied at each time step k, but in\nthe following we omit the subscript k for simplicity. Also, the\nalgorithm can be specialized to exploit the specific structure of\nthe dynamics for linear and PWA systems. Before proceeding,\nwe introduce some additional notation that will be used in the\nalgorithm. We will indicate the FSUs as subsystems Ai, for\ni = 1, . . . , NFSU, where NFSU ≤|U|, and the collection of\nall the FSUs as A = {A1, . . . , ANFSU}. Also, we will use the\nadditional set L ⊂X, indicating the state nodes that remain\nto be assigned to the FSUs after preliminary operations are\ncomplete.\nThe general algorithm for selecting FSUs consists of three\nmain steps that we present in the following in detail. These\nsteps will be performed in a specific order as a part of the\niterative procedure illustrated in Alg. 1:\n1) Selection of the roots of the FSUs. To select the FSUs,\nwe first select their anchor points, one for each FSU. We\ncall these anchor points the roots of the FSUs. These\nroots consist of at least one input node and one state\nnode directly connected by an edge. The selection of\nthe roots is performed on the subgraph (V, Eux, wux, ˜g),\nand works as follows. We start by assuming to have one\nFSU Ai ∈A for each input node i ∈U, where the\nindexing of the FSUs in the collection A may vary as\nthe procedure progresses. Then, we consider for each\ninput node in U the set of the edges connecting them\nto the state nodes: for i ∈U, if wij ̸= 0, with j ∈X,\nwe assign the state node j to the FSU Al to which\nthe input node i belongs. If we encounter a state node\nj ∈X connected to two or more input nodes in U, the\nentire group constitutes an indivisible root, and all are\nmerged into the same FSU. Consequently, we update the\nindexing of the FSUs in the collection A. We can have a\nmaximum number of roots, and therefore of FSUs, equal\nto NFSU ≤|U|. All the unassigned state nodes remaining\nfrom the selection of the roots are collected into the list\nL that will be used in the next steps.\n2) Forward assignment of the state nodes. In this step, we\nperform the assignment of state nodes not belonging to\nthe roots of the FSUs already, and for which a directed\nedge from the state nodes in the roots exists. We call\nthis procedure forward assignment of the state nodes.\nHere we consider the graph (X, Exx, wxx, ˜g). For each\nunassigned node j ∈L, we consider the edge wij\nstarting from a state node i ∈X in the roots and ending\nin j that has the largest absolute value, i.e. maxi |wij|. If\nthis wij exists, we assign the node j ∈L to the FSU to\nwhich the state node i = arg maxi |wij| belongs, and we\nupdate the set L. If such wij does not exist, we proceed\nwith the assignment procedure for the next state node\nin L until no further forward assignments are possible.\nWe call this phase forward assignment since we look\nat the edges starting from the states in the roots of the\nFSUs and that are directed toward the periphery of the\nequivalent graph.\n3) Backward assignment of the state nodes. This step of\nthe algorithm assigns the state nodes in L that have no\nforward connection from the roots. Therefore, we call\nthis step backward assignment because we consider the\nconnections from the nodes belonging to the periphery\nof the graph toward the direction of the roots. For each\nremaining i ∈L, following the same principle in step\n2), we select the node j belonging to an FSU such that\nj = arg maxj |wij|. We repeat the procedure for the next\nstate node in L until we have checked all the nodes once.\nAlgorithm 1: Selection of FSUs\n1 Part 1 - Selection of the roots\nData: (V, Eux, wux, ˜g)\n2 Perform 1) in Sec. IV: selection of the roots\n3 return\n\u0000A[0], L[0]\u0001\n4 Part 2 - Selection of the FSUs\nData: (X, Exx, wxx, ˜g)\n5 k ←0\n6 while L[k] not empty ∨L[k] connected do\n7\nPerform 2) in Sec. IV: forward assignment\n8\n\u0000A[k+1], L[k+1]\u0001\n←\n\u0000A[k], L[k]\u0001\n9\nPerform 3) in Sec. IV: backward assignment\n10\n\u0000A[k+2], L[k+2]\u0001\n←\n\u0000A[k+1], L[k+1]\u0001\n11\nk ←k + 2\nRemark 1. If the system considered is time-varying, its\ntopology can change at each time step. For such systems, the\nalgorithm for selecting the FSUs has to be applied each time\nthere is a variation in the topological structure, i.e. in the\nedges or their weights.\n\n\n6\nRemark 2. If outputs are also specified in the dynamics of\nthe system, the procedure defined above still holds, but it is\nnecessary to assign the output nodes to each CSU in the same\nway as done for the state nodes.\nV. THE PARTITIONING STRATEGY\nIn this section, we present a novel metric that will be\nused to address the partitioning problem in a generalized\nsetting. We call this metric the partition index. Then, we\npropose an algorithmic procedure to solve the problem, which\nreturns the partition, i.e. a set of CSUs, for the required\ngranularity. This approach provides a sub-optimal solution but\nrequires polynomial time to be computed. In Sec. VI-B, we\nempirically show how this first approach offers a good trade-\noff between optimality of the solution and computational com-\nplexity. Moreover, we show that when the partition index is a\nquadratic function, the partitioning problem can be formulated\nas an integer quadratic program (IQP). This will require a\nnovel characterization of the partition index, which must be a\nquadratic function. In Sec. VI-B, we show how this approach\nprovides the best solution to the problem, but the downside is\nthat it is an NP-hard problem.\nA. The partition index\nFor a specific partition P we denote the partition index\nby pidx(P). We define this metric to account for both the\ninteractions that occur inside a CSU, and among the CSUs.\nIn this sense, the partition index is a global metric since it\nrequires global information about the system to be computed.\nConsider a partition P = {S1, . . . , SNCSU}, with NCSU ≤\nNFSU ≤p where each Si is a CSU in the sense of Definition\n2. For CSU Si we define following measures: the intra-CSU\ninteraction W intra\nSi , the inter-CSU interaction W inter\nSi , and the\nsize of the CSU W size\nSi , i.e. the number of FSUs belonging to\nSi. The components W inter\nSi , W intra\nSi , W size\nSi\nof the partition index\n(13) can be constructed through the equivalent graph represen-\ntation. For this, consider a non-linear system of the form (1),\nwith equivalent graph representation G = (V, E, w, ˜g) where\nthe dependence on k is omitted. A CSU Si belonging to such\nsystem has an equivalent graph Si = (Vi, Ei, wi, ˜gi). For CSU\nSi we can define the intra-CSU, and the inter-CSU interactions\nrespectively as\nW intra\nSi\n=\nX\ns,t∈Vi\n|wi(s, t)|\n(11)\nW inter\nSi\n=\nX\ns∈FSi\nX\nj∈NSi\nX\nt∈Ns∩Vj\n|wi(s, t)| + |wj(t, s)|\n(12)\nwhere FSi denotes the frontier of the CSU Si, NSi the set\nof neighbor CSUs of Si, and Ns the set of neighbor nodes of\nnode s. Note that W size\nSi\nis equal to the square of the number\nof FSUs A belonging to the CSU Si, i.e. W size\nSi\n= |Vi|2. The\npartition index is defined as:\nDefinition 6 (Partition Index). The partition index of a net-\nwork of CSUs for a specified set-valued function h and a given\npartition P = {S1, . . . , Sm} is a global metric given by:\npidx(P) = h\n m\nX\ni=1\nW inter\nSi ,\nm\nX\ni=1\nW intra\nSi ,\nm\nX\ni=1\nW size\nSi , α\n!\n(13)\nwhere α > 0 is a scalar parameter allowing for the selection\nof the granularity of the partitioning\nIn the general case of the nonlinear dynamics (1), pidx(P) is\ntime-dependent, because is is based on a time varying graph.\nThe framework for partitioning that we propose is based on the\noptimization of pidx(P), and works for every characterization\nof h. In the following we propose one nonlinear form for h\nthat we will use for algorithmic partitioning in Sec. V-B; and\none quadratic for the optimization-based strategy in Sec. V-D.\nOther strategies using different characterizations of h can be\ndefined by the same approach.\nB. Algorithmic partitioning\nFor the first characterization of the partition index (13),\nwe assume a function that accounts for the ratio between\nthe intra- and inter-CSU interactions. Since the algorithmic\napproach will try to maximize the partition index, then we\nassume that terms W intra\nSi\nare at the numerator, and W inter\nSi\nat\nthe denominator. This approach will favor merging strongly\ncoupled FSUs into the same CSU, while assigning weakly\ncoupled FSUs to different CSUs. This approach is inspired\nby the modularity metric [37], [38], and the motivation for\nusing it in non-centralized control is that such approaches\nmodular partitioning usually improve performances of the\narchitecture by minimizing the interaction among subsystem.\nAlso, advantages in communication costs and privacy are\npresent. However, it is important to stress that this partitioning\napproach will not provide the optimal partitioning in absolute\nterms (as we will show in Sec. VII), and which motivates the\nintroduction of the abstract formulation (13). Moreover, we\ninclude in the (13) a novel term that accounts for the size of\nthe resulting CSUs, according to the parameter α allowing for\nselecting the desired granularity of the partitioning:\npidx(P) =\nm\nX\ni=1\nW Intra\nSi\n1 +\nm\nX\ni=1\nW Inter\nSi\n+\nα\n1 +\nm\nX\ni=1\nW size\nSi\n(14)\nThis index grows when the interactions among FSUs in the\nsame CSU increase, the interactions among CSUs decrease,\nand the sizes of the CSUs decrease. A greedy algorithm max-\nimizing this index is reported in Alg. 2. The time complexity\nof this algorithm is O(N 3\nFSU) because for each of the NFSU\niterations it is necessary to evaluate the immediate gain in\n(14) of the assignment of every unassigned FSU to each\nCSU, which can be at most NFSU, and this operation has to\nbe performed NFSU times, until no more FSUs have to be\nassigned.\nIn the algorithm, we start by creating a list of unassigned\nFSUs L, and an empty partition P, which is a list with NFSU\nentries, where the i-th entry is itself a sub-list denote by Pi,\nalso initialized as empty. The partition index corresponding to\nthe empty allocation at the first step is pidx(P) = 0. At each\niteration, the algorithm tries all possible assignments of the\nelements in L to all the sub-lists Pi. For each assignment, it\nevaluates which element of L maximizes the immediate return\n\n\n7\ngiven by the difference in (14) evaluated for P and for PNew,\nwhere the latter is the candidate next partitioning under evalua-\ntion for an element of L. This operation provides the maximum\nimmediate gain ∆pmax, which is re-initialized to minus infinity\nat each iteration of the while-loop. If the assignment of FSU\nAi to the sub-list Pj gives a ∆pidx > ∆pmax, then we store the\ncandidate best allocation in the variables ANext, jNext. When all\npossible evaluations in L are performed, the resulting ANext is\nassigned to PjNext, and removed from L. The algorithm stops\nwhen L is empty, returning the partition P.\nAlgorithm 2: Greedy algorithm for partitioning\nData: A = {A1, . . . , ANFSU}\nResult: P\n1 L ←A, P ←{P1, . . . , PNFSU}, pidx(P) ←0\n2 while length(L) > 0 do\n3\n∆pmax ←−∞, ANext ←None, jNext ←None\n4\nfor i = 1, . . . , length(L) do\n5\nAi ←Li\n6\nfor j = 1, . . . , length(P) do\n7\nPNew ←P s.t. Pj.append(Ai)\n8\n∆pidx ←pidx(PNew) −pidx(P)\n9\nif ∆pidx > ∆pmax then\n10\nANext ←Ai, jNext ←j, ∆pmax ←∆pidx\n11\nP ←P s.t. PjNext.append(ANext), L.remove(ANext)\nC. Improved algorithmic partitioning\nWe improve the partition quality returned by the greedy\nAlgorithm 2 by introducing a local refinement step at the\nexpense of higher computational complexity. Specifically, once\nthe assignment of an FSU to a CSU is performed, we can run\na nested evaluation of the partition, going through all the FSUs\nalready allocated and checking whether relocating them to a\ndifferent CSU improves the value of the partition index, and\niterating this procedure until no further improving by swapping\nis possible. A similar idea was proposed in [26], where\nan analogous procedure constitutes the overall partitioning\nstrategy. The local refinement routine is reported in Alg. 3, and\nthe improved algorithm is obtained by performing the steps in\nAlg. 3 right after the last step in the while loop of Algorithm 2,\ni.e. after L.remove(ANext) in line 11. Since in the worst case\nAlg. 3 re-runs three times through all the FSUs in a while\nloop, the overall computational complexity of the improved\nalgorithm is O(N 4\nFSU).\nD. Optimization-based partitioning\nThe characterization of the partition index proposed in (14)\ncan be interpreted as a nonlinear mixed-integer metric, for\nwhich, in general, it is not possible to formulate programming\nproblems allowing for a global optimum. To address this\nlimitation, in this section we propose a characterization of the\npartition index (13) that is a mixed-integer quadratic function.\nWith this choice, we will define a multi-objective optimization\nproblem that will try to balance the difference between W inter\nSi\nAlgorithm 3: Partition refinement\nData: P\nResult: P\n1 ∆pmax ←0, Exit Condition ←False\n2 while Exit Condition = False do\n3\nANext ←None, jNext ←None, jPrev ←None\n4\nfor i = 1, . . . , length(P) do\n5\nfor j = 1, . . . , length(Pi) do\n6\nATest ←Pi,j\n7\nPNew ←P s.t. Pi.remove(ATest)\n8\nfor k = 1, . . . , length(P) do\n9\nif k != i then\n10\nPNew ←PNew s.t. Pk.append(ATest)\n11\n∆pidx ←pidx(PNew) −pidx(P)\n12\nif ∆pidx > ∆pmax then\n13\nANext ←ATest\n14\njNext ←k, jPrev ←i\n15\nif ANext != None then\n16\nP ←P s.t. PjPrev.remove(ANext)\n17\nP ←P s.t. PjNext.append(ANext)\n18\nelse\n19\nExit Condition ←True\nand W intra\nSi , while accounting for the size of the resulting\nCSUs through the product of α and W size\nSi . The approach\nis conceptually similar to the one used to derive (14), but\nprovides a mixed-integer quadratic metric. Consequently, the\noptimization-based strategy for partitioning is based on the\nsolution of an Integer Quadratic Programming (IQP) problem,\nfor which an optimal solution can be obtained.\nUnder these considerations, we introduce the binary vari-\nables δij ∈{0, 1} to specify whether or not an FSU Ai belongs\nto a CSU Sj:\nδij = 1 ⇐⇒Ai ∈Sj\n(15)\nAll decision variables δij are collected into the vector δ. The\ntotal number of entries of δ is N 2\nFSU ≤p2, since the range of\ni, j is 1, . . . , NFSU. The number of non-empty CSUs Si that\nthe optimization procedure will generate is unknown a priori,\nbut it cannot exceed the number of FSUs by definition. Using\nthe binary variables δ, we can rewrite the terms in (12)-(13)\nrespectively as:\nW inter(δ) =\nNFSU\nX\nm=1\nNFSU\nX\ni=1\nNFSU\nX\nj=1\nj̸=i\nNFSU\nX\nl=1\nl̸=m\nδi,mδj,l (|w(i, j)|\n+|w(j, i)|)\n(16)\nW intra(δ) =\nNFSU\nX\nm=1\nNFSU\nX\ni=1\nNFSU\nX\nj=1\nδi,mδj,m (|w(i, i)|\n+|w(i, j)| + |w(j, i)| + |w(j, j)|)\n(17)\n\n\n8\nW size(δ) =\nNFSU\nX\nm=1\n NFSU\nX\ni=1\nδi,m\n!2\n(18)\nThe network partition is thus expressed as a function of vector\nδ, i.e. P(δ). Additionally, we include a set of constraints on as-\nsigning the FSUs to the CSUs so that we get a non-overlapping\npartitioning. This requirement is codified by imposing that the\nsum of variables δi,j over the index j is equal to one, i.e.\neach FSU must belong only to one CSU. The resulting IQP\nproblem is:\nmin\nδ\npidx(P(δ)) = W inter(δ) −W intra(δ) + αW size(δ)\ns.t.\nNFSU\nX\nj=1\nδij = 1\n∀i\n(19)\nδij ∈{0, 1}\nSolving the IQP problem (19) provides the partition minimiz-\ning the partition index (13) for a given choice of the granularity\nparameter α. Such optimization problems are known to be\nNP-hard [39]; therefore, their scalability is limited, as will be\nshown in the examples in Sec. VI-A.\nRemark 3. For the problem definition (19), and for the\nalgorithmic approach in Sec. V-B, it is possible to find a range\nfor the selection of α. In this range, for large values of α, the\nresulting partitioning will be the set of individual FSUs, i.e.\nP = {A1, . . . , ANFSU}. Conversely, for small values of α, the\nresulting partitioning will be a single CSUs comprehending\nall FSUs, which is the entire system, i.e. P = {S1} with\nS1 = {A1, . . . , ANFSU}. Other choices of α in the range will\nlead to different levels of aggregation of the FSUs.\nVI. EXAMPLES: APPLICATION OF THE GENERALIZED\nPARTITIONING STRATEGY\nA. Selection of the FSUs\nTo show the features of Algorithm 1, we consider a dy-\nnamical system with 100 states and 20 inputs. The system has\nan equivalent graph represented in Fig. 1a. This graph can\nbe interpreted as a snapshot of the dynamical relationships\noccurring through system variables at a certain time instant.\nThe complexity of such graph makes the expert selection of\nFSUs, i.e. FSUs, a long and challenging task that quickly\nbecomes intractable as the group size grows. In practice, no\nclear structure can be identified in such graph at first sight. The\napplication of Algorithm 1 reveals the existence of 17 FSUs, as\nshown in Fig. 1b. For such FSUamental units, all the couplings\nwith their neighbors are guaranteed to occur through dynamic\nrelationships, and no inputs are shared among them. As shown\nin Fig. 1b, this property might require the aggregation of some\ninput nodes together to form single FSUs. It is interesting to\nnotice that the existence of a forward path from an input to a\nstate node inside an FSU implies, for discrete-time systems,\nthat the input will affect that state in a number of time steps\nequal to the length of the path (an analogous consideration is\npossible for continuous-time systems in the number of their\ntime derivatives [40]). Also, certain states, or groups of them,\nu1\nu2\nu3\nu4\nu5\nu6\nu7\nu8\nu9\nu10\nu11\nu12\nu13\nu14\nu15\nu16\nu17\nu18\nu19\nu20\nx1\nx2\nx3\nx4\nx5\nx6\nx7\nx8\nx9\nx10\nx11\nx12\nx13\nx14\nx15\nx16\nx17\nx18\nx19\nx20\nx21\nx22\nx23\nx24\nx25\nx26\nx27\nx28\nx29\nx30\nx31\nx32\nx33\nx34\nx35\nx36\nx37\nx38\nx39\nx40\nx41\nx42\nx43\nx44\nx45\nx46\nx47\nx48\nx49\nx50\nx51\nx52\nx53\nx54\nx55\nx56\nx57\nx58\nx59\nx60\nx61\nx62\nx63\nx64\nx65\nx66\nx67\nx68\nx69\nx70\nx71\nx72\nx73\nx74\nx75\nx76\nx77\nx78\nx79\nx80\nx81\nx82\nx83\nx84\nx85\nx86\nx87\nx88\nx89\nx90\nx91\nx92\nx93\nx94\nx95\nx96\nx97\nx98\nx99\nx100\n(a)\nu1\nx10\nx81\nx12\nx25\nx39\nx56\nx70\nx55\nx87\nu2\nx57\nx97\nx80\nx98\nu3\nu15\nx13\nx50\nx51\nx4\nx37\nx62\nx76\nx93\nu4\nx65\nx79\nx18\nx59\nx68\nu5\nx35\nx47\nx5\nu6\nx11\nx17\nu7\nx7\nx33\nx28\nu8\nu14\nx45\nx78\nx92\nx8\nx19\nx26\nx41\nx63\nx75\nx66\nx86\nu9\nx67\nx100\nu10\nx52\nx6\nx36\nx71\nx88\nx49\nx85\nu11\nu20\nx31\nx90\nx94\nx9\nx20\nx22\nx40\nx42\nx72\nx91\nx38\nx48\nx53x54x83\nu12\nx32\nx95\nx58\nu13\nx43\nx46\nx73\nx84\nu16\nx44\nx96\nx21\nx60\nu17\nx77\nx99\nx23\nu18\nx16\nx34\nx1\nu19\nx61\nx64\nx2\nx3\nx14\nx15\nx27\nx69\nx89\nx24\nx30\nx74\nx82\nx29\n(b)\nFig. 1.\nRepresentations of the equivalent graph of a system with 20 input\n(in red) and 100 states (in cyan) variables. In (Fig. 1a) the concentric degree-\nbased representation, where input nodes are positioned in the most outer part\nof a circle, while state nodes on concentric rings representing their degree,\nwith the most outer being degree one, and growing of one each ring toward\nthe center. In (Fig. 1b) the representation of the FSUs after the application\nof the selection procedure. Each of the 17 FSU is represented by the green\narea, where dynamic relationships among the variables in the same FSU are\nrepresented by with black arrows, and among the FSUs by green arrows.\nonly exhibit a backward effect on the FSU they belong to.\nFor such states, no forward path exists from any input node,\nmaking them unreachable states. Still, we assign them to their\nclosest FSU according to their coupling strength.\nB. Partitioning a Modular Network\nWe now show the application of our partitioning strategy\nto a modular network [37] [41] of FSUs reported in Fig. 2.\nSuch network is characterized by a basic pattern of four FSUs\nstrongly coupled, but weakly coupled with similarly config-\nured groups. This type of connection structure is repeated\nthroughout the network at different levels of aggregation. We\npropose this example to show three important aspects of the\n\n\n9\nproposed approach: 1) the use of the partition index (13) allows\nto obtain partitions that aggregate together strongly connected\ncomponents of the network while minimizing the coupling\nwith the others; 2) the function of the granularity parameter\nα defined in (13), which allows selecting the relevance of the\nlevel of aggregation of the components of the partition, thus\nreturning different configurations; 3) the respective advantages\nand limitations of the optimization-based and algorithmic\npartitioning approaches.\nIn the network in Fig. 2, a basic pattern of four FSUs that\nare strictly connected can easily identified by inspection. This\npattern repeats at a coarser level if we consider a group of 16\nFSUs connected by their corner links. Further, the structure\nrepeats if we consider the entire network of 64 FSUs. To\npartition this network, we first apply the optimization-based\nstrategy of Section V-D. To this end, we need to select a\nvalue for the granularity parameter α. In our testing, we used\na value related to the weights of the edges of the graphs, i.e.\nα =\n\u0000κ\nwmin\n\u00012, where κ is a scalar free to select by the user,\nand wmin is the minimum weight of the edges of the graph.\nBy varying κ, we obtain different α and consequently different\npartitions. We tried a broad spectrum of values α; however,\nwe retrieved only four distinct partitions, which actually cover\nall the possible aggregations of the FSUs in Fig. 2 at different\ngranularities that we can identify by inspection. Specifically,\nfor κ = 1, 0.1, 0.01, 0.001 we have α = 106, 104, 102, 1.\nThe partitions obtained, which we denoted respectively as\npartitions P1, . . . , P4, are represented in the corresponding\nFigs. 2a-2d. As expected, the larger the value of α is, the\nhigher the relevance of the size of the resulting CSU in the\npartition. If the value of α becomes sufficiently large, then\nthe resulting partition is constituted by all individual FSUs.\nConversely, if α is small enough, the network partition is\none individual CSU aggregating all the FSUs together. The\nother two selections of α return groups of 4 or 16 FSUs,\nrepresenting the other two optimal aggregations of FSUs.\nWhen applying algorithmic partitioning to the same problem,\nwe have retrieved two of the four optimal results obtained\nwith optimization-based partitioning. In particular, using the\nmetric (14), with a choice of α = 25 we get the partition in\nFig. 2a, where every FSU is allocated into a distinct CSU. By\nreducing α to 1, we get the partition in Fig. 2a, with 16 CSUs\ncontaining 4 FSUs each. For α smaller than 1, we obtained\nvarious combinations of the basic groups of four FSUs, but\nnever the exact partition in Fig. 2c\nThe last aspect to consider in this example is the computa-\ntional cost of the two approaches. We solved the optimization-\nbased problem using the Gurobi Optimizer software [42], run-\nning with parallel computing on a CPU Intel XEON E5-6248R\nwith 24 cores at 3GHz. In the worst case scenario, that is for\nthe partition in Fig. 2a with 64 individual FSUs, we let the\noptimizer run for 3 hours and 34 minutes before prematurely\nstopping it while reaching a solution gap of 7.07%. For the\nother cases, the solver could get the solution faster, with\ncomputation times as small as 15 minutes. The algorithmic\napproach is executed as a single-threaded task, given its\nsequential nature. The solution has been retrieved consistently\nin less than 600 seconds, with a clear computational advantage\ncompared to the optimization-based approach.\nVII. CASE STUDIES: THE ROLE OF PARTITIONING IN\nDISTRIBUTED PREDICTIVE CONTROL\nIn this section, we illustrate the applicability of the par-\ntitioning techniques developed throughout the paper to the\ndistributed MPC (DMPC) control of networks. The DMPC\narchitecture we will use for this aim is based on the alternating\ndirection method of multipliers (ADMM) [43], which is a\nwell-established framework [44], [45]. We will show how our\npartitioning technique can improve the performance of DMPC-\nADMM, both in terms of stage and computation cost. This will\nbe achieved first by applying DMPC-ADMM on the modular\nnetwork of 64 FSUs presented in Sec. VII-A and assuming\nthat each FSU is a linear system. Then in Sec. VII-B, for\nthe same network, we assume that each FSU is a piecewise\naffine (PWA) system [36], [46], [47], for which, at each\nADMM iteration, we deploy the MPC technique formulated\nin [48]. The performance of DMPC-ADMM improved w.r.t.\nthe conventional formulation, where all FSUs are considered\nindependently, while retaining the same pattern in computation\ntimes and costs obtained for linear systems. Finally, in Sec.\nVII-C, we propose a case study of a random network of\n50 FSUs for which we will compare optimization-based and\nalgorithmic partitioning.\nA. DMPC-ADMM for a modular network of linear FSUs\nWe consider each FSU a linear systems with matrices A\nand B such that the dynamics is stable and the network\nis controllable. We consider A(i,i) = 0.5, B(i,i) = 1, and\nvalues A(i,j) = 0.1 or 0.01 for strong and weak couplings\nrespectively, as represented by the thickness of the edges in\nFig. 2. For this system, we consider a prediction horizon of 30\nin time steps. Centralized MPC [14], [49] is applied to partition\nP4, while DMPC-ADMM is used for the others, according to\nthe formulations [44], [45], and selecting parameters ϵ = 0.001\n(consensus threshold), ρ = 0.01 (penalty parameter). The\ncontrol objective is to track a sinusoidal reference of unit\namplitude and with frequency w = 2π/15, with bounds\nu[i] ∈[−0.5; 0.5], x[j] ∈[−0.9; 0.9] ∀i, j. We assume a zero\ninitial condition. Given the maturity of the MPC framework,\nit is not surprising that the controller achieves excellent\nperformance for CMPC, and without any substantial variation\nfor the DMPC-ADMM alternatives. Control objectives are met\neasily; thus, we omit them, but they are accessible in the\nrepository [18]. Instead, we are interested in evaluating the\nresults through the performance metrics reported in Tab. I,\nwhere in the first column, each row corresponds to control\napplied using one of the partitions derived in Sec. VI-B. In the\nsecond column, we report the number of CSUs in the partition,\ncorresponding to the number of computing cores necessary\nto deploy a fully distributed control architecture. Next, we\nhave the value of the cumulative stage cost at the final step\nof the simulation. We see that the performance deteriorates\nminimally with a growing number of CSUs, not significantly\nenough to prefer the selection of a strategy w.r.t. another,\nbut also indicating that each distributed approach based on\n\n\n10\n(a) Partition P1\n(b) Partition P2\n(c) Partition P3\n(d) Partition P4\nFig. 2. Graphs related to the partitioning of a modular network with 64 FSUs. The strength of the connection is represented by the thickness of the links.\nPartitions P1-P4 are obtained for values α = 106, 104, 102, 1 respectively.\n1\nP4\n4\nP3\n16\nP2\n64\nP1\nNumber of cores\n200\n400\n600\nSeconds [s]\nComputation time comparison\n(a)\n1\nP4\n4\nP3\n16\nP2\n64\nP1\nNumber of cores\n2500\n5000\n7500\nCore Seconds [s]\nComputation cost comparison\n(b)\nFig. 3.\nComparison of the computation times (Fig. 3a), and computation\ncosts (Fig. 3b) for the different partitions. For the former, the gray trend line\nhighlights a diminishing return in computation speed as the number of cores\nincreases, while for the latter the almost linear relation between the number\nof cores and their associated computation cost.\nthe partitions obtained is effective in controlling the network.\nWe also evaluate the different approaches through parallel\ncomputation time, and computational cost, and for the latter\nwe use core seconds, i.e. the total amount of seconds the\ncomputing cores operate to deploy a fully distributed strategy.\nWe compute core seconds by picking at each ADMM iteration\nthe computation time of the slowest core and multiplying it by\nthe number of cores, then summing all these values throughout\nthe simulation. If we consider the parallel computation time\nin the fourth column, we see that the higher the number of\nCSUs is, the faster the DMPC-ADMM strategy is. To put\nthe numbers in perspective, in the fifth column we consider\nthe time required by fastest architecture P1 as a reference\nunit, and compute the ratio with the other approaches. It\nfollows that the partition P2 is only 1.35 times slower than\nP4, while the centralized MPC 4.66 times. Therefore, even\nif P1 is much faster than CMPC, there is a diminishing\nreturn w.r.t. P2. This fact can also be observed in Fig. 3a,\nwhere we report the computation times. Finally, we compare\nthe computational costs in the last two columns of Tab. I.\nAccordingly, considering CMPC in P1 the least expensive\nstrategy, the computation cost increase trend is almost linear in\nthe number of cores. This fact can be seen in Fig. 3b, where we\nplot the computation costs. The analysis of the results shows\nthat while the computation cost increases almost linearly\nwith the number of CSUs, the computation time decreases\nexponentially, highlighting a pattern of diminishing returns.\nAccordingly, it is comparatively more expensive to increase\nthe number of CSUs from the computation perspective. In this\ncase, partition P2 is 1.35 times slower than P1 while being\n2.94 cheaper, and requiring 48 computing cores less to be\nexecuted. Considerations of this nature can vastly differentiate\nthe selection of different approaches in practice. While DMPC-\nADMM performance as a function of a selected partition is\nnot an issue for linear systems, computation times can be.\nAs an example, we considered a partition manually selected\nto maximize the inter-CSU coupling, accessible at [18]. As\na result of our testing of the same control problem of the\nother cases, even if the ADMM approach can converge, we\nget an estimated parallel computation time of 160 hours for\nthe same simulation compared to less than 400 seconds for the\nother approaches, a clear demonstration of the value of proper\nselection of partitions.\nB. DMPC-ADMM for a modular network of hybrid FSUs\nWe now assume that each CSU in the network Fig. 2 is a\nhybrid system, specifically a PWA system of the form:\nx[i](k + 1) = 0.5x[i](k) + u[i](k)+\n+\nX\nj∈Ni\nA(i,j)x[j](k)\nif\nx[i](k) ≥0\n(20)\nx[i](k + 1) = −0.5x[i](k) + u[i](k)+\n+\nX\nj∈Ni\nA(i,j)x[j](k)\nif\nx[i](k) < 0\n(21)\nwhere, as in the previous case, A(i,j) = 0.1 or 0.01 as repre-\nsented in Fig. 2. Also, we retain the bounds u[i] ∈[−0.5; 0.5],\nx[j] ∈[−0.9; 0.9] ∀i, j. The objective is to track a sinusoidal\nreference with unit amplitude and frequency w = 2π/20 from\na zero initial condition. Here, we reduce the prediction horizon\nto 6 time steps to reduce the complexity of the problems.\nIn this case we first convert the system into Mixed Logical\n\n\n11\nTABLE I\nCOMPARISON OF DMPC-ADMM PERFORMANCE APPLIED TO A NETWORK OF LINEAR SYSTEMS FOR DIFFERENT PARTITIONING STRATEGIES\nPartition\nCores\nCost function value\nComputation time [s]\nComputation time ratio\nCore seconds [s]\nCore seconds ratio\nP4\n1\n465.4703\n680.88\n4.6619\n680.88\n1.0000\nP3\n4\n465.4709\n369.56\n2.5303\n1478.23\n2.1710\nP2\n16\n465.5153\n198.52\n1.3592\n3176.39\n4.6651\nP1\n64\n465.5158\n146.05\n1.0000\n9347.37\n13.7283\nTABLE II\nCOMPARISON OF DMPC-ADMM PERFORMANCE APPLIED TO A NETWORK OF HYBRID SYSTEMS FOR DIFFERENT PARTITIONING STRATEGIES\nPartition\nCores\nCost function value\nComputation time [s]\nComputation time ratio\nCore seconds [s]\nCore seconds ratio\nP4\n1\n24949.16\n849.57\n2.7691\n849.57\n1.0000\nP3\n4\n24945.76\n632.35\n2.0611\n2529.41\n2.9772\nP2\n16\n24945.77\n488.05\n1.5907\n7808.80\n9.1914\nP1\n64\n24946.04\n306.80\n1.0000\n19635.58\n23.1123\n0\n10\n20\n30\n40\n50\n60\nTime step k\n−0.5\n0.0\n0.5\nu(k)\nInput evolution\n(a)\n0\n10\n20\n30\n40\n50\n60\nTime step k\n−1\n0\n1\nx(k)\nState evolution\n(b)\nFig. 4. Evolution of the inputs (Fig. 4a) and of the states (Fig. 4b) for the\nnetwork of PWA FSUs. For the former, all signals are approximately the same\nand they respect the constraints.For the latter, when the value of a state crosses\nthe threshold at zero, we have a change of dynamics, which is reflected in\na different behavior. Accordingly, the input signal changes too, but reaches\nsaturation before the state is able to reproduce the reference.\nDynamical (MLD) form [36], and then we apply MPC as\ndescribed in [48] for centralized control, and use the same\nprocedure in combination with the DMPC-ADMM strategy\n[43]–[45]. In this case, at each step we are required to solve a\nMixed Integer Quadratic Program (MIQP) [39] to obtain the\ncontrol action. Even if for nonconvex and possibly nonsmooth\noptimization it is not always guaranteed that the ADMM\nalgorithm will converge [50], similar approaches have been\ndeployed successfully in literature [51]. We use the same\nhardware specified in Sec. VII-A. We report the results of the\nsimulations in Figs. 4a, 4b and in Tab. II. The results show a\nmarginally improved cumulative stage cost. Also, the patterns\nfound in the previous section for linear systems regarding\ncomputation times and costs repeat here.\nC. DMPC-ADMM for a random network of 50 hybrid systems\nWe consider a network with 50 FSUs with hybrid dynamics\n(20) that are connected through a randomly generated topol-\nogy, reproducible through the software in [18]. The network\nis in Fig. 6. We apply both optimization-based and algorith-\nmic partitioning, obtaining different configurations of CSUs\nfor which we will test CMPC and DMPC-ADMM control\nstrategies as above. All the partitions obtained are reported in\nFig. 6a - 6i. For both partitioning approaches, the endpoints\nof the range of α return two partitions that we denote by\nPADMM (with 50 individual CSUs) and PCMPC (with one CSU\nfor the whole network). The superscript notation refers to the\nfact that we deploy the conventional DMPC-ADMM strategy\nfor the former partition, and for the latter CMPC. Selecting\nthe weights as described in Sec. VI-B, for optimization-based\npartitioning we obtain five partitions POpt\ni\n, i = 0, . . . , 4, for α\nvarying, which are in Figs. 6a, 6b, 6e, 6h, 6i. For algorithmic\npartitioning we have the six partitions PAlg\ni\n, i = 0, . . . , 5,\nshown in Figs. 6a, 6c, 6d, 6f, 6g, 6i. Respective weights are\nindicated below the figures, where the CSUs are represented\nby the colored areas collecting together FSUs. Given such\npartitions, we apply the same DMPC strategy deployed in Sec.\nVII-B. This time, we assume a different initial condition for\neach CSU and a different sinusoidal reference to track, both\nrandomly generated. Data resulting from the simulations and\nscripts to reproduce them can be accessed at [18]. Reporting\nthe graphs of each simulation is prohibitive. Therefore, here\nwe propose in Figs. 5a, 5b respectively the evolution of the\nmaximum error and the maximum absolute value of the states\nacross all FSUs for each partition, showing that the controller\ncan achieve the objectives. Results for all partitions are in\nTab. III. Here we notice that, while for PCMPC we still get the\nbest performance at the expense of the highest computation\ntime, for PADMM we have a different situation w.r.t. what we\nfound previously in Sec. VII-B. For PADMM, there is now a\nsubstantial difference in performance w.r.t. the other partitions,\nwhich poses the question of the impact of topology in network\ncontrol. The best-performing strategies are now in order PAlg\n3\nand POpt\n2 , with a difference in the cost value of 0.16%, 0.24%\nrespectively, but a significantly lower computation time for\nPOpt\n2 , which can be considered as the most suitable partition\nin this case. To further compare the results, Figs. 7a, 7b show\nthe graphs relating computation time and cost to the number\nof CSUs in the network, and consequently of computing\ncores necessary for the implementation of the distributed\n\n\n12\nTABLE III\nCOMPARISON OF DMPC-ADMM PERFORMANCE APPLIED TO A RANDOM NETWORK OF HYBRID SYSTEMS FOR DIFFERENT PARTITIONING STRATEGIES\nPartition\nCores\nCost fun. value\nOpt. loss %\nComp. time [s]\nComp. time ratio\nCore seconds [s]\nCore seconds ratio\nPCMPC\n1\n6899.9750\n0.00\n2628.04\n26.4870\n2628.04\n1.3736\nPADMM\n50\n7749.2102\n12.31\n99.22\n1.0000\n4960.99\n2.5930\nPOpt\n1\n15\n6976.9122\n1.12\n279.25\n2.8145\n4188.73\n2.1893\nPOpt\n2\n6\n6916.7114\n0.24\n436.29\n4.3972\n2617.71\n1.3682\nPOpt\n3\n2\n6918.2608\n0.27\n1368.36\n13.7918\n2736.72\n1.4304\nPAlg\n1\n11\n6982.5798\n1.20\n173.93\n1.7530\n1913.28\n1.0000\nPAlg\n2\n9\n6975.5149\n1.09\n353.81\n3.5660\n3184.27\n1.6643\nPAlg\n3\n5\n6911.0475\n0.16\n2818.69\n28.4085\n14093.47\n7.3661\nPAlg\n4\n4\n6923.4294\n0.34\n1681.59\n16.9481\n6726.37\n3.5156\n0\n10\n20\n30\n40\n50\n60\nTime step k\n0.2\n0.4\n0.6\n0.8\ne(k)\nMaximum error evolution\nPADMM\nPOpt\n1\nPOpt\n2\nPOpt\n3\nPCMP C\nPAlg\n1\nPAlg\n2\nPAlg\n3\nPAlg\n4\n(a)\n0\n10\n20\n30\n40\n50\n60\nTime step k\n0.4\n0.6\n0.8\nMax |x(k)|\nMaximum absolute value for the state\nPADMM\nPOpt\n1\nPOpt\n2\nPOpt\n3\nPCMP C\nPAlg\n1\nPAlg\n2\nPAlg\n3\nPAlg\n4\n(b)\nFig. 5. Evolution of the maximum error (Fig. 5a) and absolute value (Fig.\n5b) across all CSUs for different partitions.\narchitecture, as done in Sec. VII-B. Here, we see that for\noptimization-based partitions, we still get the same trend in\nthe scaling of the computation w.r.t. the number of CSUs as\nin Fig. 3a, and an almost linear scaling in the computation\ncost as in Fig. 3b. However, for algorithmic partitioning, we\nget two outliners with PAOpt\n4\nand PAOpt\n5\n, both relatively high\nin the computation time and cost, but different in the overall\nperformance. This analysis suggests that partitions obtained\nwith the optimization-based approach tend to provide more\nreliable performances according to these evaluation criteria.\nHowever, algorithmic partition remains a valid alternative,\noften providing good results with low computational cost.\nVIII. CONCLUSIONS AND FUTURE WORK\nThis work establishes foundations for the systematic par-\ntitioning of networks of control systems. We have pursued\nthis goal by formalizing the concept of equivalent graph of a\ndynamical system, extending analogous previous propositions;\nand introducing the novel concepts of fundamental system unit\n(FSU), composite system unit (CSU), aggregation operation,\nand control partition. This has allowed us to propose a\ngeneralized partitioning technique consisting of two steps: first\nwe select FSUs; then we aggregate them to obtain a collection\nof CSUs, which is the control partition. The former step is\nachieved through a selection algorithm; while for the latter\nwe propose both an optimization-based and an algorithmic\napproach. These new approaches have been validated through\nexamples, and case studies on networks of linear and hybrid\nsystems of different topologies. Empirical evidence shows\nthat, through partitioning, we can achieve superior distributed\ncontrol performance, computation efficiency, and lower costs.\nFuture work could focus on improving the real-time appli-\ncability of these techniques and exploring the impact of the\ntopology in distributed network control in terms of perfor-\nmance and stability. Other areas for extensions are the inclu-\nsion in the equivalent graph of external signals or disturbances\nthrough additional nodes, and applying generalized partition-\ning to control strategies different from predictive control.\nREFERENCES\n[1] S. An, B. Lee, and D. Shin, “A survey of intelligent transportation\nsystems,” in 2011 Third International Conference on Computational\nIntelligence, Communication Systems and Networks, 2011, pp. 332–337.\n[2] B. Chen and H. H. Cheng, “A review of the applications of agent\ntechnology in traffic and transportation systems,” IEEE Transactions on\nIntelligent Transportation Systems, vol. 11, pp. 485–497, 2010.\n[3] A. Riccardi, L. Laurenti, and B. De Schutter, “A benchmark for the\napplication of distributed control techniques to the electricity network of\nthe European economic area,” in Control Systems Benchmarks. Spinger,\n2025.\n[4] ——, “Code underlying the publication: A benchmark for the application\nof distributed control techniques to the electricity network of the\nEuropean economic area,” 2025.\n[5] J. M. Carrasco, L. G. Franquelo, J. T. Bialasiewicz, E. Galvan, R. C.\nPortilloGuisado, M. A. M. Prats, J. I. Leon, and N. Moreno-Alfonso,\n“Power-electronic systems for the grid integration of renewable energy\nsources: A survey,” IEEE Transactions on Industrial Electronics, vol. 53,\npp. 1002–1016, 2006.\n[6] D. K. Molzahn, F. D¨orfler, H. Sandberg, S. H. Low, S. Chakrabarti,\nR. Baldick, and J. Lavaei, “A survey of distributed optimization and\ncontrol algorithms for electric power systems,” IEEE Transactions on\nSmart Grid, vol. 8, pp. 2941–2962, 2017.\n[7] S. K. Pandey, S. R. Mohanty, and N. Kishor, “A literature survey\non load–frequency control for conventional and distribution generation\npower systems,” Renewable and Sustainable Energy Reviews, vol. 25,\npp. 318–334, 2013.\n[8] X. Zhang, Q. Han, X. Ge, D. Ding, L. Ding, D. Yue, and C. Peng,\n“Networked control systems: A survey of trends and techniques,”\nIEEE/CAA Journal of Automatica Sinica, vol. 7, pp. 1–17, 2020.\n[9] C. Ocampo-Martinez, D. Barcelli, V. Puig, and A. Bemporad, “Hier-\narchical and decentralised model predictive control of drinking water\nnetworks: application to barcelona case study,” IET Control Theory &\nApplications, vol. 6, pp. 62–71, 2012.\n[10] Y. Cao, W. Yu, W. Ren, and G. Chen, “An overview of recent progress\nin the study of distributed multi-agent coordination,” IEEE Transactions\non Industrial Informatics, vol. 9, pp. 427–438, 2013.\n[11] J. Cort´es and M. Egerstedt, “Coordinated control of multi-robot sys-\ntems: A survey,” SICE Journal of Control, Measurement, and System\nIntegration, vol. 10, pp. 495–503, 2017.\n\n\n13\nA0\nA1\nA2\nA3\nA4\nA5\nA6\nA7\nA8\nA9\nA10\nA11\nA12\nA13\nA14\nA15\nA16\nA17\nA18\nA19\nA20\nA21\nA22\nA23\nA24\nA25\nA26\nA27\nA28\nA29\nA30\nA31\nA32\nA33\nA34 A35\nA36\nA37 A38\nA39\nA40\nA41\nA42\nA43\nA44\nA45\nA46\nA47\nA48\nA49\n(a) PADMM ≡POpt\n0\n≡PAlg\n0\nA0\nA20\nA24\nA33\nA45\nA36\nA15\nA6\nA41\nA44\nA38\nA42\nA37\nA1\nA2\nA9\nA35\nA32\nA22\nA16\nA3\nA21\nA25\nA27\nA47\nA11\nA8\nA48\nA17\nA26\nA49\nA39\nA7\nA13\nA18\nA43\nA10\nA5\nA40\nA34\nA23\nA28\nA46\nA29\nA4\nA19\nA31\nA12\nA14\nA30\n(b) POpt\n1\nA3\nA21\nA25\nA36\nA15\nA6\nA8\nA48\nA42\nA47\nA1\nA2\nA9\nA35\nA37\nA11\nA32\nA10\nA5\nA40\nA34\nA7\nA13\nA18\nA43\nA4\nA19\nA31\nA12\nA14\nA30\nA16\nA27\nA17\nA26\nA49\nA39\nA22\nA29\nA41\nA44\nA0\nA20\nA24\nA33\nA38\nA45\nA23\nA28\nA46\n(c) PAlg\n1\nA3\nA21\nA25\nA22\nA23\nA29\nA36\nA41\nA44\nA0\nA15\nA20\nA24\nA33\nA28\nA38\nA46\nA6\nA45\nA8\nA48\nA42\nA47\nA1\nA2\nA9\nA35\nA37\nA11\nA32\nA10\nA5\nA40\nA34\nA7\nA13\nA18\nA43\nA4\nA19\nA31\nA12\nA14\nA30\nA16\nA27\nA17\nA26\nA49\nA39\n(d) PAlg\n2\nA22\nA29\nA41\nA44\nA0\nA20\nA24\nA33\nA38\nA45\nA16\nA3\nA21\nA25\nA27\nA8\nA48\nA7\nA23\nA36\nA13\nA15\nA18\nA43\nA28\nA46\nA6\nA10\nA17\nA26\nA42\nA47\nA49\nA1\nA2\nA5\nA40\nA9\nA34\nA35\nA37\nA11\nA32\nA39\nA4\nA19\nA31\nA12\nA14\nA30\n(e) POpt\n2\nA17\nA26\nA42\nA47\nA49\nA1\nA2\nA9\nA35\nA37\nA11\nA32\nA39\nA16\nA3\nA21\nA25\nA27\nA10\nA5\nA40\nA34\nA4\nA7\nA19\nA22\nA23\nA29\nA31\nA36\nA41\nA44\nA0\nA12\nA13\nA14\nA15\nA18\nA20\nA24\nA33\nA43\nA28\nA38\nA46\nA6\nA30\nA45\nA8\nA48\n(f) PAlg\n3\nA22\nA29\nA41\nA44\nA0\nA20\nA24\nA33\nA38\nA45\nA10\nA16\nA17\nA26\nA42\nA47\nA49\nA1\nA2\nA3\nA5\nA21\nA25\nA27\nA40\nA9\nA34\nA35\nA37\nA11\nA32\nA39\nA8\nA48\nA4\nA7\nA19\nA23\nA31\nA36\nA12\nA13\nA14\nA15\nA18\nA43\nA28\nA46\nA6\nA30\n(g) PAlg\n4\nA4\nA7\nA19\nA22\nA23\nA29\nA31\nA36\nA41\nA44\nA0\nA12\nA13\nA14\nA15\nA18\nA20\nA24\nA33\nA43\nA28\nA38\nA46\nA6\nA30\nA45\nA8\nA10\nA16\nA17\nA26\nA42\nA47\nA49\nA1\nA2\nA3\nA5\nA21\nA25\nA27\nA40\nA48\nA9\nA34\nA35\nA37\nA11\nA32\nA39\n(h) POpt\n3\nA4\nA7\nA8\nA10\nA16\nA17\nA19\nA22\nA23\nA26\nA29\nA31\nA36\nA41\nA42\nA44\nA47\nA49\nA0\nA1\nA2\nA3\nA5\nA12\nA13\nA14\nA15\nA18\nA20\nA21\nA24\nA25\nA27\nA33\nA40\nA43\nA48\nA9\nA28\nA34\nA35\nA37\nA38\nA46\nA6\nA11\nA30\nA32\nA45\nA39\n(i) PCMPC ≡POpt\n4\n≡PAlg\n5\nFig. 6. Partitions of a random network with 50 FSUs. In the following, for each partition it is indicated the number of resulting CSUs, and the granularity\nparameter α used in optimization-based and algorithmic partitioning. PADMM ≡POpt\n0\n≡PAlg\n0\n: 50 CSUs, αOpt\n0\n= 4.3 · 106, αAlg\n0\n= 100; POpt\n1\n: 15 CSUs,\nαOpt\n1\n= 4.3 · 104; PAlg\n1\n: 11 CSUs, αAlg\n1\n= 30; PAlg\n2\n: 9 CSUs, αAlg\n2\n= 20; POpt\n2\n: 6 CSUs, αOpt\n2\n= 4.3 · 103; PAlg\n3\n: 5 CSUs, αAlg\n3\n= 10; PAlg\n4\n: 4 CSUs,\nαAlg\n4\n= 1; POpt\n3\n, 6 CSUs, αOpt\n3\n= 4.3 · 102; PCMPC ≡POpt\n4\n≡PAlg\n5\n, 1 CSU, αOpt\n4\n= 4.3 · 101, αAlg\n5\n= 10−13.\n[12] M. Kordestani, A. A. Safavi, and M. Saif, “Recent survey of large-scale\nsystems: Architectures, controller strategies, and industrial applications,”\nIEEE Systems Journal, vol. 15, pp. 5440–5453, 2021.\n[13] D. D. ˇSiljak, Decentralized Control of Complex Systems.\nAcademic\nPress, Inc., 1991.\n[14] R. Scattolini, “Architectures for distributed and hierarchical model\npredictive control – a review,” Journal of Process Control, vol. 19, pp.\n723–731, 2009.\n[15] J. M. Maestre and R. R. Negenborn, Eds., Distributed Model Predictive\nControl Made Easy.\nSpringer, 2014, vol. 69.\n[16] A. Riccardi, L. Laurenti, and B. De Schutter, “A generalized partitioning\nstrategy for distributed control,” in 2024 63nd IEEE Conference on\nDecision and Control (CDC), 2024, p. 8.\n[17] ——, “Code underlying the publication: A generalized partitioning\nstrategy for control,” 2024. [Online]. Available: https://doi.org/10.4121/\n90ada13d-a6c9-4e4c-a046-2b984595bcdd.v1\n[18] ——,\n“Code\nunderlying\nthe\npublication:\nA\ngeneral\npartitioning\nstrategy\nfor\nnon-centralized\ncontrol,”\n2025.\n[Online].\nAvailable:\nhttps://doi.org/10.4121/0e7dd651-66d7-451e-889b-d558e7d5b986\n[19] P. Chanfreut, J. M. Maestre, and E. F. Camacho, “A survey on clustering\nmethods for distributed and networked control systems,” Annual Reviews\nin Control, vol. 52, pp. 75–90, 2021.\n[20] S. E. Schaeffer, “Graph clustering,” Computer Science Review, vol. 1,\npp. 27–64, 2007.\n[21] J. M. Hernandez and P. van Mieghem, “Classification of graph metrics,”\nDelft University of Technology, Faculty of Electrical Engineering,\nMathematics, and Computer Science, Tech. Rep., 2011.\n[22] S. Fortunato, “Community detection in graphs,” Physics Reports, vol.\n486, pp. 75–174, 2010.\n[23] F. D. Malliaros and M. Vazirgiannis, “Clustering and community de-\ntection in directed networks: A survey,” Physics Reports, vol. 533, pp.\n95–142, 2013.\n[24] P. Segovia, V. Puig, E. Duviella, and L. Etienne, “Distributed model pre-\ndictive control using optimality condition decomposition and community\ndetection,” Journal of Process Control, vol. 99, pp. 54–68, 2021.\n[25] W. Tang, A. Allman, D. B. Pourkargar, and P. Daoutidis, “Optimal\ndecomposition for distributed optimization in nonlinear model predictive\n\n\n14\n0\n10\n20\n30\n40\n50\nNumber of cores\n0\n1000\n2000\nSeconds [s]\nComputation time comparison\n(a)\n0\n10\n20\n30\n40\n50\nNumber of cores\n5000\n10000\nCore seconds [s]\nComputation cost comparison\n(b)\nFig. 7.\nComputation time in [s] (Fig. 7a), and computation cost in core-\nseconds (Fig. 7b) required to perform the simulation related to the number of\nrequired cores. The results for each strategy are reported by a marker, square-\nred for optimization-based partitioning, circle-green for algorithmic.\ncontrol through community detection,” Computers & Chemical Engi-\nneering, vol. 111, pp. 43–54, 2018.\n[26] V. D. Blondel, J. Guillaume, R. Lambiotte, and E. Lefebvre, “Fast\nunfolding of communities in large networks,” Journal of Statistical\nMechanics: Theory and Experiment, vol. 2008, p. 13, 2008.\n[27] S. Siniscalchi-Minna, F. D. Bianchi, C. Ocampo-Martinez, J. L.\nDom´ınguez-Garc´ıa, and B. De Schutter, “A non-centralized predictive\ncontrol strategy for wind farm active power control: A wake-based\npartitioning approach,” Renewable Energy, vol. 150, pp. 656–669, 2020.\n[28] J. Barreiro-Gomez, Partitioning for large-scale systems: Sequential\nDMPC design.\nSpringer, 2019, pp. 163–178.\n[29] F. Fele, J. M. Maestre, and E. F. Camacho, “Coalitional control:\nCooperative game theory and control,” IEEE Control Systems Magazine,\nvol. 37, pp. 53–69, 2017.\n[30] F. Fele, E. Debada, J. M. Maestre, and E. F. Camacho, “Coalitional\ncontrol for self-organizing agents,” IEEE Transactions on Automatic\nControl, vol. 63, pp. 2883–2897, 2018.\n[31] W. Ananduta, T. Pippia, C. Ocampo-Martinez, J. Sijs, and B. De Schut-\nter, “Online partitioning method for decentralized control of linear\nswitching large-scale systems,” Journal of the Franklin Institute, vol.\n356, pp. 3290–3313, 2019.\n[32] W. Ananduta and C. Ocampo-Martinez, “Event-triggered partitioning for\nnon-centralized predictive-control-based economic dispatch of intercon-\nnected microgrids,” Automatica, vol. 132, p. 9, 2021.\n[33] P. R. Baldivieso Monasterios, P. A. Trodden, and M. Cannon, “On\nfeasible sets for coalitional mpc,” in IEEE 58th Conference on Decision\nand Control (CDC), 2019, pp. 4668–4673.\n[34] P. Chanfreut, J. M. Maestre, T. Hatanaka, and E. F. Camacho, “Fast\nclustering for multi-agent model predictive control,” IEEE Transactions\non Control of Network Systems, vol. 9, pp. 1544–1555, 2022.\n[35] B. Bollob´as, Modern Graph Theory.\nSpringer, 2001.\n[36] W. P. M. H. Heemels, B. De Schutter, and A. Bemporad, “Equivalence\nof hybrid dynamical models,” Automatica, vol. 37, pp. 1085–1091, 2001.\n[37] M. E. J. Newman, “Modularity and community structure in networks,”\nProceedings of the National Academy of Sciences, vol. 103, pp. 8577–\n8582, 2006.\n[38] U.\nBrandes,\nD.\nDelling,\nM.\nGaertler,\nR.\nGoerke,\nM.\nHoefer,\nZ. Nikoloski, and D. Wagner, “Maximizing modularity is hard,” 2006.\n[Online]. Available: https://doi.org/10.48550/arXiv.physics/0608255\n[39] M. Conforti, G. Cornu´ejols, and G. Zambelli, Integer Programming.\nSpringer, 2014, vol. 271.\n[40] P. Daoutidis and C. Kravaris, “Structural evaluation of control config-\nurations for multivariable nonlinear processes,” Chemical Engineering\nScience, vol. 47, pp. 1091–1107, 1992.\n[41] U. Brandes, D. Delling, M. Gaertler, R. Gorke, M. Hoefer, Z. Nikoloski,\nand D. Wagner, “On modularity clustering,” IEEE Transactions on\nKnowledge and Data Engineering, vol. 20, pp. 172–188, 2008.\n[42] Gurobi\nOptimization,\nLLC,\n“Gurobi\nOptimizer,”\n2024.\n[Online].\nAvailable: https://www.gurobi.com/\n[43] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, “Distributed\noptimization and statistical learning via the alternating direction method\nof multipliers,” Foundations and Trends in Machine Learning, vol. 3,\npp. 1–122, 2010.\n[44] T. H. Summers and J. Lygeros, “Distributed model predictive consensus\nvia the alternating direction method of multipliers,” in 50th Annual\nAllerton Conference on Communication, Control, and Computing, 2012,\npp. 79–84.\n[45] R. Rostami, G. Costantini, and D. G¨orges, “ADMM-based distributed\nmodel predictive control: Primal and dual approaches,” in 2017 IEEE\n56th Annual Conference on Decision and Control (CDC), 2017, pp.\n6598–6603.\n[46] E. Sontag, “Nonlinear regulation: The piecewise linear approach,” IEEE\nTransactions on Automatic Control, vol. 26, pp. 346–358, 1981.\n[47] P. Tabuada, Verification and Control of Hybrid Systems: A Symbolic\nApproach.\nSpringer, 2009.\n[48] A. Bemporad and M. Morari, “Control of systems integrating logic,\ndynamics, and constraints,” Automatica, vol. 35, pp. 407–427, 1999.\n[49] D. Q. Mayne, J. B. Rawlings, C. V. Rao, and P. O. M. Scokaert, “Con-\nstrained model predictive control: Stability and optimality,” Automatica,\nvol. 36, pp. 789–814, 2000.\n[50] Y. Wang, W. Yin, and J. Zeng, “Global convergence of ADMM in\nnonconvex nonsmooth optimization,” Journal of Scientific Computing,\nvol. 78, pp. 29–63, 2019.\n[51] X. Luan, B. De Schutter, L. Meng, and F. Corman, “Decomposition and\ndistributed optimization of real-time traffic management for large-scale\nrailway networks,” Transportation Research Part B: Methodological,\nvol. 141, pp. 72–97, 2020.\nIX. BIOGRAPHY SECTION\nAlessandro Riccardi is a PhD candidate in the Delft\nCenter for Systems and Control at the Delft Univer-\nsity of Technology. He received a BS in automation\nengineering, and an MS in control engineering both\nfrom the University Sapienza of Rome. His research\ninterests include predictive control and reinforce-\nment learning for complex large-scale systems, and\nlearning-based modeling of system dynamics.\nLuca Laurenti (Member, IEEE) is a tenure-track\nassistant professor at the Delft Center for Systems\nand Control at TU Delft and co-director of the\nHERALD Delft AI Lab. He received his PhD from\nthe Department of Computer Science, University of\nOxford (UK). Luca has a background in stochastic\nsystems, control theory, formal methods, and ar-\ntificial intelligence. His research work focuses on\ndeveloping data-driven systems provably robust to\ninteractions with a dynamic and uncertain world.\nBart De Schutter (IEEE member since 2008, senior\nmember since 2010, fellow since 2019) is a full\nprofessor and head of department at the Delft Center\nfor Systems and Control of Delft University of\nTechnology in Delft, The Netherlands.\nBart De Schutter is senior editor of the IEEE\nTransactions on Intelligent Transportation Systems.\nHis current research interests include integrated\nlearning- and optimization-based control, with ap-\nplications in transportation and energy networks.\n\n\n15\nAPPENDIX A\nADDITIONAL PROFS\nProof of Prop. 1.\nProof. The statement is verified by construction. Consider two\nCSUs, S1,k = (V1, E1,k, w1,k, ˜g1), S2,k = (V2, E2,k, w2,k, ˜g2),\nwith V1 = X1 ∪U1, V2 = X2 ∪U2. By definition of\nCSU, for both S1,k and S2,k, it holds that no edges are\npresent from the set of nodes Ui to the set of states Xj,\nfor i, j\n=\n1, 2, and i\n̸=\nj. Now we define the CSU\nresulting from their aggregation as S(1,2),k = (S1,k ⊎S2,k)|Gk\n=\n\u0000V1 ∪V2, E1,k ∪E2,k ∪E(1,2),k, w(1,2),k, (˜g1, ˜g2)\n\u0001\n,\nwhere\nE(1,2),k contains the edges in Ek linking the two sets of nodes,\nbut that are not present in the individual sets of edges. The\noperation does not add any edge connecting the set of input\nnodes U to the aggregated set of states X1 ∪X2. Thus, the\nCSU resulting from the aggregation is a CSU according to\nDefinition 2.\nAPPENDIX B\nADDITIONAL FIGURES\nFig. 8.\nA partition selected to maximize the interaction among CSUs used\nin Sec. VII-A\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21126v1.pdf",
    "total_pages": 15,
    "title": "A general partitioning strategy for non-centralized control",
    "authors": [
      "Alessandro Riccardi",
      "Luca Laurenti",
      "Bart De Schutter"
    ],
    "abstract": "Partitioning is a fundamental challenge for non-centralized control of\nlarge-scale systems, such as hierarchical, decentralized, distributed, and\ncoalitional strategies. The problem consists of finding a decomposition of a\nnetwork of dynamical systems into system units for which local controllers can\nbe designed. Unfortunately, despite its critical role, a generalized approach\nto partitioning applicable to every system is still missing from the\nliterature. This paper introduces a novel partitioning framework that\nintegrates an algorithmic selection of fundamental system units (FSUs),\nconsidered indivisible entities, with an aggregative procedure, either\nalgorithmic or optimization-based, to select composite system units (CSUs) made\nof several FSUs. A key contribution is the introduction of a global network\nmetric, the partition index, which quantitatively balances intra- and inter-CSU\ninteractions, with a granularity parameter accounting for the size of CSUs,\nallowing for their selection at different levels of aggregation. The proposed\nmethod is validated through case studies in distributed model predictive\ncontrol (DMPC) for linear and hybrid systems, showing significant reductions in\ncomputation time and cost while maintaining or improving control performance\nw.r.t. conventional strategies.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}