{
  "id": "arxiv_2502.20946v1",
  "text": "Generative Uncertainty in Diffusion Models\nMetod Jazbec*1\nEliot Wong-Toi2\nGuoxuan Xia3\nDan Zhang4\nEric Nalisnick5\nStephan Mandt2\n1UvA-Bosch Delta Lab, University of Amsterdam\n2University of California, Irvine\n3Imperial College, London\n4Bosch Center for AI\n5Johns Hopkins University\nAbstract\nDiffusion models have recently driven significant\nbreakthroughs in generative modeling. While state-\nof-the-art models produce high-quality samples\non average, individual samples can still be low\nquality. Detecting such samples without human\ninspection remains a challenging task. To address\nthis, we propose a Bayesian framework for esti-\nmating generative uncertainty of synthetic sam-\nples. We outline how to make Bayesian inference\npractical for large, modern generative models and\nintroduce a new semantic likelihood (evaluated in\nthe latent space of a feature extractor) to address\nthe challenges posed by high-dimensional sample\nspaces. Through our experiments, we demonstrate\nthat the proposed generative uncertainty effectively\nidentifies poor-quality samples and significantly\noutperforms existing uncertainty-based methods.\nNotably, our Bayesian framework can be applied\npost-hoc to any pretrained diffusion or flow match-\ning model (via the Laplace approximation), and\nwe propose simple yet effective techniques to min-\nimize its computational overhead during sampling.\n1\nINTRODUCTION\nDiffusion (and flow-matching) models [Sohl-Dickstein\net al., 2015, Song et al., 2020a,b, Lipman et al., 2022] have\nrecently pushed the boundaries of generative modeling due\nto their strong theoretical underpinnings and scalability.\nAcross various domains, they have enabled the generation\nof increasingly realistic samples [Rombach et al., 2022,\nEsser et al., 2024, Li et al., 2024]. Despite the impressive\nprogress, state-of-the-art models can still generate low\nquality images that contain artefacts and fail to align\nwith the provided conditioning information. This poses a\n*Corresponding author: <m.jazbec@uva.nl>\nchallenge for deploying diffusion models, as it can lead to\na poor user experience by requiring multiple generations\nto manually find an artefact-free sample.\nBayesian inference has long been applied to detect\npoor-quality predictions in predictive models [MacKay,\n1992b, Gal et al., 2016, Wilson, 2020, Arbel et al., 2023].\nBy capturing the uncertainty of the model parameters due\nto limited training data, each prediction can be assigned\na predictive uncertainty, which, when high, serves as a\nwarning that the prediction may be unreliable. Despite its\nwidespread use for principled uncertainty quantification\nin predictive models, Bayesian methodology has been far\nless commonly applied to detecting poor generations in\ngenerative modeling. This raises a key question: How can\nBayesian principles help us detect poor generations?\nIn this work, we propose a Bayesian framework for esti-\nmating generative uncertainty in modern generative models,\nsuch as diffusion. To scale Bayesian inference for large\ndiffusion models, we employ the (last-layer) Laplace ap-\nproximation [MacKay, 1992a, Ritter et al., 2018, Daxberger\net al., 2021a]. Additionally, to address the challenge posed\nby the high-dimensional sample spaces of data such as nat-\nural images, we introduce a semantic likelihood, where we\nleverage pretrained image encoders (such as CLIP [Radford\net al., 2021]) to compute variability in a latent, semantic\nspace instead. Through our experiments, we demonstrate\nthat generative uncertainty is an effective tool for detecting\nlow-quality samples and propose simple strategies to\nminimize the sampling overhead introduced by Bayesian in-\nference. In particular, we make the following contributions:\n1. We formalize the notion of generative uncertainty and\npropose a method to estimate it for modern generative\nmodels (Section 3). Analogous to how predictive\nuncertainty helps identify unreliable predictions in\npredictive models, generative uncertainty can be used\nto detect low-quality generations in generative models.\n2. We show that our generative uncertainty strongly\noutperforms previous uncertainty-based approaches\narXiv:2502.20946v1  [cs.LG]  28 Feb 2025\n\n\nfor filtering out poor samples [Kou et al., 2024, De Vita\nand Belagiannis, 2025]. Additionally, we achieve\ncompetitive performance with non-uncertainty-based\nmethods, such as realism score [Kynkäänniemi et al.,\n2019] and rarity score [Han et al., 2023] (Section 4.1),\nwhile also highlighting the complementary benefits\nof uncertainty (Appendix A.5).\n3. We propose effective strategies to reduce the sampling\noverhead of Bayesian uncertainty (Section 4.2) and\ndemonstrate the applicability of our framework beyond\ndiffusion models by applying it to a (latent) flow\nmatching model (Section A.7).\n2\nBACKGROUND\n2.1\nGENERATIVE MODELING\nSampling in Generative Models\nModern deep gen-\nerative models like variational autoencoders (VAEs)\n[Kingma, 2013], generative adversarial networks (GANs)\n[Goodfellow et al., 2014], and diffusion models differ in\ntheir exact probabilistic frameworks and training schemes,\nyet share a common sampling recipe: start with random\nnoise and transform it into a new data sample [Tomczak,\n2022]. Specifically, let x ∈X denote a data sample and\nz ∈Z an initial noise. A new sample is generated by:\nz ∼p(z) ,\nˆx = gθ(z) ,\nwhere p(z) is an initial noise (prior) distribution, typically a\nstandard Gaussian N(0, I), and gθ : Z →X is a generator\nfunction with model parameters θ ∈RP .\nDiffusion Models\nThe primary focus of this work is on\ndiffusion models [Sohl-Dickstein et al., 2015]. These mod-\nels operate by progressively corrupting data into Gaussian\nnoise and learning to reverse this process. For a data sample\nx0 ∼q(x), the forward (noising) process is defined as\nxt = √¯αtx0 +\n√\n1 −¯αtϵ, ϵ ∼N(0, I)\nwhere ¯αt = Qt\ns=1(1 −βs) and {βs}T\ns=1 is a noise schedule\nchosen such that xT ∼N(0, I) (approximately). In the\nbackward process, a denoising network fθ is learned via\na simplified regression objective (among various possible\nparameterizations, see Song et al. [2020b] or Karras et al.\n[2022]):\nL(θ; D) = Et,x0,ϵ\nh\f\f\f\ffθ(√¯αtx0+\n√\n1−¯αtϵ, t) −ϵ\n\f\f\f\f2\n2\ni\n(1)\nwhere D = {xn}N\nn=1 denotes a training dataset of images.\nAfter training, diffusion models generate new samples via\na generator function, gˆθ, which consists of sequentially\napplying the learned denoiser, fˆθ, and following specific\ntransition rules from samplers such as DDPM [Ho et al.,\n2020] or DDIM [Song et al., 2020a].\n2.2\nBAYESIAN DEEP LEARNING\nBayesian neural networks (BNNs) go beyond point\npredictions and allow for principled uncertainty quantifi-\ncation [Buntine and Weigend, 1991, Neal, 1995, Kendall\nand Gal, 2017, Jospin et al., 2022]. Let hψ : X →Y\ndenote a predictive model with parameters ψ ∈RO and\nD = {(xn, yn)}N\ni=1 denote training data. Instead of finding\na single fixed set of parameters, ˆψ = arg max L(ψ; D),\nthat maximizes an objective function L, BNNs specify a\nprior p(ψ) over model parameters and define a likelihood\np(y|hψ(x)), which together yield a posterior distribution\nvia Bayes rule: p(ψ|D)\n∝\np(ψ) QN\nn=1 p(yn|hψ(xn)).\nUnder this Bayesian view, a predictive model for a new\ntest point x∗is then obtained via the posterior predictive\ndistribution [Murphy, 2022]:\np(y|x∗, D) = Ep(ψ|D)\n\u0002\np(y|hψ(x∗))\n\u0003\n.\nFor large models, finding the exact posterior distribution is\ncomputationally intractable, hence an approximate posterior\nq(ψ|D) is used instead. Popular approaches for approximate\ninference include deep ensembles [Lakshminarayanan\net al., 2017], variational inference [Blundell et al., 2015,\nZhang et al., 2018], SWAG [Mandt et al., 2017, Maddox\net al., 2019], and Laplace approximation [Daxberger et al.,\n2021a]. Moreover, to alleviate computational overhead, it is\ncommon to give a ‘Bayesian treatment’ only to a subset of\nparameters [Kristiadi et al., 2020, Daxberger et al., 2021b,\nSharma et al., 2023]. Finally, the intractable expectation\nintegral in the posterior predictive is approximated via\nMonte-Carlo (MC) sampling:\np(y|x∗, D)≈1\nM\nM\nX\nm=1\np(y|hψm(x∗)), ψm ∼q(ψ|D),\n(2)\nwith M denoting the number of MC samples. By measuring\nthe variability of the posterior predictive distribution, e.g.,\nits entropy, one can obtain an estimate of the model’s pre-\ndictive uncertainty for a given test point u(x∗). The utility\nof such uncertainties has been demonstrated on a wide\nrange of tasks such as out-of-distribution (OOD) detection\n[Daxberger et al., 2021a], active learning [Gal et al., 2017],\nand detection of influential samples [Nickl et al., 2024].\n3\nGENERATIVE UNCERTAINTY\nVIA BAYESIAN INFERENCE\nWhile Bayesian neural networks (BNNs) have traditionally\nbeen applied to predictive models to estimate predictive un-\ncertainty, in this section we demonstrate how to apply them\nto diffusion to estimate generative uncertainty (see Figure 1\nand Algorithm 1 for an overview of our method). Later in\nSection 4, we show that generative uncertainty can be used\nto detect poor-quality samples. Our focus is on generative\n\n\n…\n…\nθi ∼q(θ|D)\ngθ1\ngθm\ngθM\ncϕ\ncϕ\ncϕ\nLow u(z1)\n…\n…\nz1 ∼p(z)\n…\n…\ncϕ\ncϕ\ncϕ\n…\n…\nHigh u(z2)\ngθ1\ngθm\ngθM\nz2 ∼p(z)\nu(z) := H(p(x|z, D)\nFigure 1: Illustration of how we compute generative uncertainty for each random noise z. For a generative model gθ, we\nsample M sets of model parameters from our posterior distribution q(θ|D) and generate M images. Then, we evaluate\nthe semantic likelihood for each by computing feature embeddings with a pretrained encoder cϕ (e.g., CLIP) and take the\nuncertainty (e.g., entropy) over these embeddings. Random noises z with low uncertainty (left) tend to lead to consistent,\nhigh quality generations while random noises with high uncertainty (right) lead to poor, discordant generations.\nmodels for natural images, where x ∈RH×W ×C. For ease\nof exposition, we consider unconditional generation in\nthis section, though our methodology can also be applied\ndirectly to conditional models (see Section 4.1).\n3.1\nGENERATIVE UNCERTAINTY\nAs in traditional Bayesian predictive models (cf. Sec-\ntion 2.2), the central principle for obtaining a Bayesian\nnotion of uncertainty in diffusion models is the posterior\npredictive distribution:\np(x|z, D) = Ep(θ|D)\n\u0002\np(x|gθ(z))\n\u0003\n.\n(3)\nHere, we use z (with a slight abuse of notation) to denote\nthe entire randomness involved in the diffusion sampling\nprocess.1 Generative uncertainty is then defined as the\nvariability of the posterior predictive:\nu(z) := V(p(x|z, D))\n(4)\nwhere V(·) denotes the variability measure, such as entropy.\nWe propose a tractable estimator of the posterior predictive\nlater in Eq. 8.\nIn the same way that the predictive uncertainty u(x∗),\nof a predictive model provides insight into the quality\nof its prediction for a new test point x∗, the generative\nuncertainty u(z) of a generative model gθ should offer\ninformation about the quality of the generation gθ(z) for\na ‘new’ random noise sample z. We demonstrate this\n1In DDIM [Song et al., 2020a] and ODE sampling [Song et al.,\n2020b], randomness is only present at the start of the sampling\nprocess (akin to VAEs and GANs). In contrast, in DDPM [Ho\net al., 2020] and SDE sampling [Song et al., 2020b], randomness\nis introduced at every step throughout the sampling process.\nrelationship experimentally in Section 4. Next, we discuss\nhow to make Bayesian inference on (large) diffusion models\ncomputationally tractable.\n3.2\nLAST-LAYER LAPLACE APPROXIMATION\nState-of-the-art diffusion models are extremely large\n(100M to 1B+ parameters) and can take weeks to train.\nConsequently, the computational overhead of performing\nBayesian inference on such large models is of signif-\nicant concern. To address this, we adopt the Laplace\napproximation [MacKay, 1992a, Shun and McCullagh,\n1995] to approximate the posterior q(θ|D). The Laplace\napproximation is among the most computationally efficient\napproximate inference methods while still offering com-\npetitive performance [Daxberger et al., 2021a]. Moreover, a\nparticularly appealing feature of the Laplace approximation\nis that it can be applied post-hoc to any diffusion model.\nWe leverage this property in Section 4, where we apply it\nto a variety of popular diffusion and flow-matching models.\nThe Laplace approximation of the posterior is given by:\nq(θ|D) = N(θ|ˆθ, Σ), Σ =\n\u0000∇2\nθL(θ; D)\n\f\fˆθ\n\u0001−1,\n(5)\nwhere ˆθ represents the parameters of a pre-trained diffusion\nmodel, and Σ is the inverse Hessian of the diffusion training\nloss from Eq. 1. To reduce the computational cost further,\nwe apply a ‘Bayesian’ treatment only to the last layer of\nthe denoising network fθ.\nWe note that the use of last-layer Laplace approximation for\ndiffusion models has been previously proposed in BayesDiff\n[Kou et al., 2024]. While our implementation of the Laplace\napproximation closely follows theirs, there are significant\ndifferences in how we utilize the approximate posterior,\n\n\nq(θ|D). Specifically, in our approach, we use it within\nthe traditional Bayesian framework (Eq. 3) to sample new\ndiffusion model parameters, leaving the diffusion sampling\nprocess, gθ, unchanged. In contrast, BayesDiff resamples\nnew weights from q(θ|D) at every diffusion sampling\nstep t, which necessitates substantial modifications to\nthe diffusion sampling process through their variance\npropagation approach. We later demonstrate in Section 4\nthat modifications such as variance propagation are\nunnecessary for obtaining Bayesian generative uncertainty\nand staying closer to the traditional Bayesian setting leads\nto the best empirical performance.\n3.3\nSEMANTIC LIKELIHOOD\nWe next discuss the choice of likelihood for estimating gen-\nerative uncertainty in diffusion models. Since the denoising\nproblem in diffusion is modeled as a (multi-output) regres-\nsion problem, the most straightforward approach is to place\na simple Gaussian distribution over the generated sample:\np(x|gθ(z)) = N(x | gθ(z), σ2I),\n(6)\nwhere σ2 represents the observation noise.\nHowever, as we will demonstrate in Section 4, this\nlikelihood leads to non-informative estimates of gener-\native uncertainty (Eq. 4). The primary issue is that the\nsample space of natural images is high-dimensional (i.e.,\n|X|\n=\nHWC). Consequently, placing the likelihood\ndirectly in the sample space causes the variability of the\nposterior predictive distribution to be based on pixel-level\ndifferences. This is problematic because it is well-known\nthat two images can appear nearly identical to the human eye\nwhile exhibiting a large L2-norm difference in pixel space\nX (see, for example, the literature on adversarial examples\n[Szegedy, 2013]). To get around this, we propose to map the\ngenerated samples to a ‘semantic’ latent space, S, via a pre-\ntrained feature extractor, cϕ : X →S (e.g., an inception-net\n[Szegedy et al., 2016] or a CLIP encoder [Radford et al.,\n2021]). The resulting semantic likelihood has the form\np(x|gθ(z); ϕ) = N(e(x) | cϕ\n\u0000gθ(z)\n\u0001\n, σ2I)\n(7)\nwhere e(x) ∈S is the (random) vector of semantic features.\nBy combining the (last-layer) Laplace approximate poste-\nrior and the semantic likelihood, we can now approximate\nthe posterior predictive (Eq. 3) as\np(x|z, D) ≈N\n\u0000e(x)\n\f\f ¯e, Diag\n\u0000 1\nM\nM\nX\nm=1\ne2\nm −¯e2\u0001\n+ σ2I\n\u0001\n,\n¯e = 1\nM\nM\nX\nm=1\nem,\nem = cϕ\n\u0000gθm(z)\n\u0001\n, θm ∼q(θ|D),\n(8)\nwhere M denotes the number of Monte Carlo samples.\nAdditionally, we approximate the posterior predictive with a\nAlgorithm 1: Diffusion Sampling with Generative Unc.\nInput\n:random noise z, pretrained diffusion model\ngˆθ, Laplace posterior q(θ|D) (Eq. 5), number\nof MC samples M, semantic feature extractor\ncϕ, semantic likelihood noise σ\nOutput :generated sample ˆx0, generative uncertainty\nestimate u(z)\n1 Generate a sample ˆx0 = gˆθ(z)\n2 Get semantic features e0 = cϕ(ˆx0)\n3 for m = 1 →M do\n4\nθm ∼q(θ|D)\n5\nˆxm = gθm(z)\n6\nem = cϕ(ˆxm)\n7 end\n8 Compute p(x|z, D) using {em}M\nm=0 (Eq. 7)\n9 Compute the entropy u(z) = H(p(x|z, D))\n10 return ˆx0, u(z)\nsingle Gaussian via moment matching here, a common prac-\ntice in Bayesian neural networks for regression problems\n[Lakshminarayanan et al., 2017, Antorán et al., 2020].\nUnlike in the posterior predictive for predictive models\n(Eq. 2), where it is used to obtain both the prediction and the\nassociated uncertainty, the generative posterior predictive\n(Eq. 8) is used solely to estimate the generative uncertainty\nu(z). The actual samples ˆx are still generated using the pre-\ntrained diffusion model gˆθ (see Algorithm 1). As a variabil-\nity measure V(·) in our generative uncertainty framework,\nwe propose to use entropy (denoted with H(·) in Algo-\nrithm 1) due to its simplicity and widespread use in quantify-\ning predictive uncertainty. However, we note that alternative\nmeasures of variability, such as pairwise-distance estimators\n(PAiDEs) [Berry and Meger, 2023], can also be employed.\n3.4\nEPISTEMIC UNCERTAINTY\nUncertainty is commonly decomposed into two components:\naleatoric and epistemic [Hüllermeier and Waegeman, 2021,\nSmith et al., 2024]. Aleatoric uncertainty represents the\nirreducible uncertainty inherent in the data-generating\nprocess, while epistemic uncertainty arises from observing\nonly a limited amount of training data. In our framework, we\nfix the observation noise in the semantic likelihood (Eq. 7)\nto a small constant value (e.g., σ = 0.001). As a result, the\ngenerative uncertainty we capture is primarily epistemic,\nreflecting uncertainty about the model parameters θ due\nto limited training data via q(θ|D). Since the parameters\nϕ of the semantic feature extractor cϕ are kept fixed in the\nsemantic likelihood, the resulting generative uncertainty\nu(z) continues to reflect the epistemic uncertainty of the\ndiffusion model parameters θ. Extending our framework\nto capture the aleatoric uncertainty of a generative process\npresents an interesting avenue for future research.\n\n\n4\nEXPERIMENTS\nIn our experiments, we demonstrate that generative uncer-\ntainty is an effective method for detecting poor samples\nin diffusion models (Section 4.1). We also discuss the\nsampling overhead introduced by our Bayesian approach\nand show that it can be effectively minimized (Section 4.2).\nFinally, we extend our Bayesian framework beyond diffu-\nsion by applying it to detect low-quality samples in a (latent)\nflow matching model (Appendix A.7). Our code is available\nat https://github.com/metodj/DIFF-UQ.\n4.1\nDETECTING LOW-QUALITY GENERATIONS\nTo evaluate whether our newly introduced generative uncer-\ntainty can be used to detect low-quality generations, we fol-\nlow the experimental setup from prior work on uncertainty-\nbased filtering [Kou et al., 2024, De Vita and Belagiannis,\n2025]. Specifically, we generate 12K samples using a given\ndiffusion model and compute the uncertainty estimate for\neach sample. We then select the 10K samples with the lowest\nuncertainty. If uncertainty reliably reflects the visual quality\nof generated samples, filtering based on it should yield\ngreater improvements in population-level metrics (such as\nFID) compared to selecting a random subset of 10K images.\nImplementation Details\nTo ensure a fair comparison\nwith BayesDiff [Kou et al., 2024], we adopt their proposed\nimplementation of the last-layer Laplace approximation.\nSpecifically, we use an Empirical Fisher approximation\nof the Hessian with a diagonal factorization [Daxberger\net al., 2021a]. When computing the posterior predictive\ndistribution (Eq. 8), we use M = 5 Monte Carlo samples.\nFor the semantic feature extractor cϕ, we leverage a\npretrained CLIP encoder [Radford et al., 2021]. Additional\nimplementation details are provided in Appendix B.\nBaselines We first compare our proposed generative\nuncertainty to existing uncertainty-based approaches for\ndetecting low-quality samples: BayesDiff and the aleatoric\nuncertainty (AU) approach proposed by De Vita and Bela-\ngiannis [2025]. BayesDiff estimates epistemic uncertainty in\ndiffusion models using a last-layer Laplace approximation\nand tracks this uncertainty throughout the entire sampling\nprocess. In contrast, in AU aleatoric uncertainty is computed\nby measuring the sensitivity of intermediate diffusion\nscores to random perturbations. Unlike our approach, both\nmethods estimate uncertainty directly in pixel space.\nImportantly, we also compare our method against non-\nuncertainty-based sample-level metrics, such as the realism\nscore [Kynkäänniemi et al., 2019] and the rarity score [Han\net al., 2023]. These metrics work by measuring the distance\nof a generated sample from the data manifold (derived\nfrom a reference dataset) in a semantic space spanned by\nthe inception-net features [Szegedy et al., 2016]. Notably,\nprior work [Kou et al., 2024, De Vita and Belagiannis,\n2025] has not considered such comparisons, which we\nbelieve are essential for assessing the practical utility of\nuncertainty-based filtering.\nEvaluation Metrics\nIn addition to the widely used\nFréchet Inception Distance (FID) [Heusel et al., 2017] for\nevaluating the quality of a filtered set of images, we also\nreport precision and recall metrics [Sajjadi et al., 2018,\nKynkäänniemi et al., 2019]. To compute these quantities\nwe fit two manifolds in feature space: one for the generated\nimages and another for the reference (training) images.\nPrecision is the proportion of generated images that lie in\nthe reference image manifold while recall is the proportion\nof reference images that lie in the generated image manifold.\nPrecision measures the quality (or fidelity) of generated\nsamples, whereas recall quantifies their diversity (or\ncoverage over the reference distribution).\nResults\nWe present our main results on the ImageNet\ndataset in Table 1. We first observe that existing uncertainty-\nbased approaches (BayesDiff and AU) result in little to no\nimprovement in metrics that assess sample quality (FID and\nprecision). In contrast, our generative uncertainty method\nleads to significant improvements in terms of both FID\nand precision. For example, on the UViT model [Bao et al.,\n2023], a subset of images selected based on our uncertainty\nmeasure achieves an FID of 7.89, significantly outper-\nforming both the Random baseline (9.45) and existing\nuncertainty-based methods (BayesDiff 9.16, AU 9.20).\nNext, in order to qualitatively demonstrate the effectiveness\nof our approach, we show 25 samples with the highest\nand lowest generative uncertainty (out of the original\n12K samples) according to our method in Figure 2.\nHigh-uncertainty samples exhibit numerous artefacts, and\nin most cases, it is difficult to determine what exactly they\ndepict. Combined with the quantitative results in Table 1,\nthis supports our hypothesis that (Bayesian) generative\nuncertainty is an effective metric for identifying low-quality\nsamples. Conversely, the lowest-uncertainty samples are of\nhigh quality, with most appearing as ‘canonical’ examples\nof their respective (conditioning) class.\nFor comparison, in Figure 5 we also depict the 25 ‘worst’\nand ‘best’ samples according to the uncertainty estimate\nfrom BayesDiff. It is evident that their uncertainty is less\ninformative for sample quality than ours. Moreover, their\nuncertainty measure appears to be very sensitive to the\nbackground pixels. Most images with the highest uncer-\ntainty have a ‘cluttered’ background, whereas most images\nwith the lowest uncertainty have a ‘clear’ background.\nWe attribute this issue to the fact that in BayesDiff the\nuncertainty is computed directly in the pixel space, unlike\nin our approach where we use the semantic likelihood\n(Section 3.3) to move away from the (high-dimensional)\n\n\nTable 1: Image generation results for 10K filtered samples (out of 12K). Our generative uncertainty outperforms previously\nproposed uncertainty-based approaches in terms of image quality (AU [De Vita and Belagiannis, 2025], BayesDiff [Kou\net al., 2024]), as indicated by higher FID and precision scores, and is competitive with non-uncertainty methods (Realism\n[Kynkäänniemi et al., 2019], Rarity [Han et al., 2023]). We report mean values along with standard deviations over 3 runs\nwith different random seeds.\nADM (DDIM), ImageNet 128×128\nUViT (DPM), ImageNet 256×256\nFID (↓)\nPrecision (↑)\nRecall (↑)\nFID (↓)\nPrecision (↑)\nRecall (↑)\nRandom\n11.31 ± 0.07\n58.90 ± 0.36\n70.68 ± 0.38\n9.46 ± 0.12\n60.94 ± 0.24\n73.82 ± 0.33\nBayesDiff\n11.20 ± 0.05\n58.80 ± 0.05\n70.62 ± 0.32\n9.16 ± 0.17\n61.77 ± 0.19\n73.72 ± 0.38\nAU\n11.39 ± 0.05\n58.82 ± 0.42\n70.70 ± 0.38\n9.20 ± 0.12\n61.80 ± 0.33\n73.46 ± 0.24\nOurs\n10.14 ± 0.08\n61.26 ± 0.26\n69.60 ± 0.49\n7.89 ± 0.12\n64.14 ± 0.17\n71.92 ± 0.35\nRealism\n9.76 ± 0.04\n67.95 ± 0.19\n66.32 ± 0.40\n8.24 ± 0.09\n70.29 ± 0.15\n69.12 ± 0.32\nRarity\n10.09 ± 0.02\n64.99 ± 0.16\n67.73 ± 0.47\n8.37 ± 0.11\n67.21 ± 0.10\n67.76 ± 0.48\nFigure 2: Images with highest (left) and lowest (right) generative uncertainty amongst 12K generations using a UViT\ndiffusion model [Bao et al., 2023]. Generative uncertainty correlates with visual quality: high-uncertainty samples exhibit\nnumerous artefacts, whereas low-uncertainty samples resemble canonical images of their respective conditioning class.\nFigure 3: Images with the highest (bottom) and the lowest (top) generative uncertainty among 128 generations using a UViT\ndiffusion model for 2 classes: black swan (left) and Tibetan terrier (right).\n\n\nsample space. To further verify the importance of the\nsemantic likelihood, in Figure 7 we perform an ablation\nwhere we compute our generative uncertainty directly in\nthe pixel-space. It is clear that without semantic likelihood,\nour uncertainty becomes overly sensitive to the background\npixels in the same way as in BayesDiff.\nReturning to Table 1, we observe that filtering based on\nour generative uncertainty results in some loss of sample\ndiversity, as evidenced by lower recall scores (e.g., 73.82 for\nRandom vs. 71.92 for our method on the UViT model). We\nattribute this to the fact that, in our main experiment, 12K\nimages are generated such that all 1000 ImageNet classes\nare represented.2 Since certain classes produce images\nwith higher uncertainty (see Appendix A.6 for a detailed\nanalysis), filtering based on uncertainty inevitably alters the\nclass distribution among the selected samples. Moreover,\nthe trade-off between improving sample quality (precision)\nand reducing diversity (recall) has been observed before,\nsee for example the literature on classifier-free guidance\n[Ho and Salimans, 2022].\nLastly, we compare our proposed method with non-\nuncertainty-based approaches—a comparison missing in\nprior literature [Kou et al., 2024, De Vita and Belagiannis,\n2025]. For realism, we retain the 10K images with the\nhighest scores, whereas for rarity, we keep those with\nthe lowest scores. As shown in Table 1, our generative\nuncertainty is the only uncertainty-based method that\napproaches realism and rarity in terms of FID (e.g., 7.89\nfor ours vs. 8.24 for realism and 8.37 for rarity on UViT).\nHowever, a large gap remains in precision (e.g., 64.14 for\nours vs. 70.29 for realism and 67.21 for rarity on UViT).\nNotably, realism and rarity sacrifice the most sample\ndiversity, as indicated by their lowest recall scores (e.g.,\n69.12 for realism and 67.76 for rarity on UViT).\nFurthermore, Table 2 shows that our score can be effectively\ncombined with realism or rarity scores. Specifically, com-\nbining our score with realism yields an FID of 7.60 on UViT,\ncompared to 8.26 when combining realism and rarity. We at-\ntribute higher benefits from ensembling our score to the fact\nthat, while realism and rarity exhibit a strong negative Spear-\nman correlation (-0.85), our uncertainty measure is less cor-\nrelated with them (-0.27 with realism, 0.38 with rarity), as\nshown in Figure 10. Taken together, these results indicate\nthat our uncertainty score captures (somewhat) different de-\nsirable properties of images compared to realism and rarity.\n4.2\nIMPROVING SAMPLING EFFICIENCY\nWe next examine the sampling costs associated with\nBayesian inference in diffusion sampling. As shown in\n2Following Kou et al. [2024], we use class-conditional dif-\nfusion models but randomly sample a class for each of the 12K\ngenerated samples.\nFigure 4: FID results for 10K ImageNet-filtered images us-\ning our generative uncertainty on ADM model [Dhariwal\nand Nichol, 2021]. We vary the number of Monte Carlo sam-\nples M and diffusion sampling steps T (see Algorithm 1).\nBy default, we use M=5 with T=50 ( ), incurring an addi-\ntional 250 NFEs for uncertainty estimation. Encouragingly,\nsetting M=1 and T=25 ( ) still achieves competitive perfor-\nmance while reducing the sampling overhead by 10x. Lower\nleft is best: better FID and greater computational efficiency.\nAlgorithm 1, obtaining an uncertainty estimate u(z) for\na generated sample ˆx0 = gθ(z) requires generating M\nadditional samples, resulting in MT additional network\nfunction evaluations (NFEs). For the results presented in\nTable 1, we use M = 5 and the default number of sampling\nsteps T = 50 ( ), leading to an additional 250 NFEs for\nuncertainty estimation—on top of the 50 NFEs required\nto generate the original sample. Since this overhead may\nbe prohibitively expensive in certain deployment scenarios,\nwe next explore strategies to reduce the sampling cost\nassociated with our generative uncertainty.\nThe most straightforward approach is to reduce the number\nof Monte Carlo samples M. Encouragingly, reducing M\nto as few as 1 still achieves highly competitive performance\n(see Figure 4). Further efficiency gains can be achieved\nby reducing the number of sampling steps T, leveraging\nthe flexibility of diffusion models to adjust T on the fly.\nImportantly, we lower T only for the additional M samples\nused for uncertainty assessment while keeping the default\nT for the original sample ˆx0 to ensure that the generation\nquality is not compromised. Taken together, reducing M\nand T significantly improves the efficiency of our generative\nuncertainty. Using the ADM model [Dhariwal and Nichol,\n2021], our generative uncertainty method with M = 1 and\nT = 25 ( ) achieves an FID of 10.36, which still strongly\noutperforms both the Random (11.31) and BayesDiff\n(11.20) baselines while requiring only 25 additional NFEs.\n5\nRELATED WORK\nUncertainty quantification in diffusion models has\nrecently gained significant attention. Most related to our\n\n\nwork are BayesDiff [Kou et al., 2024], which uses a Laplace\napproximation to track epistemic uncertainty throughout\nthe sampling process, and De Vita and Belagiannis [2025],\nwhich captures aleatoric uncertainty via the sensitivity\nof diffusion score estimates. Our work extends both by\nintroducing an uncertainty framework that is more general\n(applicable beyond diffusion), simpler (requiring no sam-\npling modifications), and more effective (see Section 4.1).\nAlso related is DECU [Berry et al., 2024], which employs\nan efficient variant of deep ensembles [Lakshminarayanan\net al., 2017] to capture the epistemic uncertainty of condi-\ntional diffusion models. However, DECU does not consider\nusing uncertainty to detect poor-quality generations, as its\nframework provides uncertainty estimates at the level of the\nconditioning variable, whereas ours estimates uncertainty\nat the level of initial random noise. Similarly, in Chan\net al. [2024] the use of hyper-ensembles is proposed to\ncapture epistemic uncertainty in diffusion models for\ninverse problems such as super-resolution, but, as in DECU,\ntheir approach does not provide uncertainty estimates\nin unconditional settings or in conditional settings with\nlow-dimensional conditioning (such as class-conditional\ngeneration). Moreover, both DECU [Berry et al., 2024]\nand Chan et al. [2024] require modifying and retraining\ndiffusion model components, whereas our approach oper-\nates post-hoc with any pretrained diffusion model via the\nLaplace approximation [Daxberger et al., 2021a]. A recent\napproach, PUNC [Franchi et al., 2024], focuses only on\ntext-to-image models. The uncertainty of image generation\nwith respect to text conditioning is measured through the\nalignment between a caption generated from a generated\nimage and the original prompt used to generate said image.\nAdditionally, a large body of work explores conformal\nprediction for uncertainty quantification in diffusion models\n[Angelopoulos et al., 2022, Sankaranarayanan et al., 2022,\nTeneggi et al., 2023, Belhasin et al., 2023]. However, these\napproaches are primarily designed for inverse problems\n(e.g., deblurring), and cannot be directly applied to detect\nlow-quality samples in unconditional generation.\nBayesian inference in generative models has been\nexplored previously outside the domain of diffusion\nmodels. Prominent examples include Saatci and Wilson\n[2017] where a Bayesian version of a GAN is proposed,\nshowing improvements for semi-supervised learning,\nand Daxberger and Hernández-Lobato [2019], where a\nBayesian VAE [Tran et al., 2023] is shown to provide\nmore informative likelihood estimates for the unsupervised\nout-of-distribution detection compared to the non-Bayesian\ncounterparts [Nalisnick et al., 2018]. Since diffusion models\ncan be interpreted as neural ODEs [Song et al., 2020b],\nanother relevant work is Ott et al. [2023], which employs a\nLaplace approximation to quantify uncertainty when solving\nneural ODEs [Chen et al., 2018]. However, Ott et al. [2023]\nfocuses solely on low-dimensional regression problems.\nNon-uncertainty based approaches for filtering out\npoor generations include the realism [Kynkäänniemi et al.,\n2019], rarity [Han et al., 2023], and anomaly scores [Hwang\net al., 2024]. Our work is the first to establish a connection\nbetween these scores and uncertainty-based methods,\nwhich we hope will inspire the development of even better\nsample-level metrics in the future. Additionally, a large\nbody of work focuses on specially designed sample-quality\nscoring models [Gu et al., 2020, Zhao et al., 2024] or,\nalternatively, on leveraging large pretrained vision-language\nmodels (VLMs) [Zhang et al., 2024] for scoring generated\nimages. However, these approaches require either access to\nsample-quality labels or rely on (expensive) external VLMs.\nIn contrast, our uncertainty-based method requires neither,\nmaking it a more accessible and scalable alternative.\n6\nLIMITATIONS\nWhile we have demonstrated in Section 4 that semantic like-\nlihood is essential for addressing the over-sensitivity of prior\nwork to background pixels [Kou et al., 2024], our reliance\non a pretrained image encoder like CLIP [Radford et al.,\n2021] limits the applicability of our diffusion uncertainty\nframework to natural images. Removing the dependence on\nsuch encoders would unlock the application our Bayesian\nframework to other modalities where diffusion models are\nused, such as molecules [Hoogeboom et al., 2022, Cornet\net al., 2024] or text [Gong et al., 2022, Yi et al., 2024]. Ex-\nploring whether insights from the literature on uncovering\nsemantic features in diffusion models [Kwon et al., 2022,\nLuo et al., 2024, Namekata et al., 2024] could help achieve\nthis represents a promising direction for future work.\nMoreover, the large size of modern diffusion models neces-\nsitates the use of cheap and scalable Bayesian approximate\ninference techniques, such as the (diagonal) last-layer\nLaplace approximation employed in our work (following\n[Kou et al., 2024]). A more comprehensive comparison of\navailable approximate inference methods could be valuable,\nas improving the quality of the posterior approximation\nmay further enhance the detection of low-quality samples\nbased on Bayesian generative uncertainty.\n7\nCONCLUSION\nWe introduced generative uncertainty and demonstrated how\nto estimate it in modern generative models such as diffusion.\nOur experiments showed the effectiveness of generative\nuncertainty in filtering out low-quality samples. For future\nwork, it would be interesting to explore broader applications\nof Bayesian principles in generative modeling beyond\ndetecting poor-quality generations. Promising directions\ninclude guiding synthetic data generation, detecting memo-\nrized samples, and optimizing diffusion hyperparameters via\nthe marginal likelihood using the Laplace approximation.\n\n\nAcknowledgements\nThis project was generously supported by the Bosch Center\nfor Artificial Intelligence. Eric Nalisnick did not utilize\nresources from Johns Hopkins University for this project.\nStephan Mandt acknowledges funding from the National\nScience Foundation (NSF) through an NSF CAREER\nAward IIS-2047418, IIS-2007719, the NSF LEAP Center,\nthe IARPA WRIVA program, and the Hasso Plattner\nResearch Center at UCI.\nReferences\nMichael S Albergo, Nicholas M Boffi, and Eric Vanden-\nEijnden.\nStochastic interpolants: A unifying frame-\nwork for flows and diffusions.\narXiv preprint\narXiv:2303.08797, 2023.\nAnastasios N Angelopoulos, Amit Pal Kohli, Stephen\nBates, Michael Jordan, Jitendra Malik, Thayer Alshaabi,\nSrigokul Upadhyayula, and Yaniv Romano.\nImage-\nto-image regression with distribution-free uncertainty\nquantification and applications in imaging. In Interna-\ntional Conference on Machine Learning, pages 717–730.\nPMLR, 2022.\nJavier Antorán, James Allingham, and José Miguel\nHernández-Lobato. Depth uncertainty in neural networks.\nAdvances in neural information processing systems, 33:\n10620–10634, 2020.\nJulyan Arbel, Konstantinos Pitas, Mariia Vladimirova, and\nVincent Fortuin. A primer on bayesian neural networks:\nreview and debates. arXiv preprint arXiv:2309.16314,\n2023.\nFan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan\nLi, Hang Su, and Jun Zhu. All are worth words: A vit\nbackbone for diffusion models. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern\nrecognition, pages 22669–22679, 2023.\nOmer Belhasin, Yaniv Romano, Daniel Freedman, Ehud\nRivlin, and Michael Elad. Principal uncertainty quan-\ntification with spatial correlation for image restoration\nproblems. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 2023.\nLucas Berry and David Meger.\nEscaping the sample\ntrap: Fast and accurate epistemic uncertainty estimation\nwith pairwise-distance estimators.\narXiv preprint\narXiv:2308.13498, 2023.\nLucas Berry, Axel Brando, and David Meger. Shedding\nlight on large generative networks: Estimating epistemic\nuncertainty in diffusion models. In The 40th Conference\non Uncertainty in Artificial Intelligence, 2024.\nCharles Blundell, Julien Cornebise, Koray Kavukcuoglu,\nand Daan Wierstra. Weight uncertainty in neural network.\nIn International conference on machine learning, pages\n1613–1622. PMLR, 2015.\nWray L. Buntine and Andreas S. Weigend.\nBayesian\nback-propagation. Complex Syst., 5, 1991. URL https:\n//api.semanticscholar.org/CorpusID:\n14814125.\nMatthew Albert Chan, Maria J Molina, and Christopher Met-\nzler. Estimating epistemic and aleatoric uncertainty with\na single model. In The Thirty-eighth Annual Conference\non Neural Information Processing Systems, 2024.\nRicky TQ Chen, Yulia Rubanova, Jesse Bettencourt,\nand David K Duvenaud. Neural ordinary differential\nequations. Advances in neural information processing\nsystems, 31, 2018.\nFrançois Cornet, Grigory Bartosh, Mikkel N Schmidt, and\nChristian A Naesseth. Equivariant neural diffusion for\nmolecule generation.\nIn 38th Conference on Neural\nInformation Processing Systems, 2024.\nQuan Dao, Hao Phung, Binh Nguyen, and Anh Tran.\nFlow matching in latent space.\narXiv preprint\narXiv:2307.08698, 2023.\nErik Daxberger and José Miguel Hernández-Lobato.\nBayesian\nvariational\nautoencoders\nfor\nunsuper-\nvised out-of-distribution detection.\narXiv preprint\narXiv:1912.05651, 2019.\nErik Daxberger, Agustinus Kristiadi, Alexander Immer,\nRuna Eschenhagen, Matthias Bauer, and Philipp Hen-\nnig. Laplace redux-effortless bayesian deep learning.\nAdvances in Neural Information Processing Systems, 34:\n20089–20103, 2021a.\nErik Daxberger, Eric Nalisnick, James U Allingham, Javier\nAntorán, and José Miguel Hernández-Lobato. Bayesian\ndeep learning via subnetwork inference. In International\nConference on Machine Learning, pages 2510–2521.\nPMLR, 2021b.\nMichele De Vita and Vasileios Belagiannis.\nDiffusion\nmodel guided sampling with pixel-wise aleatoric\nuncertainty estimation. IEEE/CVF Winter Conference\non Applications of Computer Vision (WACV), 2025.\nPrafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis.\nAdvances in neural\ninformation processing systems, 34:8780–8794, 2021.\nPatrick Esser, Sumith Kulal, Andreas Blattmann, Rahim\nEntezari, Jonas Müller, Harry Saini, Yam Levi, Dominik\nLorenz, Axel Sauer, Frederic Boesel, Dustin Podell,\nTim Dockhorn, Zion English, and Robin Rombach.\n\n\nScaling rectified flow transformers for high-resolution\nimage synthesis.\nIn Forty-first International Con-\nference on Machine Learning, 2024.\nURL https:\n//openreview.net/forum?id=FPnUhsQJ5B.\nGianni\nFranchi,\nDat\nNguyen\nTrong,\nNacim\nBelkhir, Guoxuan Xia, and Andrea Pilzer.\nTo-\nwards\nunderstanding\nand\nquantifying\nuncer-\ntainty for text-to-image generation, 2024.\nURL\nhttps://arxiv.org/abs/2412.03178.\nYarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep\nbayesian active learning with image data. In International\nconference on machine learning, pages 1183–1192.\nPMLR, 2017.\nYarin Gal et al. Uncertainty in deep learning. 2016.\nShansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu,\nand LingPeng Kong. Diffuseq: Sequence to sequence\ntext generation with diffusion models. arXiv preprint\narXiv:2210.08933, 2022.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial\nnets. Advances in neural information processing systems,\n27, 2014.\nShuyang Gu, Jianmin Bao, Dong Chen, and Fang Wen.\nGiqa: Generated image quality assessment. In Computer\nVision–ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23–28, 2020, Proceedings, Part XI 16, pages\n369–385. Springer, 2020.\nJiyeon Han, Hwanil Choi, Yunjey Choi, Junho Kim,\nJung-Woo Ha, and Jaesik Choi. Rarity score : A new\nmetric to evaluate the uncommonness of synthesized\nimages. In The Eleventh International Conference on\nLearning Representations (ICLR), 2023.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained\nby a two time-scale update rule converge to a local nash\nequilibrium. Advances in neural information processing\nsystems, 30, 2017.\nJonathan Ho and Tim Salimans. Classifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022.\nJonathan Ho, Ajay Jain, and Pieter Abbeel.\nDenoising\ndiffusion probabilistic models.\nAdvances in neural\ninformation processing systems, 33:6840–6851, 2020.\nEmiel Hoogeboom, Vıctor Garcia Satorras, Clément\nVignac, and Max Welling.\nEquivariant diffusion for\nmolecule generation in 3d. In International conference\non machine learning, pages 8867–8887. PMLR, 2022.\nEyke Hüllermeier and Willem Waegeman.\nAleatoric\nand epistemic uncertainty in machine learning: An\nintroduction to concepts and methods. Machine learning,\n110(3):457–506, 2021.\nJaehui Hwang, Junghyuk Lee, and Jong-Seok Lee. Anomaly\nscore: Evaluating generative models and individual gen-\nerated images based on complexity and vulnerability. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 8754–8763, 2024.\nAlexander Immer, Matthias Bauer, Vincent Fortuin, Gunnar\nRätsch, and Khan Mohammad Emtiyaz.\nScalable\nmarginal likelihood estimation for model selection in\ndeep learning. In International Conference on Machine\nLearning, pages 4563–4573. PMLR, 2021.\nLaurent Valentin Jospin, Hamid Laga, Farid Boussaid,\nWray Buntine, and Mohammed Bennamoun. Hands-on\nbayesian neural networks—a tutorial for deep learning\nusers. IEEE Computational Intelligence Magazine, 17\n(2):29–48, 2022.\nTero Karras, Miika Aittala, Timo Aila, and Samuli\nLaine. Elucidating the design space of diffusion-based\ngenerative models.\nAdvances in neural information\nprocessing systems, 35:26565–26577, 2022.\nAlex Kendall and Yarin Gal. What uncertainties do we need\nin bayesian deep learning for computer vision? Advances\nin neural information processing systems, 30, 2017.\nDiederik P Kingma.\nAuto-encoding variational bayes.\narXiv preprint arXiv:1312.6114, 2013.\nSiqi Kou, Lei Gan, Dequan Wang, Chongxuan Li, and Zhijie\nDeng.\nBayesdiff: Estimating pixel-wise uncertainty\nin diffusion via bayesian inference.\nIn The Twelfth\nInternational Conference on Learning Representations\n(ICLR), 2024.\nAgustinus Kristiadi, Matthias Hein, and Philipp Hennig.\nBeing bayesian, even just a bit, fixes overconfidence in\nrelu networks. In International conference on machine\nlearning, pages 5436–5446. PMLR, 2020.\nMingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusion\nmodels already have a semantic latent space.\narXiv\npreprint arXiv:2210.10960, 2022.\nTuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko\nLehtinen, and Timo Aila. Improved precision and recall\nmetric for assessing generative models.\nAdvances in\nneural information processing systems, 32, 2019.\nBalaji Lakshminarayanan, Alexander Pritzel, and Charles\nBlundell.\nSimple and scalable predictive uncertainty\nestimation using deep ensembles. Advances in neural\ninformation processing systems, 30, 2017.\n\n\nZehui Li, Yuhao Ni, Guoxuan Xia, William Beardall,\nAkashaditya Das, Guy-Bart Stan, and Yiren Zhao.\nAbsorb & escape: Overcoming single model limitations\nin generating heterogeneous genomic sequences.\nIn\nThe Thirty-eighth Annual Conference on Neural In-\nformation Processing Systems, 2024.\nURL https:\n//openreview.net/forum?id=XHTl2k1LYk.\nYaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maxim-\nilian Nickel, and Matt Le. Flow matching for generative\nmodeling. arXiv preprint arXiv:2210.02747, 2022.\nXingchao Liu, Chengyue Gong, and Qiang Liu.\nFlow\nstraight and fast: Learning to generate and transfer data\nwith rectified flow. arXiv preprint arXiv:2209.03003,\n2022.\nCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan\nLi, and Jun Zhu.\nDpm-solver: A fast ode solver for\ndiffusion probabilistic model sampling in around 10 steps.\nAdvances in Neural Information Processing Systems, 35:\n5775–5787, 2022.\nGrace Luo, Lisa Dunlap, Dong Huk Park, Aleksander\nHolynski, and Trevor Darrell. Diffusion hyperfeatures:\nSearching through time and space for semantic corre-\nspondence. Advances in Neural Information Processing\nSystems, 36, 2024.\nDavid JC MacKay.\nBayesian interpolation.\nNeural\ncomputation, 4(3):415–447, 1992a.\nDavid JC MacKay. Information-based objective functions\nfor active data selection.\nNeural computation, 4(4):\n590–604, 1992b.\nWesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P\nVetrov, and Andrew Gordon Wilson. A simple baseline\nfor bayesian uncertainty in deep learning. Advances in\nneural information processing systems, 32, 2019.\nStephan Mandt, Matthew D Hoffman, David M Blei, et al.\nStochastic gradient descent as approximate bayesian\ninference. Journal of Machine Learning Research, 18\n(134):1–35, 2017.\nKevin P Murphy.\nProbabilistic machine learning: an\nintroduction. MIT press, 2022.\nEric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan\nGorur, and Balaji Lakshminarayanan. Do deep generative\nmodels know what they don’t know?\narXiv preprint\narXiv:1810.09136, 2018.\nKoichi Namekata, Amirmojtaba Sabour, Sanja Fidler, and\nSeung Wook Kim.\nEmerdiff: Emerging pixel-level\nsemantic knowledge in diffusion models. arXiv preprint\narXiv:2401.11739, 2024.\nRadford M Neal. Bayesian learning for neural networks,\nvolume 118. Springer Science & Business Media, 1995.\nPeter Nickl, Lu Xu, Dharmesh Tailor, Thomas Möllenhoff,\nand Mohammad Emtiyaz E Khan.\nThe memory-\nperturbation equation: Understanding model’s sensitivity\nto data.\nAdvances in Neural Information Processing\nSystems, 36, 2024.\nKatharina Ott, Michael Tiemann, and Philipp Hennig.\nUncertainty and structure in neural ordinary differential\nequations. arXiv preprint arXiv:2305.13290, 2023.\nWilliam Peebles and Saining Xie. Scalable diffusion models\nwith transformers.\nIn Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages\n4195–4205, 2023.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al.\nLearning transferable visual models from natural\nlanguage supervision. In International conference on\nmachine learning, pages 8748–8763. PMLR, 2021.\nHippolyt Ritter, Aleksandar Botev, and David Barber. A\nscalable laplace approximation for neural networks. In\n6th international conference on learning representations,\nICLR 2018-conference track proceedings, volume 6. Inter-\nnational Conference on Representation Learning, 2018.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Björn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pages 10684–10695, June\n2022.\nYunus Saatci and Andrew G Wilson.\nBayesian gan.\nAdvances in neural information processing systems, 30,\n2017.\nMehdi SM Sajjadi, Olivier Bachem, Mario Lucic, Olivier\nBousquet, and Sylvain Gelly.\nAssessing generative\nmodels via precision and recall.\nAdvances in neural\ninformation processing systems, 31, 2018.\nSwami\nSankaranarayanan,\nAnastasios\nAngelopoulos,\nStephen Bates, Yaniv Romano, and Phillip Isola.\nSemantic uncertainty intervals for disentangled latent\nspaces. In NeurIPS, 2022.\nMrinank Sharma, Sebastian Farquhar, Eric Nalisnick, and\nTom Rainforth. Do bayesian neural networks need to\nbe fully stochastic?\nIn International Conference on\nArtificial Intelligence and Statistics, pages 7694–7722.\nPMLR, 2023.\n\n\nZhenming Shun and Peter McCullagh. Laplace approxima-\ntion of high dimensional integrals. Journal of the Royal\nStatistical Society Series B: Statistical Methodology, 57\n(4):749–760, 1995.\nFreddie Bickford Smith, Jannik Kossen, Eleanor Trollope,\nMark van der Wilk, Adam Foster, and Tom Rainforth.\nRethinking aleatoric and epistemic uncertainty. arXiv\npreprint arXiv:2412.20892, 2024.\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli. Deep unsupervised learning using\nnonequilibrium thermodynamics.\nIn International\nconference on machine learning, pages 2256–2265.\nPMLR, 2015.\nJiaming Song, Chenlin Meng, and Stefano Ermon. De-\nnoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020a.\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma,\nAbhishek Kumar, Stefano Ermon, and Ben Poole. Score-\nbased generative modeling through stochastic differential\nequations. arXiv preprint arXiv:2011.13456, 2020b.\nC Szegedy. Intriguing properties of neural networks. arXiv\npreprint arXiv:1312.6199, 2013.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\nShlens, and Zbigniew Wojna. Rethinking the inception\narchitecture for computer vision.\nIn Proceedings of\nthe IEEE conference on computer vision and pattern\nrecognition, pages 2818–2826, 2016.\nJacopo Teneggi, Matthew Tivnan, Web Stayman, and\nJeremias Sulam. How to trust your diffusion model: A\nconvex optimization approach to conformal risk control.\nIn International Conference on Machine Learning, pages\n33940–33960. PMLR, 2023.\nLucas Theis.\nWhat makes an image realistic?\narXiv\npreprint arXiv:2403.04493, 2024.\nLucas Theis, Aäron van den Oord, and Matthias Bethge. A\nnote on the evaluation of generative models, 2016. URL\nhttps://arxiv.org/abs/1511.01844.\nJakub M. Tomczak. Deep Generative Modeling. Springer,\nGermany, February 2022. ISBN 978-3-030-93157-5. doi:\n10.1007/978-3-030-93158-2.\nBa-Hien Tran, Babak Shahbaba, Stephan Mandt, and Mau-\nrizio Filippone. Fully bayesian autoencoders with latent\nsparse gaussian processes. In International Conference\non Machine Learning, pages 34409–34430. PMLR, 2023.\nJoost Van Amersfoort, Lewis Smith, Yee Whye Teh, and\nYarin Gal. Uncertainty estimation using a single deep\ndeterministic neural network. In International conference\non machine learning, pages 9690–9700. PMLR, 2020.\nAndrew Gordon Wilson.\nThe case for bayesian deep\nlearning. arXiv preprint arXiv:2001.10995, 2020.\nQiuhua Yi, Xiangfan Chen, Chenwei Zhang, Zehai Zhou,\nLinan Zhu, and Xiangjie Kong. Diffusion models in text\ngeneration: a survey. PeerJ Computer Science, 10:e1905,\n2024.\nCheng Zhang, Judith Bütepage, Hedvig Kjellström, and\nStephan Mandt.\nAdvances in variational inference.\nIEEE transactions on pattern analysis and machine\nintelligence, 41(8):2008–2026, 2018.\nZicheng Zhang, Haoning Wu, Chunyi Li, Yingjie Zhou, Wei\nSun, Xiongkuo Min, Zijian Chen, Xiaohong Liu, Weisi\nLin, and Guangtao Zhai. A-bench: Are lmms masters\nat evaluating ai-generated images?\narXiv preprint\narXiv:2406.03070, 2024.\nGanning Zhao, Vasileios Magoulianitis, Suya You, and\nC-C Jay Kuo. A lightweight generalizable evaluation\nand enhancement framework for generative models and\ngenerated samples. In Proceedings of the IEEE/CVF\nWinter Conference on Applications of Computer Vision,\npages 450–459, 2024.\n\n\nAPPENDIX\nThe supplementary material is organized as follows:\n• In Appendix A.1, we qualitatively compare our method with BayesDiff [Kou et al., 2024].\n• In Appendix A.2, we perform ablations on our semantic likelihood (Section 3.3).\n• In Appendix A.3, we demonstrate how to use our generative uncertainty for pixel-wise uncertainty.\n• In Appendix A.4, we show that diffusion’s own likelihood is not useful for filtering out poor samples.\n• In Appendix A.5, we further compare our generative uncertainty to realism [Kynkäänniemi et al., 2019] and rarity\n[Han et al., 2023] scores.\n• In Appendix A.6, we investigate the drop in sample diversity by looking at the average generative uncertainty per\nconditioning class.\n• In Appendix A.7, we apply our generative uncertainty to detect low-quality samples in a latent flow matching model\n[Dao et al., 2023].\n• In Appendix B, we provide implementation and experimental details.\nA\nADDITIONAL RESULTS\nA.1\nQUALITATIVE COMPARISON WITH BAYESDIFF\nTo further highlight the differences between our generative uncertainty and BayesDiff [Kou et al., 2024], we present samples\nwith the highest and lowest uncertainty according to BayesDiff in Figure 5. These samples are drawn from the same set of 12K\nImageNet ‘unconditional’ images generated using the UViT model [Bao et al., 2023] as in Figure 2. Notably, BayesDiff’s\nuncertainty score appears highly sensitive to background pixels—images with high uncertainty tend to have cluttered\nbackgrounds, while those with low uncertainty typically feature clear backgrounds. Furthermore, as reflected in BayesDiff’s\npoor performance in terms of FID and precision (see Table 1), some low-uncertainty examples exhibit noticeable artefacts,\nwhereas certain high-uncertainty samples are of rather high-quality. For example, the image of a dog in the bottom-right\ncorner of the high-uncertainty grid in Figure 5 looks quite good despite being assigned (very) high uncertainty.\nSimilarly, in Figure 6, we show low- and high-uncertainty samples according to BayesDiff for the same set of 128 images\nper class as in Figure 3. Once again, we observe that BayesDiff’s uncertainty metric is less informative regarding a sample’s\nvisual quality compared to our generative uncertainty.\nA.2\nABLATION ON SEMANTIC LIKELIHOOD\nTo highlight the importance of using a semantic likelihood (Section 3.3) when leveraging uncertainty to detect low-quality\ngenerations, we conduct an ablation study in which we replace it with a standard Gaussian likelihood applied directly in\npixel space (Eq. 6). Figure 7 presents the highest and lowest uncertainty images according to this ‘pixel-space’ generative\nuncertainty. Notably, pixel-space uncertainty is overly sensitive to background pixels, mirroring the issue observed in\nBayesDiff (see Appendix A.1). This highlights the necessity of using semantic likelihood to obtain uncertainty estimates\nthat are truly informative about the visual quality of generated samples.\nA.3\nPIXEL-WISE UNCERTAINTY\nWhile not the primary focus of our work, we demonstrate how our generative uncertainty framework (Algorithm 1) can be\nadapted to obtain pixel-wise uncertainty estimates. This is achieved by replacing our proposed semantic likelihood (Eq. 7)\nwith a standard ‘pixel-space’ likelihood (Eq. 6). Figure 8 illustrates pixel-wise uncertainty estimates for 5 generated samples.\nAlthough pixel-wise uncertainty received significant attention in past work [Kou et al., 2024, Chan et al., 2024, De Vita and\nBelagiannis, 2025], there is currently no principled method for evaluating its quality. Most existing approaches rely on quali-\ntative inspection, visualizing pixel-wise uncertainty for a few generated samples (as we do in Figure 8). This further motivates\nour focus on sample-wise uncertainty estimates, where more rigorous evaluation frameworks—such as improvements in FID\nand precision on a set of filtered images (see Table 1)—enable more meaningful comparisons between different approaches.\n\n\nFigure 5: Images with the highest (left) and the lowest (right) BayesDiff uncertainty among 12K generations using a UViT\ndiffusion model [Bao et al., 2023]. BayesDiff uncertainty correlates poorly with visual quality and is overly sensitive to the\nbackground pixels. Same set of 12K generated images is used as in Figure 2 to ensure a fair comparison.\nFigure 6: Images with the highest (bottom) and the lowest (top) BayesDiff uncertainty among 128 generations using a UViT\ndiffusion model for 2 classes: black swan (left) and Tibetan terrier (right). Same set of 128 generated images\nper class is used as in Figure 3 to ensure a fair comparison.\nA.4\nCOMPARISON WITH LIKELIHOOD\nWe compare our generative uncertainty filtering criterion with a likelihood selection approach on the 12K images generated\nby ADM trained on ImageNet 128x128. In the same way as in our other comparisons, we retain the 10K generated images\nwith highest likelihood. We utilize the implementation in Dhariwal and Nichol [2021] to compute the bits-per-dimension\nof each sample (one-to-one with likelihood). The 25 samples with lowest and highest likelihood are shown in Figure 9.\nVisually, the likelihood objective heavily prefers simple images with clean backgrounds and not necessarily image quality.\nNote that this is consistent with other works that have reported likelihood to be an inconsistent identifier of image quality\n[Theis et al., 2016, Theis, 2024]. Quantitative results for image quality were consistent with our qualitative observations. The\nFID, precision, and recall for the best 10K images according to bits-per-dimension were 11.86 ± 0.0026, 58.23 ± 0.02160,\nand 70.45 ± 0.0237 over three runs. By point estimate, all three metrics are worse or indistinguishable from the Random\nbaseline (11.31 ± 0.07, 58.90 ± 0.36, 70.68 ± 0.38). Results for all image selection methods can be found in Table 1.\n\n\nFigure 7: Images with the highest (left) and the lowest (right) ‘pixel-space’ generative uncertainty among 12K generations\nusing a UViT diffusion model. Pixel-space uncertainty correlates poorly with visual quality and is overly sensitive to the\nbackground pixels. Same set of 12K generated images is used as in Figure 2 to ensure a fair comparison.\nFigure 8: Pixel-wise uncertainty based on our generative uncertainty for 5 generated samples using UViT diffusion.\nA.5\nCOMPARISON WITH REALISM & RARITY\nTo better understand the relationship between our generative uncertainty and non-uncertainty-based approaches such as\nrealism [Kynkäänniemi et al., 2019] and rarity [Han et al., 2023] scores, we compute the Spearman correlation coefficient\nbetween different sample-level metrics on a set of 12K generated images from the experiment in Section 4.1. As shown\nin Figure 10, realism and rarity scores exhibit a strong correlation (< −0.8). This is unsurprising, as both scores are derived\nfrom the distance of a generated sample to a data manifold obtained using a reference dataset (e.g., a subset of training\ndata or a separate validation dataset).3\nIn contrast, our generative uncertainty exhibits a weaker correlation (< 0.4) with both realism and rarity scores. We attribute\nthis to the fact that our uncertainty primarily reflects the limited training data used in training diffusion models (i.e., epistemic\nuncertainty, see Section 3.4), rather than the distance to a reference dataset, as is the case for realism and rarity scores.\nNext, we investigate whether combining different scores can improve the detection of low-quality generations. When\n3Such distance-based approaches are also commonly used to estimate prediction’s quality in predictive models; see, for example,\nVan Amersfoort et al. [2020].\n\n\nFigure 9: The 25 ‘worst’ (left) and ‘best’ (right) samples generated by ADM trained on ImageNet 128x128 selected by\nlowest and highest likelihood among 12K generations.\ncombining two scores, we first rank the 12K images based on each score individually, then compute the combined ranking\nby summing the two rankings and re-ranking accordingly. The results, shown in Table 2, indicate that combining realism and\nrarity leads to minor or no improvements in FID (9.81 compared to 9.76 for realism alone on ADM. However, combining our\ngenerative uncertainty with either realism or rarity achieves the best FID performance (9.54 on ADM). These results suggest\nthat ensembling scores that capture different aspects of generated sample quality is a promising direction for future research.\nFigure 10: Spearman correlation coefficient between different sample quality metrics for 12K ImageNet images generated\nusing ADM (left) and UViT (right).\nA.6\nCLASS-AVERAGED GENERATIVE UNCERTAINTY\nTo better understand the drop in sample diversity (recall) when using our generative uncertainty to filter low-quality\nsamples in Table 1, we analyze the distribution of average entropy per conditioning class. Specifically, for each of the\n12K generated images, we randomly sample a conditioning class to mimic unconditional generation. As a result, all 1,000\nImageNet classes are represented among the 12K generated samples. Next, we compute our generative uncertainty for each\nsample and then average the uncertainties within each class. A plot of class-averaged uncertainties is shown in Figure 11.\nSince class-averaged uncertainties exhibit considerable variance, the class distribution in the 10K filtered samples deviates\nsomewhat from that of the original 12K images, thereby explaining the reduction in diversity (recall).\n\n\nTable 2: Image generation results for 10K filtered samples (out of 12K) based on combined metrics. Combining our\ngenerative uncertainty outperforms combining realism and recall in terms of FID. We report mean values along with standard\ndeviation over 3 runs with different random seeds.\nADM (DDIM), ImageNet 128×128\nUViT (DPM), ImageNet 256×256\nFID (↓)\nPrecision (↑)\nRecall (↑)\nFID (↓)\nPrecision (↑)\nRecall (↑)\nRealism + Rarity\n9.81 ± 0.06\n67.06 ± 0.29\n66.73 ± 0.37\n8.26 ± 0.07\n69.01 ± 0.33\n69.86 ± 0.36\nOurs+ Realism\n9.54 ± 0.04\n66.41 ± 0.15\n67.04 ± 0.47\n7.60 ± 0.10\n68.33 ± 0.09\n69.75 ± 0.42\nOurs + Rarity\n9.56 ± 0.06\n65.44 ± 0.26\n67.36 ± 0.54\n7.56 ± 0.12\n67.48 ± 0.18\n70.18 ± 0.40\nFigure 11: A histogram of class-averaged generative uncer-\ntainties for 12K generated samples using UViT.\nWhile our primary focus in this work is on providing\nper-sample uncertainty estimates u(z), we can also ob-\ntain uncertainty estimates for the conditioning variable\nu(y) (e.g., a class label), by averaging over all samples\ncorresponding to a particular y ∈Y as done in Figure 11.\nThese estimates resemble the epistemic uncertainty scores\nproposed in DECU [Berry et al., 2024] and could be used\nto identify conditioning variables for which generated\nsamples are likely to be of poor quality. We leave fur-\nther exploration of generative uncertainty at the level of\nconditioning variables for future work.\nA.7\nFLOW MATCHING\nTo demonstrate that our generative uncertainty framework\n(Section 3) extends beyond diffusion models, we apply\nit here to the recently popularized flow matching approach [Lipman et al., 2022, Liu et al., 2022, Albergo et al., 2023].\nSpecifically, we consider a latent flow matching formulation [Dao et al., 2023] with a DiT backbone [Peebles and Xie, 2023].\nFor sampling, we employ a fifth-order Runge-Kutta ODE solver (dopri5). In Figure 12, we illustrate the samples with the\nhighest and lowest generative uncertainty among 12K generated samples. On a filtered set of 10K images, our generative\nuncertainty framework achieves an FID of 10.48 and a precision of 64.71, significantly outperforming a random baseline,\nwhich yields an FID of 11.80 and a precision of 61.04.\nB\nIMPLEMENTATION DETAILS\nAll our experiments can be conducted on a single A100 GPU, including the fitting of the Laplace posterior (Section 3.2).\nCode for reproducing our experiments is publicly available at https://github.com/metodj/DIFF-UQ.\nAll Params.\nLL Params.\nLL Name\nADM\n∼421 × 106\n∼14 × 103\nout.2\nUViT\n∼500 × 106\n∼18 × 103\ndecoder_pred\nDiT\n∼131 × 106\n∼1.2 × 106\nfinal_layer\nTable 3: Details of our last-layer (LL) Laplace approximation.\nThe first column presents the total number of model parame-\nters, while the second and third columns indicate the number\nof parameters in the last layer and its name, respectively\nLaplace Approximation\nWhen fitting a last-layer\nLaplace approximation (Section 3.2), we closely follow\nthe implementation from BayesDiff [Kou et al., 2024].\nSpecifically, we use the empirical Fisher approximation\nwith a diagonal factorization for Hessian computation.\nThe prior precision parameter and observation noise are\nfixed at γ = 1 and σ = 1, respectively. For Hessian\ncomputation, we utilize 1% of the training data for Ima-\ngeNet 128×128 and 2% for ImageNet 256×256. Further\ndetails about the last layer of each diffusion model are\nprovided in Table 3, where we observe that fewer than\n1% of the parameters receive a ‘Bayesian treatment’. We\nutilize laplace4 library in our implementation.\n4https://github.com/aleximmer/Laplace\n\n\nFigure 12: Images with the highest (left) and the lowest (right) generative uncertainty (Eq. 8) among 12K generations\nusing a latent flow matching model [Dao et al., 2023]. Generative uncertainty correlates with visual quality, as high-\nuncertainty samples exhibit numerous artefacts, whereas low-uncertainty samples resemble canonical images of their\nrespective conditioning class.\nAs discussed in Section 6, improving the quality of the Laplace approximation—such as incorporating both first and last\nlayers instead of only the last layer [Daxberger et al., 2021b, Sharma et al., 2023] or optimizing Laplace hyperparameters\n(e.g., prior precision and observation noise) [Immer et al., 2021]—could further enhance the quality of generative uncertainty\nand represents a promising direction for future work.\nSampling with Generative Uncertainty\nFor our main experiment in Section 4.1, we generate 12K images using the\npretrained ADM model [Dhariwal and Nichol, 2021] for ImageNet 128×128 and the UViT model [Bao et al., 2023] for\nImageNet 256×256. Following BayesDiff [Kou et al., 2024], we use a DDIM sampler [Song et al., 2020a] for the ADM\nmodel and a DPM-2 sampler [Lu et al., 2022] for the UViT model, both with T = 50 sampling steps.\nTo compute generative uncertainty (Algorithm 1), we first sample M = 5 sets of weights from the posterior q(θ|D). Then,\nfor each of the initial 12K random seeds, we generate M additional samples. The same set of model weights {θm}M\nm=1 is\nused for all 12K samples for efficiency reasons. For semantic likelihood (Eq. 7), we use a pretrained CLIP encoder [Radford\net al., 2021] and set the semantic noise to σ2 = 0.001 .\nBaselines\nFor all baselines, we use the original implementation provided by the respective papers, except for [De Vita\nand Belagiannis, 2025], which we reimplemented ourselves since we were unable to get their code to run. Moreover, we\nuse the default settings (e.g., hyperparameters) recommended by the authors for all baselines. For realism [Kynkäänniemi\net al., 2019] and rarity [Han et al., 2023] we use InceptionNet [Szegedy et al., 2016] as a feature extractor and a subset of\n50K ImageNet training images as the reference dataset. For samples where the rarity score is undefined (i.e., those that lie\noutside the estimated data manifold), we set it to inf.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20946v1.pdf",
    "total_pages": 18,
    "title": "Generative Uncertainty in Diffusion Models",
    "authors": [
      "Metod Jazbec",
      "Eliot Wong-Toi",
      "Guoxuan Xia",
      "Dan Zhang",
      "Eric Nalisnick",
      "Stephan Mandt"
    ],
    "abstract": "Diffusion models have recently driven significant breakthroughs in generative\nmodeling. While state-of-the-art models produce high-quality samples on\naverage, individual samples can still be low quality. Detecting such samples\nwithout human inspection remains a challenging task. To address this, we\npropose a Bayesian framework for estimating generative uncertainty of synthetic\nsamples. We outline how to make Bayesian inference practical for large, modern\ngenerative models and introduce a new semantic likelihood (evaluated in the\nlatent space of a feature extractor) to address the challenges posed by\nhigh-dimensional sample spaces. Through our experiments, we demonstrate that\nthe proposed generative uncertainty effectively identifies poor-quality samples\nand significantly outperforms existing uncertainty-based methods. Notably, our\nBayesian framework can be applied post-hoc to any pretrained diffusion or flow\nmatching model (via the Laplace approximation), and we propose simple yet\neffective techniques to minimize its computational overhead during sampling.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}