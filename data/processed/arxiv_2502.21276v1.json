{
  "id": "arxiv_2502.21276v1",
  "text": "Boosting Prediction with Data Missing Not\nat Random\nYuan Bian\nDepartment of Statistical and Actuarial Sciences\nUniversity of Western Ontario\nand\nGrace Y. Yi\nDepartment of Statistical and Actuarial Sciences\nDepartment of Computer Science\nUniversity of Western Ontario\nand\nWenqing He∗\nDepartment of Statistical and Actuarial Sciences\nUniversity of Western Ontario\nMarch 3, 2025\nAbstract\nBoosting has emerged as a useful machine learning technique over the past three\ndecades, attracting increased attention. Most advancements in this area, however,\nhave primarily focused on numerical implementation procedures, often lacking rig-\norous theoretical justifications. Moreover, these approaches are generally designed\nfor datasets with fully observed data, and their validity can be compromised by the\npresence of missing observations. In this paper, we employ semiparametric estimation\napproaches to develop boosting prediction methods for data with missing responses.\nWe explore two strategies for adjusting the loss functions to account for missingness\neffects. The proposed methods are implemented using a functional gradient descent\nalgorithm, and their theoretical properties, including algorithm convergence and es-\ntimator consistency, are rigorously established. Numerical studies demonstrate that\nthe proposed methods perform well in finite sample settings.\nKeywords: Adjusted loss function, Boosting, Consistency, Missing data, Semiparametric\nestimation.\n∗corresponding author: whe@stats.uwo.ca\n1\narXiv:2502.21276v1  [stat.ME]  28 Feb 2025\n\n\n1\nIntroduction\nBoosting, a useful machine learning method, transforms weak learners into strong learners\nthrough iterative processes (Schapire and Freund, 2012). The AdaBoost algorithm (Freund\nand Schapire, 1997) is widely recognized as the first practically feasible boosting algorithm,\nregarded as the best off-the-shelf classifier (Breiman, 1998). Breiman (1998, 1999) demon-\nstrated that the AdaBoost algorithm can be interpreted as a steepest descent algorithm in\na function space spanned by weak learners. Friedman et al. (2000) and Friedman (2001)\ndeveloped a general statistical framework, providing a direct interpretation of boosting as\na function estimation method. They also extended boosting from binary classification to\nregression and multiclass classification. B¨uhlmann and Yu (2003) introduced a computa-\ntionally simple boosting algorithm for regression and classification problems using the L2\nloss function.\nThese boosting methods typically require access to complete data without missing val-\nues, which is, however, often not true in practice. Recognizing this limitation, Hothorn\net al. (2006), Barnwal et al. (2022), and Chen and Yi (2024), among others, extended\nboosting algorithms to accommodate censored response variables. In this paper, we focus\non extending boosting algorithms to handle incomplete data scenarios where the response\nvariable is subject to missingness.\nWhen data are missing completely at random (MCAR), conventional boosting algo-\nrithms can be directly applied to the complete observations, yielding valid results as they\nstill constitute a random subsample. However, directly applying boosting procedures to\ndata missing not at random (MNAR) poses a significant challenge, as it can lead to biased\nresults. This issue is further complicated by the inherent non-identifiability problems as-\nsociated with MNAR data, as discussed by Yi and Cook (2002), Wang et al. (2014), Miao\nand Tchetgen Tchetgen (2016), Sun et al. (2018), and Morikawa and Kim (2021), among\nothers.\n2\n\n\nIn this paper, we incorporate two strategies to adjust the loss function: the Buckley-\nJames-type and the inverse propensity weight adjustments, with the missingness effects\naccounted for. These adjustments allow the flexible application of boosting methods to\nincomplete data with MNAR through a functional gradient descent algorithm. However,\nthe usual optimization procedures are complicated by the involvement of unknown func-\ntions. To address this difficulty, we describe semiparametric optimal estimation approaches\nthat provide consistent estimators for these unknown functions. We rigorously establish\nthe theoretical properties for the resultant estimators. In addition, we adapt conditions\nconsidered by Morikawa and Kim (2021) to ensure model identifiability.\nThe remainder of the paper is organized as follows. Section 2 introduces conventional\nboosting prediction with full data. Section 3 addresses incomplete data with MNAR re-\nsponses and presents boosting prediction approaches that accommodate the missingness\neffects. Section 4 describes semiparametric approaches for estimating the unknown func-\ntions involved. Section 5 establishes the theoretical results, followed by simulation studies\nin Section 6. In Section 7, we analyze a real dataset to illustrate the use of the proposed\nmethods. We conclude the article with discussions in Section 8 and defer technical details\nto the Supplementary Materials.\n2\nConventional Boosting with full Data\n2.1\nObjective and Data\nLet Y denote the continuous response variable, and let X denote the p-dimensional random\nvector of covariates, where Y ∈Y, X ∈X, and Y and X are the sample spaces for Y and\nX, respectively. The goal is to find a function of X, say f(X), that can effectively predict\nY . Let F = {f : X →Y | f is continuous and bounded in ℓ∞} denote the set of real\nvalued functions satisfying condition (B9) in the Supplementary Materials. For f ∈F,\n3\n\n\nlet L : Y × Y −→R denote the loss function that describes the discrepancy of using\nf(X) to predict Y , which is assumed to be differentiable (almost everywhere) and convex\nwith respect to the second argument, as specified in condition (B3) of the Supplementary\nMaterials.\nGiven the loss function L(·, ·), for f ∈F, define the risk function as\nR(f) = E{L (Y, f(X))},\n(1)\nwhere the expectation is taken with respect to the joint distribution of X and Y . To find\na function f ∈F that predicts Y well, we minimize the risk function:\nf ∗= argmin\nf∈F\nR(f).\n(2)\nIn practice, the joint distribution of X and Y is unknown, and we typically have access to\nonly a random sample of n independent observations of them, denoted by Ofull ≜{{Xi, Yi} :\ni = 1, . . . , n}. Consequently, replacing the expectation in (1) with its empirical counterpart,\nwe estimate f ∗by minimizing the empirical risk:\nˆffull = argmin\nf∈F\n(\nn−1\nn\nX\ni=1\nL(Yi, f(Xi))\n)\n.\n(3)\n2.2\nConventional Boosting Prediction Procedure\nFinding ˆffull in (3) can be achieved by employing the boosting method (Freund and Schapire,\n1997). This method, also known as the functional gradient descent algorithm (e.g., B¨uhlmann\nand Yu, 2003; B¨uhlmann and Hothorn, 2007; Schapire and Freund, 2012), minimizes the\nempirical risk through steepest gradient descent in a function space to iteratively improve\nthe estimate of ˆffull (Friedman, 2001), where the function space is spanned by a class of\nconstrained continuous functions, denoted by C. At iteration m, given the current esti-\nmate f (m)(·), the next estimate f (m+1) of ˆffull is updated by adding an increment term,\nˆα(m+1)ˆh(m+1)(·), where ˆh(m+1)(·) ∈C, and ˆα(m+1) is a learning rate.\n4\n\n\nSpecifically, let\nˆh(m+1) = argmin\nh(m+1)∈C\n\"\nn−1\nn\nX\ni=1\n\b\n∂L\n\u0000Yi, f (m)(Xi)\n\u0001\nh(m+1)(Xi)\n\t\n#\nand\nˆα(m+1) = argmin\nα(m+1)∈R\n(\nn−1\nn\nX\ni=1\nL\n\u0010\nYi, f (m)(Xi) + α(m+1)ˆh(m+1)(Xi)\n\u0011)\n,\nwhere ∂L\n\u0000Yi, f (m)(Xi)\n\u0001\n≜∂L(u,v)\n∂v\n\f\f\f\nu=Yi,v=f(m)(Xi) for i = 1, . . . , n. Then, at iteration (m+1),\nthe updated function f (m+1)(·) is given by\nf (m+1)(·) = f (m)(·) + ˆα(m+1)ˆh(m+1)(·),\nor equivalently,\nf (m+1)(·) = f (0)(·) +\nm+1\nX\nj=1\nˆα(j)ˆh(j)(·).\nThe iteration procedure terminates when a specified stopping criterion is met, say after\n˜m iterations. The final estimator of f ∗is then given by ˆffull(·) ≜f ( ˜m)(·). A common stop-\nping criterion evaluates the difference in the values of L(·, ·) between successive estimates,\nf (m)(·) and f (m−1)(·). The iteration stops when this difference, measured in a certain norm\n(e.g., L1 or L2), falls below a prespecified threshold value (e.g., 10−6).\n3\nBoosting Prediction with MNAR Data\nThe development in Section 2 builds upon the assumption that a random sample of full\ndata, Ofull, is available. In applications, however, this assumption is often not true. Here,\nwe focus on the scenario where the response is subject to missingness while the covariate\nvector is always observed.\nFor i = 1, . . . , n, let Ri denote the response missingness indicator, which equals 1 if Yi\nis observed and 0 otherwise. Throughout the paper, we use lowercase letters yi, xi, and ri\nto represent realizations of Yi, Xi, and Ri, respectively. The observed data form a sample,\ndenoted as Omissing, which includes {yi, xi, ri = 1} or {xi, ri = 0} for i = 1, . . . , n. For\nsimplicity, we occasionally drop the subject index i from the notation.\n5\n\n\n3.1\nMissing not at Random and Identifiablity\nLet f(y|X = x) denote the conditional probability density of Y given X, and let π(y, x) ≜\nPr(R = 1|Y = y, X = x) denote the propensity of observing the response, i.e., the condi-\ntional probability of observing Y , given Y and X. If π(y, x) does not depend on Y , the\nresulting missing data mechanism is called missing at random (MAR). When the missing\nmechanism is missing not at random (MNAR) or nonignorable, the propensity π(y, x) de-\npends on Y , as well as X, irrespective of whether Y is missing or observed. Under MNAR,\nnon-identifiability is often a concern (Robins and Ritov, 1997). When both f(y|X = x)\nand π(y, x) are left fully unspecified, the joint distribution of Y and R, given X, becomes\nnon-identifiable (Robins and Ritov, 1997).\nTo address non-identifiability issues, certain assumptions can be imposed. For example,\nWang et al. (2014) assumed the existence of a nonresponse instrumental variable (aka\na shadow variable) (Miao and Tchetgen Tchetgen, 2016), which is a component of the\ncovariate vector X that is associated with the response Y but is conditional independent of\nthe missingness indicator R, given Y and other components of X. Alternatively, Sun et al.\n(2018) assumed the existence of another version of instrumental variables: a subset of the\ncovariate vector X that is independent of the response Y but conditionally dependent on\nR, given Y and other components of X. While utilizing instrumental variables can mitigate\nnon-identifiability issues, identifying an appropriate instrumental variable can be difficult\nin applications. Most importantly, those conditions are not testable solely based on the\nobserved data (Morikawa and Kim, 2021).\nInstead of relying on the existence of instrumental variables, Morikawa and Kim (2021)\nproposed a set of identification conditions, listed as (A1) - (A4) in the Supplementary\nMaterials. Those conditions are testable using observed data. Even if the model does not\nsatisfy those conditions, a doubly-normalized exponential transformation (Morikawa and\nKim, 2021) can be applied to artificially make the model identifiable, albeit at the cost of\n6\n\n\nsacrificing the estimator consistency. In our development here, we adopt the identification\nconditions of Morikawa and Kim (2021) to emphasize our key ideas.\nImposing parametric assumptions on both f(y|X = x) and π(y, x) can help address\nthe non-identifiability issue, but the results are often sensitive to model misspecification\n(Kenward, 1998). As a remedy, an intermediate approach may be considered, where one\nfunction is modeled parametrically while leaving the other unspecified; this is the strategy\nwe adopt in the following development.\n3.2\nAdjusting Loss Functions with MNAR Data\nThe boosting prediction procedure described in Section 2.2 cannot be applied directly to\nOmissing, as the response Yi is not observed for every subject in the study. To address this,\nwe construct a new loss function, denoted L∗(yi, f(xi), ri), using the observed data in the\nsample Omissing. Following the idea of Chen and Yi (2024), we construct an adjusted loss\nfunction L∗(·, ·, ·) that maintains the same risk function as the original loss function L(·, ·),\ni.e., E{L∗(Yi, f(Xi), Ri)} = E{L(Yi, f(Xi))}. Therefore, minimizing the risk function of\nthe adjusted loss function E{L∗(Yi, f(Xi), Ri)} is equivalent to minimizing the risk function\nR(f) defined in (1), as if all Yi were observed.\nFollowing Hothorn et al. (2006), one way to adjust the loss function is through the\ninverse propensity weight (IPW) scheme:\nLIPW(yi, f(xi), ri) = riL(yi, f(xi))\nπ(yi, xi)\n,\n(4)\nwhere, to ensure (4) to be well-defined, π(y, x) is assumed to be bounded away from 0,\nas stated in condition (B4) in the Supplementary Materials. The adjusted loss function\n(4) uses only complete measurements and discards partial information from subjects whose\nresponses are missing.\nTo maximize the use of all available measurements, we follow the idea of Chen and Yi\n7\n\n\n(2024) to construct a Buckley-James (BJ)-type adjusted loss function:\nLBJ(yi, f(xi), ri) = riL(yi, f(xi)) + (1 −ri)Ψ0(xi),\n(5)\nwhere Ψ0(x) ≜E{L(Y, f(X))|X = x, R = 0}, determined by\nR\nL(y, f(x))f(y|X = x, R =\n0)dy.\nDetermining Ψ0(x) requires f(y|X = x, R = 0), which is not available since the outcome\nis not observed for this subpopulation. To get around this, as in Kim and Yu (2011), we\nuse Bayes’ rule to express\nf(y|X = x, R = 0) = f(y|X = x, R = 1)O(y, x)\nE{O(Y, X)|X = x, R = 1}, with O(y, x) ≜1 −π(y, x)\nπ(y, x)\n;\n(6)\nf(y|X = x, R = 1) is the conditional probability density of Y , given X = x and R = 1;\nand E{O(Y, X)|X = x, R = 1} is determined by\nR\nO(y, x)f(y|X = x, R = 1)dy.\nWhile incorporating the conditional expectation Ψ0(xi) in (5) facilitates contributions\nfrom subjects with missing responses, it is more restrictive than (4) because (5) requires\ntwo working models f(y|X = x, R = 1) and π(y, x). The following proposition justifies that\nthe proposed adjusted loss functions accommodate the missingness effects while recovering\nthe expectation of the original loss function. The proof is placed in the Supplementary\nMaterials.\nProposition 1. The proposed adjusted loss functions (4) and (5) have the same expectation\nas L(Yi, f(Xi)). That is,\n(a) E{LIPW(Yi, f(Xi), Ri)} = E{L(Yi, f(Xi))};\n(b) E{LBJ(Yi, f(Xi), Ri)} = E{L(Yi, f(Xi))},\nwhere the expectations are evaluated with respect to the joint distribution of the associated\nrandom variables.\nProposition 1 shows that our proposed adjusted loss functions, (4) and (5), have the\nsame expectation as the original loss function constructed using the full data Ofull, as if\n8\n\n\nthey were available. Therefore, the risk function for an adjusted loss function, (7) or (8),\nderived from incomplete data Omissing, is identical to that derived from the original full\ndata Ofull. Consequently, the optimization problem (3) based on the full data Ofull is now\nconverted to the following problem, which is computed from the observed incomplete data\nOmissing:\nˆf AL = argmin\nf∈F\n(\nn−1\nn\nX\ni=1\nL∗(yi, f(xi), ri)\n)\n.\n(7)\n4\nImplementation Details\nThe use (4) or (5) for the implementation of (7) requires consistent estimation of π(y, x)\nand f(y|X = x, R = 1). In this section, we describe methods for estimating them and\npresent a boosting prediction procedure for handling MNAR data.\n4.1\nConstruction of Consistent Estimators\nWe estimate π(y, x) by modeling it parametrically. Suppose π(y, x) is correctly modeled by\na parametric form, say π(y, x; γ) with the parameter γ, as stated in identification condition\n(A1) in the Supplementary Materials. The likelihood of γ derived from the binary missing\ndata indicator is\nL(γ) ≜\nn\nY\ni=1\n{π(yi, xi; γ)}ri {1 −π(yi, xi; γ)}1−ri ,\nyielding the score function\nS(γ) ≜∂log L(γ)\n∂γ\n=\nn\nX\ni=1\nSri(yi, xi; γ),\n(8)\nwhere\nSr(y, x; γ) ≜\n\u001a∂π(y, x; γ)\n∂γ\n\u001b \u0014\nr −π(y, x; γ)\nπ(y, x; γ){1 −π(y, x; γ)}\n\u0015\n(9)\nis a vector of the same dimension as γ.\nSince some yi values are missing (i.e., when ri = 0), directly setting (8) to 0 to solve\nfor γ is not feasible. To address this, Morikawa and Kim (2021) proposed an alternative\n9\n\n\nestimation method. Let\nO(y, x; γ) = 1 −π(y, x; γ)\nπ(y, x; γ)\nand\nE∗{S0(Y, X; γ)|X = x} ≜E{π−1(Y, X; γ)O(Y, X; γ)S0(Y, X; γ)|X = x, R = 1}\nE{π−1(Y, X; γ)O(Y, X; γ)|X = x, R = 1}\n.\n(10)\nLet S0(Y, X; γ) denote (9) with r set to 0.\nLet ˆE∗{S0(Y, X; γ)|X = x} represent an\nestimate of (10), with E(·|X = x, R = 1) estimated using the conditional probability\ndensity function f(y|X = x, R = 1).\nSolving\nn\nX\ni=1\n\u0014\u001a\n1 −\nri\nπ(yi, xi; γ)\n\u001b\nˆE∗{S0(Yi, Xi; γ)|Xi = xi}\n\u0015\n= 0\n(11)\nfor γ gives an estimator of γ. While this approach estimates γ, its implementation requires\nevaluation of (10), which relies on the availability of f(y|X = x, R = 1). In the remainder\nof this subsection, we outline parametric and nonparametric estimation procedures for\nf(y|X = x, R = 1).\nFirst, we model f(y|X = x, R = 1) by a parametric model, say f(y|X = x, R = 1; β)\nwith the parameter β. The estimator of β, denoted ˆβp, can be obtained by maximizing the\nconditional likelihood:\nˆβp = argmax\nβ\n( n\nX\ni=1\nri log f(yi|Xi = xi, Ri = 1; β)\n)\n.\nThus, the estimate of f(y|X = x, R = 1), denoted ˆfp(y|X = x, R = 1), is given by\nf(y|X = x, R = 1; ˆβp).\nWith ˆβp, (10) can be estimated accordingly. If the conditional expectations in (10) can\nbe derived in closed-form, then ˆE∗{S0(Y, X; γ)|X = x} in (11) is estimated as:\nˆE∗\np{S0(Y, X; γ)|X = x} = E{π−1(Y, X; γ)O(Y, X; γ)S0(Y, X; γ)|X = x, R = 1; ˆβp}\nE{π−1(Y, X; γ)O(Y, X; γ)|X = x, R = 1; ˆβp}\n, (12)\nwhere the conditional expectations are evaluated with respect to f(y|X = x, R = 1; ˆβp).\nOtherwise, ˆE∗{S0(Y, X; γ)|X = x} can be approximated using the fractional weights ap-\n10\n\n\nproach (Kim, 2011) by\nˆE∗\np{S0(Y, X; γ)|X = x}\n=\nn\nX\nk=1\n\n\nrkπ−1(yk, x; γ)O(yk, x; γ) ˆfp(yk|X = x, R = 1)/C (yk, x)\nPn\nl=1\nn\nrlπ−1(yl, x; γ)O(yl, x; γ) ˆfp(yl|X = x, R = 1)/C (yl, x)\noS0(yk, x; γ)\n\n,\n(13)\nwhere C(y, x) = Pn\nt=1 rt ˆfp(y|Xt = x, Rt = 1).\nUnder identification conditions (A1) - (A4) and regularity conditions (C1) - (C6),\nlisted in the Supplementary Materials, a consistent estimator of γ, denoted ˆγp, can be\nobtained by solving (11), with ˆE∗{S0(Y, X; γ)|X = x} replaced by (12) or (13). Hence,\nˆπp(y, x) ≜π(y, x; ˆγp) provides a consistent estimate for π(y, x). While the performance of\nˆfp(y|X = x, R = 1) may be sensitive to the assumed parametric form f(y|X = x, R = 1; β),\nMorikawa and Kim (2021) justified that ˆγp remains consistent even if f(y|X = x, R = 1; β)\nis misspecified. Moreover, when f(y|X = x, R = 1; β) is correctly specified, ˆγp attains the\nsemiparametric efficiency bound.\nAlternatively, f(y|X = x, R = 1) can be estimated nonparametrically to avoid possible\nmodel misspecification. For illustration, consider the case where Yi is continuous and Xi\nis univariate and continuous; the procedure can be easily generalized to discrete variables\nor multivariate Xi. Using kernel density estimation, f(y, x|R = 1) and f(x|R = 1) can be\nestimated by:\nˆfnp(y, x|R = 1) = n−1\nn\nX\nk=1\nrkKhx(x −xk)Khy(y −yk),\nand\nˆfnp(x|R = 1) = n−1\nn\nX\nl=1\nrlKhx(x −xl),\nwhere Kh(u) = K\n\u0000 u\nh\n\u0001\n, with K(·) denoting a kernel function and h denoting the bandwidth.\nHere, hx and hy are bandwidths corresponding to Xi and Yi, respectively. Consequently,\n11\n\n\n(10) can be estimated using the Nadaraya-Watson estimator:\nˆE∗\nnp{S0(Yi, Xi; γ)|Xi = xi}\n=\nPn\nk=1 rkKhx(xi −xk)π−1(yk, xi; γ)O(yk, xi; γ)S0(yk, xi; γ)\nPn\nl=1 rlKhx(xi −xl)π−1(yl, xi; γ)O(yl, xi; γ)\n.\n(14)\nUnder identification conditions (A1) - (A4) and regularity conditions (C1) - (C3) and\n(C7) - (C12), stated in the Supplementary Materials, a consistent estimator of γ, denoted\nˆγnp, which attains the semiparametric efficiency bound, can be obtained by solving (11)\nwith ˆE∗{S0(Yi, Xi; γ)|Xi = xi} replaced by (14). Therefore, ˆπnp(y, x) ≜π(y, x; ˆγnp) is a\nconsistent estimator for π(y, x).\nIt is worth emphasizing that ˆfnp(y|X = x, R = 1) and ˆπnp(y, x) are robust, as they do\nnot rely on any parametric modeling assumptions for f(y|X = x, R = 1). However, when X\nis multivariate or high dimensional, these estimates become less practical due to increased\ncomputational complexity and the higher possibility of collinearity among covariates.\n4.2\nAlgorithm for Boosting Prediction with MNAR Data\nWith the estimates of f(y|X = x, R = 1) and π(y, x), f(y|X = x, R = 0) can be esti-\nmated using (6), where f(y|X = x, R = 1) and π(y, x) are replaced by their estimates. Let\nˆf(y|X = x, R = 0) denote the resulting estimate. Let ˆL∗(yi, f(xi), ri) denote the adjusted\nloss functions in (4) or (5) with π(yi, xi) and f(y|Xi = xi, Ri = 1) replaced by their esti-\nmates described in Section 4.1. For the conditional mean in (5), we employ an approximate\nmethod: we specify a large positive integer Ny and independently take Ny random draws,\ndenoted\nn\ny(1)\ni0 , . . . , y(Ny)\ni0\no\n, from ˆf(y|X = xi, R = 0); then we estimate LBJ(yi, f(xi), ri) in\n(5) by its empirical version:\nLBJ(yi, f(xi), ri) = riL(yi, f(xi)) + (1 −ri)N −1\ny\nNy\nX\nk=1\nL\n\u0010\ny(k)\ni0 , f(xi)\n\u0011\n.\n(15)\nThe goal is to find f ∈F by modifying (7) to be\nˆf AL\nn\n= argmin\nf∈F\n(\nn−1\nn\nX\ni=1\nˆL∗(yi, f(xi), ri)\n)\n,\n(16)\n12\n\n\nwhich can be implemented using Algorithm 1.\nAlgorithm 1 Boosting Prediction for MNAR Data with A Modified Loss Function\nTake an initial function f (0) ∈F and set η as a small positive number;\nfor iteration (m + 1) with m = 0, 1, 2, . . . do\n(i) calculate ∂ˆL∗\u0000yi, f (m)(xi), ri\n\u0001\n≜∂ˆL∗(u,v,w)\n∂v\n\f\f\f\nu=yi,v=f(m)(xi),w=ri for i = 1, . . . , n;\n(ii) find ˆh(m+1) by solving\nˆh(m+1) = argmin\nh(m+1)∈C\n\"\nn−1\nn\nX\ni=1\nn\n∂ˆL∗\u0000yi, f (m)(xi), ri\n\u0001\nh(m+1)(xi)\no#\n;\n(iii) find ˆα(m+1) by solving\nˆα(m+1) = argmin\nα(m+1)∈R\n(\nn−1\nn\nX\ni=1\nˆL∗\u0010\nyi, f (m)(xi) + α(m+1)ˆh(m+1)(xi), ri\n\u0011)\n;\n(iv) update f (m+1)(xi) = f (m)(xi) + ˆα(m+1)ˆh(m+1)(xi) for i = 1, . . . , n;\nif at iteration ˜m + 1 ,\n\f\f\f\f\fn−1\nn\nX\ni=1\nˆL∗\u0000yi, f ( ˜m)(xi), ri\n\u0001\n−n−1\nn\nX\ni=1\nˆL∗\u0000yi, f ( ˜m+1)(xi), ri\n\u0001\n\f\f\f\f\f ≤η\n(17)\nthen stop iteration and define the final estimator as\nˆf AL\nn (·) = f ( ˜m)(·)\nend if\nend for\n4.3\nSpecification of the class C\nThe boosting prediction procedure for MNAR data described in Section 4.2 depends on\nthe specification of the class C. Similar to Chen and Yi (2024), we consider the regression\nspline method to characterize the functions in C. Instead of using the truncated power basis\nfunctions, which are known to be numerically unstable in practice (Hastie et al., 2009), we\n13\n\n\nemploy more computationally stable B-spline basis functions (de Boor, 2001).\nSpecifically, any function h(·) in C is expressed in an additive form:\nh(x) = h0 + h1(x1) + . . . + hp(xp),\nwith h0 denoting the intercept and hk(xk) = PT\nt=1 ξktbkt(xk) for k = 1, . . . , p, where x =\n(x1, . . . , xp)⊤; bk(xk) = (bk1(xk), . . . , bkT(xk))⊤is the vector of the B-spline basis functions\nof order M, with T −M + 1 interior knots at suitably chosen quantiles of Xk and M ≥2\nbeing an integer; and ξk = (ξk1, . . . , ξkT)⊤is the unknown parameter vector.\n5\nTheoretical Results\nIn this section, we develop theoretical results for the proposed methods, based on the\nassumption that π(y, x) and f(y|X = x, R = 1) are consistently estimated, as stated\nin condition (B5) in the Supplementary Materials. First, we discuss the convergence of\nthe proposed iterated algorithm, described in Algorithm 1, and defer the proofs to the\nSupplementary Materials.\nProposition 2. Assume regularity conditions (B1) - (B8) in the Supplementary Materials.\nSuppose that Algorithm 1 is run to a random sample Omissing with the given size n considered\nin Section 3. For any initial function f (0) ∈F, let f (m+1) denote the updated estimate of\nthe function at iteration (m + 1) of Algorithm 1. Then there exist positive constants c∗and\nC∗with c∗C∗> 1 such that\nR\n\u0000f (m+1)\u0001\n−R (f ∗) ≤\n\u0012\n1 −\n1\nC∗c∗\n\u0013m \b\nR\n\u0000f (0)\u0001\n−R(f ∗)\n\t\n,\n(18)\nwhere R(·) and f ∗are defined in (1) and (2), respectively.\nProposition 2 demonstrates that for Algorithm 1 and any m = 0, 1, 2, . . ., the difference\nbetween R\n\u0000f (m+1)\u0001\nand R (f ∗) is upper bounded by the difference between R\n\u0000f (0)\u0001\nand\nR (f ∗) that is multiplied by the power m of a positive constant smaller than 1. As m →∞,\nthis upper bound approaches zero, as summarized as follows.\n14\n\n\nTheorem 1. Under the setup of Proposition 2,\nlim\nm→∞R\n\u0000f (m+1)\u0001\n= R (f ∗) .\nWhile the convergence of Algorithm 1 is guaranteed by Theorem 1, in applications,\nAlgorithm 1 stops at a finite number to avoid excessively running an unnecessarily large\nnumber of iterations. Similar to Chen and Yi (2024), we may use (18), in combination with\n(17), to decide an upper bound for the stopping iteration number ˜m in Algorithm 1:\n˜m < 1 +\nlog\n\u001a\nη\n|R(f(0))−R(f∗)|(2−\n1\nC∗c∗)\n\u001b\nlog\n\u00001 −\n1\nC∗c∗\n\u0001\n,\n(19)\nwhere η is a given threshold in Algorithm 1. The upper bound of (19) shows that the\nnumber of iterations may vary with the choice of an initial function f (0) and the required\naccuracy η, as noted by Boyd and Vandenberghe (2004) and Chen and Yi (2024).\nNext, we show the consistency of the estimator ˆf AL\nn , defined in (16).\nTheorem 2. Assume the conditions in Theorem 1 and condition (B9) in the Supplementary\nMaterials. Suppose that Algorithm 1 is run to a sequence of random samples with a varying\nsize n. Then for any ϵ > 0,\nP\n\u0010\r\r\r ˆf AL\nn\n−f ∗\r\r\r\n∞≤ϵ\n\u0011\n−→1 as n →∞,\nwhere\n\r\r\r ˆf AL\nn\n−f ∗\r\r\r\n∞= max1≤i≤n\n\f\f\f ˆf AL\nn (Xi) −f ∗(Xi)\n\f\f\f is the L∞norm of ˆf AL\nn\n−f ∗, evaluated\nover {Xi : i = 1, . . . , n}.\nTheorem 2 indicates that the difference between the proposed estimator ˆf AL\nn\nand its\ntarget f ∗, expressed in terms of the L∞-norm, converges in probability as n →∞.\n6\nSimulation Studies\nIn this section, we assess the finite sample performance of the proposed methods. Five\nhundreds simulations are run for each parameter configuration discussed below, with the\nsample size n set to 1000.\n15\n\n\n6.1\nSimulation Design and Data Generation\nWe consider two settings with different dimensions p for covariates Xi: Setting 1 has p = 2,\nwhich is similar to the simulation settings in Morikawa and Kim (2021), and Setting 2\nincludes a larger number of covariates with p = 9.\nFor the two settings, we use slightly different ways to generate covariates Xi yet the\nsame procedure to generate responses Yi and missing indicators Ri.\nIn Setting 1, we\ngenerate X1,i independently from N (0, σ2\nX) for i = 1, . . . , n; and given X1,i, X2,i is drawn\nfrom N (ζX1,i, σ2\nX), with ζ = −0.25 and σ2\nX = 1\n2. In Setting 2, Xi = (X1,i, . . . , X9,i)⊤is\nindependently generated for i = 1, . . . , n from the multivariate normal distribution with\nzero mean and covariance matrix given by the 9 × 9 matrix with element (u, v) being\n0.5|u−v|/\n√\n2 for u, v = 1, . . . , 9.\nFor Setting j with j = 1, 2, given the covariates Xi, we generate the missing data\nindicator Ri from Bernoulli(˜πj(Xi)), with\n˜πj(Xi) =\n1\n1 + exp{gj(Xi)}\nand\ngj(Xi) = 1\n2γ2\nyσ2 −νj(Xi) −γyµj(Xi),\n(20)\nwhere for Setting 1, we specify ν1(Xi) = γ1,0 + γ1,1X1,i + γ1,2X2,i and µ1(Xi) = β1,0 +\nβ1,1X1,i + β1,2X2,i + β1,3X1,iX2,i with β1,1 = β1,2 = 0.4 and γ1,1 = γ1,2 = −0.5; and for\nSetting 2, we set ν2(Xi) = γ2,0 + P8\nk=1 γ2,kXk,i and µ2(Xi) = β2,0 + P9\nk=1 β2,kXk,i with\nβ2,1 = . . . = β2,9 = 0.4 and γ2,1 = . . . = γ2,8 = −0.5. For both settings, σ = 0.5.\nCondition (A4) in the Supplementary Materials implies that setting β1,3 ̸= 0 ensures the\nmodel to be identifiable. Noted by Morikawa and Kim (2021) through simulation studies,\nalmost half of the simulations failed to converge if β1,3 is taken as 0. Consequently, here\nwe only consider settings with a nonzero β1,3, set as β1,3 = 0.8.\nWe set γ1,0 = γ2,0 = 0.405 to achieve approximately 40% missing observations at the\nbaseline when taking Xi and Yi to be zero in both settings. Furthermore, to represent\ndifferent missing data mechanisms, for Setting 1, we set γy = 0 to represent an MAR\nand set γy = −1 to reflect an MNAR; and for Setting 2, we set γy = 0 for an MAR and\n16\n\n\nγy = 1 for an MNAR. The average missingness proportions in our simulated samples are\nabout 40.4%, 41.3%, 43.6%, and 41.1% for Setting 1 (MAR), Setting 1 (MNAR), Setting\n2 (MAR), and Setting 2 (MNAR), respectively.\nAs shown in the Supplementary Materials, in Setting 1, by taking E(Yi) = 0, β1,0 is\ndetermined by\nγyσ2{1 −Pr(Ri = 1)} −ζβ1,3σ2\nX,\n(21)\nleading to β1,0 = 0.1 for the MAR scenario and β1,0 = 0 for the MNAR scenario, if we\nset Pr(Ri = 1) = 0.6. Similarly, for Setting 2, if E(Yi) = 0 and Pr(Ri = 1) = 0.6, β2,0 is\ndetermined by\nγyσ2{1 −Pr(Ri = 1)},\n(22)\nas detailed in the Supplementary Materials. Thus, β2,0 = 0 for the MAR scenario and\nβ2,0 = 0.1 for the MNAR case.\nConsequently, given Xi and Ri for i = 1, . . . , n, the response Yi is generated from\nN(µj(Xi) −(1 −Ri)γyσ2, σ2),\n(23)\nas detailed in the Supplementary Materials.\nNext, in both settings, the generated data are split randomly by the 4 : 1 ratio, and\nwe let OTR ≜{yi, xi, ri : i = 1, · · · , n1} and OTE ≜{yi, xi, ri : i = n1 + 1, · · · , n1 + n2}\ndenote them, respectively, where n1 = 800 and n2 = 200.\nThe training data OTR\nmissing,\nwhich includes {yi, xi, ri = 1} or {xi, ri = 0} with i = 1, . . . , n1, are used to obtain the\nestimated ˆf ∗\nn1(·); and the test data OTE\nmissing, which includes {yi, xi, ri = 1} or {xi, ri = 0}\nwith i = n1 + 1, . . . , n1 + n2, are utilized to evaluate the performance of ˆf PO\nn1 (·). Define\nOTR\nfull ≜{yi, xi : i = 1, · · · , n1}.\n6.2\nEvaluation Metrics\nTo evaluate the performance of the proposed boosting prediction methods, we calculate\nthe following two metrics for the target function f ∗as in (2).\n17\n\n\nThe first metric represents the sample-based maximum absolute error (S-MAE) (Chen\nand Yi, 2024), defined as the infinity norm of the difference between the estimate and the\ntrue function with respect to the sample:\n\r\r\r ˆf ∗\nn1 −f ∗\r\r\r\n∞=\nmax\nn1+1≤i≤n1+n2\n\f\f\f ˆf ∗\nn1(xi) −f ∗(xi)\n\f\f\f ,\nand the second metric reports square root of the sample-based mean squared error (S-\nRMSE), defined as:\n\r\r\r ˆf ∗\nn1 −f ∗\r\r\r\n2 =\nv\nu\nu\ntn2−1\nn1+n2\nX\ni=n1+1\nn\nˆf ∗\nn1(xi) −f ∗(xi)\no2\n.\nThese two metrics take different perspectives to reflect the discrepancies of ˆf ∗\nn1 from its\ntarget f ∗.\n6.3\nAnalysis Methods\nWe analyze the simulated data using five methods, where we set M = 2 and T = 3 for\nthe specification of the class C, discussed in Section 4.3. For the stopping criterion, we set\nη = 10−6.\nThe first two methods, called R (reference) and N (naive), address two extreme sce-\nnarios: applying the conventional boosting procedure introduced in Section 2.2 to the\nfull dataset OTR\nfull and to the dataset OTR\nmissing, respectively. The next three methods ap-\nply boosting with modified loss functions described in Algorithm 1. Specifically, we em-\nploy the inverse propensity weight adjusted loss functions (4) with parametrically modeled\nf(y|X = x, R = 1) and nonparametrically estimated f(y|X = x, R = 1), respectively,\ndenoted IPW and IPWN, where nonparametric estimation uses a Gaussian kernel with\na rule-of-thumb bandwidth hx = (Pn1\ni=1 ri)−1\n5 ˆσx, with ˆσx denoting the sample standard\ndeviation of the covariate, as suggested by Morikawa and Kim (2021). Additionally, we\nconsider the Buckley-James-type adjusted loss function (5), referred to as the BJ method,\nemploying (15) with Ny = 20.\n18\n\n\nLet L1, L2, and H denote the L1 loss, the L2 loss, and the Huber loss, respectively,\ngiven by L (Yi, f(Xi)) = |Yi −f(Xi)|, L (Yi, f(Xi)) = 1\n2 {Yi −f(Xi)}2, and\nL (Yi, f(Xi)) =\n\n\n\n\n\n\n\n1\n2 {Yi −f(Xi)}2 ,\nif |Yi −f(Xi)| ≤η,\nη\n\u0000|Yi −f(Xi)| −η\n2\n\u0001\n,\notherwise,\nwhere η is a transition point, which can be chosen as the αth quantile of the set {|Yi −\nf(Xi)| : i = 1, . . . , n}, with 0 ≤α ≤100 (Friedman, 2001). For the Huber loss, η is itera-\ntively specified as the 50th-quantile of {Ri|Yi −f (m−1)(Xi)| : i = 1, . . . , n} at iteration m,\nas in Friedman (2001) and B¨uhlmann and Hothorn (2007). As discussed by B¨uhlmann and\nHothorn (2007) and Hastie et al. (2009), the solution f ∗in (2) corresponds to median(Yi|Xi)\nand mean(Yi|Xi) for the L1 loss and L2 loss functions, respectively. Here “median(Yi|Xi)”\nand “mean(Yi|Xi)” represent the median and mean of the conditional probability function,\nfj(y|Xi), of Yi given Xi, which is given by\nfj(y|Xi) = ˜πj(Xi)fj(y|Xi, Ri = 1) + {1 −˜πj(Xi)}fj(y|Xi, Ri = 0)\n(24)\nfor Setting j = 1 or 2, where fj(y|Xi, Ri = 0) and fj(y|Xi, Ri = 1) is defined in the\nSupplementary Materials. As shown in the Supplementary Materials, corresponding to\nSetting j with j = 1, 2, mean(Yi|Xi) equals\nµj(Xi) −{1 −˜πj(Xi)}γyσ2,\n(25)\nand median(Yi|Xi) can be obtained by solving\nR q\n−∞fj(y|Xi)dy = 0.5 for q.\nIn the MAR scenario, since fj(y|Xi) = fj(y|Xi, Ri = 1) = fj(y|Xi, Ri = 0), which is\nidentical to the probability density function (pdf) of (S.23) in the Supplementary Materials,\nmedian(Yi|Xi) coincides with mean(Yi|Xi) in (25). Given that the Huber loss behaves as the\nL2 loss when |Yi −f(Xi)| ≤η, and as a linear form of the L1 loss otherwise, the solution f ∗\nin (2) also corresponds to (25) if the Huber loss is used. However, in the MNAR scenario,\nmedian(Yi|Xi) does not necessarily equal mean(Yi|Xi), so f ∗generally lacks an analytic\nform when using the Huber loss.\n19\n\n\n6.4\nSimulation Results\nIn Figures 1 and 2, we summarize the S-MAE and S-RMSE values for 500 simulations as\nboxplots. Figure 1 presents results for the MAR scenario with three loss functions, while\nFigure 2 focuses on the MNAR scenario using only L1 and L2 loss functions. In Figure 1\nwith MAR, all methods perform similarly, with the L2 loss yielding the smallest median of\nS-MAE and S-RMSE values. In Figure 2 with MNAR, for Setting 1, the N methods produce\nthe largest S-MAE and S-RMSE, whereas for Setting 2, all methods using the same loss\nfunction yield similar S-MAE and S-RMSE values. The L1 loss function produces larger\nvalues and the L2 loss function yields smaller values. The IPW, IPWN, and BJ methods\nprovide comparable results to the R methods.\nloss\nH\nL1\nL2\n1\n2\n3\n4\n5\nR\nN\nIPW\nIPWN\nBJ\nvalue\n0.3\n0.4\n0.5\n0.6\n0.7\nR\nN\nIPW\nIPWN\nBJ\nvalue\n0\n2\n4\n6\nR\nN\nIPW\nIPWN\nBJ\nvalue\n0.25\n0.50\n0.75\n1.00\nR\nN\nIPW\nIPWN\nBJ\nvalue\nFigure 1: Prediction results in simulation studies in the MAR scenario. Top and bottom\nrows correspond to Settings 1 and 2, respectively; left and right columns correspond to the\nvalues for S-MAE and S-RMSE, respectively.\nTo visualize predicted values derived from different methods, Figure 3 shows boxplots\nof the average predicted values, n2−1 Pn2\ni=1 ˆf ∗\nn1(Xi), across 500 simulations, along with the\nexpected value E(Yi), whose expression is provided in the Supplementary Materials. Under\nMAR, all methods yield similar results, though the N method tends to deviate more from\n20\n\n\nloss\nL1\nL2\n1\n2\n3\n4\n5\nR\nN\nIPW\nIPWN\nBJ\nvalue\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nR\nN\nIPW\nIPWN\nBJ\nvalue\n0.0\n2.5\n5.0\n7.5\n10.0\nR\nN\nIPW\nIPWN\nBJ\nvalue\n0.25\n0.50\n0.75\nR\nN\nIPW\nIPWN\nBJ\nvalue\nFigure 2: Prediction results in simulation studies in the MNAR scenario. Top and bottom\nrows correspond to Settings 1 and 2, respectively; left and right columns correspond to the\nvalues for S-MAE and S-RMSE, respectively.\nthe R method than the proposed methods. However, with MNAR in Setting 1, the R and\nproposed methods (i.e., IPW, IPWN, and BJ) produce fairly close results, while the N\nmethods exhibit noticeable bias. In Setting 2, with a large number of correlated covariates,\nIPWN does not perform satisfactorily, as the MNAR estimation procedure of Morikawa\nand Kim (2021) assumes independent covariates.\nFinally, we assess the convergence of the proposed boosting methods. The results con-\nfirm the convergence of our boosting algorithm, as long as estimates of π2(y, x), defined in\nthe Supplementary Materials, using the method of Morikawa and Kim (2021) are available;\nfurther details are provided in the Supplementary Materials. We note that the method pro-\nposed by Morikawa and Kim (2021) is specifically designed for MNAR data and may not\nperform well for MAR data, particularly in scenarios with high-dimensional covariates. In\nsuch cases, divergence can occur frequently, as acknowledged by Morikawa and Kim (2021),\nthus preventing the estimation of π2(y, x) and halting the boosting procedure. This issue\nmay stem from various factors related to starting values for the nonlinear optimization\n21\n\n\nloss\nH\nL1\nL2\n−0.1\n0.0\n0.1\n0.2\nR\nN\nIPW\nIPWN\nBJ\nvalue\n−0.2\n0.0\n0.2\nR\nN\nIPW\nIPWN\nBJ\nvalue\n−0.50\n−0.25\n0.00\n0.25\n0.50\nR\nN\nIPW\nIPWN\nBJ\nvalue\n−0.2\n0.0\n0.2\n0.4\nR\nN\nIPW\nIPWN\nBJ\nvalue\nFigure 3: Prediction results in simulation studies: Boxplots of n2−1 Pn2\ni=1 ˆf ∗\nn1(Xi) obtained\nfrom the five methods in combination with the three loss functions, for 500 simulations,\nwhere the grey line indicates the value of E(Yi). Top and bottom rows correspond to Settings\n1 and 2, respectively; left and right columns correspond to the MAR and MNAR settings,\nrespectively.\nfunction, the complexity of the optimization process, and the large number of parameters.\n6.5\nModel Misspecification\nIn addition to Sections 3.2 and 4.1, we further conduct a simulation study to assess the\nperformance of the IPW and IPWN methods under model misspecification for πj(Yi, Xi)\nand fj(y|Xi, Ri = 1), with j = 1, 2.\nWe use the same procedure as in Section 6.1 to generate data, where fj(y|Xi, Ri = 1)\nand πj(Yi, Xi) are respectively taken as the pdf of (S.23) and (S.24). However, when fitting\nthe data with the IPW methods, we intentionally misspecify fj(y|Xi, Ri = 1) and πj(Yi, Xi)\nas the pdf of N (βj,1X1,i, σ2) and exp (γj,0) /{1+exp (γj,0)}, respectively. Specifically, we let\nIPW1, IPW2, IPW3, and IPW4 represent cases where both fj(y|Xi, Ri = 1) and πj(Yi, Xi)\nare correctly specified, only fj(y|Xi, Ri = 1) is misspecified, only πj(Yi, Xi) is misspecified,\n22\n\n\nand both fj(y|Xi, Ri = 1) and πj(Yi, Xi) are misspecified, respectively. For the IPWN\nmethod, we let IPWN1 and IPWN2 represent cases where πj(Yi, Xi) is correctly specified\nand misspecified, respectively. The results are displayed in Figures 4 - 6 in a manner similar\nto those in Figures 1 - 3, where in contrast, we also include results obtained from the R\nand N methods.\nIn the MAR scenario of Setting 1, all IPW and IPWN methods perform similarly. In the\nMNAR scenario of Setting 1, the IPW and IPWN methods with misspecified πj (Yi, Xi),\nnamely, IPW3, IPW4, and IPWN2, exhibit performance similar to the N method, whereas\nthe IPW and IPWN methods with correctly specified πj (Yi, Xi), namely, IPW1, IPW2, and\nIPWN1, yield results closer to those obtained from the R method. For the IPW methods,\nwhen there is a single model misspecification (either for πj (Yi, Xi) or fj (y|Xi, Ri = 1)),\nmisspecifying the latter has less noticeable effects compared to misspecifying the former,\nwhich aligns with the discussions in Sections 3.2 and 4.1. For the MAR scenario of Setting\n2, the results are similar to those in Setting 1, with the N, IPW2, IPW3, and IPW4\nmethods (but not the IPWN methods) exhibiting more biased results. However, in the\nMNAR scenario of Setting 2, only the IPW1 method yields results comparable to the R\nmethod.\n7\nAnalysis of KLIPS Data\nTo illustrate the proposed methods, we apply them to analyze a dataset from the Korean\nLabor and Income Panel Survey (KLIPS), which contains demographics information for\n2501 Korean workers. The KLIPS data are frequently analyzed in the MNAR literature,\nincluding Kim and Yu (2011), Wang et al. (2014), Shao and Wang (2016), Morikawa et al.\n(2017), and Morikawa and Kim (2021).\nFor subject i, let the response variable Yi denote the income (in 106 Korean Won) of\nworker i in 2008; let Xi1 denote the income (in 106 Korean Won) of worker i in 2007; let\n23\n\n\nloss\nH\nL1\nL2\n1\n2\n3\n4\n5\nR\nN\nIPW1\nIPW2\nIPW3\nIPW4 IPWN1 IPWN2\nvalue\n0.3\n0.4\n0.5\n0.6\n0.7\nIPW1\nIPW2\nIPW3\nIPW4\nIPWN1\nIPWN2\nNA\nvalue\n2\n4\n6\nR\nN\nIPW1\nIPW2\nIPW3\nIPW4 IPWN1 IPWN2\nvalue\n0.25\n0.50\n0.75\n1.00\nR\nN\nIPW1\nIPW2\nIPW3\nIPW4 IPWN1 IPWN2\nvalue\nFigure 4: Performance under model misspecification in the MAR scenario. Top and bottom\nrows correspond to Settings 1 and 2, respectively; left and right columns correspond to the\nvalues for S-MAE and S-RMSE, respectively.\nloss\nL1\nL2\n1\n2\n3\n4\n5\nR\nN\nIPW1\nIPW2\nIPW3\nIPW4 IPWN1 IPWN2\nvalue\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nR\nN\nIPW1\nIPW2\nIPW3\nIPW4 IPWN1 IPWN2\nvalue\n2.5\n5.0\n7.5\n10.0\nR\nN\nIPW1\nIPW2\nIPW3\nIPW4 IPWN1 IPWN2\nvalue\n0.25\n0.50\n0.75\nR\nN\nIPW1\nIPW2\nIPW3\nIPW4 IPWN1 IPWN2\nvalue\nFigure 5: Performance under model misspecification in the MNAR scenario.\nTop and\nbottom rows correspond to Settings 1 and 2, respectively; left and right columns correspond\nto the values for S-MAE and S-RMSE, respectively.\nXi2 be 1 if the age is less than 35, 2 if the age is between 35 and 51, and 3 otherwise; and\nlet Xi3 be 1 if male and 2 if female. The response variable has about 30.63% missing values\n24\n\n\nloss\nH\nL1\nL2\n−0.2\n−0.1\n0.0\n0.1\n0.2\n0.3\nR\nN\nIPW1\nIPW2\nIPW3\nIPW4 IPWN1 IPWN2\nvalue\n−0.4\n−0.2\n0.0\n0.2\n0.4\nR\nN\nIPW1\nIPW2\nIPW3\nIPW4 IPWN1 IPWN2\nvalue\n−0.50\n−0.25\n0.00\n0.25\n0.50\nR\nN\nIPW1\nIPW2\nIPW3\nIPW4 IPWN1 IPWN2\nvalue\n−0.2\n0.0\n0.2\n0.4\nR\nN\nIPW1\nIPW2\nIPW3\nIPW4 IPWN1 IPWN2\nvalue\nFigure 6: Performance under model misspecification: Boxplots of n−1 Pn\ni=1 ˆf ∗\nn(Xi) obtained\nfrom the eight methods in combination with the three loss functions, for 500 simulations,\nwhere the grey line indicates the value of E(Yi). Top and bottom rows correspond to Settings\n1 and 2, respectively; left and right columns correspond to the MAR and MNAR settings,\nrespectively.\nwhile all covariate values are observed.\nAssume that the missing data indicator Ri follows the Bernoulli distribution, Bernoulli(π(Yi, Xi)),\nwith\nlogit{π(Yi, Xi)} = γ0 + γ1X1,i + γ2X2,i + γ3X3,i + γ4Yi,\nwhere γj is the parameter for j = 0, 1, . . . , 4. Given the missing indicator Ri = 1 and\ncovariates Xi,1, Xi,2 = j, Xi3 = k for j = 1, 2, 3 and k = 1, 2, the response Yi fol-\nlows N(β0,j,k + β1,j,kXi,1 + β2,j,kX2\ni,1 + β3,j,kX3\ni,1 + β4,j,kX4\ni,1, σ2\nj,k), with parameters βj,k =\n(β0,j,k, β1,j,k, β2,j,k, β3,j,k, β4,j,k)⊤and σj,k. Following Morikawa and Kim (2021), for each\ncombination of j and k, we use the stepwise AIC to select the best model.\nWe analyze the data using the methods considered in Section 6.3 except the R method.\nLet ˆf ∗\nn(Xi) denote the predicted value for the observed covariates Xi with i = 1, . . . , n,\nobtained from each method. To visualize those estimates, for each estimate ˆf ∗\nn(·), we fix\n25\n\n\nthe second argument to be 1, 2, or 3, and the third argument to be 1 or 2, and then in\nFigure 7, we plot ˆf ∗\nn(·) against the first argument, denoted X1. The results yielded from the\nproposed methods are similar and tend to deviate from those produced by the N method\nat the left and right ends of the first argument’s range.\nAdditionally, in Figure 8, we\nplot boxplots for the predicted values\nn\nˆf ∗\nn(Xi) : i = 1, . . . , n\no\nobtained from all considered\nmethods.\nloss\nH\nL1\nL2\nmethods\nBJ\nIPW\nIPWN\nN\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\nx1\nvalue\n1\n2\n3\n4\n5\n2\n4\n6\nx1\nvalue\n2\n4\n6\n2\n4\n6\nx1\nvalue\n1.0\n1.5\n2.0\n2.5\n3.0\n1\n2\n3\nx1\nvalue\n1\n2\n3\n4\n0\n1\n2\n3\n4\n5\nx1\nvalue\n1\n2\n3\n4\n0\n1\n2\n3\n4\n5\nx1\nvalue\nFigure 7: Plots of the predicted values ˆf ∗\nn(Xi) versus its first argument Xi1 for i = 1, . . . , n,\nobtained from the proposed methods and the N method, with the second argument of ˆf ∗\nn(Xi)\nfixed as 1, 2, or 3 (indicated by the left to right columns, respectively) and with the third\nargument of ˆf ∗\nn(Xi) fixed as 1 or 2 (indicated by the top to bottom rows, respectively).\n2\n4\n6\nN\nIPW\nIPWN\nBJ\nvalue\nloss\nH\nL1\nL2\nFigure 8: Boxplots of\nn\nˆf ∗\nn(Xi) : i = 1, . . . , n\no\nobtained from the proposed methods and the\nN method.\n26\n\n\n8\nDiscussion\nIn this paper, we introduce two strategies – the Buckley-James (BJ)-type adjustment and\nthe IPW adjustment – for modifying loss functions to mitigate the impact of MNAR data\nand develop a boosting prediction procedure. The BJ-type adjustment requires determining\nthe conditional expectation of the loss function, which relies on knowledge of the conditional\ndistribution of Y given X and R = 0. In contrast, the IPW adjustment weights the loss\nfunction by π(y, x), derived from modeling the conditional distribution of R given Y and\nX. Neither method is universally superior; their effectiveness depends on the feasibility of\ntheir respective modeling assumptions. When model reliability is uncertain, applying both\nmethods to compare results can help assess sensitivity to assumptions.\nWhile our numerical studies focus on three loss functions: L1, L2, and the Huber loss,\nour theoretical results are applicable to other loss functions as well. The proposed strategies\ncan be extended to accommodate other machine learning techniques, such as support vector\nmachines, tree-based methods, and neural networks, which can be carried out by modifying\ntheir respective loss functions with the proposed schemes designed to handle MNAR data.\nThe validity of the proposed methods depends on certain conditions, as outlined in\nSection S.1 of the Supplementary Materials. These include identification conditions, the\nconsistent estimation of key components such as π(y, x) and/or f(y|X = x, R = 1), and\nstandard conditions related to the loss function and covariates. These assumptions are\nimportant for establishing the theoretical guarantees of our methods. While some assump-\ntions can be verified directly, many are challenging to validate in practice and may not\nalways hold. Consequently, the practical performance of the proposed methods depends on\nhow well these assumptions are satisfied in specific applications.\nOur primary objective is to identify an optimal function of covariates for predicting the\nresponse variable, rather than focusing on inference about the underlying model parameters.\nWe prioritize predictive performance within the MNAR framework and do not directly\n27\n\n\naddress hypothesis testing or parameter estimation uncertainty.\nSupplementary Materials\nText document: A .pdf file contains regularity conditions, proofs of Propositions 1 and\n2, Theorems 1 and 2, and additional simulation results.\nR-code: A .zip file, named “code.zip”, includes R scripts for the simulations and data\nanalyses presented in Sections 6 and 7.\nAcknowledgments\nThe authors thank the review team for their comments on the initial version of the\nmanuscript and Dr.\nMorikawa for providing the KLIPS data and the R code used in\nMorikawa and Kim (2021). Yi is a Canada Research Chair in Data Science (Tier 1). Her\nresearch is supported by the Natural Sciences and Engineering Research Council of Canada\n(NSERC) and the Canada Research Chairs Program.\nReferences\nBarnwal, A., Cho, H., and Hocking, T. (2022), “Survival regression with accelerated failure\ntime model in XGBoost,” Journal of Computational and Graphical Statistics, 31, 1292–\n1302.\nBoyd, S. P. and Vandenberghe, L. (2004), Convex Optimization, Cambridge: Cambridge\nUniversity Press.\nBreiman, L. (1998), “Arcing classifier,” The Annals of Statistics, 26, 801–849.\n— (1999), “Prediction games and arcing algorithms,” Neural Computation, 11, 1493–1517.\n28\n\n\nB¨uhlmann, P. and Hothorn, T. (2007), “Boosting algorithms: Regularization, prediction\nand model fitting,” Statistical Science, 22, 477–505.\nB¨uhlmann, P. and Yu, B. (2003), “Boosting with the L2 loss: Regression and classification,”\nJournal of the American Statistical Association, 98, 324–339.\nChen, L.-P. and Yi, G. Y. (2024), “Unbiased Boosting Estimation for Censored Survival\nData,” Statistica Sinica, 34, 439–458.\nde Boor, C. (2001), A Practical Guide to Splines, New York: Springer, Revised edition.\nFreund, Y. and Schapire, R. E. (1997), “A decision-theoretic generalization of on-line learn-\ning and an application to boosting,” Journal of Computer and System Sciences, 55,\n119–139.\nFriedman, J. H. (2001), “Greedy function approximation: A gradient boosting machine,”\nThe Annals of Statistics, 29, 1189–1232.\nFriedman, J. H., Hastie, T. J., and Tibshirani, R. (2000), “Additive logistic regression: A\nstatistical view of boosting,” The Annals of Statistics, 28, 337–407.\nHastie, T. J., Tibshirani, R., and Friedman, J. H. (2009), The Elements of Statistical\nLearning, New York: Springer, Second edition.\nHothorn, T., B¨uhlmann, P., Dudoit, S., Molinaro, A., and van der Laan, M. J. (2006),\n“Survival ensembles,” Biostatistics, 7, 355–373.\nKenward, M. G. (1998), “Selection models for repeated measurements with non-random\ndropout: An illustration of sensitivity,” Statistics in Medicine, 17, 2723–2732.\nKim,\nJ. K. (2011),\n“Parametric fractional imputation for missing data analysis,”\nBiometrika, 98, 119–132.\n29\n\n\nKim, J. K. and Yu, C. L. (2011), “A semiparametric estimation of mean functionals with\nnonignorable missing data,” Journal of the American Statistical Association, 106, 157–\n165.\nMiao, W. and Tchetgen Tchetgen, E. J. (2016), “On varieties of doubly robust estimators\nunder missingness not at random with a shadow variable,” Biometrika, 103, 475–482.\nMorikawa, K. and Kim, J. K. (2021), “Semiparametric optimal estimation with nonignor-\nable nonresponse data,” The Annals of Statistics, 49, 2991–3014.\nMorikawa, K., Kim, J. K., and Kano, Y. (2017), “Semiparametric maximum likelihood\nestimation with data missing not at random,” The Canadian Journal of Statistics, 45,\n393–409.\nRobins, J. M. and Ritov, Y. (1997), “Toward a curse of dimensionality appropriate (CODA)\nasymptotic theory for semi-parametric models,” Statistics in Medicine, 16, 285–319.\nSchapire, R. E. and Freund, Y. (2012), Boosting: Foundations and Algorithms, Mas-\nsachusetts: MIT Press.\nShao, J. and Wang, L. (2016), “Semiparametric inverse propensity weighting for nonignor-\nable missing data,” Biometrika, 103, 175–187.\nSun, B., Liu, L., Miao, W., Wirth, K., Robins, J. M., and Tchetgen Tchetgen, E. J. (2018),\n“Semiparametric estimation with data missing not at random using an instrumental\nvariable,” Statistica Sinica, 28, 1965–1983.\nWang, S., Shao, J., and Kim, J. K. (2014), “An instrumental variable approach for identifi-\ncation and estimation with nonignorable nonresponse,” Statistica Sinica, 24, 1097–1116.\nYi, G. Y. and Cook, R. J. (2002), “Marginal methods for incomplete longitudinal data\narising in clusters,” Journal of the American Statistical Association, 97, 1071–1080.\n30\n\n\nSupplementary Materials of\n“Boosting Prediction with Data\nMissing Not at Random”\nYuan Bian\nDepartment of Statistical and Actuarial Sciences\nUniversity of Western Ontario\nand\nGrace Y. Yi\nDepartment of Statistical and Actuarial Sciences\nDepartment of Computer Science\nUniversity of Western Ontario\nand\nWenqing He∗\nDepartment of Statistical and Actuarial Sciences\nUniversity of Western Ontario\nMarch 3, 2025\nThe supplementary materials include regularity conditions and derivations for the the-\noretical results, together with additional simulation results.\nWrite θ =\n\u0000γ⊤, β⊤\u0001⊤. For x ∈X, y ∈Y, and r ∈{0, 1}, define\nS(y, x; β) ≜∂log f(y|X = x, R = 1; β)\n∂β\nand\nU(y, x, r; θ) ≜\n\u001a\n1 −\nr\nπ(y, x; γ)\n\u001b\nˆE∗\np{S0(Y, X; γ)|X = x; β}.\nLet B and Γ denote the parameter space of β and γ, respectively. Let γ0 denote the true\nvalue of γ, and let β0 denote the true value of β.\n∗corresponding author: whe@stats.uwo.ca\n1\narXiv:2502.21276v1  [stat.ME]  28 Feb 2025\n\n\nFor a vector a = (a1, . . . , ad)⊤, let ∥a∥=\nqPd\nj=1 a2\nj denote the L2-norm of a. For vectors\na = (a1, . . . , ad)⊤and b = (b1, . . . , bd)⊤, let a ⪯b represent aj ≤bj for any j = 1, . . . , d.\nFor an s × t matrix A = [ajk] with the (j, k) element, let ∥A∥F =\nqPs\nj=1\nPt\nk=1 a2\njk denote\nthe Frobenius norm.\nS.1\nRegularity Conditions\nIdentification Conditions:\n(A1) π(y, x) is assumed to be parametrically modeled by π(y, x; γ) for γ ∈Γ.\n(A2) E{O(Y, X; γ)|X, R = 1} exists and is bounded almost surely.\n(A3) Pr (infγ∈Γ ∥E∗{S0(Y, X; γ)|X}∥> 0) > 0, where E∗{S0(Y, X; γ)|X} is defined in (10).\nFor any γ ∈Γ, elements of the vector E∗{S0(Y, X; γ)|X = x} are linearly indepen-\ndent, i.e., any element in this vector cannot be expressed as a linear combination of\nother elements in the vector.\n(A4) E{O(Y, X|γ)|X = x, R = 1} = E{O(Y, X|˜γ)|X = x, R = 1} almost surely implies\nγ = ˜γ.\nCondition (A1) says that we handle the missing data process parametrically, as de-\ntailed in Section 4.1. Condition (A2) implies that the logistic model is a valid candidate\nfor π(y, x; γ), though the probit model does not satisfy condition (A2) (?).\nCondition\n(A3) ensures that E∗{S0(Y, X; γ)|X = x} is a nonzero vector for any given x. Condition\n(A4) requires that E{O(Y, X|γ)|X, R = 1} can be distinguished for different values of the\nparameter γ. Conditions (A2) - (A4) ensure the identifiability of γ in condition (A1), as\ndiscussed by ?.\nRegularity Conditions:\nFor the boosting estimation, we assume the following regularity conditions:\n2\n\n\n(B1) The covariates X are bounded pointwisely.\nThat is, there exist finite vectors of\nconstants, xl and xu, such that xl ⪯X ⪯xu.\n(B2) The Lipschitz condition holds for the loss functions L(u, v) in the second argument\nv in a uniform manner with respect to the first argument u. That is, there exists a\nconstant ζ1 > 0 such that for v1, v2 ∈Y,\nsup\nu∈Y\n|L(u, v1) −L(u, v2)| ≤ζ1|v1 −v2|,\nwhere Y is defined in Section 2.1.\n(B3) L(u, v) is a convex and twice differentiable function with respect to the second ar-\ngument v (almost everywhere), and there exists a constant C > 0 such that for any\nv ∈Y,\n∂2L (u, v)\n∂v2\n≤C holds uniformly for u.\n(B4) π(y, x) and its estimate ˆπ(y, x) satisfy the following conditions:\n(i) both π(y, x) and ˆπ(y, x) are bounded away from zero. That is, there exists a\nconstant 0 < δ < 1 such that π(y, x) > δ and ˆπ(y, x) > δ for any y and x;\n(ii) ˆπ(y, x) is continuous in both y and x;\n(iii) ˆπ(y, x) consistently estimate π(y, x) in the sense that\nsup\n(y,x)\n|ˆπ(y, x) −π(y, x)| →0 as n →∞.\n(B5) For given x, ˆf(y|X = x, R = 1) is continuous in y, and with probability approaching\none,\nsup\ny∈Y\n\f\f\f ˆf(y|X = x, R = 1) −f(y|X = x, R = 1)\n\f\f\f →0 as n →∞.\n(B6)\nn\n∂2\n∂v2\nR\nL(y, v)f(y|X = x, R = 0)dy\no\f\f\f\nv=f(x) =\nZ \u001a\n∂2L(y,v)\n∂v2\n\f\f\f\nv=f(x)\n\u001b\nf(y|X = x, R =\n0)dy and\n3\n\n\nn\n∂2\n∂v2\nR\nL(y, v) ˆf(y|X = x, R = 0)dy\no\f\f\f\nv=f(x) =\nZ \u001a\n∂2L(y,v)\n∂v2\n\f\f\f\nv=f(x)\n\u001b\nˆf(y|X = x, R =\n0)dy, with\nˆf(y|X = x, R = 0) ∝ˆf(y|X = x, R = 1)1−ˆπ(y,x)\nˆπ(y,x) .\n(B7) ˆL∗(Y, f(X), R)\np\n−−→L∗(Y, f(X), R)\nas n →∞.\n(B8) Let ∂ˆR(f) denote the vector with the jth entry being\n∂{n−1 Pn\ni=1 ˆL∗(Yi,v,Ri)}\n∂v\n\f\f\f\f\nv=f(Xj)\n,\nand let f ∗and f (m) be “parametrized” as {f ∗(X1), . . . , f ∗(Xn)} and\n\b\nf (m)(X1), . . . , f (m)(Xn)\n\t\n,\nrespectively. Assume that\ninf\nx1,...,xn\n\n\ninf\nm∈N\n\r\r\r∂ˆR\n\u0000f (m)\u0001\r\r\r\n∥f ∗−f (m)∥\n\f\f\f\f\f\f\nX1=x1,...,Xn=xn\n\n\n> 0.\n(B9) Real-valued functions in F are continuous, and there exists a constant M0 such that\nsupf∈F ∥f∥∞≤M0. Moreover, for any function f ∈F, there exists ˜ϵ > 0 such that\nfor any a with |a| < ˜ϵ,\nmin\n1≤i≤n E\nn\nˆL∗(Yi, f(Xi) + a, Ri) −ˆL∗(Yi, f(Xi), Ri)\no\n≥˜ϵa2.\n(S.1)\nConditions (B1) - (B9) are made in ? to derive the convergence and consistency of the\nboosting estimation procedure. Conditions similar to (B2) and (B3) were also employed\nby ?. Condition (B2) implies that for i = 1, · · · , n,\nsup\nyi∈Y\n|L(yi, f1(xi)) −L(yi, f2(xi))| ≤ζ1|f1(xi) −f2(xi)|,\n(S.2)\nfor any f1, f2 ∈F and any xi ∈X. Moreover, with condition (B4), the Lipschitz condi-\ntion in condition (B2) also holds for the proposed adjusted loss functions, as justified in\nSection S.3; and the boundness in condition (B3) yields the boundness of the second order\nderivatives of the adjusted loss function, in combination of conditions (B4) and (B6), as\nillustrated in Section S.3. Condition (B3), together with Theorem 10.1 of ?, implies that\nL(·, ·) is continuous. Condition (B4), assumed by ? and others, ensures that the IPW ad-\njusted loss function (4) and BJ adjusted loss function (5) are well defined. Condition (B7)\n4\n\n\nensures that the estimated version of the adjusted loss function converges in probability\nto the adjusted loss function as n →∞. This condition may be met with the applica-\ntion of conditions (B3) - (B5) to practically used loss functions and their estimates. The\nparametrizations in condition (B8) were also used by ?, Section 10.10. The requirement of\nthe existence of M0 and (S.1) in condition (B9) are similar to those of ?.\nFor the consistency of the estimators in Section 4.1, we require the following regularity\nconditions:\n(C1) The parameter space Γ for γ is a compact set in a Euclidean space.\n(C2) Random variables {{Yi, Xi, Ri} : i = 1, . . . , n} are independent and identically dis-\ntributed.\n(C3) The parameter space for β, denoted B, is a compact set of a Euclidean space.\n(C4) We have the following assumptions about S(y, x; β):\n(i) each element of S(Y, X; β) is continuously differentiable at each β ∈B with\nprobability one;\n(ii) there exists a function M1(y, x, r), with E{M1(Y, X, R)} < ∞, such that for any\nβ ∈B, ∥S(y, x; β)∥≤M1(y, x, 1);\n(iii) E[S(Y, X; β)] = 0 has a unique solution, denoted β∗∈B;\n(iv) each element of ∂S(Y,X;β)\n∂β⊤\nis continuous at β∗with probability one;\n(v) there is a neighborhood of β∗, say N1 (β∗), such that\n\r\r\r\r\rE\n(\nsup\nβ∈N1(β∗)\n∂S(Y, X; β)\n∂β⊤\n)\r\r\r\r\r\nF\n< ∞.\n(C5) Assume the following conditions for U(Y, X, R; θ):\n(i) ∂U(Y,X,R;θ)\n∂θ⊤\nis continuous at\n\u0000γ⊤\n0 , β∗⊤\u0001⊤with probability one;\n(ii) E\nn\n∂U(Y,X,R;θ)\n∂θ⊤\no\nis nonsingular at\n\u0000γ⊤\n0 , β∗⊤\u0001⊤;\n5\n\n\n(iii) there is a neighborhood of\n\u0000γ⊤\n0 , β∗⊤\u0001⊤, say N2 (γ0, β∗), such that\n\r\r\r\r\rE\n(\nsup\nθ∈N2(γ0,β∗)\n∂U(Y, X, R; θ)\n∂θ⊤\n)\r\r\r\r\r\nF\n< ∞.\n(C6) Each element of U(Y, X, R; θ) is continuously differentiable at each θ ∈Γ × B with\nprobability one, and there exists a function M2(y, x, r), with E{M2(Y, X, R)} < ∞,\nsuch that\n∥U (y, x, r; θ) ∥≤M2(y, x, r).\n(C7) The conditions (C5) and (C6) also hold when replacing β∗with β0.\n(C8) For any x ∈X, f(x|R = 1) > 0 and E{π(Y, X; γ0)|X = x, R = 1} > 0, where γ0\ndenotes the true value of γ.\n(C9) The kernel function K(·) has bounded derivatives of order d, satisfies\nR\nK(v)dv = 1,\nand has zero moments of order up to m −1 and a nonzero mth order moment.\n(C10) For any y ∈Y, π(y, x; γ0) and\n∂π(y,x;γ)\n∂γ\n\f\f\f\nγ=γ0 are differentiable up to order d and are\nbounded on an open set containing X, where d is a positive integer.\n(C11) Let a1(Y, X) = 1, and a2(Y, X) = S0 (Y, X; γ0). Then for any X ∈X and k = 1, 2,\nthere exists v ≥4 such that E {|π−1(Y, X; γ0)O(Y, X; γ0)ak(Y, X))|v| R = 1} and\nE{|π−1(Y, X; γ0)O(Y, X; γ0)ak(Y, X)|v|X = x, R = 1}f(x|R = 1) are bounded.\n(C12) The bandwidth hn satisfies that as n →∞,\nhn →0, n1−(2/v)hp\nn/ log(n) →∞, √nhp+2d\nn\n/ log(n) →∞and √nh2m\nn\n→0,\nwhere p is the dimension of Xi, m is determined in condition (C9), d is given in\ncondition (C10), and v is identified in condition (C11).\nConditions (C1) - (C6) are assumed to guarantee that ˆγp is a consistent estimator for\nγ0 when a parametric working model f(y|X = x, R = 1; β) is assumed for the observed\n6\n\n\nresponse process; yet the consistency of ˆγp does not require f(y|X = x, R = 1; β) to be\ncorrectly specified, as noted by ?. Conditions (C1) - (C3) and (C7) - (C12) are commonly\nmade to ensure the consistency of ˆγnp for γ0 when employing the kernel methods for the\nobserved response process, which are also required by ?.\nS.2\nProof of Proposition 1\nProof.\n(a)\nE {LIPW(Yi, f(Xi), Ri)} = E\n\u001aRiL(Yi, f(Xi))\nπ(Yi, Xi)\n\u001b\n= E\n\u0014\nE\n\u001a RiL(Yi, f(Xi))\nπ(Yi, Xi)\n\f\f\f\f Yi, Xi\n\u001b\u0015\n= E\n\u001aL(Yi, f(Xi))\nπ(Yi, Xi)\nE (Ri| Yi, Xi)\n\u001b\n= E{L(Yi, f(Xi))},\nwhere the first step uses (4), the second and third steps come from the properties of the\nconditional expectation, and the last step is due to the definition of π(Yi, Xi).\n(b)\nE{LBJ(Yi, f(Xi), Ri)} = E{RiL(Yi, f(Xi)) + (1 −Ri)Ψ(Xi, Ri = 0)}\n= E{L(Yi, f(Xi))|Ri = 1} Pr(Ri = 1)\n+ E [E{L(Yi, f(Xi))|Xi, Ri = 0}|Ri = 0] Pr(Ri = 0)\n= E{L(Yi, f(Xi))|Ri = 1} Pr(Ri = 1)\n+ E{L(Yi, f(Xi))|Ri = 0} Pr(Ri = 0)\n= E{L(Yi, f(Xi))},\nwhere the first step uses (5), the second and last steps are due to the law of total expectation,\nand the third step comes from the property that\nE{E(U|V, W)|W} = E(U|W)\n(S.3)\n7\n\n\nfor any random variables U, V , and W.\nS.3\nJustification of Conditions (B2) and (B3) for the\nAdjusted Loss Functions\nConditions (B2) and (B3) assume that the Lipschitz condition, convexity, and differentia-\nbility for the original loss function L(·, ·) defined in (1), here we justify that conditions\n(B2) and (B3) also apply to the IPW loss function (4) and BJ loss function (5), defined in\nSection 3.2.\nS.3.1\nJustification of Condition (B2) for the Adjusted Loss Func-\ntions\nFor f1, f2 ∈F, xi ∈X, and ri = 0, 1,\nsup\nyi∈Y\n|LIPW(yi, f1(xi), ri) −LIPW(yi, f2(xi), ri)|\n= sup\nyi∈Y\n\f\f\f\f\nriL(yi, f1(xi))\nπ(yi, xi)\n−riL(yi, f2(xi))\nπ(yi, xi)\n\f\f\f\f\n= sup\nyi∈Y\n\u001a\nri\nπ(yi, xi) |L(yi, f1(xi)) −L(yi, f2(xi))|\n\u001b\n,\nwhich equals 0 if ri = 0, where the first step comes from (4), and the second step is due to\ncondition (B4) and ri ≥0. Therefore, by (S.2),\nsup\nyi∈Y\n|LIPW(yi, f1(xi), 1) −LIPW(yi, f2(xi), 1)|\n≤\n \nsup\n(yi,xi)\n1\nπ(yi, xi)\n!\nsup\nyi∈Y\n|L(yi, f1(xi)) −L(yi, f2(xi))|\n≤ζIPW|f1(xi) −f2(xi)|\nwith ζIPW = ζ1\nδ , where sup(yi,xi)\n1\nπ(yi,xi) ≤1\nδ by condition (B4). Therefore, for ri = 0, 1,\nsup\nyi∈Y\n|LIPW(yi, f1(xi), ri) −LIPW(yi, f2(xi), ri)| ≤ζIPW|f1(xi) −f2(xi)|.\n(S.4)\n8\n\n\nFor the BJ loss function, we have that for f1, f2 ∈F, and any xi ∈X and ri = 0, 1,\nsup\nyi∈Y\n|LBJ(yi, f1(xi), ri) −LBJ(yi, f2(xi), ri)|\n≤sup\nyi∈Y\n{ri |L(yi, f1(xi)) −L(yi, f2(xi))|}\n+ (1 −ri)\n\f\f\f\f\nZ\nL(y, f1(xi))f(y|X = xi, R = 0)dy −\nZ\nL(y, f2(xi))f(y|X = xi, R = 0)dy\n\f\f\f\f\n≤riζ1|f1(xi) −f2(xi)| + (1 −ri)\nZ\n|L(y, f1(xi)) −L(y, f2(xi))| f(y|X = xi, R = 0)dy\n≤riζ1|f1(xi) −f2(xi)| + (1 −ri)\nZ\nζ1|f1(xi) −f2(xi)|f(y|X = xi, R = 0)dy\n= riζ1|f1(xi) −f2(xi)| + (1 −ri)ζ1|f1(xi) −f2(xi)|\n= ζBJ|f1(xi) −f2(xi)|,\n(S.5)\nwhere the first step is due to (5) and the triangle inequality, the second step comes from\n(S.2) and the inequality of exchanging the absolute value and the integral, the third step\nis due to (S.2), and the last step uses ζBJ ≜ζ1.\nS.3.2\nJustification of Condition (B3) for the Adjusted Loss Func-\ntions\nThe second-order derivatives of the IPW and BJ adjusted loss functions in (4) and (5) are\nrespectively given by\n∂2LIPW (u, v, w)\n∂v2\n\f\f\f\f\nv=f(xi)\n=\n\u001a\nw\nπ(u, xi)\n\u001b (\n∂2L (u, v)\n∂v2\n\f\f\f\f\nv=f(xi)\n)\n,\nand\n∂2LBJ (u, v, w)\n∂v2\n\f\f\f\f\nv=f(xi)\n= w ∂2L (u, v)\n∂v2\n\f\f\f\f\nv=f(xi)\n+ (1 −w)\nZ (\n∂2L(y, v)\n∂v2\n\f\f\f\f\nv=f(xi)\n)\nf(y|X = xi, R = 0)dy,\nwhere interchangeability between the operations of differentiation and integration is as-\nsumed in condition (B6).\n9\n\n\nSimilar to the derivations about ζIPW and ζBJ from ζ1 respectively in (S.4) and (S.5),\nwe can derive a constant C1 > 0 from C, and show that for any f ∈F and any xi ∈X,\n∂2L∗(u, v, w)\n∂v2\n\f\f\f\f\nv=f(xi)\n≤C1\n(S.6)\nuniformly in u and w, where L∗(u, v, w) represents the IPW loss function (4) or the BJ\nloss function (5).\nS.3.3\nJustification of Conditions (B2) and (B3) for Estimated\nVersions of Adjusted Loss Functions\nAs defined in Section 4.2, ˆL∗(u, v, w) is an estimated version of L∗(u, v, w) with f(u|X =\nx, R = 1) and π(u, x) replaced by their estimates satisfying conditions (B4) - (B6).\nSimilar to the derivations about ζIPW and ζBJ from using ζ1 respectively in (S.4) and\n(S.5), and about C1 from using C in (S.6), we can find positive constants ζ∗and C∗from\nusing ζ1 and C respectively, such that\nsup\nyi∈Y\n\f\f\fˆL∗(yi, f1(xi), ri) −ˆL∗(yi, f2(xi), ri)\n\f\f\f ≤ζ∗|f1(xi) −f2(xi)|\nand for any f ∈F and any xi ∈X,\n∂2 ˆL∗(u, v, w)\n∂v2\n\f\f\f\f\f\nv=f(xi)\n≤C∗\n(S.7)\nuniformly in u and w.\nS.4\nProofs of Proposition 2 and Theorems 1 - 2\nProof of Proposition 2. For f ∈F and given {Xi, Ri = 0} and {Xi, Yi, Ri = 1} with\ni = 1, · · · , n, let ˆR(f) = n−1 Pn\ni=1 ˆL∗(Yi, f(Xi), Ri) with ˆL∗(·, ·, ·) defined in Section 4.2.\nBy condition (B3) and the definition of ˆL(·, ·, ·) and ˆR(·), for any f ∈F, ˆR (f) is a convex\nfunction of f.\n10\n\n\nBy proposition 1, condition (B7), and the law of large numbers, we have that for any\nf ∈F,\nˆR(f)\np\n−−→R(f)\nas n →∞.\n(S.8)\nFor f ∈F, in contrast to that ∂ˆR(f) denoting the vector with the jth entry being\n∂{n−1 Pn\ni=1 ˆL∗(Yi,v,Ri)}\n∂v\n\f\f\f\f\nv=f(Xj)\ndefined on condition (B8), let ∂2 ˆR(f) denote the diagonal ma-\ntrix with the (j, j) entry being\n∂2{n−1 Pn\ni=1 ˆL∗(Yi,v,Ri)}\n∂v2\n\f\f\f\f\nv=f(Xj)\n. Then, (S.7) implies that for\nj = 1, . . . , n,\n∂2 n\nn−1 Pn\ni=1 ˆL∗(Yi, v, Ri)\no\n∂v2\n\f\f\f\f\f\f\nv=f(Xj)\n≤C∗.\n(S.9)\nConsider f (m+1) = f (m) +α(m+1)h(m+1) for any nonnegative integer m, where h(m+1) ∈C\nand α(m+1) is a constant. As in ?, Section 10.10 and ?, we “parametrize” f ∗, f (m+1), f (m),\nand h(m+1) as {f ∗(X1), . . . , f ∗(Xn)},\n\b\nf (m+1)(X1), . . . , f (m+1)(Xn)\n\t\n,\n\b\nf (m)(X1), . . . , f (m)(Xn)\n\t\n,\n\b\nh(m+1)(X1), . . . , h(m+1)(Xn)\n\t\n, respectively. By the definition of C in Section 2.2, there ex-\nists a positive constant C2 such that\n\r\rh(m+1)\r\r ≤C2.\n(S.10)\nApplying the second order Taylor series expansion to ˆR\n\u0000f (m+1)\u0001\naround f (m) and by (S.9),\nwe have that\nˆR\n\u0000f (m+1)\u0001\n≤ˆR\n\u0000f (m)\u0001\n+ α(m+1) n\n∂ˆR\n\u0000f (m)\u0001o⊤\nh(m+1) + 1\n2\n\u0000α(m+1)\u00012 \r\rh(m+1)\r\r2 C∗\n≤ˆR\n\u0000f (m)\u0001\n+ α(m+1)C2\n\r\r\r∂ˆR\n\u0000f (m)\u0001\r\r\r + 1\n2\n\u0000α(m+1)\u00012 C2\n2C∗\n= ˆR\n\u0000f (m)\u0001\n−\n1\n2C∗\n\r\r\r∂ˆR\n\u0000f (m)\u0001\r\r\r\n2\n,\n(S.11)\nwhere the second step uses the Cauchy-Schwarz inequality and (S.10), and the third step\nis due to the specification of α(m+1) as −\n1\nC2C∗\n\r\r\r∂ˆR\n\u0000f (m)\u0001\r\r\r.\nLet\n1\nc∗= min\n\u0012\nC∗\n2 , infm∈N ∥∂ˆR(f(m))∥\n2∥f∗−f(m)∥\n\u0013\n, which is a positive constant by condition (B8).\nThen by the first-order condition (?, p.70) for the convex function, we obtain that for any\n11\n\n\nm,\nˆR (f ∗) ≥ˆR\n\u0000f (m)\u0001\n+\nn\n∂ˆR\n\u0000f (m)\u0001o⊤\u0000f ∗−f (m)\u0001\n≥ˆR\n\u0000f (m)\u0001\n−\n\r\r\r∂ˆR\n\u0000f (m)\u0001\r\r\r ·\n\r\rf ∗−f (m)\r\r\n≥ˆR\n\u0000f (m)\u0001\n−c∗\n2\n\r\r\r∂ˆR\n\u0000f (m)\u0001\r\r\r\n2\n,\n(S.12)\nwhere the second step is due to the Cauchy-Schwarz inequality. Then (S.12) implies that\n\r\r\r∂ˆR\n\u0000f (m)\u0001\r\r\r\n2\n≥2\nc∗\nn\nˆR\n\u0000f (m)\u0001\n−ˆR (f ∗)\no\n.\n(S.13)\nCombining (S.11) and (S.13) gives\nˆR\n\u0000f (m+1)\u0001\n≤ˆR\n\u0000f (m)\u0001\n−\n1\nC∗c∗\nn\nˆR\n\u0000f (m)\u0001\n−ˆR (f ∗)\no\n.\n(S.14)\nSubtracting ˆR(f ∗) on both sides of (S.14), we obtain that\nˆR\n\u0000f (m+1)\u0001\n−ˆR(f ∗) ≤ˆR\n\u0000f (m)\u0001\n−ˆR(f ∗) −\n1\nC∗c∗\nn\nˆR\n\u0000f (m)\u0001\n−ˆR (f ∗)\no\n=\n\u001a\n1 −\n1\nC∗c∗\n\u001b n\nˆR\n\u0000f (m)\u0001\n−ˆR (f ∗)\no\n.\n(S.15)\nTherefore, applying (S.8) to (S.15) gives that\nR\n\u0000f (m+1)\u0001\n−R(f ∗) ≤\n\u0012\n1 −\n1\nC∗c∗\n\u0013 \b\nR\n\u0000f (m)\u0001\n−R (f ∗)\n\t\n,\n(S.16)\nand then applying (S.16) recursively yields (18).\nProof of Theorem 1. By (2), we have that R (f ∗) = minf∈F R(f), and therefore,\nR\n\u0000f (m+1)\u0001\n−R (f ∗) ≥0.\n(S.17)\nCombining (18) and (S.17) gives that\n0 ≤R\n\u0000f (m+1)\u0001\n−R (f ∗) ≤\n\u0012\n1 −\n1\nC∗c∗\n\u0013m \b\nR\n\u0000f (0)\u0001\n−R(f ∗)\n\t\n.\nBy the definition of c∗, we have that 0 <\n\u00001 −\n1\nC∗c∗\n\u0001\n< 1. Applying the squeeze theorem\nto it, we obtain that\nlim\nm→∞R\n\u0000f (m+1)\u0001\n= R (f ∗) .\n12\n\n\nProof of Theorem 2. Let P and P denote the empirical and probability measures of X, Y ,\nand R, and write\nPˆL∗≜n−1\nn\nX\ni=1\nˆL∗(Yi, f(Xi), Ri)\nand\nP ˆL∗≜E\nn\nˆL∗(Yi, f(Xi), Ri)\no\n.\n(S.18)\nBy Lemma S3.2 of ? with condition (B9), there exists a constant κ > 0 which may\ndepend on ˜ϵ, defined in condition (B9), such that\nκ\n\r\r\r ˆf AL\nn\n−f ∗\r\r\r\n∞≤−\nZ n\nˆL∗(Yi, ˆf AL\nn (Xi), Ri) −ˆL∗(Yi, f ∗(Xi), Ri)\no\nd{P −P},\n(S.19)\nwhere by definitions of ∥·∥∞and κ,\nκ\n\r\r\r ˆf AL\nn\n−f ∗\r\r\r\n∞≥0.\n(S.20)\nNow we examine the right-hand-side of (S.19):\n−\nZ n\nˆL∗\u0010\nYi, ˆf AL\nn (Xi), Ri\n\u0011\n−ˆL∗(Yi, f ∗(Xi), Ri)\no\nd{P −P}\n= −\nZ\nˆL∗\u0010\nYi, ˆf AL\nn (Xi), Ri\n\u0011\ndP +\nZ\nˆL∗(Yi, f ∗(Xi), Ri)dP\n+\nZ\nˆL∗\u0010\nYi, ˆf AL\nn (Xi), Ri\n\u0011\ndP −\nZ\nˆL∗(Yi, f ∗(Xi), Ri)dP\n= −1\nn\nn\nX\ni=1\nˆL∗\u0010\nYi, ˆf AL\nn (Xi), Ri\n\u0011\n+ 1\nn\nn\nX\ni=1\nˆL∗(Yi, f ∗(Xi), Ri)\n+ E\nn\nˆL∗\u0010\nYi, ˆf AL\nn (Xi), Ri\n\u0011o\n−E\nn\nˆL∗(Yi, f ∗(Xi), Ri)\no\n,\n(S.21)\nwhere the second step is due to (S.18).\nUsing the definition of ˆR(·), and Proposition 2 with conditions (B4) and (B5), as n →∞,\n(S.21) can be rewritten as\n−ˆR\n\u0010\nˆf AL\nn\n\u0011\n+ ˆR (f ∗) + R\n\u0010\nˆf AL\nn\n\u0011\n−R (f ∗)\n=\nn\nˆR(f ∗) −R(f ∗)\no\n+\nn\nR\n\u0010\nˆf AL\nn\n\u0011\n−ˆR\n\u0010\nˆf AL\nn\n\u0011o\n.\n(S.22)\nCombining (S.19) with (S.8), (S.20), (S.21), and (S.22), we obtain that as n →∞,\n0 ≤κ\n\r\r\r ˆf AL\nn\n−f ∗\r\r\r\n∞= op(1),\n13\n\n\nshowing that for any ϵ > 0,\nP\n\u0010\r\r\r ˆf AL\nn\n−f ∗\r\r\r\n∞> ϵ\n\u0011\n−→0\nas n →∞.\nS.5\nDerivations of the Distribution Forms for Simula-\ntion Studies in Section 6\nFor Setting j with j = 1, 2, conditional on Xi and Ri = 1, consider the conditional pdf of\nYi, fj(y|Xi, Ri = 1), given by\nN(µj(Xi), σ2),\n(S.23)\nand specify propensity score πj(Yi, Xi) as\nexp{νj(Xi) + γyYi}\n1 + exp{νj(Xi) + γyYi},\n(S.24)\nwhere µj(Xi) and νj(Xi) are defined in Section 6.1.\nIn Sections S.5.1 and S.5.2, we show that the models of generating data in Section 6.1\ncan be derived from (S.23) and (S.24).\nS.5.1\nDerivations of fj(y|Xi, Ri) in (23) using (S.23) and (S.24)\nBy (S.24), we obtain that O(Yi, Xi) in (6) can be simplified as:\nO(Yi, Xi) = exp {−νj(Xi) −γyYi} .\n(S.25)\nBy the model assumption for (S.23), we have that\nfj(y|Xi, Ri = 1) =\n1\nσ\n√\n2π exp\n\"\n−1\n2\n\u001ay −µj(Xi)\nσ\n\u001b2#\n.\n(S.26)\nTherefore, applying (S.25) and (S.26) to (6), we obtain that\nfj(y|Xi, Ri = 0) ∝exp\n\"\n−1\n2\n\u001ay −µj(Xi)\nσ\n\u001b2#\nexp(−γyy),\n14\n\n\nsuggesting that conditional on Xi and Ri = 0, Yi follows\nN\n\u0000µj(Xi) −γyσ2, σ2\u0001\n.\n(S.27)\nTherefore, combining this with (S.26) shows that Yi, given Xi and Ri, follows N(µj(Xi) −\n(1 −Ri)γyσ2, σ2). That is, (23) holds.\nS.5.2\nDerivations of ˜πj(Xi) in (20), as well as fj(y|Xi) in (24),\nusing (S.23) and (S.24)\nUsing Bayes’ rule, we have the following relationship:\nfj(y|Xi) =\nfj(y|Xi, Ri = 1)/πj(y, Xi)\nR\nfj(y|Xi, Ri = 1)/πj(y, Xi)dy.\n(S.28)\nBy (S.24) and (S.26), the numerator of (S.28) is\nfj(y|Xi, Ri = 1)/πj(y, Xi) = fj(y|Xi, Ri = 1) [1 + exp {−νj(Xi) −γyy)}]\nand therefore, the denominator of (S.28) can be written as\nZ\nfj(y|Xi, Ri = 1)/πj(y, Xi)dy\n=\nZ\nfj(y|Xi, Ri = 1) [1 + exp {−νj(Xi) −γyy)}] dy\n=\nZ\nfj(y|Xi, Ri = 1)dy + exp {−νj(Xi)}\nZ\nfj(y|Xi, Ri = 1) exp(−γyy)dy\n= 1 + exp {−νj(Xi)} exp\n\u001a\u00121\n2γ2\nyσ2 −γyµj(Xi)\n\u0013\u001b\n= 1 + exp\n\u001a\u00121\n2γ2\nyσ2 −νj(Xi) −γyµj(Xi))\n\u0013\u001b\n= 1 + exp{gj(Xi)},\nwhere the third step results from the definition of the probability density function, the\nmodel form (S.23), and the moment generating function of the corresponding Normal dis-\ntribution; and the last step comes from (20). Therefore, (S.28) becomes\nfj(y|Xi) = [1 + exp{−νj(Xi) −γyµj(Xi)}] × fj(y|Xi, Ri = 1)\n1 + exp{gj(Xi)}.\n(S.29)\n15\n\n\nNext, we show that ˜πj(Xi) assumes the forms (20) as follows:\n˜πj(Xi) =\nZ\nPr(Ri = 1, y|Xi)dy\n=\nZ\nπj(y, Xi)fj(y|Xi)dy\n=\n1\n1 + exp{g(Xi)}\nZ\nπj(y, Xi)fj(y|Xi, Ri = 1) [1 + exp {−νj(Xi) −γyy)}] dy\n=\nR\nfj(y|Xi, Ri = 1)dy\n1 + exp{gj(Xi)}\n=\n1\n1 + exp{gj(Xi)},\n(S.30)\nwhere the second step is due to the Bayes’ rule; the third and fourth steps use (S.29) and\n(S.24), respectively; and the fifth step results from the definition of the probability density\nfunction.\n(S.29) together with (S.27) and (S.30) implies that\nfj(y|Xi) = ˜πj(Xi)fj(y|Xi, Ri = 1) + {1 −˜πj(Xi)}fj(y|Xi, Ri = 0).\nS.5.3\nDerivations of the formula for β1,0 in (21)\nBy that Xi = (X1,i, X2,i)⊤, with X1,i ∼N (0, σ2\nX) and X2,i|X1,i ∼N (ζX1,i, σ2\nX), we obtain\nthat\nE(X1,i) = 0,\n(S.31)\nE(X2,i) = E{E(X2,i|X1,i)} = E(ζX1,i) = 0,\n(S.32)\nand\nE(X1,iX2,i) = E{E(X1,iX2,i|X1,i)}\n= E{X1,iE(X2,i|X1,i)}\n= E(ζX2\n1,i)\n= ζVar(X1,i)\n= ζσ2\nX.\n(S.33)\n16\n\n\nTherefore, we obtain that\nE(Yi) = E{E(Yi|Xi, Ri)}\n= E\n\b\nβ1,0 + β1,1X1,i + β1,2X2,i + β1,3X1,iX2,i −(1 −Ri)γyσ2\t\n= β1,0 + β1,1E(X1,i) + β1,2E(X2,i) + β1,3E(X1,iX2,i) −{1 −E(Ri)}γyσ2\n= β1,0 + α1β1,3σ2\nX −γyσ2{1 −Pr(Ri = 1)},\n(S.34)\nwhere the second step is due to (23), and the last step uses (S.31), (S.32), and (S.33). By\n(S.34), setting E(Yi) = 0 implies that\nβ1,0 = γyσ2{1 −Pr(Ri = 1)} −ζβ1,3σ2\nX.\nS.5.4\nDerivations of the formula for β2,0 in (22)\nBy that Xi = (X1,i, . . . , X9,i)⊤, with Xi follows multivariate normal with mean 0, we obtain\nthat for k = 1, . . . , 9,\nE(Xk,i) = 0,\n(S.35)\nTherefore, we obtain that\nE(Yi) = E{E(Yi|Xi, Ri)}\n= E\n(\nβ2,0 +\n9\nX\nk=1\nβ2,kXk,i −(1 −Ri)γyσ2\n)\n= β2,0 +\n9\nX\nk=1\nβ2,kE(Xk,i) −{1 −E(Ri)}γyσ2\n= β2,0 −γyσ2{1 −Pr(Ri = 1)},\n(S.36)\nwhere the second step is due to (23), and the last step uses (S.35). By (S.36), setting\nE(Yi) = 0 implies that\nβ2,0 = γyσ2{1 −Pr(Ri = 1)}.\n17\n\n\nS.5.5\nDerivations for E(Yi|Xi) in (25)\nBy E{E(U|V, W)|W} = E(U|W) and (23), we have that\nE(Yi|Xi) = E{E(Yi|Xi, Ri)|Xi}\n= E{µj(Xi) −(1 −Ri)γyσ2|Xi}\n= µj(Xi) −{1 −E(Ri|Xi)}γyσ2\n= µj(Xi) −{1 −˜πj(Xi)}γyσ2,\nwhere the last step is due to the definition of ˜πj(Xi), with ˜πj(Xi) given by (20).\nS.6\nAdditional Simulation Results\nFor f ∈F, let ˆR(f) denote the approximation of the empirical risk function, defined as\nn−1 Pn\ni=1 ˆL∗(yi, f(xi), ri). We plot the number of iteration m and the corresponding values\nof ˆR(f (m)) for one random simulation of the MAR and MNAR scenarios of Settings 1 and\n2 considered in Section 6 of the main text, respectively. Clearly, ˆR(f (m)) approaches zero\nas the iteration number m increases, showing the convergence of the algorithm for the\nproposed boosting methods.\n18\n\n\n0.13\n0.14\n0.15\n0.16\n0\n10\n20\n30\niteration\nR^(f(m))\n0.50\n0.52\n0.54\n0\n10\n20\n30\niteration\nR^(f(m))\n0.21\n0.22\n0.23\n0.24\n0.25\n0.26\n0\n10\n20\niteration\nR^(f(m))\n0.125\n0.130\n0.135\n0.140\n4\n8\n12\niteration\nR^(f(m))\n0.47\n0.48\n0.49\n0.50\n0.51\n0\n10\n20\n30\n40\niteration\nR^(f(m))\n0.18\n0.19\n0.20\n0.21\n0.22\n0\n10\n20\n30\n40\niteration\nR^(f(m))\n0.12\n0.13\n0.14\n0.15\n0\n10\n20\n30\n40\niteration\nR^(f(m))\n0.48\n0.50\n0.52\n4\n8\n12\niteration\nR^(f(m))\n0.20\n0.22\n0.24\n0\n10\n20\niteration\nR^(f(m))\n0.125\n0.130\n0.135\n0.140\n0.145\n0.150\n2\n4\n6\niteration\nR^(f(m))\n0.49\n0.50\n0.51\n0.52\n0.53\n2\n4\n6\niteration\nR^(f(m))\n0.18\n0.20\n0.22\n0.24\n0\n10\n20\n30\n40\n50\niteration\nR^(f(m))\n0.135\n0.140\n0.145\n0.150\n2\n4\n6\n8\niteration\nR^(f(m))\n0.50\n0.51\n0.52\n0.53\n0.54\n1\n2\n3\n4\niteration\nR^(f(m))\n0.20\n0.22\n0.24\n0\n10\n20\n30\niteration\nR^(f(m))\nFigure S.1: Plots of ˆR(f (m)) versus the number of iterations under the MAR scenario of\nSetting 1: Top to bottom rows correspond to the R, N, IPW, IPWN, and BJ methods,\nrespectively; left to right columns correspond the Huber, L1, and L2 loss functions, respec-\ntively.\n19\n\n\n0.15\n0.16\n0.17\n0.18\n0\n5\n10\n15\n20\n25\niteration\nR^(f(m))\n0.53\n0.55\n0.57\n0.59\n0\n5\n10\n15\n20\n25\niteration\nR^(f(m))\n0.22\n0.24\n0.26\n0.28\n0.30\n0\n20\n40\n60\niteration\nR^(f(m))\n0.13\n0.14\n0\n5\n10\n15\n20\niteration\nR^(f(m))\n0.48\n0.50\n0.52\n0\n5\n10\n15\n20\niteration\nR^(f(m))\n0.18\n0.20\n0.22\n0\n10\n20\n30\n40\niteration\nR^(f(m))\n0.15\n0.16\n0.17\n5\n10\n15\n20\niteration\nR^(f(m))\n0.52\n0.54\n0.56\n0.58\n0.60\n4\n8\n12\niteration\nR^(f(m))\n0.24\n0.26\n0.28\n0.30\n5\n10\n15\niteration\nR^(f(m))\n0.145\n0.150\n0.155\n0.160\n0.165\n0.170\n2\n4\n6\niteration\nR^(f(m))\n0.54\n0.56\n0.58\n1.0\n1.5\n2.0\n2.5\n3.0\niteration\nR^(f(m))\n0.23\n0.25\n0.27\n0.29\n0\n10\n20\n30\niteration\nR^(f(m))\n0.150\n0.155\n0.160\n0.165\n0.170\n5\n10\n15\niteration\nR^(f(m))\n0.53\n0.54\n0.55\n0.56\n0.57\n0.58\n2\n4\n6\n8\niteration\nR^(f(m))\n0.24\n0.25\n0.26\n0.27\n0.28\n0.29\n2\n4\n6\niteration\nR^(f(m))\nFigure S.2: Plots of ˆR(f (m)) versus the number of iterations under the MNAR scenario\nof Setting 1: Top to bottom rows correspond to the R, N, IPW, IPWN, and BJ methods,\nrespectively; left to right columns correspond the Huber, L1, and L2 loss functions, respec-\ntively.\n20\n\n\n0.25\n0.50\n0.75\n1.00\n0\n20\n40\niteration\nR^(f(m))\n0.6\n0.8\n1.0\n1.2\n1.4\n0\n10\n20\n30\niteration\nR^(f(m))\n0.4\n0.8\n1.2\n0\n25\n50\n75\n100\niteration\nR^(f(m))\n0.4\n0.6\n0.8\n1.0\n5\n10\niteration\nR^(f(m))\n0.6\n0.8\n1.0\n1.2\n0\n50\n100\n150\n200\niteration\nR^(f(m))\n0.5\n1.0\n0\n50\n100\n150\n200\niteration\nR^(f(m))\n0.25\n0.50\n0.75\n1.00\n0\n20\n40\n60\niteration\nR^(f(m))\n0.75\n1.00\n1.25\n5\n10\niteration\nR^(f(m))\n0.4\n0.8\n1.2\n1.6\n0\n50\n100\n150\n200\n250\niteration\nR^(f(m))\n0.25\n0.50\n0.75\n0\n10\n20\n30\n40\n50\niteration\nR^(f(m))\n0.6\n0.8\n1.0\n1.2\n5\n10\n15\n20\niteration\nR^(f(m))\n0.5\n1.0\n0\n25\n50\n75\niteration\nR^(f(m))\n0.25\n0.50\n0.75\n1.00\n0\n10\n20\n30\niteration\nR^(f(m))\n0.6\n0.8\n1.0\n1.2\n0\n20\n40\niteration\nR^(f(m))\n0.5\n1.0\n1.5\n0\n25\n50\n75\n100\n125\niteration\nR^(f(m))\nFigure S.3: Plots of ˆR(f (m)) versus the number of iterations under the MAR scenario of\nSetting 2: Top to bottom rows correspond to the R, N, IPW, IPWN, and BJ methods,\nrespectively; left to right columns correspond the Huber, L1, and L2 loss functions, respec-\ntively.\n21\n\n\n0.25\n0.50\n0.75\n1.00\n0\n20\n40\n60\niteration\nR^(f(m))\n0.6\n0.8\n1.0\n1.2\n0\n20\n40\n60\niteration\nR^(f(m))\n0.5\n1.0\n1.5\n0\n50\n100\niteration\nR^(f(m))\n0.25\n0.50\n0.75\n0\n20\n40\n60\niteration\nR^(f(m))\n0.8\n1.0\n1.2\n0\n10\n20\n30\niteration\nR^(f(m))\n0.5\n1.0\n0\n50\n100\n150\n200\niteration\nR^(f(m))\n0.25\n0.50\n0.75\n0\n20\n40\n60\n80\niteration\nR^(f(m))\n0.6\n0.8\n1.0\n1.2\n0\n50\n100\n150\niteration\nR^(f(m))\n0.5\n1.0\n0\n25\n50\n75\n100\niteration\nR^(f(m))\n0.2\n0.4\n0.6\n0.8\n5\n10\n15\niteration\nR^(f(m))\n0.8\n1.0\n1.2\n5\n10\n15\n20\niteration\nR^(f(m))\n0.5\n1.0\n0\n50\n100\n150\n200\niteration\nR^(f(m))\n0.25\n0.50\n0.75\n1.00\n0\n20\n40\n60\niteration\nR^(f(m))\n0.6\n0.8\n1.0\n1.2\n0\n10\n20\n30\n40\niteration\nR^(f(m))\n0.25\n0.50\n0.75\n1.00\n1.25\n0\n10\n20\n30\n40\niteration\nR^(f(m))\nFigure S.4: Plots of ˆR(f (m)) versus the number of iterations under the MNAR scenario\nof Setting 2: Top to bottom rows correspond to the R, N, IPW, IPWN, and BJ methods,\nrespectively; left to right columns correspond the Huber, L1, and L2 loss functions, respec-\ntively.\n22\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21276v1.pdf",
    "total_pages": 52,
    "title": "Boosting Prediction with Data Missing Not at Random",
    "authors": [
      "Yuan Bian",
      "Grace Y. Yi",
      "Wenqing He"
    ],
    "abstract": "Boosting has emerged as a useful machine learning technique over the past\nthree decades, attracting increased attention. Most advancements in this area,\nhowever, have primarily focused on numerical implementation procedures, often\nlacking rigorous theoretical justifications. Moreover, these approaches are\ngenerally designed for datasets with fully observed data, and their validity\ncan be compromised by the presence of missing observations. In this paper, we\nemploy semiparametric estimation approaches to develop boosting prediction\nmethods for data with missing responses. We explore two strategies for\nadjusting the loss functions to account for missingness effects. The proposed\nmethods are implemented using a functional gradient descent algorithm, and\ntheir theoretical properties, including algorithm convergence and estimator\nconsistency, are rigorously established. Numerical studies demonstrate that the\nproposed methods perform well in finite sample settings.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}