{
  "id": "arxiv_2502.20855v1",
  "text": "MAMUT: A Novel Framework for Modifying Mathemati-\ncal Formulas for the Generation of Specialized Datasets for\nLanguage Model Training\nJonathan Drechsel\njonathan.drechsel@uni-passau.de\nFaculty of Computer Science and Mathematics, University of Passau, DE\nAnja Reusch\nanja@campus.technion.ac.il\nTaub Faculty for Computer Science, Technion - Israel Institute of Technology, IL\nSteffen Herbold\nsteffen.herbold@uni-passau.de\nFaculty of Computer Science and Mathematics, University of Passau, DE\nAbstract\nMathematical formulas are a fundamental and widely used component in various scientific\nfields, serving as a universal language for expressing complex concepts and relationships.\nWhile state-of-the-art transformer models excel in processing and understanding natural\nlanguage, they encounter challenges with mathematical notation, which involves a complex\nstructure and diverse representations. This study focuses on the development of specialized\ntraining datasets to enhance the encoding of mathematical content. We introduce Math\nMutator (MAMUT), a framework capable of generating equivalent and falsified versions\nof a given mathematical formula in LATEX notation, effectively capturing the mathematical\nvariety in notation of the same concept. Based on MAMUT, we have generated four large\nmathematical datasets containing diverse notation, which can be used to train language\nmodels with enhanced mathematical embeddings.\n1\nIntroduction\nMathematical formulas are a fundamental and widely used component in various scientific fields, serving as\na universal language for expressing complex concepts and relationships. Their context-dependent symbols,\nnested operations, and diverse notations pose distinct challenges for machine learning models due to their\nsymbolic and structural differences from natural language (Zanibbi et al., 2020; Peng et al., 2021).\nDespite the success of transformer-based language models (Vaswani et al., 2017) in natural language tasks,\nthey encounter challenges in comprehending mathematical notation (Hendrycks et al., 2021; Gong et al.,\n2022; Petersen et al., 2023; Dao & Le, 2023; Shen et al., 2023; Reusch et al., 2024; Qiao et al., 2024).\nRaw Datasets\n\\frac{d}{dx} f(x) = \\lim_{h \\to 0} \\frac{f(x+h)-f(x)}{h}\nRaw Datasets\nAMPS, ARQMath (Section 2.2), NMFT (Section 4)\nGeneration of Formula Versions\nEquVG (Section 5.1), FalseVG (Section 5.2)\nGenerated Datasets\ng’(y) = \\lim_{h \\to 0} \\left(g(h+y) - g(y)\\right)/h\nMF, MT, NMF, MFR (Section 6)\nFigure 1: MAMUT: Modifying formulas to generate large and diverse mathematical datasets.\n1\narXiv:2502.20855v1  [cs.CL]  28 Feb 2025\n\n\nDataset\nDescription\nExample(s)\nMathematical\nFormulas (MF)\nMathematical formulas with\nhigh variance\nx · xN = x1+N\n(a −b)/(b ∗a) = −1/a + 1\nb\nMathematical\nTexts (MT)\nTexts combining natural lan-\nguage and mathematical for-\nmulas\nIdentify P∞\nn=0(yn −L) where yn+1 = (1+yn)\n1\n3 and\nL3 = L + 1. Let y > 2 and let f(y) = (1 + y)\n1\n3 . Let\nf n(y) be the n th iterate of f(y). Let L be . . .\nNamed\nMathe-\nmatical Formulas\n(NMF)\nHigh variance formulas of fa-\nmous named identities\nName: Pythagorean Thm., Formula: c2 = b2 + a2\nName: Binomial Formula, Formula: (α + z)2 =\nz2 + α2 + 2 · α · z\nMathematical\nFormula Retrieval\n(MFR)\nPairs of formulas with labels\nindicating identical or different\nmathematical concepts\nFormula 1:\n1 · 2 · 3 · . . . · n = n!, Formula 2:\nm! := Qm\nk=1 k, Label: Equivalent\nFormula 1: a2 + b2 = c2, Formula 2: a2 + 2b = c2\nLabel: Not Equivalent\nTable 1: Overview of generated datasets. The examples are shown as rendered LATEX.\nThese challenges stem from the complex formula structure, diverse formula representations, and ambiguous\nimplicit semantics (Peng et al., 2021).\nFor example, x =\n−b±\n√\nb2−4ac\n2a\ninvolves nested operations, while\ndifferent notations, such as x\ny , x/y, x ÷ y, and x · y−1 can represent the same mathematical relationship,\nalongside the contextual meanings of symbols (e.g., i as an index or imaginary unit) further complicate\nthe understanding. These difficulties highlight the need for rich, specialized datasets to train models for\nmathematical content. However, existing datasets face scalability constraints due to expert curation or lack\ndiversity in problem types and notation.\nTo address the need, we propose a framework, Math Mutator (MAMUT), for generating high-quality and\ndiverse mathematical formulas to enhance the training and comprehension capabilities of mathematical\nlanguage models. MAMUT allows for the creation of mathematically equivalent formulas (EquVG) and\nchallenging non-equivalent ones that appear similar (FalseVG). This includes random alterations in variable\nand function identifiers and variations in LATEX notation that leverage mathematical properties such as\ncommutativity and symmetry. Additionally, we extend this approach to text containing mathematical LATEX\nnotation, ensuring consistent changes in identifiers and notation styles across textual contexts. We apply\nMAMUT to generate four datasets (see Figure 1 and Table 1) designed for the training of mathematical\nlanguage models, e.g., for further mathematical pre-training on equation completion tasks.\n2\nRelated Work\nThis section covers language models, datasets, and data augmentation techniques in mathematical contexts.\n2.1\nMathematical Language Models\nThe success of transformer-based models (Vaswani et al., 2017), such as BERT (Devlin et al., 2019), led\nto the development of domain-specific models, including SciBERT for scientific texts (Beltagy et al., 2019)\nand CodeBERT for programming (Feng et al., 2020). These models improve over general-purpose models\nby training on domain-specific data. Similarly, specialized models have been developed for mathematics,\nsuch as MathBERT (Peng et al., 2021), MathBERTa (Novotný & Štefánik, 2022), and others (Reusch et al.,\n2022; Liu et al., 2023; Shao et al., 2024). They typically employ additional mathematical tokens and were\npre-trained on mathematical datasets (see Section 2.2).\nA key application of mathematical language models is in Mathematical Information Retrieval (MIR) (Dadure\net al., 2024; Zanibbi et al., 2025), where the goal is to retrieve relevant documents based on a user’s query,\n2\n\n\nQuery\nRelevant Documents\nNot Relevant Documents\n(a + b)2 = a2 + 2ab + b2\na2 + 2ab + b2 = (a + b)2\n(a + b)2 + a2 = 2ab + b2\n(c + d)2 = c2 + 2cd + d2\n(a + b)2 = a2 + 2ab + a2\n(a + b)2 = a2 + b2 + 2ab\n(a + b)2 = a2 + b2\na2 + b2 = c2\nc2 = a2 + b2\na2 + b2 + c2\nPythagorean Theorem\na2 + b2 = c2\nPythagorean Identity\nP∞\nn=1\n1\nn\nP∞\nk=1 k−1\nP∞\nn=1\n1\nn2\nf ′(x) = limh→0\nf(x+h)−f(x)\nh\nd\ndzg(z) = limd→0\ng(z+d)−g(z)\nd\nlimx→0 f(x) = 0\nTable 2: Examples for MIR queries including relevant and not relevant documents. Note that Pythagorean\nIdentity (sin2(x) + cos2(x) = 1) is not relevant for the query Pythagorean Theorem (a2 + b2 = c2).\nwhere both may contain mathematical content (see Table 2 for examples). Traditional MIR systems rely\non keyword matching or simple embeddings (Kim et al., 2012; Greiner-Petter et al., 2020), while more\nsophisticated techniques leverage explicit mathematical knowledge, such as operator trees or formula unifi-\ncation (Kristianto et al., 2016; Mansouri et al., 2019; 2022b; Aizawa & Kohlhase, 2021; Peng et al., 2021).\nTransformer-based models offer new possibilities for MIR by addressing key challenges such as integrating\nnatural and mathematical language, directly processing LATEX input, and handling diverse notations. As a\nresult, mathematical language models have been successfully adapted to MIR (Novotný & Štefánik, 2022;\nReusch et al., 2022; Zhong et al., 2023).\nDespite their promising performance, specialized mathematical models still face challenges in understanding\nmathematical notation (Gong et al., 2022; Shen et al., 2023), especially when it comes to handling variable\nnames and recognizing mathematical equivalence beyond superficial textual similarities (Reusch et al., 2024).\nThis motivates the creation of specialized datasets that reflect the unique roles of variables and aim to improve\nmathematical modeling.\n2.2\nMathematical Datasets\nThe need for mathematical models has led to the development of various collections aimed at enhancing and\nevaluating language model capabilities in context of mathematics. Manually curated datasets like MATH\n(Hendrycks et al., 2021), GSM8K (Cobbe et al., 2021), and MathOdyssey (Fang et al., 2024) test problem-\nsolving skills but are typically small, and reliant on expert input. Synthetically generated datasets like the\nMathematics Dataset (Saxton et al., 2019), AMPS (Hendrycks et al., 2021), and HARDMath (Fan et al.,\n2024) offer scalability but may lack diversity in problem topics, as they rely on generation rules, although\nthey can produce a wide variety of similar but distinct problems (with changing numbers, symbols, . . . ),\nwhich can be beneficial for models learning to generalize across formula representations.\nDatasets like\nNTCIR (Zanibbi et al., 2016) and ARQMath (Mansouri et al., 2022a), sourced from the existing repositories\narXiv, Wikipedia, and the Mathematical Stack Exchange, provide a broad range of real-world mathematical\nproblems. However, they lack controlled variations of specific formulas, an important aspect for training\nMIR models to classify whether two symbolic representations are mathematically equivalent.\n2.3\nMathematical Data Augmentation Techniques\nRecent advancements in mathematical data augmentation have introduced various innovative methods aimed\nat enhancing the diversity and depth of training materials. InfinityMath (Zhang et al., 2024) utilize GPT-4\n(Achiam et al., 2023) to transform specific mathematical problems into generic templates. These templates\ncan then generate multiple variations of the original problem, altering numerical values or structural com-\nplexity, thereby increasing the dataset’s variety. Similarly, Li et al. (2024) propose a method to formalize\n3\n\n\nNatural Language\nMathematical Language\nPurpose\nGeneral human communication, includ-\ning opinions and feelings\nPrecise description of mathematical concepts\nVocabulary Large set of words (language depen-\ndent), sometimes with ambiguous mean-\ning (e.g., love, happy, data)\nSmall set of well-defined symbols (e.g., x, +,\nR, sin, ∀, ∞,\nR\n) and terms (e.g., Eigenvalue,\nDerivative, Field) with precise meanings\nGrammar\nRather flexible\nStrict rules\nClarity\nOften imprecise, open to interpretation\nSingle, unambiguous interpretation\nEvolution\nEvolves over time naturally, new words,\nphrases, and idioms emerge or disappear\nEvolves slower,\nchanges are introduced by\nmathematicians and are backward compatible\nWriting\nStyle\nLinear structure in sentences and para-\ngraphs using standard formats\nRequires specialized formats (e.g., LATEX) to\nrepresent complex notation in a structured way\nTable 3: Comparison of natural and mathematical language.\nmathematical problems written in natural language, alter the difficulty by adjusting the problem’s opera-\ntions, and then informalize these changes back into natural language using GPT-4, preserving mathematical\nintegrity across different levels of complexity. MathGenie (Lu et al., 2024) augments step-by-step solutions\nby generating modified candidate solutions with a Llama model (Touvron et al., 2023) with verified correct-\nness, and then back-propagating these solutions to a modified question. You et al. (2024) augment data by\napplying different strategies, including rephrasing and reorganization with LLM, and question alteration.\nThese approaches primarily focus on diversifying problem content rather than varying mathematical notation\n(e.g., (a + b)2 = a2 + 2ab + b2 vs. (x + y)2 = x2 + y2 + 2yx). In contrast, Reusch et al. (2024) explore\nvariable renaming in training data to prevent models from taking shortcuts in problem-solving, such as\nrelying only on variable overlap. Building on this idea, our study enhances mathematical formula diversity\nthrough substitutions and other techniques.\n3\nNatural and Mathematical Language\nMathematical language differs fundamentally from natural languages such as English, German, or Chi-\nnese. While natural language is used for general communication and often conveys subjective information,\nmathematical language serves a highly specialized purpose to precisely describe mathematical topics, such as\ndefinitions, theorems, and proofs. It consists of both symbolic expressions (e.g., a2+b2 = c2 and\nR b\na sin(x), dx)\nand specialized terminology (e.g., derivative and eigenvalue). Despite their differences, natural and mathe-\nmatical languages share some structural similarities. Both use symbols arranged in a syntax that conveys\nmeaning, and both can be represented in textual form. However, there are some key differences (Ilany et al.,\n2010; Scarlatos & Lan, 2023) summarized in Table 3. A crucial challenge for mathematical language models\nis symbol abstraction. In mathematical expressions, certain symbols act essentially as wildcards and can be\nreplaced without changing the mathematical meaning. These symbols are either variables (e.g., x, α, A) or\ngeneric functions (e.g., f, g or φ), i.e., functions not tied to a specific mathematical object (e.g., Euler’s\nGamma function Γ(z)). For example, a model should recognize that the first binomial formula,\n(a + b)2 = a2 + 2ab + b2,\n(1)\nis equivalent to (c+d)2 = c2 +2cd+d2, despite different variable names. In contrast, (a+b)2 = a2 +2ab+a2\nuses the same variables but in a mathematically non-derivable way. Likewise, the modified formula (a+b)2 =\na2 + b2 + 2ab appears different from Eq. (1), but it is, in fact, mathematically equivalent.\nAnother important aspect is the structure of mathematical formulas. Consider the two formulas 2x and x2.\nAssuming a character-wise LATEX tokenization, both formulas use the same tokens but in a different order.\nA model should not treat x2 as equivalent to 2x (but instead to x · x). These structural variations highlight\n4\n\n\nNames\nFactorial, Definition of a factorial\nVersion 1\nn! = 1 · 2 · . . . · n\nVersion 2\nn! = Qn\ni=1 i\nVersion 3\n∀n ∈N : (n + 1)! = (n + 1) · n!, 0! = 1\nVersion 4\nFor any natural number n, we have n! is defined as n! := Qn\ni=1 i.\nVersion 5\nFor any natural number n, n! can be obtained by multiplying all natural\nnumbers from 1 to n together.\nSimilar Formula\nBinomial Coefficient Formula\nFalse Version 1\nn! = 1 · 2 · 3 · 4 · n\nFalse Version 2\n∀n ∈N : (n + 1)! = (n −1) · n!, 0! = 0\nFalsifying Replacements\nQ →P, N →R, “natural“ →“real“\nTable 4: Example entry for the definition of a factorial from the NMFT dataset (partially).\nthe need for a model that goes beyond simple token matching and actually understands mathematical\nmeaning. Transformer language models (Vaswani et al., 2017) have shown their capabilities in modeling\nnatural language, hence, it is worth exploring their potential to precisely capture mathematical language.\n4\nNamed Mathematical Formula Templates (NMFT)\nPrevious datasets provide formulas and mathematical texts with significant variance across a wide range of\nmathematical topics. For the purpose of our proposed data augmentation methods introduced in the next\nsection, it is necessary to parse formulas into symbolic expressions. Real-world datasets contain formulas with\nvarious notations, some of them might be parsed incorrectly, or the parsing even fails completely. Therefore,\nwe created a dataset consisting of only a few but high-quality parsable formulas. This dataset includes 71\nwell-known mathematical identities that are easily recognizable and associated with one or multiple names.\nFor example, a2 + b2 = c2 represents the Pythagorean theorem, while (a + b)2 = a2 + 2ab + b2 represents\nthe first binomial formula (Eq. 1). Since mathematical formulas are associated with its name, we call this\ndataset Named Mathematical Formula Templates (NMFT), as the formulas serve as templates for deriving\nmodified versions. An example entry can be found in Table 4, and Table 6 lists all identities.\nEach identity provides multiple representations, such as ∀a, b ∈R : (a + b)2 = a2 + 2ab + b2 as another, more\ndetailed version of the first binomial formula. Additionally, some representations are provided as descriptive\ntext, e.g., “In a right-angled triangle with side lengths a, b, and c, where c represents the length of the\nhypotenuse, the equation a2 + b2 = c2 holds true”. Others paraphrase formulas textually, e.g., “a2 + b2\nis equal to c2”, reinforcing associations between the equals sign = and “equals”. These textual versions\nintentionally exclude the name of the identity to make MIR tasks harder, where the name serves as the\nquery. For each provided identity version, the variables and function symbols are explicitly given to assist\nthe parsing and version generation. To enhance the generation of challenging falsified versions, similar-looking\nformulas are provided (e.g., the first binomial formula for the Pythagorean theorem, as both identities contain\nmultiple powers of two), or hints to falsify any given representation by a string replacement (e.g., removing\n“right-angled” to falsify the previous descriptive example of the Pythagorean theorem). The descriptive\ntext versions have been partially generated by using GPT-3.5 (Brown et al., 2020) and manually verified\nfor validity. Typically, we call entries of NMFT and of datasets generated from it formulas, but both, pure\nmathematical formulas and textual descriptions of formulas are meant.\n5\n\n\n5\nMath Mutator (MAMUT)\nOur goal is to create high-quality, large, and diverse mathematical datasets to enhance mathematical mod-\neling. We introduce Math Mutator (MAMUT), a framework consisting of two core algorithms designed to\ngenerate both equivalent and falsified versions of a given formula. The first algorithm, Equivalent Version\nGeneration (EquVG), presented in Section 5.1, automatically generates various versions of a given formula,\nexpanding the training data and enabling the model to learn math-specific language rules, such as treating\nvariables as placeholders that can be substituted without changing the validity of an expression. The sec-\nond algorithm, Falsified Version Generation (FalseVG), introduced in Section 5.2, slightly modifies formulas\nslightly to create mathematically not equivalent versions of the original formula, offering challenging negative\nexamples for MIR tasks.\n5.1\nEquVG: Variations of Mathematical Formulas\nThe key idea of this section can be summarized as follows: Given a mathematical formula, our aim is\nto generate mathematically equivalent variations of this formula, called equivalent versions. For instance,\nconsider the formula\n(a + b)2 = a2 + 2 · a · b + b2.\n(2)\nIn this context, we observe that all the following formulas describe the same mathematical relationship,\nnamely the first binomial formula:\n(b + a)2 = a2 + b2 + 2 · b · a,\n(3)\n(a + b)2 = a · a + 2 · a · b + b2,\n(4)\na2 + 2 · a · b + b2 = (a + b)2,\n(5)\n(c + d)2 = c2 + 2 · c · d + d2,\n(6)\n(λ + Z)2 = λ2 + 2 · λ · Z + Z2.\n(7)\nEquation (3) can be derived from Eq. (2) by applying both, additive and multiplicative commutativity. In\nEq. (4), the exponentiation a2 is replaced by its definition a · a. Since equality is a symmetric relation,\nequations remain valid after interchanging the sides, as done in Eq. (5). The final two equations can be\nobtained from Eq. (2) by substituting variables. This section is dedicated to the automated generation of\nsuch equivalent versions. Note that for a complete mathematical expression, it would be necessary to specify\nthe range of values (e.g., of variables) for which the statement holds. For example, a complete expression\nof Eq. (2) could be ∀a, b ∈R : (a + b)2 = a2 + 2 · a · b + b2. However, in practical applications like MIR\nsystems, the shortened version Eq. (2) may also be used, for the sake of simplicity. Therefore, the somewhat\nless precise mathematical formulations in Eqs. (2)-(7) are often sufficient.\nThe complete workflow of our proposed Equivalent Version Generation (EquVG) algorithm is depicted in\nFigure 2. The input consists of a formula written in LATEX format. To implement transformations, as seen in\nEqs. (2)-(7), we can identify two steps: the substitution of symbols and the modification of the mathematical\nnotation. For both of these purposes, it is helpful to represent the formula not as a string but as a structured\ndata format that captures the mathematical relationships and dependencies. This representation is achieved\nby parsing the LATEX formulas into a symbolic expression format, essentially creating an operator tree. The\nsymbolic representation categorizes elements into numbers, variables, functions, and other mathematical\nobjects, establishing a structured relationship between them. This organization enables the identification\nand substitution of variables (x, \\alpha, . . . ) and generic functions (f, g, . . . ) to derive a mathematically\nequivalent representation using different symbols. This substituted expression is then converted back into\nLATEX format during the printing process, which includes the desired modifications of mathematical notation,\nsuch as writing a · a instead of a2. In the following, we will provide a more detailed explanation of the three\nsteps of EquVG: parsing, substituting, and printing.\nLATEX Parsing\nParsing a LATEX formula into a symbolic expression presents certain challenges.\nFor\ninstance, if a letter precedes parentheses, it can be interpreted either as a multiplication (with omitted\n6\n\n\nInput Formula\nLATEX Parsing\nSymbolic Expression\nSymbol\nSubstitution\nNew Symbolic Expression\nGenerated Formula\nRandomized\nLATEX Printing\n\\frac{d}{dx} f(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}\nEQU(DERIV(FUNC(f, VAR(x))), LIM(DIV(SUB(FUNC(f, ADD(VAR(x),\nVAR(h))), FUNC(f, VAR(x))), VAR(h)), VAR(h), INT(0)))\nVAR(x) →VAR(y), FUNC(f) →FUNC(g)\nEQU(DERIV(FUNC(g, VAR(y))), LIM(DIV(SUB(FUNC(g, ADD(VAR(y),\nVAR(h))), FUNC(g, VAR(y))), VAR(h)), VAR(h), INT(0)))\ng’(y) = \\lim_{h \\to 0} \\left(g(h+y) - g(y)\\right)/h\nFigure 2: Visualization of the EquVG algorithm.\nInput Formula\nLATEX Parsing\nSymbolic Expression\nSymbol\nSubstitution\nNew Symbolic Expression\nGenerated Formula\nRandomized\nLATEX Printing\nThe derivative of a function $f$, referred to as $\\frac{d}{dx}f(x)$, is defined\nas the limit of $\\frac{f(x+h) - f(x)}{h}$ as $h$ approaches $0$.\nTEXT(\"The derivative of a function “, FUNC(f), “, referred to as “, DERIV(FUNC(f),\nVAR(x)), “, is defined as the limit of “, DIV(SUB(FUNC(f, ADD(VAR(x), VAR(h))),\nFUNC(f, VAR(x))), VAR(h)), “ as “, VAR(h), “ approaches “, INT(0), “.”)\nVAR(x) →VAR(y), FUNC(f) →FUNC(g)\nTEXT(\"The derivative of a function “, FUNC(g), “, referred to as “, DERIV(FUNC(g),\nVAR(y)), “, is defined as the limit of “, DIV(SUB(FUNC(g, ADD(VAR(y), VAR(h))),\nFUNC(g, VAR(y))), VAR(h)), “ as “, VAR(h), “ approaches “, INT(0), “.”)\nThe derivative of a function $g$, referred to as $g’(y)$, is defined\nas the limit of $\\left(g(h+y) - g(y)\\right)/h$ as $h$ approaches $0$.\nFigure 3: Visualization of the EquVG algorithm for a mathematical text.\nmultiplication symbol, e.g., v(x + y) = v · (x + y)) or as a function call (v(x + y)). The symbol e could be\neither a variable or Euler’s number e ≈2.718. Likewise, the symbol i might function as a variable (e.g., as\na summation index in Pn\ni=1 i2) or as the imaginary unit, sometimes expressed in LATEX as i (\\mathrm{i})\nto avoid ambiguity. It is crucial for our purposes to determine whether a symbol is a substitutable variable\nor a constant. Otherwise, the imaginary unit might be incorrectly substituted by another symbol, resulting\nin a non-equivalent expression. We have addressed this issue, partially by applying heuristics that consider\nthe context (i = 1 within the formula indicates a variable, while iπ indicates the imaginary unit, as in eiπ).\nFurthermore, we introduce a safeguard to handle cases where the parser is uncertain about whether to treat a\nsymbol as a variable or the imaginary unit. In these cases, the symbol is represented in a way that prevents\nsubstitution while maintaining its appearance as the plain i, without enabling complex unit formatting\noptions. Despite these measures, parsing can still fail in cases with unusual or malformed notation.\nThe formula parsing can be conceptually expanded to include the parsing of text containing LATEX formulas.\nSuch texts are referred to as mathematical texts in this study. The text parts remain unchanged during\nsubstitution and printing, only the formula parts are processed consistently by EquVG as shown in Figure 3.\nThis allows to consistently change symbols in a mathematical text throughout all its formulas. Within a\nmathematical text, formulas are defined as text in between dollar signs ($...$), as used in LATEX documents\nto write inline mathematical formulas.\nSymbol Substitution\nThe symbolic expression format allows the substitution of symbols by simply\nreplacing all occurrences of a given symbol within the expression. The generation of a substitution (i.e.,\na mapping of symbols) involves two steps. Firstly, a subset of all symbols in the expression is randomly\nselected. Secondly, a new symbol is chosen for each selected symbol. The aim of the substitution process is\n7\n\n\nto generate diverse formulas that are similar to formulas occurring in real-world scenarios. It is important\nto note that, intuitively, Eq. (6) with variables c and d appears more familiar for a binomial formula than\nEq. (7), which uses Greek and uppercase Latin letters (λ and Z) that are not commonly used as variables in\nthe context of binomial formulas. When variables are selected entirely at random from a uniform distribution\nover all Latin and Greek letters, unfamiliar symbol usage is more likely. This motivates the introduction\nof symbol groups, which categorizes similar variables or functions together.\nThe defined symbol groups\ncan be found in Table 10, along with a description of a typical mathematical context for each group. For\ninstance, we have the group of indices {i, j, k, l} and group of vectors {u, v, w}. It is worth noting that\nvariables can belong to multiple groups. Given a symbol that should be substituted with a new symbol,\nall symbols from the relevant symbol group(s) are candidates. Additionally, the most common variable x\nis a candidate in every variable group to reflect its common usage. To add variety, a random symbol may\nbe added by chance to the set of candidates, selected from commonly used lowercase or uppercase Latin\nletters or lowercase Greek letters. Symbols that refer to constants in certain contexts, such as e, i, and\nπ, are excluded.\nGiven the set of candidates, a random candidate is chosen.\nHowever, it is sometimes\nnecessary (or at least useful) to make the symbol selection dependent on multiple substitution symbols. For\ninstance, consider the Fundamental Theorem of Calculus:\nR b\na f(x) dx = F(b) −F(a). Here, we have two\ngeneric functions, f and F. Whenever a symbol has related variants in the formula, such as uppercase and\nlowercase forms or corresponding Greek letters (e.g., a, A, α), the algorithm automatically preserves these\nrelationships by restricting possible substitutions accordingly. For instance, substituting f and F by g and\nG, respectively, yields an equivalent version\nR b\na g(x) dx = G(b) −G(a). Again, this is rather mathematically\nimprecise but aligns with the implicit assumptions on mathematical notation found in real-world datasets.\nAnother possible generated version is\nR a2\na1 f(x) dx = F(a2)−F(a1) where a and b are substituted by indexed\nvariables a1 and a2 respectively. In cases where multiple variables of the same variable group appear in the\nsame formula, the generation algorithm may randomly perform such indexed substitutions. The indexing\nenforces the model not only to attend to the variable itself but also its modifications, in this case, its index.\nRandomized LATEX Printing\nIn the final step of EquVG, the symbolic expression is converted back\ninto LATEX format. To further increase the variety of generated formulas, the LATEX printer makes ran-\ndomized printing decisions. These variations can be categorized into two main sources: mathematical and\nLATEX notation. The parsed and printed formulas in Figure 3 are illustrating these differences. For example,\nthe input text used the explicit notation for a derivative, \\frac{d}{dx}f(x), while the printed substituted\nexpression uses the shorthand notation g’(y). We developed a list of equivalent mathematical notations,\nwhere the printer randomly selects one of the available ones for printing. As another example in Figure 3,\ninstead of the fraction notation with \\frac, the printer used the forward slash / to denote division. Since\naddition is commutative, h+y is printed instead of y+h. These examples represent mathematical variations,\nas they express a mathematical concept in an equivalent way. In contrast, the usage of \\left and \\right\ncommands represents LATEX variations, since these commands are not essential for mathematical reasons but\nonly for a differently rendered text. In addition to the already covered examples, the randomization of the\nLATEX printer includes the notation of\n• equalities (x = y vs. y = x),\n• inequalities (x > 0 vs. 0 < x),\n• multiplication symbols (a \\cdot b vs. a * b vs. a \\times b vs. ab),\n• divisions (2/n vs. 2 \\cdot n^{-1} vs. \\frac{2}{n} vs. \\frac2n),\n• integer powers (a^3 vs. a^2\\cdot a vs. a \\cdot a \\cdot a),\n• inverse trigonometric functions (\\asin(x) vs. \\arcsin(x) vs. \\sin^{-1}(x) vs. (\\sin(x))^{-1}),\n• higher order derivatives (f’’’(x) vs. f^{(3)}(x) vs. \\frac{d^3}{dx^3} f(x)),\n• expected values (\\mathbb{E}[X] vs. \\operatorname{E}[X] vs. E[X]),\n• matrix determinants (\\det(A) vs. |A|),\n8\n\n\n• binomial coefficients (\\binom{n}{k} vs. {n \\choose k}),\n• empty sets (\\emptyset vs. \\varnothing vs. \\{\\}), and\n• natural logarithms (\\ln(x) vs. \\log_e(x)).\nAs real-world data uses different styles of notations, language models should be capable of understanding all\ncommonly used notations. This is similar to the notion of synonyms in natural language. The randomized\nLATEX printing provides an automation to diversify training data, such that models can learn the different\nnotations. The combination of parsing, substituting, and printing results as part of EquVG is a powerful\ntool to increase the training data size significantly. Additionally, research has shown that using training data\nwith substituted query-document pairs for MIR helps the model to less focus on shallow features such as\nvariable overlapping (Reusch et al., 2024), confirming the usage of substitutions in EquVG.\n5.2\nFalseVG: Generating Challenging Negative Examples\nWe believe that a classification task determining whether two formulas describe the same mathematical\nconcept helps the model to encode mathematics more effectively. To train models on such a task, we require\nboth positive and negative formula pairs, similar to a MIR training. While positive pairs are often readily\navailable in datasets, identifying meaningful negative pairs is more challenging, as datasets rarely contain\nexplicit negative examples.\nA common approach to extract negative pairs is random sampling from datasets by pairing two random\nformulas. However, this may lead to simplified feature extractions. For instance, the model might learn to\nsimply check for the presence of an important function, like whether both formulas contain the determinant\nfunction \\det. Given a random negative document, a naive classifier that checks if a determinant is part of\nthe formula would likely perform well, due to the rarity of the determinant function across most mathematical\ncontexts. The language model may adapt to this behavior during training.\nTo prevent models from learning such easy shortcuts instead of the true semantic understanding, researchers\nhave successfully used challenging negative examples in other domains (Cai & Liu, 2020; Qiu et al., 2021).\nWe introduce the Falsified Version Generation (FalseVG) algorithm to generate falsified versions of a given\nformula, meaning a similar-looking but not mathematically equivalent formula.\nSince the formulas are\nalready parsed into a symbolic expression for EquVG, we can simply use and modify this representation.\nWe have developed and implemented eight modification strategies, which are described below.\nTable 9\nprovides illustrative examples of each strategy. Similar to EquVG, these strategies can also be applied to\nmathematical texts, by applying the strategies to the text’s formulas.\nEquality\nFalsifying an equality can be achieved by inserting or removing a term on one side of the equality.\nThis can be done either at the outermost level (e.g., changing sin(x) = . . . to sin(x) + 1 = . . .) or within a\nsub-expression (e.g., changing sin(x) = . . . to sin(x+1) = . . .). When inserting a term, the algorithm selects\neither a subexpression from the entire formula, a random new variable, or a random number. Importantly,\nthe algorithm avoids modifications that will not change the validity of an equality, such as adding zero\nor multiplying by one.\nThis strategy enforces the model to focus on the entire formula and long-term\ndependencies.\nInequality\nTo falsify an inequality, we simply invert the inequality symbol. Thus, the symbol ≤is replaced\nby > and vice versa. The same replacement holds for ≥and <. The not equals symbol ̸= can be replaced\nby =, but not vice versa (as = indicates an equality). Similarly to the strategy equality, the model is forced\nto encode long-term dependencies using this strategy.\nSwap\nThe strategy swap involves altering unary and binary functions. Unary functions such as sine,\nsquare root, or logarithm get replaced by different random unary functions. In the case of binary non-\ncommutative functions, we swap the order of the two arguments. These non-commutative functions are\nsubtraction, division, and exponentiation (e.g., x2 becomes 2x). These changes enforce the model to rely on\nthe order of operands rather than just token occurrences in a random order.\n9\n\n\nInput Formula\n\\frac{d}{dx} f(x) = \\lim_{h \\to 0} \\frac{f(x+h)-f(x)}{h}\nLATEX Parsing\nEQU(DERIV(FUNC(f, VAR(x))), LIM(DIV(SUB(FUNC(f, ADD(VAR(x),\nVAR(h))), FUNC(f, VAR(x))), VAR(h)), VAR(h), INT(0)))\nSymbolic Expression\nStrategy\nSelection\nand\nApplication\nEquality\nInequality\nVariable\nConstant\nSwap\nRemove the dividend VAR(h) of quotient DIV\nNo change\nReplace VAR(h) by VAR(a) in limit LIM\nReplace INT(0) by random number INT(1)\nSwap the order in subtraction SUB\nFalsified Symbolic Expression\nEQU(DERIV(FUNC(f, VAR(x))), LIM(SUB(FUNC(f, VAR (x)), FUNC(f,\nADD(VAR(x), VAR(h)))), VAR(h), INT(1)))\nSymbol\nSubstitution\nVAR(x) →VAR(y), FUNC(f) →FUNC(g)\nSubstituted Symbolic Expression\nEQU(DERIV(FUNC(g, VAR(y))), LIM(SUB(FUNC(g, VAR (y)),\nFUNC(g, ADD(VAR(y), VAR(h)))), VAR(a), INT(1)))\nRandomized\nLATEX Printing\nGenerated Formula\ng’(y) = \\lim_{a \\to 1} g(y) - g(h+y)\nFigure 4: Visualization of the FalseVG algorithm.\nVariable\nThe strategy variable essentially aims to split a single variable (e.g., a in (a+b)2 = a2+2ab = b2)\ninto two (e.g., into a and c in (c + b)2 = a2 + 2cb + b2). Specifically, if a variable occurs at least twice in\nthe formula, it might be randomly replaced by another variable for a proper and nonempty subset of its\noccurrences (i.e., at least one occurrence is replaced and at least one occurrence remains unchanged). This\nstrategy enforces the model to check for a consistent use of symbols in the entire formula.\nConstant\nThe strategy constant focuses on numbers (1, 2, e, π, ∞, . . . ) as well as variables that are\ntypically considered to be constant within an expression, such as the upper limit n of an indexed sum like\nPn\ni=1 i2. These constants are replaced by other constants enforcing the model to learn what tokens a certain\nformula should contain.\nDistribute\nThe strategy distribute is inspired by the distributive law, a fundamental mathematical rule\nrelating two binary functions. A standard example for real numbers is that multiplication distributes over\naddition since x·(y+z) = x·y+x·z holds for all real numbers x, y, z. This rule motivates this strategy, which\napplies a modified distributive law to non-distributive functions. Specifically, for a unary function f and a\nbinary function ⊕in infix notation, the relation f(x⊕y) = f(x)⊕f(y) is (falsely) assumed. We use addition\nand multiplication as binary functions and the logarithm, factorial, power with fixed base, and trigonometric\nfunctions for the unary function. This readily results in examples where commonly known identities are\nfalsified, e.g., the falsified product of powers rule is 2x · 2y = 2x·y (instead of the correct 2x · 2y = 2x+y). The\nfalsified sine additivity yields sin(x+y) = sin(x)+sin(y) (instead of sin(x+y) = sin(x) cos(y)+cos(x) sin(y)).\nThis strategy enforces the model to notice the presence of parantheses and to enhance its understanding of\noperator relationships, including precedence.\nManual\nWhile all previous strategies focused on modifying a formula by applying generally valid trans-\nformation rules to falsify it, this strategy relies on manual transformation or replacement rules. These rules\nrefer to the specifically newly created NMFT dataset (see Section 4). The rules can be explicitly given\nfalsified versions (e.g., ∀n ∈N : n! = 1 · 2 · n), references to different but similar formulas (e.g., law of cosines\nfor the Pythagorean theorem), or falsifying replacement rules. For example, a formula replacement might\n10\n\n\nName\nHugging Face Identifier\nOriginal\nRaw\nGenerated\n\u001f v.p.f.\nMax\nDataset(s)\nEntries\nVersions\nv.p.f.\nMF\nddrg/math_formulas\nAMPS\n30,985\n958,735\n30.9\n101\nARQMath\n55,894\n2,257,826\n43.3\n101\nBoth\n82,765\n3,198,108\n38.6\n101\nMT\nddrg/math_text\nAMPS\n62,099\n2,542,015\n40.9\n101\nARQMath\n690,333\n4,480,369\n6.5\n96\nBoth\n752,428\n7,022,384\n9.3\n101\nNMF\nddrg/named_math_formulas\nNMFT\n71/ 522\n23,707,392\n333,906\n400,000\nMFR\nddrg/math_formula_retrieval\nNMFT\n71/ 522\n23,702,560\n334,092\n400,000\nTable 5: Summary of the generated datasets. The abbreviation v.p.f. stands for versions per formula. For\nMF, the Generated Entries values do not sum up from AMPS and Answer Retrieval for Questions on Math\n(ARQMath) to Both due to duplicate removal. The raw values of NMF and MFR refer to the number of\nmathematical identities and the total number of provided version templates of these identities, respectively.\nchange ∀n ∈N to ∀n ∈R if the quantified term only holds for natural but not for real numbers. These rules\nare also applicable to mathematical texts, where, for instance, natural can be replaced by real.\nRandom\nThe simplest approach to generate a falsified formula is to use a random formula, meaning\nan earlier generated equivalent version of a different formula.\nThis approach is especially important to\nincrease the models’ robustness in real-world applications, where most of the input pairs are not inherently\nchallenging.\nThe complete FalseVG algorithm is summarized in Figure 4.\nIt involves applying a random subset of\nthe strategies to a parsed symbolic expression.\nNote that some strategies are not applicable to certain\nformulas, resulting in no changes. However, if at least one strategy succeeds, a falsified symbolic expression\nis generated. Finally, a random symbol substitution and randomized LATEX printing are performed to create\nthe final formula as a string, identically to EquVG.\n6\nGenerated Datasets\nThis section presents four datasets generated using MAMUT employing EquVG and FalseVG. Our imple-\nmentation, built on SymPy (Meurer et al., 2017), is detailed in Appendix B.1. Table 5 summarizes key\nstatistics of the generated datasets, including Hugging Face identifiers, while Appendix B.2 reports example\nentries. All entries ensure uniqueness at the string level. Our dataset generation code is publicly available1.\nTwo of the generated datasets (NMF and MFR) are based on the specifically created NMFT dataset (see\nSection 4), while the other two datasets (MF and MT) are derived from two existing diverse sources that\ncombine natural language with mathematical notation: ARQMath (Mansouri et al., 2022a) and the Khan\nAcademy problems in AMPS (Hendrycks et al., 2021). While we focus on these sources, MAMUT is applica-\nble to any mathematical corpus containing LATEX notation. ARQMath, sourced from the Mathematics Stack\nExchange, benefits from a user-rating system that ensures high-quality discussions and problem-solving con-\ntent. The Khan Academy problems in AMPS provide structured exercises used for educational purposes.\nExample dataset entries are shown in Appendix A.\nMathematical Formulas (MF)\nThis dataset consists exclusively of mathematical formulas extracted\nfrom AMPS and ARQMath, enriched with variations by the EquVG algorithm. However, not all formulas\nfrom these raw datasets are included in the MF dataset. Only formulas are selected being suitable for a\n1https://github.com/aieng-lab/math-mutator\n11\n\n\nMasked Language Modeling (MLM) task (Devlin et al., 2019), where a masked token’s value can be concluded\nby the remaining context of the formula. For example, a masked formula such as π > [MASK] has infinite\nalgebraic solutions, like 3, 0, or any other value that can fill the masked position in a mathematical valid\nsense. Therefore, the formulas are restricted to equalities and implications to ensure meaningful inferences.\nAdditionally, the (general) validity of each used expression is verified using SymPy to ensure high data\nquality. In case of an equation without general validity (e.g., x2 = 2) but with existing solution(s) found by\nSymPy, the equation can be transformed into an implication (e.g., x2 = 2 ⇒x = −\n√\n2 or x =\n√\n2). A few\nexamples of extracted formulas are (original LATEX formatting is preserved):\ntan(x) = sin(x)/ cos(x),\n−2\n5 ÷ −1\n6 = −2\n5 × −6\n1,\n3x = 210 ⇒x = 70,\n3\n13 −2\n13 = 1\n13,\ne2πi = (eπi)2 = (−1)2 = 1,\n√\n25 = 5,\n(n + 1) × (n −1)! = (n + 1) × n × (n −1)!\nn\n= (n + 1)!\nn\n.\nMathematical Texts (MT)\nWhile the previous dataset MF focuses exclusively on mathematical formu-\nlas, MT focuses on the relationship between mathematical formulas and natural language. Similarly to MF,\nMT is generated using the AMPS and ARQMath datasets, along with applying EquVG, which consistently\nchanges variable names across the text and prints the LATEX formulas in different ways. We only consider\ntexts containing at least five formulas. Questions and answers of ARQMath are treated as separate text,\nwhile the AMPS data is treated as a single text where question and hints are concatenated. We generate up\nto 100 additional versions for each suitable input.\nNamed Mathematical Formulas (NMF)\nThis dataset associates the name of a mathematical identity\nwith either its formula or a describing text. It is derived from NMFT by applying both, EquVG and FalseVG,\nresulting in diverse positive and negative pairs. This data could be used to train a classifier that predicts\nwhether a formula is a valid representation of an identity’s name, using a Next Sentence Prediction (NSP)-like\ntask (Devlin et al., 2019). In a typical NSP task, each positive pair is matched with a random negative pair,\nwhich changes when the positive pair is reused. To enhance training, we create an imbalanced dataset with\nfour times more negative than positive pairs. This allows for training where positive pairs remain unchanged\nacross epochs, while negative pairs vary between epochs (and remain challenging). With a maximum of four\nepochs, the model encounters unique negative pairs in each iteration. NMF originates from 71 mathematical\nidentities, each with multiple base versions used to generate up to 400k versions per identity. About 60%\nof the NMF entries are textual descriptions, and the rest are pure mathematical formulas. For 20 of the\n71 mathematical identities, fewer than 400k versions exist, as they offer fewer possibilities for generating\nversions, such as limited substitution options or fewer opportunities for creating randomized LATEX.\nMathematical Formula Retrieval (MFR)\nThis dataset consists of formula pairs, classified as either\nmathematical equivalent or not. It is constructed by pairing each true formula version from NMF with an\nequivalent version and four falsified versions of that identity, all randomly sourced from NMF. This approach\npreserves the positive-to-negative pair ratio while ensuring that negative pairs remain challenging. MFR can\nbe used to train a MIR system for querying relevant formulas based on a similar formula, like a NSP task.\n7\nConclusion\nMathematical formulas are essential to communicate complex and abstract concepts in various scientific fields.\nTo effectively encode the unique structure of mathematical language, specialized mathematical language\nmodels are required. We developed MAMUT, a framework based on SymPy (Meurer et al., 2017) that\ngenerates equivalent and falsified versions of LATEX formulas through parsing, substituting, possibly falsifying,\n12\n\n\nand printing again into LATEX format. MAMUT diversifies and expands datasets, as demonstrated by four\ngenerated large, high-quality datasets: MF, MT, NMF and MFR, all publicly available on Hugging Face2 (see\nTable 5). These datasets can be leveraged for further mathematical pre-training of language models utilizing\ntasks such as Masked Language Modeling (MLM) and Causal Language Modeling (CLM) to predict equation\nparts, an Next Sentence Prediction (NSP) variant that predicts if equations are equivalent, or contrastive\nlearning between positive and negative samples to learn equation embeddings.\nAcknowledgments\nThe authors gratefully acknowledge the computing time made available to them on the high-performance\ncomputer at the NHR Center of TU Dresden. This center is jointly supported by the Federal Ministry of\nEducation and Research and the state governments participating in the NHR3. This paper is based on work\nconducted during J.D.’s Master’s thesis at Dresden University of Technology. A.R. was a doctoral researcher\nat Dresden University of Technology during this time and was funded through the Azrieli international\npostdoctoral fellowship and the Ali Kaufman postdoctoral fellowship.\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\nAlmeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.\nGpt-4 technical report.\narXiv\npreprint arXiv:2303.08774, 2023.\nAkiko Aizawa and Michael Kohlhase. Mathematical Information Retrieval, pp. 169–185. Springer Singapore,\nSingapore, 2021. ISBN 978-981-15-5554-1. doi: 10.1007/978-981-15-5554-1_12. URL https://doi.org/\n10.1007/978-981-15-5554-1_12.\nIz Beltagy, Kyle Lo, and Arman Cohan. SciBERT: A pretrained language model for scientific text. In\nKentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP), pp. 3615–3620, Hong Kong, China, November 2019. Association\nfor Computational Linguistics. doi: 10.18653/v1/D19-1371. URL https://aclanthology.org/D19-1371/.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens\nWinter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language\nmodels are few-shot learners. In Proceedings of the 34th International Conference on Neural Information\nProcessing Systems, NIPS ’20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.\nWenjie Cai and Qiong Liu. Image captioning with semantic-enhanced features and extremely hard negative\nexamples. Neurocomputing, 413:31–40, 2020. ISSN 0925-2312. doi: https://doi.org/10.1016/j.neucom.\n2020.06.112. URL https://www.sciencedirect.com/science/article/pii/S0925231220311012.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word\nproblems. arXiv preprint arXiv:2110.14168, 2021. doi: 10.48550/arXiv.2110.14168.\nPankaj Dadure, Partha Pakray, and Sivaji Bandyopadhyay. Mathematical information retrieval: A review.\nACM Comput. Surv., 57(3), November 2024.\nISSN 0360-0300.\ndoi: 10.1145/3699953.\nURL https:\n//doi.org/10.1145/3699953.\nXuan-Quy Dao and Ngoc-Bich Le. Investigating the effectiveness of ChatGPT in mathematical reasoning\nand problem solving: Evidence from the vietnamese national high school graduation examination. arXiv\npreprint arXiv:2306.06331, abs/2306.06331, 2023. URL https://arxiv.org/abs/2306.06331.\n2https://huggingface.co/ddrg\n3https://www.nhr-verein.de/en/our-partners\n13\n\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidi-\nrectional transformers for language understanding. In North American Chapter of the Association for\nComputational Linguistics, 2019. URL https://api.semanticscholar.org/CorpusID:52967399.\nJingxuan Fan, Sarah Martinson, Erik Y Wang, Kaylie Hausknecht, Jonah Brenner, Danxian Liu, Nianli\nPeng, Corey Wang, and Michael P Brenner. HARDMath: A benchmark dataset for challenging problems\nin applied mathematics. arXiv preprint arXiv:2410.09988, 2024.\nMeng Fang, Xiangpeng Wan, Fei Lu, Fei Xing, and Kai Zou. Mathodyssey: Benchmarking mathematical\nproblem-solving skills in large language models using odyssey math data. arXiv preprint arXiv:2406.18321,\n2024.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, and Ming Zhou. CodeBERT: A pre-trained model for programming and natural\nlanguages.\nIn Trevor Cohn, Yulan He, and Yang Liu (eds.), Findings of the Association for Compu-\ntational Linguistics: EMNLP 2020, pp. 1536–1547, Online, November 2020. Association for Computa-\ntional Linguistics.\ndoi: 10.18653/v1/2020.findings-emnlp.139.\nURL https://aclanthology.org/2020.\nfindings-emnlp.139/.\nZheng Gong, Kun Zhou, Wayne Xin Zhao, Jing Sha, Shijin Wang, and Ji-Rong Wen. Continual pre-training\nof language models for math problem understanding with syntax-aware memory network. In Proceedings\nof the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\npp. 5923–5933, 2022.\nAndré Greiner-Petter, Abdou Youssef, Terry Ruas, Bruce R Miller, Moritz Schubotz, Akiko Aizawa, and\nBela Gipp. Math-word embedding in math search and semantic extraction. Scientometrics, 125:3017–3046,\n2020.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021.\nBat-Sheva Ilany, Bruria Margolin, et al. Language and mathematics: Bridging between natural language\nand mathematical language in solving problems in mathematics. Creative Education, 1(03):138, 2010.\nShinil Kim, Seon Yang, and Youngjoong Ko. Mathematical equation retrieval using plain words as a query.\nIn Proceedings of the 21st ACM international conference on Information and knowledge management, pp.\n2407–2410, 2012.\nGiovanni Yoko Kristianto, Goran Topic, and Akiko Aizawa. MCAT math retrieval system for NTCIR-12\nMathIR Task. In NTCIR, 2016.\nZenan Li, Zhi Zhou, Yuan Yao, Yu-Feng Li, Chun Cao, Fan Yang, Xian Zhang, and Xiaoxing Ma. Neuro-\nsymbolic data generation for math reasoning, 2024. URL https://arxiv.org/abs/2412.04857.\nWentao Liu, Hanglei Hu, Jie Zhou, Yuyang Ding, Junsong Li, Jiayi Zeng, Mengliang He, Qin Chen, Bo Jiang,\nAimin Zhou, et al. Mathematical language models: A survey. arXiv preprint arXiv:2312.07622, 2023.\nZimu Lu, Aojun Zhou, Houxing Ren, Ke Wang, Weikang Shi, Junting Pan, Mingjie Zhan, and Hongsheng\nLi. Mathgenie: Generating synthetic data with question back-translation for enhancing mathematical\nreasoning of llms. arXiv preprint arXiv:2402.16352, 2024.\nBehrooz Mansouri, Shaurya Rohatgi, Douglas W Oard, Jian Wu, C Lee Giles, and Richard Zanibbi. Tangent-\nCFT: An embedding model for mathematical formulas. In Proceedings of the 2019 ACM SIGIR interna-\ntional conference on theory of information retrieval, pp. 11–18, 2019.\nBehrooz Mansouri, Vít Novotn`y, Anurag Agarwal, Douglas W Oard, and Richard Zanibbi. Overview of\narqmath-3 (2022): Third clef lab on answer retrieval for questions on math. In International Conference\nof the Cross-Language Evaluation Forum for European Languages, pp. 286–310. Springer, 2022a.\n14\n\n\nBehrooz Mansouri, Douglas W. Oard, and Richard Zanibbi.\nContextualized formula search using math\nabstract meaning representation.\nIn Proceedings of the 31st ACM International Conference on In-\nformation & Knowledge Management, CIKM ’22, pp. 4329–4333, New York, NY, USA, 2022b. As-\nsociation for Computing Machinery.\nISBN 9781450392365.\ndoi:\n10.1145/3511808.3557567.\nURL\nhttps://doi.org/10.1145/3511808.3557567.\nAaron Meurer, Christopher P. Smith, Mateusz Paprocki, Ondřej Čertík, Sergey B. Kirpichev, Matthew\nRocklin, AMiT Kumar, Sergiu Ivanov, Jason K. Moore, Sartaj Singh, Thilina Rathnayake, Sean Vig,\nBrian E. Granger, Richard P. Muller, Francesco Bonazzi, Harsh Gupta, Shivam Vats, Fredrik Johansson,\nFabian Pedregosa, Matthew J. Curry, Andy R. Terrel, Štěpán Roučka, Ashutosh Saboo, Isuru Fernando,\nSumith Kulal, Robert Cimrman, and Anthony Scopatz. SymPy: symbolic computing in python. PeerJ\nComputer Science, 3:e103, January 2017.\nISSN 2376-5992.\ndoi: 10.7717/peerj-cs.103.\nURL https:\n//doi.org/10.7717/peerj-cs.103.\nVít Novotný and Michal Štefánik. Combining sparse and dense information retrieval. In Guglielmo Faggioli,\nNicola Ferro, Allan Hanbury, and Martin Potthast (eds.), Proceedings of the Working Notes of CLEF 2022,\npp. 104–118. CEUR-WS, 2022. URL http://ceur-ws.org/Vol-3180/paper-06.pdf.\nShuai Peng, Ke Yuan, Liangcai Gao, and Zhi Tang. Mathbert: A pre-trained model for mathematical formula\nunderstanding. ArXiv, abs/2105.00377, 2021. URL https://arxiv.org/abs/2105.00377.\nFelix Petersen, Moritz Schubotz, Andre Greiner-Petter, and Bela Gipp.\nNeural machine translation for\nmathematical formulae, 2023. URL https://arxiv.org/abs/2305.16433.\nRunqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue,\nShanglin Lei, Zhe Wei, Miaoxuan Zhang, et al. We-math: Does your large multimodal model achieve\nhuman-like mathematical reasoning? arXiv preprint arXiv:2407.01284, 2024.\nYao Qiu, Jinchao Zhang, Huiying Ren, and Jie Zhou. Challenging instances are worth learning: Generating\nvaluable negative samples for response selection training. arXiv preprint arXiv:2109.06538, 2021. URL\nhttps://arxiv.org/abs/2109.06538.\nAnja Reusch, Maik Thiele, and Wolfgang Lehner. Transformer-encoder and decoder models for questions\non math. In Conference and Labs of the Evaluation Forum, 2022. URL https://ceur-ws.org/Vol-3180/\npaper-07.pdf.\nAnja Reusch, Julius Gonsior, Claudio Hartmann, and Wolfgang Lehner. Investigating the usage of formulae\nin mathematical answer retrieval. In European Conference on Information Retrieval, pp. 247–261. Springer,\n2024.\nDavid Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning\nabilities of neural models. arXiv preprint arXiv:1904.01557, 2019.\nAlexander Scarlatos and Andrew Lan. Tree-based representation and generation of natural and mathematical\nlanguage.\nIn Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3714–\n3730, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.\nacl-long.205. URL https://aclanthology.org/2023.acl-long.205/.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan\nZhang, YK Li, Y Wu, et al.\nDeepseekmath: Pushing the limits of mathematical reasoning in open\nlanguage models. arXiv preprint arXiv:2402.03300, 2024.\nRuoqi Shen, Sébastien Bubeck, Ronen Eldan, Yin Tat Lee, Yuanzhi Li, and Yi Zhang. Positional description\nmatters for transformers arithmetic, 2023. URL https://arxiv.org/abs/2311.14737.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample.\nLlama: Open and efficient foundation language models, 2023.\nURL\nhttps://arxiv.org/abs/2302.13971.\n15\n\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30,\n2017.\nWeihao You, Shuo Yin, Xudong Zhao, Zhilong Ji, Guoqiang Zhong, and Jinfeng Bai. Mumath: Multi-\nperspective data augmentation for mathematical reasoning in large language models. In Findings of the\nAssociation for Computational Linguistics: NAACL 2024, pp. 2932–2958, 2024.\nRichard Zanibbi, Akiko Aizawa, Michael Kohlhase, Iadh Ounis, Goran Topic, and Kenny Davila. Ntcir-12\nmathir task overview. In NTCIR, 2016.\nRichard Zanibbi, Douglas W Oard, Anurag Agarwal, and Behrooz Mansouri. Overview of ARQMath 2020:\nCLEF lab on answer retrieval for questions on math. In Experimental IR Meets Multilinguality, Multimodal-\nity, and Interaction: 11th International Conference of the CLEF Association, CLEF 2020, Thessaloniki,\nGreece, September 22–25, 2020, Proceedings 11, pp. 169–193. Springer, 2020.\nRichard Zanibbi, Behrooz Mansouri, Anurag Agarwal, et al. Mathematical information retrieval: Search\nand question answering. Foundations and Trends® in Information Retrieval, 19(1-2):1–190, 2025.\nBo-Wen Zhang, Yan Yan, Lin Li, and Guang Liu. Infinity <scp>math:</scp> a scalable instruction tuning\ndataset in programmatic mathematical reasoning. In Proceedings of the 33rd ACM International Confer-\nence on Information and Knowledge Management, CIKM ’24, pp. 5405–5409. ACM, October 2024. doi:\n10.1145/3627673.3679122. URL http://dx.doi.org/10.1145/3627673.3679122.\nWei Zhong, Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin. One blade for one purpose: advancing\nmath information retrieval using hybrid search. In Proceedings of the 46th International ACM SIGIR\nConference on Research and Development in Information Retrieval, pp. 141–151, 2023.\nA\nOriginal Datasets\nIn Table 6, we present one version of each mathematical identity of NMFT, while this entire raw dataset\nis available on Hugging Face4 as part of the NMF dataset files. Subsequently, Table 7 provides an example\nentry of ARQMath (Mansouri et al., 2022a) from the Mathematical Stack Exchange, while Table 8 shows\nan example of Auxiliary Mathematics Problems and Solutions (AMPS) (Hendrycks et al., 2021).\nName\nFormula\nAddition Theorem for Cosine\n∀α, β ∈R : cos(α + β) = cos(α) cos(β) −sin(α) sin(β)\nAddition Theorem for Cosine\n∀α, β ∈R : cos(α + β) = cos(α) cos(β) −sin(α) sin(β)\nAddition Theorem for Sine\n∀α, β ∈R : sin(α + β) = sin(α) cos(β) + cos(α) sin(β)\nAddition Theorem for Tangent\n∀α, β ∈R : tan(α + β) =\ntan(α)+tan(β)\n1−tan(α) tan(β)\nAlternating Harmonic Series\nP∞\nn=1\n(−1)n+1\nn\n= 1 −1\n2 + 1\n3 −1\n4 + . . . = ln(2)\nBasel Problem\nP∞\nn=1\n1\nn2 =\n1\n12 + 1\n22 + 1\n32 + 1\n42 + 1\n52 + 1\n62 + . . . = π2\n6\nBayes’ Theorem\nP(A|B) = P(B|A)·P(A))\nP(B)\nBernouilli Inequality\n∀x ≥−1, ∀α > 1 ⇒(1 + x)α ≥1\nBinomial Coefficient Formula\n∀n, k ∈N, n ≥k :\n\u0000n\nk\n\u0001\n=\nn!\nk!(n−k)!\nBinomial Distribution\nP(X = k) =\n\u0000n\nk\n\u0001\npk · (1 −p)n−k\nTable 6: The 71 mathematical identities of the NMFT dataset.\n4https://huggingface.co/datasets/ddrg/named_math_formulas/blob/main/data.json\n16\n\n\n(Continued from previous page)\nName\nFormula\nBinomial Series\n∀α, x ∈C > 0 : |x| < 1 ⇒(1 + x)α = P∞\nk=0\n\u0000α\nk\n\u0001\nxk\nBinomial Theorem\n∀a, b ∈R∀n ∈N : (a + b)n = Pn\nk=0\n\u0000n\nk\n\u0001\nan−kbk\nChain Rule\nd\ndx [f(g(x))] = f ′(g(x)) · g′(x)\nComplex Number Division\n∀a, b, c, d ∈R : a+bi\nc+di = (ac+bd)+(bc−ad)i\nc2+d2\nComplex Number Inverse\n∀z ∈C : z = a + bi ⇒z−1 =\na\na2+b2 −\nb\na2+b2 i\nComplex Number Multiplication\n∀a, b, c, d ∈R : (a + bi) · (c + di) = (ac −bd) + (ad + bc)i\nComplex Number Sum\n∀a, b, c, d ∈R : (a + bi) + (c + di) = (a + c) + (b + d)i\nCosine Function Definition\n∀x ∈R : cos(x) = P∞\nn=0\n(−1)n\n(2n)! x2n\nCovariance\nCov[X, Y ] = E[(X −E[X])(Y −E[Y ])]\nDe Morgan Law\n∀x, y : ¬(x ∧y) = ¬x ∨¬y\nDerivative of Inverse Function\nd\ndx\n\u0002\nf −1(x)\n\u0003\n=\n1\nf ′(f −1(x))\nDerivative of a Function\nf ′(x) = limh→0\nf(x+h)−f(x)\nh\nDeterminant of 2x2 Matrix\ndet ( a b\nc e ) = a · e −b · c\nDeterminant of 3x3 Matrix\ndet\n\u0012\na b c\nd e f\ng h j\n\u0013\n= a · det\n\u0010\ne f\nh j\n\u0011\n−b · det\n\u0010\nd f\ng j\n\u0011\n+ c · det\n\u0000 d e\ng h\n\u0001\nDistributive Law of Sets\nA ∪(B ∩C) = (A ∪B) ∩(A ∪C)\nEuler’s Formula\n∀α ∈C : eiα = cos(α) + i sin(α)\nEuler’s Formula for Polyhedra\nV −E + F = 2\nEuler’s Identity\neiπ + 1 = 0\nEuler’s Number\ne = limn→∞(1 + 1/n)n\nExpected Value\nE(X) = Pn\ni=1 xiP(X = xi)\nExponential Function\n∀x ∈R : limn→∞(1 + x/n)n = ex\nFactorial\n∀n ∈N : n! = 1 · 2 · 3 · 4 · . . . · n\nFirst Binomial Formula\n∀a, b ∈R : (a + b)2 = a2 + 2ab + b2\nFundamental Theorem of Calculus\nR b\na f(x) dx = F(b) −F(a)\nGamma Function\n∀n ∈N : Γ(n) =\nR ∞\n0\nxn−1e−xdx = (n −1)!\nGaussian Integral\nR ∞\n−∞exp(−x2)dx = √π\nGeometric Series\nP∞\nn=0 rn =\n1\n1−r\nGregory-Leibniz Series\nP∞\nn=0(−1)n ·\n1\n2n+1 = π\n4\nHarmonic Series\nP∞\nn=1\n1\nn = ∞\nHölder Inequality\n∀p, q > 1, 1\np + 1\nq = 1, ∀x, y ∈Rn\n⇒Pn\ni=1 |xiyi| ≤(Pn\ni=1 |xi|p)\n1\np · (Pn\ni=1 |yi|q)\n1\nq\nIntegration by Parts\nR\nf(x)g′(x) dx = f(x)g(x) −\nR\ng(x)f ′(x) dx\nInverse of 2x2 Matrix\n\u0000 a b\nc d\n\u0001−1 =\n1\nad−bc\n\u0000 d\n−b\n−c a\n\u0001\nLaw of Cosines\nc2 = a2 + b2 −2ab cos(C)\nTable 6: The 71 mathematical identities of the NMFT dataset.\n17\n\n\n(Continued from previous page)\nName\nFormula\nLaw of Large Numbers\nlimn→∞1\nn\nPn\ni=1 xi = [E](X)\nLaw of Sines\nsin(A)\na\n= sin(B)\nb\n= sin(C)\nc\nLaw of Total Probability\nP(A) = Pn\ni=1 P(A|Bi)P(Bi)\nLogarithm Power Rule\n∀b ∈R, b > 0, b ̸= 1, ∀x, r ∈R, x > 0 : logb(xr) = r · logb(x)\nLogarithm Product Rule\n∀b ∈R, b > 0, b ̸= 1, ∀x, y > 0 : logb(xy) = logb(x) + logb(y)\nLogarithm Quotient Rule\n∀b ∈R, b > 0, b ̸= 1, ∀x, y > 0 : logb(x/y) = logb(x) −logb(y)\nMinkowski Inequality\n∀p > 1 ⇒Pn\ni=1 |xi + yi|\n1\np ≤(Pn\ni=1 |xi|p)\n1\np + (Pn\ni=1 |yi|p)\n1\np\nMultiplication of 2x2 Matrix\nA =\n\u0000 a b\nc d\n\u0001\n, B =\n\u0010\ne f\ng h\n\u0011\n⇒A · B =\n\u0010\nae+bg af+bh\nce+dg cf+dh\n\u0011\nNormal Distribution\nf(x) =\n1\nσ\n√\n2πe−(x−µ)2/(2σ2)\nPascal’s Rule\n∀n, k ∈N :\n\u0000n+1\nk+1\n\u0001\n=\n\u0000 n\nk+1\n\u0001\n+\n\u0000n\nk\n\u0001\nPoisson Distribution\nP(X = k) = e−λλk\nk!\nPower Rule\n∀n ∈R, n ̸= 0 :\nd\ndx (xn) = nxn−1\nPrinciple of Inclusion-Exclusion\n|A ∪B| = |A| + |B| −|A ∩B|\nProduct Rule\nd\ndx[u(x) · v(x)] = u′(x) · v(x) + u(x) · v′(x)\nPythagorean Identity\n∀α ∈R : sin2(α) + cos2(α) = 1\nPythagorean Theorem\na2 + b2 = c2\nQuadratic Formula\n∀a, b, c ∈R, a ̸= 0 : a · x2 + b · x + c = 0 ⇒x1,2 = −b±\n√\nb2−4ac\n2a\nQuotient Rule\n∀b ∈R, b > 0, b ̸= 1, ∀x, y > 0 : logb(x/y) = logb(x) −logb(y)\nRiemann Zeta Function\n∀z ∈C, Re(z) > 1 : ζ(z) = P∞\nn=1\n1\nnz\nRule de l’Hôpital\nlimx→a\nf(x)\ng(x) = limx→a\nf ′(x)\ng′(x)\nSecond Binomial Formula\n∀a, b ∈R : (a −b)2 = a2 −2a · b + b2\nSine Function Definition\n∀x ∈R : sin(x) = P∞\nn=0(−1)n/(2n + 1)!x2n+1\nStirling Approximation\n∀n ∈N : n! ≈\n√\n2πn\n\u0000 n\ne\n\u0001n\nTaylor Series\nf(x) = P∞\nn=0\nf (n)(a)\nn!\n(x −a)n\nThird Binomial Formula\n∀a, b ∈R : (a + b)(a −b) = a2 −b2\nVariance\nVar[X] = E\n\u0002\n(X −E[X])2\u0003\nWallis Product\nQ∞\nn=1\n4n2\n4n2−1 = π\n2\nYoung Inequality\n∀p, q > 1, 1/p + 1/q = 1, ∀a, b ≥0 ⇒ab ≤ap\np + bq\nq\npq Formula\n∀p, q ∈R : x2 + px + q = 0 ⇒x1,2 = −p\n2 ±\nq\np2\n4 −q\nTable 6: The 71 mathematical identities of the NMFT dataset.\nB\nMAMUT\nTable 9 shows examples of the strategies for generating falsified formulas of FalseVG. Table 10 reports the\nused symbol groups for the symbol substitution of EquVG.\n18\n\n\nTitle\nDerivative of sigmoid function σ(x) =\n1\n1+e−x\nQuestion\nIn my AI textbook there is this paragraph, without any explanation.\nThe sigmoid\nfunction is defined as follows: “σ(x) =\n1\n1+e−x .\nThis function is easy to differentiate\nbecause dσ(x)\nd(x) = σ(x) · (1 −σ(x)).“ It has been a long time since I’ve taken differential\nequations, so could anyone tell me how they got from the first equation to the second?\nAnswer 1\nConsider f(x) =\n1\nσ(x) = 1 + e−x . Then, on the one hand, the chain rule gives f ′(x) =\nd\ndx\n\u0012\n1\nσ(x)\n\u0013\n= −σ′(x)\nσ(x)2 , and on the other hand, f ′(x) =\nd\ndx\n\u00001 + e−x\u0001\n= −e−x = 1 −f(x) =\n1 −\n1\nσ(x) = σ(x)−1\nσ(x) . Equate the two expressions, and voilà!\nAnswer 2\nLet’s denote the sigmoid function as σ(x) =\n1\n1 + e−x . The derivative of the sigmoid is\nd\ndxσ(x) = σ(x)(1 −σ(x)) Here’s a detailed derivation:\nd\ndxσ(x) = d\ndx\n\u0014\n1\n1 + e−x\n\u0015\n= d\ndx\n\u00001 + e−x\u0001−1\n= −(1 + e−x)−2(−e−x)\n=\ne−x\n(1 + e−x)2\n=\n1\n1 + e−x ·\ne−x\n1 + e−x\n=\n1\n1 + e−x · (1 + e−x) −1\n1 + e−x\n=\n1\n1 + e−x ·\n\u00121 + e−x\n1 + e−x −\n1\n1 + e−x\n\u0013\n=\n1\n1 + e−x ·\n\u0012\n1 −\n1\n1 + e−x\n\u0013\n= σ(x) · (1 −σ(x))\nTable 7: Example entry of the ARQMath dataset with preserved LATEX formatting (post ID 78575, answer\nIDs 78578 and 1225116).\nProblem\nSimplify the following expression: y = p2 −3p −54\np −9\nAnswer/\nHints\nFirst factor the polynomial in the numerator. p2−3p−54 = (p−9)(p+6). So we can rewrite\nthe expression as: y = (p −9)(p + 6)\np −9\n. We can divide the numerator and denominator by\n(p −9) on condition that p ̸= 9. Therefore y = p + 6; p ̸= 9.\nTable 8: Example entry of the AMPS dataset (file amps/khan/504/1607900679.json).\n19\n\n\nOriginal Formula\nFalsified Formula\nDescription\nEquality\na2 + b2 = c2\na2 + b2 = c2 −1\nSubtracted 1 from right side\na2 = c2\nRemoved b2\na2 + b2+x = c2\nInserted +x in exponent of b2\nInequality\nx > y\nx ≤y\nInverted > to ≤\nab ≤a2 + b2\n2\nab > a2 + b2\n2\nInverted ≤to >\nx ̸= 0\nx = 0\nInverted ̸= to =\nSwap\na2 + b2 = c2\na2 + 2b = c2\nSwapped b and 2 in b2\nF(a) −F(b)\nF(b) −F(a)\nSwapped order of arguments\nln\n\u0012x\ny\n\u0013\n= ln(x) −ln(y)\nln\n\u0012x\ny\n\u0013\n= sin(x) −ln(y)\nReplaced ln by sin in ln(x)\nsin(α)\na\n= sin(β)\nb\nlog(α)\na\n= sin(β)\nb\nReplaced sin by log in sin(α)\nVariable\nn! = 1 · 2 · . . . · n\nk! = 1 · 2 · . . . · n\nReplaced n by k in n!\nn\nX\ni=1\ni2\nn\nX\ni=1\nk2\nReplaced i by k only in i2\nConstant\neiπ = −1\n3iπ = −1\nReplaced e by 3\ne1π = −1\nReplaced i by 1\neie = −1\nReplaced π by e\n42iπ = −1\nReplaced e by 42\n∞\nX\ni=1\n1\ni2 = π2\n6\nn\nX\ni=1\n1\ni2 = π2\n6\nReplaced ∞by n\nDistribute\nsin(x) + sin(y)\nsin(x + y)\nApplied sine additivity\n\u0012n\nk\n\u0013\n=\nn!\nk!(n −k)!\n\u0012n\nk\n\u0013\n=\nn!\n(k · (n −k))!\nApplied faculty multiplicity\n\u0012n\nk\n\u0013\n=\nn!\nk! · (n! −k!)\nApplied faculty multiplicity\nManual\n∀n ∈N : n! = . . .\n∀n ∈R : n! = . . .\nReplaced N by R\na2 + b2 = c2\na2 = b2 + c2\n−2bc cos(α)\nSimilar formula\nIn any right-\nIn any right-angled\nReplaced “triangle” by\nangled triangle . . .\nsquare . . .\n“square”\nRandom\na2 + b2 = c2\nsin2(α) + cos2(α) = 1\nRandom formula\nIn any right-\nThe derivative of a\nRandom text\nangled triangle . . .\nfunction f is . . .\nTable 9: Examples of the strategies for generating falsified formulas (FalseVG).\n20\n\n\nSymbol Groups\nTypical Context\nExample\nVariables\na, b, c, d, e, f, g, h\nParameters\nax2 + bx + c = 0\ni, j, k, l\nIndices\nCij = P\nk AikBkj\nk, l, m, n\nCounts\n\u0000n\nk\n\u0001\n=\nn!\nk!(n−k)!\np, q, r, s, t\nParameters, Points\nx2 + px + q = 0\nu, v, w\nVectors\nu × v = w\nx, y, z\nUnknowns\nx + 2y + 3z = 4\nA, B, C, D, E, F, G, H\nMatrices, Sets\nA ∪(B ∩C) = (A ∪B) ∩(A ∪C)\nQ, R, S, T, U, V, W, X, Y, Z\nRandom Variables\nX = Y −Z\nα, β, γ, δ, θ, ϑ, ψ, ϕ, φ, ρ\nAngles\nα + β + γ = 180◦\nτ, σ, λ, µ, ν\nScalars\nλ ( x1\nx2 ) + µ ( y1\ny2 ) = 0\nFunctions\nf, g, h, u, v\nGeneric Functions\n[uv]′ = u′v + uv′\nF, G, H, U, V\nAntiderivative\nR b\na f(x)dx = F(b) −F(a)\nτ, σ, λ, µ, ν\nPermutations\nσ ◦(τ ◦µ) = (σ ◦τ) ◦µ\nTable 10: Defined symbol groups for the symbol substitution of MAMUT.\nB.1\nImplementation\nAs discussed in Section 6, the MAMUT relies on the EquVG and FalseVG algorithms, which generate\nequivalent or falsified versions of a given formula. We implemented these algorithms using the Python li-\nbrary SymPy 1.12, which is an open-source symbolic mathematics library with computer algebra system\nfeatures (Meurer et al., 2017). This library includes a LATEX parser for converting expressions into an in-\nternal SymPy representation, which can then be printed again into the LATEX format. The SymPy formula\nrepresentation is a symbolic expression, as required for EquVG and FalseVG, and supports the substitution\nof variables and generic functions. However, the built-in SymPy parser had limitations in handling various\nmathematical notations. The parsing capability has been expanded during this work, including the parsing\nof matrices, sets, derivatives, and various operators (±, ∪, ∩, E[X], Var[X], . . . ). Additionally, the SymPy\nLATEX parsing was expanded to support a wider range of mathematical expressions through the implemen-\ntation of an adaptive hybrid approach. This approach introduces a SymPy-like expression that enables safe\nstring-based substitutions. As discussed earlier, a naive string replacement is inadequate for mathematical\nsymbol substitution. For instance, if we replace x in \\exp{x}, it would also unintentionally replace the occur-\nrence of x within \\exp. To address this issue, our implementation of the safe string-based substitution detects\nsuch situations resulting in a failure to avoid invalid expressions during the generation of versions ensuring\nhigh data quality. This SymPy-like expression also utilizes a predefined list of known symbols and LATEX\ncommands that, when present in the input, are excluded from substituting. For example, the \\exp command\nis included in this list, allowing \\exp{x} to be substituted using our safe string-based approach. This method,\nwhile being less powerful than the classical SymPy expressions, extends substitution support to a wide range\nof mathematical notations that can not be parsed in the classical parser. Hence, the hybrid combination\nof the classical SymPy expression with randomized printing and the string-based substitution, supporting\na wider range of operators, aligns perfectly with our need to create a diverse, high-quality mathematical\ndataset with substituted symbols.\nIn addition, our SymPy parser implementation is adaptive. Even if an input formula can not be parsed classi-\ncally, the classical parsing still succeeds for parts of it. As a result, formulas are split at delimiter symbols such\nas : or \\Rightarrow. Using these extended parsing capabilities, the input \\forall x, y: x\\cdot y=y\\cdot x\nis parsed into two sub-expressions: \\forall x, y, which can not be parsed classically with the used imple-\nmentation, and x\\cdot y=y\\cdot x, which is parsed into a classical SymPy expression. Both sub-expressions\nsupport the substitution of x and y. This results in, for instance, \\forall \\alpha, a: \\alpha\\times a=a\\\n21\n\n\ntimes \\alpha, where randomized printing was incorporated for the right subexpression. Similarly, support\nfor parsing entire texts containing formulas enclosed within dollar symbols, denoting the LATEX mathematical\ninline mode, is integrated into the SymPy parser. To create randomized LATEX formulas from the parsed\nSymPy expressions, the SymPy LATEX printer has been enhanced to support randomized decisions. The\nprinting process is guided by randomized settings5, which define all the randomized decisions the printer\ncould make. The modified SymPy code is accessible in a forked repository on GitHub6, providing a simple\ninterface for generating equivalent and falsified versions of a formula. Additionally, the generation code for\nthe generated datasets based on AMPS, ARQMath, and NMFT is publicly available7, including the logic\nfor base formula filtering, extraction, and validation.\nB.2\nGenerated Datasets\nTo illustrate the behavior of MAMUT and the data extraction process, we provide artificial examples for\neach generated dataset based on the previously shown raw data: MF in Table 11, MT in Table 12, NMF in\nTable 13, and MFR in Table 14. Please note that not all examples are verified as part of the actual generated\ndatasets, but are selected to illustrate the diversity of MAMUT.\nB.3\nAnalysis of NMF\nFor a better understanding of the version generation algorithms, EquVG and FalseVG, we delve into a more\ndetailed analysis of the generated NMF dataset, visualized in Figure 5.\nFigure 5a shows the distribution of strategies over the mathematical identities of the NMF dataset. The\nfigure illustrates the proportions of how many falsified versions of a mathematical identity utilized a particular\nstrategy. Since multiple strategies might be applied to generate a single falsified version, the proportions\ndo not sum up to 100% per identity. Approximately half of the time, only a single strategy is applied.\nIn general, the different strategies obviously have different proportions across the mathematical identities.\nThe most common strategies are Variable and Swap because variables and swappable expressions (e.g.,\nsin(x + y) →sin(x) + sin(y)) occur in almost all formulas, often even multiple times. About 20% of the\nstrategies are intentionally completely random to avoid introducing a bias towards challenging negative\nexamples. In real-world MIR applications, random pairs are more common than challenging ones. Some\nstrategies can not be applied to certain identities, particularly the strategy Inequality, which is not applicable\nto most identities. The reason why some identities containing no inequalities still have a nonzero proportion\nfor the strategy Inequality is that this strategy can be applied even after a random or manual formula\n(containing an inequality) is chosen as a falsified version.\nAnother analysis can be deducted from Figure 5b, which shows the distribution of whether a variable, function\nor any of them has been replaced in a generated version of a mathematical identity in the NMF dataset.\nSince only ten identities contain generic function symbols, substitutions of functions can only be applied\nto those identities regularly. Again, we recognize proportions slightly above zero for many identities not\ncontaining generic functions due to function substitutions after applying the strategies Random or Manual.\nThe overall proportion of substituted formulas is 52.3%, but when considering only equivalent versions, this\nproportion rises to 81.3%. For falsified versions, the substitution proportion is about 45.1%.\n5https://github.com/jdrechsel13/sympy-random-LaTeX/blob/master/sympy/settings.py\n6https://github.com/jdrechsel13/sympy-random-LaTeX\n7https://github.com/aieng-lab/math-mutator\n22\n\n\nFormula\n1 −\n1\nσ(x) = σ(x)−1\nσ(x)\n−1\nτ(y) + 1 =\n1\nτ(y) · (−1 + τ(y))\n1\nν(x) ∗(ν(x) + (−1)) = 1 −1/ν (x)\n(λ(x) + (−1))/λ (x) = 1 −1/λ(x)\n1 −1/ν(x) = ((−1) + ν(x))/ν(x)\n−1/µ(x) + 1 =\n1\nµ(x) · (µ(x) + (−1))\nλ(x) + (−1))/λ(x) = 1 −\n1\nλ(x)\np2 −3p −54 = (p −9)(p + 6)\n(p + 9 · (−1)) · (6 + p) = p2 −p · 3 −54\np2 −3 · p + 54 · (−1) = (9 · (−1) + p) · (p + 6)\n(p + 6) · (−9 + p) = 54 · (−1) + p2 −p · 3\n(p −9)(p + 6) = p ∗p −p ∗3 + 54(−1)\nTable 11: Example entries for MF (based on Table 7 and Table 8).\nText\nIn my AI textbook there is this paragraph, without any explanation. The sigmoid function is defined as\nfollows: “σ(x) =\n1\n1+e−x . This function is easy to differentiate because dσ(x)\nd(x) = σ(x) · (1 −σ(x)).“ It has\nbeen a long time since I’ve taken differential equations, so could anyone tell me how they got from the\nfirst equation to the second?\nIn my AI textbook there is this paragraph, without any explanation. The sigmoid function is defined as\nfollows: “τ(y) = 1/(e−y + 1). This function is easy to differentiate because τ(y)(−τ(y) + 1) = τ ′(y).“ It\nhas been a long time since I’ve taken differential equations, so could anyone tell me how they got from\nthe first equation to the second?\nConsider f(x) =\n1\nσ(x) = 1+e−x . Then, on the one hand, the chain rule gives f ′(x) =\nd\ndx\n\u0012\n1\nσ(x)\n\u0013\n= −σ′(x)\nσ(x)2 ,\nand on the other hand, f ′(x) =\nd\ndx\n\u00001 + e−x\u0001\n= −e−x = 1 −f(x) = 1 −\n1\nσ(x) = σ(x)−1\nσ(x) . Equate the two\nexpressions, and voilà!\nConsider u(y) = 1/σ(y) = 1 + e−y . Then, on the one hand, the chain rule gives\nd\ndyu (y) =\nd\ndy\n1\nσ(y) =\n−\n1\nσ2(y)\nd\ndyσ(y), and on the other hand, u′(y) =\nd\ndx\n\u00001 + e−y\u0001\n= −e−y = 1 −u(y) = 1 −\n1\nσ(y) = σ(y)−1\nσ(y) .\nEquate the two expressions, and voilà!\nSimplify the following expression: y = p2 −3p −54\np −9\nFirst factor the polynomial in the numerator. p2 −\n3p −54 = (p −9)(p + 6). So we can rewrite the expression as: y = (p −9)(p + 6)\np −9\n. We can divide the\nnumerator and denominator by (p −9) on condition that p ̸= 9. Therefore y = p + 6; p ̸= 9.\nSimplify the following expression:\n−54+s2−s·3\ns−9\n= z First factor the polynomial in the numerator. s ∗s −\n3 ∗s −54 = (s −9) ∗(6 + s). So we can rewrite the expression as: z =\n1\ns−9 × (s −9) × (6 + s). We can\ndivide the numerator and denominator by −9 + s on condition that s ̸= 9. Therefore z = s + 6; s ̸= 9.\nTable 12: Example entries of MT (based on Table 7 and Table 8).\n23\n\n\nName\nFormula\nLabel\nFactorial\nd! = 1 · 2 · 3 · 4 · 5 · . . . · d\n✓\nDefinition of a factorial\n∀n ∈N : n! = Qξ\ni=1 i\n✗\nDefinition of a factorial\n∀n ∈N : (n + 1)! = (n + n) · n! ∧0! = 1\n✗\nDefinition of a factorial\nFor any natural number k we have k! is defined as k := Qk\nj=1 j.\n✗\nFactorial\nFor any natural number n, n! can be obtained by multiplying all nat-\nural numbers from 1 to Y together.\n✗\nDefinition of a factorial\n∀n, j ∈N, n ≥j :\n\u0000n\nj\n\u0001\n=\n1\nj!·(n−j)! · n!\n✗\nFactorial\n1 · 2 · 3 · 1\n4 . . . n = n!\n✗\nFactorial\n∀m ≥1 : m! = m · (m + (−1))!, 0! = 0\n✗\nDefinition of a factorial\n1 ∗2 ∗3 ∗4 . . . x = x!\n✓\nDefinition of a factorial\nk! = (1 −3) · 18 · 4 · 5/ · · · · n\n✗\nFactorial\nn! = Pn\ni=1 i\n✗\nFactorial\nThe sum of two complex numbers g1 + i · h = z and g2 + i · f = w is\ndefined as g1 + g2 + i ∗(h + f) = w + z.\n✗\nDefinition of a factorial\nθ! = 1 · 2 · ... · θ\n✓\nTable 13: Example entries of NMF (based on Table 4).\nFormula 1\nFormula 2\nLabel\nThe value of (1+1/τ)τ approaches the constant\ne as τ tends to infinity.\nAs µ approaches infinity, the expression (1 +\n1/µ)µ converges to the value of e ≈2.718.\n✓\nBy\nutilizing\nthe\ninfinite\nseries\nP∞\nn=0 z1+2n (−1)n\n(1+2n)!,\nwe\ncan\ndefine\nsin(z)\nfor all real numbers z.\nFor all real numbers x the expression sin(z)\nis defined as the infinite sum P∞\nn=0 x2·n+1 ·\n(2 · n + 1)! · (−1)−n.\n✗\nThe limit as l approaches infinity of the expres-\nsion\n\u00001 + 1\nl · y\n\u0001l converges to the exponential\nfunction ey for any real number y.\n∀x ∈C : ex = P∞\nk=0 −kx/k! = 1 + x + x2/2! +\nx ∗x2/3! + ...\n✗\nFor all real positive g with g ̸= 1 and for all\nreal positive s, y, we have logb(sy) = logb(s) +\nlogb(y).\nFor all real bases b such that 0 < b and\nb ̸= 1 and for all real positive z, y, the equality\nlogb(z/y) = logb(z) −logb(y) holds.\n✗\nThe derivative of a composite function f (g(z))\nwith respect to z is given by\nd\ndg(z)f(g (z)) ·\nd\ndzg(z).\nThe\nderivative\nof\na\ncomposite\nfunction\nf(g(y))\nwith\nrespect\nto\ny\nis\ngiven\nby\nd\ndg(u)f(g(u))/( d\ndug(u)).\n✗\n∀m ≥1 : m! = m · (m + (−1))!, 0! = 1\n∀a ∈N : (a + 1)! = (a + 1) · a!, 0! = 1\n✓\nLet c and b be real numbers. In this case, (c +\nb)(−b + c) is equal to c2 −b2.\n1\nb −b(a + b) = −b2 + a1\n✗\nTable 14: Example entries of MFR.\n24\n\n\nAddition Theorem for Cosine\nAddition Theorem for Sine\nAddition Theorem for Tangent\nAlternating Harmonic Series\nBasel Problem\nBayes’ Theorem\nBernouilli Inequality\nBinomial Coefﬁcient Formula\nBinomial Distribution\nBinomial Series\nBinomial Theorem\nChain Rule\nComplex Number Division\nComplex Number Inverse\nComplex Number Multiplication\nComplex Number Sum\nCosine Function Deﬁnition\nCovariance\nDe Morgan Law\nDerivative of Inverse Function\nDerivative of a Function\nDeterminant of 2x2 Matrix\nDeterminant of 3x3 Matrix\nDistributive Law of Sets\nEulers Formula\nEulers Formula for polyhedra\nEulers Identity\nEulers Number\nExpected Value\nExponential Function\nFactorial\nFirst Binomial Formula\nFundamental Theorem of Calculus\nGamma Function\nGaussian Integral\nGeometric Series\nGregory-Leibniz Series\nHarmonic Series\nH¨older Inequality\nIntegration by Parts\nInverse of 2x2 Matrix\nLaw of Cosines\nLaw of Large Numbers\nLaw of Sines\nLaw of Total Probability\nLogarithm Power Rule\nLogarithm Product Rule\nLogarithm Quotient Rule\nMinkowski Inequality\nMultiplication of 2x2 Matrix\nNormal Distribution\nPascal’s Rule\nPoisson Distribution\nPower Rule\nPrinciple of Inclusion-Exclusion\nProduct Rule\nPythagorean Identity\nPythagorean Theorem\nQuadratic Formula\nQuotient Rule\nRiemann Zeta Function\nRule de l’Hopital\nSecond Binomial Formula\nSine Function Deﬁnition\nStirling Approximation\nTaylor Series\nThird Binomial Formula\nVariance\nWallis Product\nYoung Inequality\npq Formula\nMathematical Identity\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nProportion\nEquality\nInequality\nSwap\nVariable\nRandom\nConstant\nDistribute\nManual\n(a) Proportions of strategies used for generating falsified versions in NMF dataset per mathematical identity.\nAddition Theorem for Cosine\nAddition Theorem for Sine\nAddition Theorem for Tangent\nAlternating Harmonic Series\nBasel Problem\nBayes’ Theorem\nBernouilli Inequality\nBinomial Coefﬁcient Formula\nBinomial Distribution\nBinomial Series\nBinomial Theorem\nChain Rule\nComplex Number Division\nComplex Number Inverse\nComplex Number Multiplication\nComplex Number Sum\nCosine Function Deﬁnition\nCovariance\nDe Morgan Law\nDerivative of Inverse Function\nDerivative of a Function\nDeterminant of 2x2 Matrix\nDeterminant of 3x3 Matrix\nDistributive Law of Sets\nEulers Formula\nEulers Formula for polyhedra\nEulers Identity\nEulers Number\nExpected Value\nExponential Function\nFactorial\nFirst Binomial Formula\nFundamental Theorem of Calculus\nGamma Function\nGaussian Integral\nGeometric Series\nGregory-Leibniz Series\nHarmonic Series\nH¨older Inequality\nIntegration by Parts\nInverse of 2x2 Matrix\nLaw of Cosines\nLaw of Large Numbers\nLaw of Sines\nLaw of Total Probability\nLogarithm Power Rule\nLogarithm Product Rule\nLogarithm Quotient Rule\nMinkowski Inequality\nMultiplication of 2x2 Matrix\nNormal Distribution\nPascal’s Rule\nPoisson Distribution\nPower Rule\nPrinciple of Inclusion-Exclusion\nProduct Rule\nPythagorean Identity\nPythagorean Theorem\nQuadratic Formula\nQuotient Rule\nRiemann Zeta Function\nRule de l’Hopital\nSecond Binomial Formula\nSine Function Deﬁnition\nStirling Approximation\nTaylor Series\nThird Binomial Formula\nVariance\nWallis Product\nYoung Inequality\npq Formula\nMathematical Identity\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nProportion\nVariable Substitution\nFunction Substitution\nSymbol Substitution\n(b) Proportion of generated (equivalent and falsified) versions with at least one variable, function, or symbol substi-\ntution (variable or function).\nFigure 5: Analysis of NMF. The mean across all identities is printed as a solid line.\n25\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20855v1.pdf",
    "total_pages": 25,
    "title": "MAMUT: A Novel Framework for Modifying Mathematical Formulas for the Generation of Specialized Datasets for Language Model Training",
    "authors": [
      "Jonathan Drechsel",
      "Anja Reusch",
      "Steffen Herbold"
    ],
    "abstract": "Mathematical formulas are a fundamental and widely used component in various\nscientific fields, serving as a universal language for expressing complex\nconcepts and relationships. While state-of-the-art transformer models excel in\nprocessing and understanding natural language, they encounter challenges with\nmathematical notation, which involves a complex structure and diverse\nrepresentations. This study focuses on the development of specialized training\ndatasets to enhance the encoding of mathematical content. We introduce Math\nMutator (MAMUT), a framework capable of generating equivalent and falsified\nversions of a given mathematical formula in LaTeX notation, effectively\ncapturing the mathematical variety in notation of the same concept. Based on\nMAMUT, we have generated four large mathematical datasets containing diverse\nnotation, which can be used to train language models with enhanced mathematical\nembeddings.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}