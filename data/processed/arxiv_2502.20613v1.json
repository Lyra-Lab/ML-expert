{
  "id": "arxiv_2502.20613v1",
  "text": "Continuous Adversarial Text Representation\nLearning for Affective Recognition\n1st Seungah Son\nCCS Graduate School of Mobility\nKorea Advanced Institute of\nScience and Technology (KAIST)\nDaejeon, Rep. of Korea, 34141\nseungahson@kaist.ac.kr\n2nd Andres Saurez\nRobotics Program\nKorea Advanced Institute of\nScience and Technology (KAIST)\nDaejeon, Rep. of Korea, 34141\nasaurez@kaist.ac.kr\n3rd Dongsoo Har\nCCS Graduate School of Mobility\nKorea Advanced Institute of\nScience and Technology (KAIST)\nDaejeon, Rep. of Korea, 34141\ndshar@kaist.ac.kr\nAbstract—While pre-trained language models excel at semantic\nunderstanding, they often struggle to capture nuanced affective\ninformation critical for affective recognition tasks. To address\nthese limitations, we propose a novel framework for enhancing\nemotion-aware embeddings in transformer-based models. Our\napproach introduces a continuous valence-arousal labeling system\nto guide contrastive learning, which captures subtle and multi-\ndimensional emotional nuances more effectively. Furthermore,\nwe employ a dynamic token perturbation mechanism, using\ngradient-based saliency to focus on sentiment-relevant tokens,\nimproving model sensitivity to emotional cues. The experimental\nresults demonstrate that the proposed framework outperforms\nexisting methods, achieving up to 15.5% improvement in the\nemotion classification benchmark, highlighting the importance\nof employing continuous labels. This improvement demonstrates\nthat the proposed framework is effective in affective represen-\ntation learning and enables precise and contextually relevant\nemotional understanding.\nIndex Terms—Technology for affective computing, Knowledge\nRepresentation Formalisms and Methods, Sentiment Analysis\nI. INTRODUCTION\nAffective recognition has gained significant attention in the\nfield of natural language processing (NLP), with a wide range\nof applications, including human-computer interaction, recom-\nmendation systems, and management information systems [1],\n[2]. To date, as a subfield of affective recognition, emotion\nrecognition has been investigated with flat or hierachical\nsensor networks [3], [4]. There is a growing interest in making\nhuman-device interactions more natural and refined through\nAI-driven technologies. This emphasis aims to enhance the\nquality of interaction between machines and humans, making\nit more seamless and sophisticated. In light of these advances,\na comprehensive understanding of the entire range of human\nemotional responses in digital communications has become\nincreasingly essential. [5]\nBeginning with the application of deep learning method-\nologies [6], pre-trained language models (PLMs), particularly\nthose based on transformer architectures like BERT [7] and\nThis work was supported by the Korea Institute for Advancement of\nTechnology (KIAT) grant funded by Ministry of Trade, Industry and Energy\n(MOTIE) (No. P0028821, Development of 22kW Variable Type EV Charging\nSystem for Building Up Global Chain of Intelligent/Universal Mechanical\nParking System).\nGPT [8], have transformed the field of NLP by effectively\nextracting semantic information and producing contextualized\ntext representations. Many sentiment analysis approaches have\nadopted PLMs as a base model, treating sentiment-related\napplications as downstream tasks. However, while PLMs excel\nat semantic understanding, they often struggle to capture\nnuanced affective information critical to sentiment analysis\ntasks without careful fine-tuning [9].\nTo address this problem, recent studies have focused on\nthe development of sentiment-aware pre-trained language\nmodels by modifying existing methodologies employed for\nsemantic similarity and incorporating sentiment information.\nAlthough these enhancements have shown promising results\nin downstream sentiment tasks, existing sentiment analysis\nmethods still face challenges in capturing nuanced emotional\ndistinctions due to several factors. Their reliance on binary or\nternary polarity labels oversimplifies the complex, multidimen-\nsional nature of human emotions, risking dimensional collapse\nand reducing generalization across contexts. Furthermore, de-\npendence on static lexicons introduces biases and hampers\nadaptability to domain, cultural, and linguistic variations. In\naddition, narrow masking strategies focus on small subsets of\ninput tokens, failing to account for broader emotional patterns\nwithin global text structures. These limitations collectively\nrestrict the effectiveness of current sentiment analysis models.\nIn this paper, we propose continuous adversarial representa-\ntion learning (CARL), which leverages transformer-based text\nencoder training to enhance emotion-aware embeddings. The\nmain contributions of our work can be summarized as follows:\n• We propose a novel training framework for affective\nrecognition using sentence-level momentum continuous\nlabel contrastive learning and token-level dynamic adver-\nsarial attack detection guided by gradients.\n• Our approach boosts models’ affective embedding capa-\nbilities, contributing to more robust and nuanced emotion-\naware text representations.\n• We achieve superior performance compared to baseline\nmethods across three key emotion recognition tasks.\n• Furthermore, our method yields higher quality in the\nemotional representation space, tested with the balance\nbetween alignment and uniformity of the emotions.\narXiv:2502.20613v1  [cs.CL]  28 Feb 2025\n\n\nII. RELATED WORKS\nText representation learning aims to extract meaningful\ninformation from textual data, employing metric learning\ntechniques that embed objects into spaces where semantic\nrelationships are preserved. These embeddings are critical\nfor downstream tasks, as they facilitate the model’s ability\nto generalize across various applications. Text representation\nlearning can generally be divided into two main categories:\nintra-sample and inter-sample predictions.\nIntra-sample prediction focuses on creating self-supervised\ntasks within individual samples. This approach has become\npopular with the introduction of BERT which uses masked\nlanguage modeling (MLM) as its primary task. In MLM,\ntokens are masked, and the model learns to predict them\nusing contextual information from the surrounding tokens.\nThe BERT post-training (BERT-PT) [10] fine-tunes the model\nwith sentiment-specific objectives using large-scale review\ndatasets and tailors the learned representations to capture\nsentiment nuances. The SentiBERT [11] extends these ideas\nby introducing a parse tree on top of the attention mechanisms\nand allows the BERT to incorporate syntactic structures into its\nsentiment representations. This integration facilitates a deeper\nunderstanding of the sentiment relationships embedded in\ncomplex sentence structures. The SentiLARE [12] leverages\nSentiWordNet lexical semantics and embeds linguistic features\nsuch as sentiment scores at the word level directly into\nthe model to enhance its ability to understand words with\nsentiments. The SentiX [13] incorporates masked emotional\nindicators such as emojis and sentiment-bearing words for\nprioritizing emotionally significant tokens in training.\nInter-sample prediction emphasizes relationships between\ndifferent samples, with contrastive learning emerging as a\npivotal mechanism for this purpose. Since the introduction of\nSimCSE [14], which demonstrated the efficacy of contrastive\nlearning in sentence embedding tasks with minimal data\naugmentation, this paradigm has gained significant traction\nin natural language processing. Building on this foundation,\nSCAPT [15] adopts supervised contrastive learning to align\nrepresentations of sentiment expressions with the same polar-\nity, effectively capturing both implicit and explicit sentiment\norientations. Similarly, SentiCSE [16] extends the contrastive\nlearning framework by integrating masked token polarity pre-\ndiction, enabling fine-grained alignment of sentiment informa-\ntion at both token and sentence levels.\nIII. PROPOSED METHOD\nTo overcome the limitations of the existing methods, the\nproposed framework integrates two pre-training tasks at the\nsentence and token levels, as illustrated in Fig. 1. For sentence-\nlevel inter-sample training, momentum continuous contrastive\nlearning (MCCL) is used, and for token-level intra-sample\ntraining, gradient-based perturbed token detection (PTD) is\nutilized.\nA. Momentum Contrastive Learning Structure\nPrevious sentiment representation studies often employed\nthe SimCSE framework, which relies on direct encoder up-\ndates. However, such updates can introduce instability in em-\nbedding updates and high sensitivity to batch size, hindering\nthe learning of robust representations.\nWe propose adopting a momentum update strategy inspired\nby BYOL [17]. The method employes two neural networks:\nan online network and a target network, both of which share\nthe same architecture but have distinct roles. The online\nnetwork learns representations through standard back propaga-\ntion, while the target network provides stable representations\nto guide the online network’s learning. The target network’s\nparameters, denoted as θt, are not updated directly by gradi-\nents. Instead, they are updated using an exponential moving\naverage of the online network’s parameters θo given as\nθt ←mθt + (1 −m)θo\n(1)\nwhere m ∈[0, 1] is the momentum coefficient. To ensure\ndynamic adaptation, m is updated during training using a\ncosine annealing scheduler, for a given training step k out\nof a total of K steps, so the updated m, mnew, is obtained as\nmnew = 1 −(1 −minitial) · cos(π · k/K) + 1\n2\n(2)\nThe embedding and the encoding layer in the online and\ntarget networks are identical to those of the base model. How-\never, the activation function in the pooler layer is removed,\nconsidering additional layers added afterward. The projection\nlayer is responsible for mapping the outputs of both the\nonline and target branches into a shared latent space suitable\nfor contrastive learning. The prediction layer further refines\nthe outputs by aligning the features generated by the online\nnetwork with those of the target network, which is critical for\npreventing model collapse.\nB. Continuous Label Contrastive Learning\nWe propose a continuous label contrastive learning frame-\nwork grounded in Russell’s circumplex model of affect [18].\nThis model represents emotions within a two-dimensional\nspace defined by valence and arousal, where valence char-\nacterizes the pleasure-displeasure axis and arousal denotes\nthe activation-deactivation spectrum, indicating emotional in-\ntensity. By leveraging these multidimensional labels, our ap-\nproach captures nuanced emotional variations and relation-\nships that binary classification methods cannot. This extends\nthe capability of conventional contrastive learning techniques,\nenabling a more refined understanding of emotional content.\nIn our framework, we utilize the in-batch similarity matrix\ncommonly employed in self-supervised contrastive learning.\nFor a given pair of embeddings, zi and zj, obtained from the\nonline and target networks, their similarity is computed using\ncosine similarity given as\nIsim,ij =\nzi · zj\np\nz2\ni ·\nq\nz2\nj\n.\n(3)\n\n\nFig. 1. Structure of proposed framework\nSimilarly, the label similarity matrix using the valence-\narousal labels is calculated using cosine similarity defined as:\nIva,ij =\nvivj + aiaj\np\nv2\ni + a2\ni ·\nq\nv2\nj + a2\nj\n.\n(4)\nwhere v and a represent valence and arousal, respectively,\nand i and j denote individual samples in the dataset.\nTo ensure compatibility, both matrices are normalized into\nprobability distributions using the softmax function given by:\nPx,ij =\nexp(Ix,ij)\nP\nk exp(Ix,ik)\n(5)\nwhere x ∈sim, va.\nWe adopt cross-entropy to achieve alignment of relative\nproportions between distributions. This approach offers a more\ntargeted mechanism for aligning distributions, prioritizing their\nstructural consistency or shape over mere numerical discrep-\nancies. Therefore, the sentence-level loss is formulated as the\nsymmetric cross entropy between the label similarity matrix\nand the embedding similarity matrix as follows:\nLMCCL = −1\nN\nN\nX\nj=1\n\" N\nX\ni=1\npsim,i,j log pva,i,j +\nN\nX\ni=1\npva,i,j log psim,i,j\n#\n= −1\nN\nN\nX\nj=1\n[CE(psim,j, pva,j) + CE(pva,j, psim,j)]\n(6)\nwhere CE denotes the cross entropy loss. This symmetric\nformulation ensures alignment between embedding similarity\nand the underlying valence-arousal labels, promoting a robust\nrepresentation of nuanced emotional content.\nC. Gradient-Based Perturbed Token Detection\nInstead of relying on traditional random masking or prede-\nfined lexicon masking, which limits the efficiency of train-\ning to only a portion of the sample, this work proposes\nusing saliency derived from gradients to identify tokens with\nthe highest sentiment relevance dynamically. Inspired by the\nreplaced token detection method employed in ELECTRA\n[19], this framework uses gradient-based perturbations instead\nof requiring a generator, thus avoiding the complexities of\ngenerator-discriminator structures. During the forward pass\nof the online branch, gradients are captured for each token\ne ∈Rd. The importance score for each token is then deter-\nmined by the magnitude of its gradient (gt), given by\ngt =\n\r\r\r\r\n∂L\n∂et\n\r\r\r\r\n2\n,\nt ∈{1, . . . , T},\n(7)\nwhere T is the sequence length and L is the loss function.\nThen, we sort and select the top-k of tokens with the highest\ngradient norms si to form the set of important tokens S and\ncreate a mask mt. Here, we create the adversarial perturbation\nsamples using the projected gradient descent (PGD) [20] per\ntoken in the embedding layer, which is a white box attack\nthat causes the model to make incorrect predictions. The\nembedding of et is given by\ne(i+1)\nt\n= ΠB(et,ϵ)\n\u0012\ne(i)\nt\n+ αsign(\n∇etL\n∥∇etL∥2\n)\n\u0013\n,\n(8)\nwhere ΠB(et,ϵ) projects the perturbed embedding back into\nthe ϵ-ball around the original embedding et, and α is the step\nsize. The subtle attacks result in alterations to the token values,\nleading to a loss of their original semantic integrity. To prevent\nthese perturbations from surpassing a predefined threshold,\nthey are constrained with Frobenius norm projection.\nThe final embeddings can be expressed as\n˜et =\n(\net + ∆et\nif t ∈S,\net\notherwise.\n(9)\nwhere ˜et represents the total perturbation matrix. Finally,\nthe replaced token detection layer is trained to determine\nwhether a token has been perturbed, employing a final sigmoid\nactivation function in this process. To address the issue of class\nimbalance, focal binary loss is utilized\nLPTD = −(1 −pt)γ log(pt),\n(10)\n\n\nwhere pt denotes the layer’s prediction of the token’s suscep-\ntibility to attack, and γ is the parameter emphasizing difficult-\nto-classify classes.\nD. Total Loss Function\nIn summary, the overall loss combines the momentum\ncontrastive learning and perturbation token detection losses:\nL = λ1LMCCL + λ2LPTD,\n(11)\nwhere λ1 and λ2 are weighting factors that control the relative\nimportance of each loss component.\nIV. EXPERIMENTS\nA. Implementation Details\nWe implemented the proposed method using PyTorch and\ntrained it on an NVIDIA RTX A6000 GPU for 2 epochs\nwith a batch size of 128. The evaluation was conducted using\ntwo transformer-based architectures: BERT-base-uncased and\nRoBERTa-base-uncased. The models were optimized with the\nAdamW optimizer, with a cosine annealing learning rate\nschedule with warm restarts and a 10% warmup period,\ninitialized with a learning rate of 2 × 10−5. We use the\nfirst token (CLS) pooling for the output of the model, with\nthe temperature parameter set to 0.05. A maximum sequence\nlength of 128 tokens is maintained, with the momentum\ncoefficient of 0.9996. For saliency token selection, we use the\ntop 10% of each sentence, and the PGD’s epsilon parameter\nwas set to 5e-9 with the alpha of 5. The λ1, λ2 values are 0.8,\n0.2. For every 10 steps, we evaluate the model’s performance\non transfer tasks to obtain the best checkpoint.\nB. Training Dataset\nThe training data is comprised of a diverse set of corpora\nannotated with valence and arousal labels, capturing emo-\ntional expressions across various domains. Specifically, we\nutilized EmoBank [21], a dataset aggregating texts from mul-\ntiple internet sources with fine-grained emotional annotations;\nIEMOCAP [22], a corpus of dyadic interactions annotated\nfor emotion categories and dimensions; the Facebook Posts\ndataset [23], containing social media texts reflecting everyday\nemotional language; and EmoTales [24], a collection of folk\ntale sentences with emotional labels. In total, our training\nset encompassed 24,392 texts with an average length of\n20.63 tokens, capturing a wide range of emotional expressions\nacross different domains. While the EmoBank and IEMOCAP\ndatasets are annotated within the range of 1 to 5, and the\nFacebook Posts and EmoTales datasets use a scale from 1\nto 9. To address the inconsistency in label ranges across\ndatasets, min-max normalization was applied to standardize\nall annotations to a common scale\nxi = 2\nxi −min(x)\nmax(x) −min(x) −1\n(12)\nwhere xi represents the normalized value, and the resulting\nrange of xi is [−1, 1].\nC. Downstream Tasks\nModel evaluation was conducted using four comprehensive\naffective recognition tasks, each aimed at assessing distinct\naspects of the model’s capabilities.\na) Valence and Arousal Alignment: The model’s capacity\nto capture continuous valence and arousal labels was evaluated\nusing linear regression models trained on sentence embed-\ndings. Predictive accuracy was assessed via mean absolute\nerror (MAE), while Pearson (r) and Spearman (ρ) correlations\nwere calculated to quantify linear and monotonic relationships\nbetween predicted and ground truth values.\nb) Polarity Classification: : To assess the performance\nof the proposed model, we applied linear probing on standard\nsentiment benchmarks from SentEval toolkit [25], including\nMR, CR, SST5, and IMDB. These datasets encompass varying\ntext lengths and sentiment granularity, ranging from binary\nclassification to granular sentiment classification.\nc) Emotion Classification: The ability of the model to\ncapture emotional nuances was evaluated using the GoE-\nmotions [26] dataset, which consists of Reddit comments\nannotated for 28 emotion categories. In this study, six emotions\nwere selected based on Ekman’s theory of basic emotions\nto facilitate a more streamlined analysis. Performance was\nassessed using accuracy, precision, recall, and F1-score.\nd) Embedding Quality: The quality of the learned em-\nbeddings was evaluated by analyzing their alignment and\nuniformity [27]. Unlike the traditional use of the semantic\ntextual similarity benchmark, we used the GoEmotions dataset\nto construct positive and negative pairs.\nD. Experimental Results\nThe proposed models demonstrated superior performance in\nfour tasks over baseline models.\nThe CARL model demonstrated significant improvements in\nvalence and arousal prediction, as shown in Table I. For the\nBERT-based models, CARL achieved a Pearson correlation\n(r) of 0.721 and a Spearman correlation (ρ) of 0.714 for\nvalence, representing an improvement of 7.1% and 7.4%\nover the baseline BERT model, respectively. For arousal,\nCARL attained r=0.629 and ρ=0.684, yielding improvements\nof 8.1% and 6.9%, respectively. In terms of mean absolute\nerror (MAE), CARL reduced the error to 0.087 for valence and\n0.101 for arousal, a reduction of 11.2% and 4.7% compared to\nbaseline BERT, achieving the lowest MAE among BERT-based\nmodels. For RoBERTa-based models, CARL outperformed\nboth the baseline and sentiment-focused models, achieving\nr=0.741 and ρ=0.738 for valence, an improvement of 9.4%\nand 9.8% over baseline RoBERTa, respectively. For arousal,\nCARL reached r = 0.634 and ρ = 0.699, representing a 2. 4%\nand 3. 1% improvement over the baseline RoBERTa. The MAE\nfor CARL was reduced to 0.083 for valence and 0.097 for\narousal, marking a 15.3% and 5.8% improvement compared\nto baseline RoBERTa. These results indicate that both models\neffectively capture fine-grained affective states in text, with the\nRoBERTa-based model having a slight edge in performance.\n\n\nTABLE I\nPERFORMANCE COMPARISON ON VALENCE AND AROUSAL ALIGNMENT\nBase Model\nModel\nValence\nArousal\nr\nρ\nMAE\nr\nρ\nMAE\nBERT\nBERT\n0.673\n0.665\n0.098\n0.582\n0.640\n0.106\nBERT-PT\n0.643\n0.640\n0.103\n0.569\n0.622\n0.108\nSentiBERT\n0.675\n0.670\n0.098\n0.584\n0.635\n0.107\nSentiX\n0.667\n0.656\n0.120\n0.572\n0.614\n0.110\nCARL\n0.721\n0.714\n0.087\n0.629\n0.684\n0.101\nRoBERTa\nRoBERTa\n0.677\n0.672\n0.098\n0.619\n0.678\n0.103\nSentiLARE\n0.697\n0.692\n0.095\n0.589\n0.665\n0.106\nSentiCSE\n0.680\n0.670\n0.090\n0.575\n0.626\n0.170\nCARL\n0.741\n0.738\n0.083\n0.634\n0.699\n0.097\nIn transfer task evaluation, the BERT-based CARL showed\nconsistent improvements across all tasks, with particularly\nnotable gains in fine-grained sentiment analysis. As shown\nin Table II, the model achieved a 12.9% improvement over\nbase BERT on the SST5 task and a 9.2% gain on IMDB\nclassification. The RoBERTa-based CARL demonstrated even\nstronger transfer capabilities, showing a 10.2% improvement\nover base RoBERTa on SST5 and a 2.7% gain on IMDB.\nIn table II, cells marked with an asterisk (*) denote that the\nmodel utilized the dataset for training. Apart from the results\nannotated with an asterisk, our model outperformed others on\naverage. Overall, on average performance, CARL achieved a\nhigh average performance improvement of 6.5% and 6.2%\nover BERT and RoBERTa, respectively. The balanced perfor-\nmance across these diverse tasks highlights the robustness and\ngeneralizability of the proposed framework.\nTABLE II\nPERFORMANCE COMPARISON ON POLARITY CLASSIFICATION\nBase Model\nModel\nSentEval Transfer Tasks\nCR\nMR\nSST5\nIMDB\nBERT\nBERT\n87.66\n81.93\n44.14\n81.18\nBERT-PT\n88.11\n85.59\n48.41\n86.48\nSentiBERT\n88.48\n86.23\n48.59\n82.17\nSentiX\n91.01\n83.57\n44.54\n82.48\nCARL\n89.17\n86.59\n49.83\n88.64\nRoBERTa\nRoBERTa\n85.91\n82.46\n47.50\n89.96\nSentiLARE\n90.64\n88.12\n51.27\n90.16\nSentiCSE\n90.71\n97.41*\n52.13\n88.43\nCARL\n91.42\n88.49\n52.34\n92.41\nIn emotion classification tasks, as presented in Table III,\nThe BERT-based CARL significantly outperformed all BERT-\nvariants, demonstrating a 12.9% accuracy improvement over\nbase BERT and a 3.6% gain over the previous best SentiBERT.\nThe model showed an 8.9% improvement in the F1-score\ncompared to base BERT and maintained consistent improve-\nments across all metrics, with particularly strong gains in\nprecision. The RoBERTa-based CARL set new benchmarks\nin emotion classification with a 15.5% accuracy improvement\nover base RoBERTa and a 3.5% gain over the SentiCSE. The\nmodel achieved an impressive 9.3% improvement in F1-score\ncompared to base RoBERTa and demonstrated balanced im-\nprovements across precision and recall metrics. As visualized\nthrough PCA in Figure 2, even without extensive training, the\nTABLE III\nPERFORMANCE COMPARISON ON EMOTION CLASSIFICATION\nBase Model\nModel\nEmotion Classification Metrics\nµAcc\nπP\nρR\nϕF1\nBERT\nBERT\n64.44\n0.660\n0.660\n0.648\nBERT-PT\n63.17\n0.614\n0.612\n0.610\nSentiBERT\n70.21\n0.689\n0.684\n0.685\nSentiX\n65.71\n0.683\n0.682\n0.682\nCARL\n72.73\n0.719\n0.701\n0.705\nRoBERTa\nRoBERTa\n63.91\n0.657\n0.639\n0.640\nSentiLARE\n71.32\n0.716\n0.713\n0.713\nSentiCSE\n72.47\n0.728\n0.723\n0.723\nCARL\n73.84\n0.766\n0.759\n0.761\nFig. 2. PCA visualization of text embeddings across models\nproposed models achieve a well-separated and coherent cluster\nseparation between emotional clusters, with the largest dif-\nferences between cluster centroids. This balance underscores\nthe representational strength of the proposed models and their\nadaptability to diverse downstream tasks.\nFigure 3 complements Table IV, visualizing the trade-off\nbetween alignment and uniformity across all models. In the\nfigure, the model labels are color-coded based on the classifi-\ncation accuracy reported in Table III. The proposed models\ndemonstrate an excellent balance between these properties\ncompared to the previous models. For example, the proposed\nRoBERTa model achieves an alignment score of 0.2649 and\na uniformity score of -0.6063, indicating that it generates\ncompact clusters of similar sentences while maintaining a\nwell-dispersed embedding space. This balance underscores the\nrepresentational strength of the proposed models and their\nadaptability to diverse downstream tasks.\nTABLE IV\nALIGNMENT AND UNIFORMITY RESULTS\nBase Model\nModel\nAlignment\nUniformity\nBERT\nBERT\n0.3633\n-0.7383\nBERT-PT\n0.6776\n-1.4091\nSentiBERT\n0.1907\n-0.3892\nSentiX\n1.0338\n-2.1505\nCARL\n0.5027\n-1.1224\nRoBERTa\nRoBERTa\n0.0088\n-0.0182\nSentiLARE\n0.0846\n-0.1719\nSentiCSE\n0.4786\n-1.0043\nCARL\n0.2649\n-0.6063\n\n\nFig. 3. The alignment-uniformity plot of the models\nOur comprehensive ablation studies presented in Table V\nreveal that removing either the token perturbation detection\nmechanism or the momentum continuous contrastive learning\nobjective led to notable performance degradation across all\ntasks. Specifically, in BERT, removing the token perturbation\ndetection (w/o TP) resulted in a 2.5% drop, while removing the\nmomentum continuous contrastive learning (w/o MCCL) led\nto a 4.2% decrease in average performance across the polarity\nclassification task. These findings highlight the importance\nof both components in improving the quality of the learned\nrepresentations and demonstrate their complementary roles.\nTABLE V\nABLATION STUDY\nSentEval\nValence\nArousal\nModel\nCR\nMR\nSST5\nIMDB\nr\nρ\nMAE\nr\nρ\nMAE\nCARL\n89.17\n86.59\n49.83\n88.64\n0.72\n0.71\n0.09\n0.63\n0.68\n0.10\nw/o TP\n88.45\n82.21\n47.56\n88.26\n0.69\n0.70\n0.10\n0.62\n0.67\n0.10\nw/o MCCL\n87.62\n81.86\n44.96\n86.60\n0.70\n0.69\n0.09\n0.60\n0.66\n0.11\nBase RWA\n86.06\n79.46\n43.78\n86.66\n0.67\n0.66\n0.10\n0.59\n0.64\n0.11\nCARL\n91.42\n88.49\n52.34\n92.41\n0.74\n0.73\n0.08\n0.63\n0.69\n0.09\nw/o TP\n89.17\n85.79\n50.43\n91.86\n0.69\n0.68\n0.10\n0.63\n0.69\n0.10\nw/o MCCL\n90.56\n85.43\n48.39\n91.42\n0.72\n0.69\n0.09\n0.62\n0.66\n0.11\nBase RWA\n80.82\n75.59\n41.49\n85.36\n0.58\n0.57\n0.11\n0.52\n0.56\n0.12\nV. CONCLUSION\nIn this work, we presented a novel framework for enhancing\nemotion-aware text embeddings by integrating momentum\ncontinuous label contrastive learning and gradient-based token\nperturbation detection. The experimental result demonstrates\nimproved performance over baseline models in affective recog-\nnition tasks with well-aligned and uniformly distributed em-\nbeddings, validated across multiple benchmarks. This frame-\nwork is useful for emotion-aware natural language processing,\nwith potential applications spanning affective computing, text\nretrieval problems, and human-computer interaction.\nREFERENCES\n[1] S. Afzal, H. A. Khan, M. J. Piran, and J. W. Lee, A comprehensive\nsurvey on affective computing: Challenges, trends, applications, and\nfuture directions, IEEE Access, vol. 12, pp. 1–21, 2023.\n[2] E. Cambria and A. Hussain, Sentiment analysis in the age of generative\nAI, IEEE Intelligent Systems, vol. 38, no. 1, pp. 12-16, 2023.\n[3] I. Park, D. Kim, and D. Har, MAC achieving low latency and energy\nefficiency in hierarchical M2M networks with clustered nodes, IEEE\nSensors Journal, vol. 15, no. 3, pp. 1657-1661, 2015.\n[4] S. Kotte, J. R. K. Dabbakuti, Smart 6G sensor network based human\nemotion analysis by machine learning architectures, Wireless Pers Com-\nmun, 2024.\n[5] Qin, Xiangyu, et al., BERT-ERC: Fine-tuning BERT is enough for emo-\ntion recognition in conversation, Proceedings of the AAAI Conference\non Artificial Intelligence, Vol. 37, No. 11, 2023.\n[6] T. Kim, L. F. Vecchietti, K. Choi, S. Lee, and D. Har, Machine Learning\nfor Advanced Wireless Sensor Networks: A Review, IEEE Sensors\nJournal, vol. 21, no. 11, pp. 12379-12397, 2021\n[7] J. Devlin., M. Chang, K. Lee, and K. Toutanova, BERT: Pre-training of\ndeep bidirectional transformers for language understanding, Proceedings\nof the 2019 Conference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Language Technologies,\nVolume 1, pp. 4171–4186, 2019.\n[8] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, Improving\nlanguage understanding by generative pre-training, OpenAI Technical\nReport, 2018.\n[9] B. Li, H. Zhou, J. He, M. Wang, Y. Yang, and L. Li, On the sentence\nembeddings from pre-trained language models, the Conference on Em-\npirical Methods in Natural Language Processing, 2020, pp. 9119–9130\n[10] H. Xu, L. Zhang, J. Qian, and L. He, BERT post-training for review read-\ning comprehension and aspect-based sentiment analysis, the Conference\nof the North American Chapter of the Association for Computational\nLinguistics, 2019.\n[11] D. Yin, F. Xie, W. Zhang, and X. Li, SentiBERT: A transferable\ntransformer-based architecture for compositional sentiment semantics,\nthe 58th Annual Meeting of the Association for Computational Linguis-\ntics, 2020.\n[12] P. Ke, T. Huang, and Y. Liu, SentiLARE: Sentiment-aware language\nrepresentation learning with linguistic knowledge, the Conference on\nEmpirical Methods in Natural Language Processing, 2020.\n[13] Z. Zhou, L. Zhang, X. Chen, and J. Wang, SentiX: A sentiment-aware\npre-trained model for cross-domain sentiment analysis, Proceedings of\nthe 30th International Conference on Computational Linguistics, 2020.\n[14] T. Gao, X. Yao, and D. Chen, SimCSE: Simple contrastive learning of\nsentence embeddings, the Conference on Empirical Methods in Natural\nLanguage Processing, 2021.\n[15] Z. Li, Y. Zou, C. Zhang, Q. Zhang, and Z. Wei. Learning implicit\nsentiment in aspect-based sentiment analysis with supervised contrastive\npretraining. Proceedings of the Conference on Empirical Methods in\nNatural Language Processing, 2021.\n[16] J. Kim, Y. Na, K. Kim, S. R. Lee, and D. K. Chae, SentiCSE:\nA sentiment-aware contrastive sentence embedding framework with\nsentiment-guided textual similarity, Proceedings of the 30th International\nConference on Computational Linguistics, 2024.\n[17] Grill, Jean-Bastien, et al., Bootstrap your own latent — a new approach\nto self-supervised learning. Advances in neural information processing\nsystems 33 : 21271-21284, 2020.\n[18] J. A. Russell, Revisiting the circumplex model of affect, Emotion\nReview, vol. 14, no. 1, pp. 3-10, 2022.\n[19] K. Clark, M. Luong, Q.V. Le, and C.D. Manning, ELECTRA: Pre-\ntraining text encoders as discriminators rather than generators, Inter-\nnational Conference on Learning Representations, 2020\n[20] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, Towards\ndeep learning models resistant to adversarial attacks, International Con-\nference on Learning Representations, 2018.\n[21] Buechel, Sven, and Udo Hahn, Emobank: Studying the impact of an-\nnotation perspective and representation format on dimensional emotion\nanalysis, Proceedings of the 15th Conference of the European Chapter\nof the Association for Computational Linguistics: Volume 2, 2017\n[22] C. Busso et al., IEMOCAP: interactive emotional dyadic motion capture\ndatabase. Language Resources and Evaluation, 42, 335-359, 2018\n[23] D. Preotiuc-Pietro et al., Modelling Valence and Arousal in Facebook\nposts, Proceedings of the 7th workshop on computational approaches to\nsubjectivity, sentiment and social media analysis, 2016\n[24] V. Francisco, R. Herv´as, F. Peinado, P. Gerv´as. EmoTales: creating a\ncorpus of folk tales with emotional annotations, Language Resources\nand Evaluation, 46, 341 - 381, 2011.\n[25] A. Conneau and D. Kiela, SentEval: An evaluation toolkit for universal\nsentence representations, Language Resources and Evaluation, 2018.\n[26] D. Demszky, E. Go, C. Shinn, and S. Kiritchenko, GoEmotions: A\ndataset of fine-grained emotions, the 58th Annual Meeting of the\nAssociation for Computational Linguistics, 2020.\n[27] T. Wang and P. Isola, Understanding contrastive representation learning\nthrough alignment and uniformity on the hypersphere, International\nConference on Machine Learning, 2020.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20613v1.pdf",
    "total_pages": 6,
    "title": "Continuous Adversarial Text Representation Learning for Affective Recognition",
    "authors": [
      "Seungah Son",
      "Andrez Saurez",
      "Dongsoo Har"
    ],
    "abstract": "While pre-trained language models excel at semantic understanding, they often\nstruggle to capture nuanced affective information critical for affective\nrecognition tasks. To address these limitations, we propose a novel framework\nfor enhancing emotion-aware embeddings in transformer-based models. Our\napproach introduces a continuous valence-arousal labeling system to guide\ncontrastive learning, which captures subtle and multi-dimensional emotional\nnuances more effectively. Furthermore, we employ a dynamic token perturbation\nmechanism, using gradient-based saliency to focus on sentiment-relevant tokens,\nimproving model sensitivity to emotional cues. The experimental results\ndemonstrate that the proposed framework outperforms existing methods, achieving\nup to 15.5% improvement in the emotion classification benchmark, highlighting\nthe importance of employing continuous labels. This improvement demonstrates\nthat the proposed framework is effective in affective representation learning\nand enables precise and contextually relevant emotional understanding.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}