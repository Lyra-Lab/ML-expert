{
  "id": "arxiv_2502.20525v1",
  "text": "Revisiting Kernel Attention with Correlated Gaussian Process Representation\nLong Minh Bui1,3\nTho Tran Huu1,2\nDuy Dinh1\nTan Minh Nguyen2,*\nTrong Nghia Hoang3,*\n1FPT Software AI Center\n2Department of Mathematics, National University of Singapore\n3School of Electrical Engineering and Computer Science, Washington State University\n*Co-last author\nAbstract\nTransformers have increasingly become the de\nfacto method to model sequential data with state-\nof-the-art performance. Due to its widespread use,\nbeing able to estimate and calibrate its model-\ning uncertainty is important to understand and de-\nsign robust transformer models. To achieve this,\nprevious works have used Gaussian processes\n(GPs) to perform uncertainty calibration for the\nattention units of transformers and attained no-\ntable successes. However, such approaches have\nto confine the transformers to the space of sym-\nmetric attention to ensure the necessary symmet-\nric requirement of their GP’s kernel specifica-\ntion, which reduces the representation capacity\nof the model. To mitigate this restriction, we pro-\npose the Correlated Gaussian Process Transformer\n(CGPT), a new class of transformers whose self-\nattention units are modeled as cross-covariance\nbetween two correlated GPs (CGPs). This allows\nasymmetries in attention and can enhance the\nrepresentation capacity of GP-based transform-\ners. We also derive a sparse approximation for\nCGP to make it scale better. Our empirical stud-\nies show that both CGP-based and sparse CGP-\nbased transformers achieve better performance\nthan state-of-the-art GP-based transformers on a\nvariety of benchmark tasks. The code for our exper-\niments is available at https://github.com/\nMinhLong210/CGP-Transformers.\n1\nINTRODUCTION\nTransformers [Vaswani et al., 2017] have recently emerged\nas the preferred models in various sequence modeling tasks,\nincluding those in computer vision [Al-Rfou et al., 2019,\nDosovitskiy et al., 2020, Ramesh et al., 2021, Radford\net al., 2021, Arnab et al., 2021, Liu et al., 2022, Zhao\net al., 2021, Guo et al., 2021], natural language process-\ning [Baevski and Auli, 2019, Dehghani et al., 2019, Devlin\net al., 2019, Al-Rfou et al., 2019, Dai et al., 2019, Brown\nand et al., 2020, Brown et al., 2020], and reinforcement\nlearning [Chen et al., 2021, Janner et al., 2021], due to\nits computational advantage in replacing expensive recur-\nrence operations in recurrent neural networks [Medsker\nand Jain, 2001] and long short-term memory (LSTM) net-\nworks [Hochreiter and Schmidhuber, 1997] with a feed-\nforward attention mechanism that allows for significantly\nmore parallelization in model training [Lin et al., 2022, Tay\net al., 2022]. Transformer-based pre-trained models can also\nbe effectively adapted to new tasks with limited supervi-\nsion [Radford et al., 2018, 2019, Devlin et al., 2019, Yang\net al., 2019, Liu et al., 2019]. In particular, the core compo-\nnent of a transformer model is the multi-head self-attention\n(MHSA), which captures sequential dependencies among\ndifferent tokens of a single sequence by having each token\nrepresented as weighted average over (learnable) functions\nof other tokens whereas the (learnable) weights characterize\nthe similarities between tokens [Cho et al., 2014, Parikh\net al., 2016, Lin et al., 2017]. Intuitively, such weights rep-\nresent the amount of attention each token needs to give\nothers to obtain its contextual representation [Bahdanau\net al., 2014, Vaswani et al., 2017, Kim et al., 2017].\nDespite its growing successes, the original transformer lacks\na mechanism for uncertainty calibration which is essential\nto provide trustworthy predictions and enable robust, risk-\naverse decision making in safety-critical tasks [Chen and Li,\n2023]. This limitation has motivated a recent line of work\n[Xue et al., 2021, Tran et al., 2019] that develops uncertainty\nquantification techniques for transformers. Particularly, the\nmost recent work of [Chen and Li, 2023] in this direction\nhas drawn a connection between the self-attention mecha-\nnism and the inference mechanism of a GP [Rasmussen and\nWilliams, 2006], which interestingly appears to share the\nsame principle of building a contextual representation for\nan input based on its similarities to other inputs.\nThis is perhaps not surprising in hindsight considering that\nGPs had been historically adopted to model various forms of\narXiv:2502.20525v1  [cs.LG]  27 Feb 2025\n\n\nspatio-temporal data [Luttinen and Ilin, 2012, Hamelijnck\net al., 2021] and their inherent temporal dependencies based\non a kernelized measure of input similarities. This also has a\ndirect translation to the attention mechanism from the lens of\nkernel attention [Tsai et al., 2019, Chen et al., 2024], albeit\nwith no uncertainty quantification. Despite this interesting\nprogress in connecting the modern literature of transformers\nto the classical research on GPs for uncertainty quantifica-\ntion, prior work [Chen and Li, 2023] in this direction has to\ncompromise the representational capacity of transformers in\norder to make such a connection. In particular, both linear\ntransformation functions for the query and value vectors of\nthe self-attention in transformers would have to be tied to\nthe same parameterization to cast attention output as the out-\nput of a GP with a valid symmetric kernel. This constraint\nreduces the model’s performance significantly, as shown\nin our experiments, which is consistent with the results for\noriginal transformers reported in [Tsai et al., 2019].\nOur Contribution. To mitigate such restriction, we intro-\nduce a new perspective of GP-based transformer which\npreserves the original modeling flexibility of self-attention,\nallowing the attention matrix to be asymmetrical as needed.\nThis is achieved via characterizing the attention output\nnot as a GP prediction but as a prediction based on cross-\ncovariance between two CGPs, which allows kernel asym-\nmetries while retaining the quantifiable uncertainty structure\nof GPs. To substantiate the above high-level idea, we have\nmade the following technical contributions:\n1. In section 3, we derive a correspondence between the\nself-attention units of the multi-head self-attention (MHSA)\nmechanism and cross-covariance between two CGPs mod-\neled as different affine transformations of a common latent\nGP where one GP distributes the function of queries while\nthe other distributes the function of keys.\n2. In Section 4 we derive a sparse approximation to the\nabove CGP-based attention unit, which removes the cubic\ndependence of CGPT’s processing cost on the number of\ninput tokens. We further develop a loss regularizer in terms\nof the log marginal likelihood of the CGPs (or sparse CGPs)\nwhich augments the conventional training loss of the trans-\nformer to balance between minimizing the uncertainty of\nattention output (i.e., CGP’s prediction) and minimizing its\ninduced classification performance.\n3. In Section 5, we empirically demonstrate that both our\nCGPT and its sparse approximation achieve better predictive\nperformance than state-of-the-art kernel-attention and GP-\nbased transformers across multiple computer vision and\nnatural language processing benchmarks.\nNotation. For the rest of this paper, we will use the nota-\ntion κ(·, ·) to denote a kernel function. The kernel matrices\nare denoted as K. All other matrices are denoted as bold\nuppercase letters. We also denote a Gaussian distribution\nwith mean µ and variance σ2 using the notation N(µ, σ2).\nWe also use subscripts to distinguish between contexts in\nwhich those kernel function and matrices are computed.\n2\nPRELIMINARIES\n2.1\nSELF-ATTENTION\nGiven an input sequence X = [x1, . . . , xn]⊤∈Rn×d of n\nd-dimensional vectors, the self-attention mechanism trans-\nforms it into the output sequence V+ = [v+\n1 , . . . , v+\nn ]⊤∈\nRn×d via two steps:\nStep 1. The input X is linearly transformed into the query\nQ, the key ˆK, and the value V matrices,\nQ\n≜\n[q1, q2, . . . , qn]⊤= XW⊤\nq ,\n(1)\nK\n≜\n[k1, k2, . . . , kn]⊤= XW⊤\nk ,\n(2)\nV\n≜\n[v1, v2, . . . , vn]⊤= XW⊤\nv ,\n(3)\nwhere Wq, Wk ∈Rs×d and Wv ∈Rs×d are weight matri-\nces. Each tuple of vectors {qi, ki, vi}n\ni=1 comprise respec-\ntively the key, query and value vectors1.\nStep 2. Given Q, K and V, the final output of the attention\nmechanism is as follow:\nV+\n=\nsoftmax\n\u0012QK⊤\n√\nd\n\u0013\n· V = AV,\n(4)\nwhere the softmax operator is applied to each row of the\nmatrix A = softmax(QK⊤/\n√\nd). Here, A is the attention\nmatrix. Eq. (4) details the softmax attention mechanism.\n2.2\nMULTI-HEAD SELF-ATTENTION (MHSA)\nMHSA helps capture more diverse patterns in the in-\nput and increase the representation capacity of trans-\nformers. A MHSA comprises h units of self-attention\nV+\n1 , V+\n2 , . . . , V+\nh where V+\ni denote the output of the i-th\nself-attention unit defined above. The output of the MHSA\nis then computed as an affine transformation of these self-\nattention units,\nH\n≜\nMultiHead\n\u0010\nV+\n1 , V+\n2 , . . . , V+\nh\n\u0011\n=\nConcatenate\n\u0010\nV+\n1 , V+\n2 , . . . , V+\nh\n\u0011\nW⊤\no ,\n(5)\nwhere Wo ∈Rd×(h·d) is the weight matrix.\n2.3\nKERNEL ATTENTION\nAs the softmax attention essentially requires computing\nthe similarity between pairs of keys and queries, Tsai\net al. [2019] proposes kernel attention which replaces the\nsoftmax operator with a kernel function κ(xa, xb). Kernel\nattention thus replaces Eq. (4) with\nV+\n=\nKV,\n(6)\nwhere the (a, b)-cell of the Gram matrix K takes value\nκ(xa, xb) = κo\n\u0010\nxaW⊤\nq , xbW⊤\nk\n\u0011\n, where κo is a valid sym-\nmetric kernel.\n1For simplicity, we assume that the key, query and value vec-\ntors have the same dimension s.\n\n\nNote that even though κo(z, z′) can be selected to be sym-\nmetric, κ(xa, xb) might not be so since\nκ(xa, xb)\n=\nκo\n\u0010\nxaW⊤\nq , xbW⊤\nk\n\u0011\n̸=\nκo\n\u0010\nxbW⊤\nq , xaW⊤\nk\n\u0011\n= κ(xb, xa).\n(7)\nThus, to construct a valid symmetric kernel in kernel at-\ntention, the key and query matrices, Wk and Wq, need\nto be identical, Wk = Wq = W. Tying the parameters\ndefining these matrices saves computational costs but will\nresult in a limitation in the representation capacity of the\nmodel, as empirically shown in Section 3.2 in [Tsai et al.,\n2019], where attention with asymmetric kernels tends to\noutperform attention with symmetric kernels.\n2.4\nGAUSSIAN PROCESSES\nA Gaussian process [Rasmussen and Williams, 2006] de-\nfines a probabilistic prior over a random function z(x) de-\nfined by mean function m(x) = 0 and kernel function\nκ(x, x′).2 These functions induce a marginal Gaussian prior\nover the evaluations z = [z(x1) . . . z(xn)]⊤on an arbitrary\nfinite subset of inputs {x1, . . . , xn}.\nLet x∗be an unseen input whose corresponding output\nz∗= z(x∗) we wish to predict. The Gaussian prior over\n[z(x1) . . . z(xn) z(x∗)]⊤implies the following conditional\ndistribution:\nz∗\n≜\nz(x∗) | z\n∼\nN\n\u0010\nk⊤\n∗K−1z, κ(x∗, x∗) −k⊤\n∗K−1k∗\n\u0011\n, (8)\nwhere k∗= [κ(x∗, x1) . . . κ(x∗, xn)]⊤and K denotes the\nGram matrix induced by κ(x, x′) on {x1, . . . , xn} whose\nvalue at cell (a, b) is κ(xa, xb). For noisy observation zi\nperturbed by Gaussian noise such that zi ∼N(z(xi), σ2),\nEq. (8) above can be integrated with N(z, σ2I) to yield:\nz∗\n≜\nz(x∗) | z\n∼\nN\n\u0010\nk⊤\n∗K−1\nσ z, κ(x∗, x∗) −k⊤\n∗K−1\nσ k∗\n\u0011\n, (9)\nwhere Kσ = K + σ2I. Eq. (9) forms the predictive distribu-\ntion of the Gaussian process (GP).\n2.5\nKERNEL ATTENTION AS GP INFERENCE\nSuppose X = [x1, . . . , xn]⊤is the input fed into a kernel-\nattention unit. Assuming that the key and query matrices\nare set to be identical, its output V+ ∈Rn×s is given as\nV+ = KV = KXW⊤\nv where\nKXW⊤\nv\n≜\nK\n \nK + σ2I\n!−1\nZ.\n(10)\nHere, we set Z = (K + σ2I)XW⊤\nv ∈Rn×s with K being\nthe induced Gram matrix of κ(xa, xb) as defined in Eq. (7)\n2For simplicity, we assume a zero mean function since we can\nalways re-center the training outputs around 0.\nabove. Thus, let νa denote the a-th column of V+ and za\ndenote the a-th column of Z, we have\nνa\n=\nK\n \nK + σ2I\n!−1\nza,\n(11)\nor equivalently, [νa]r = k⊤\nr (K + σ2I)−1za where [νa]r\nis the r-th component of the column vector νa and kr =\n[κ(xr, x1) . . . κ(xr, xn)]⊤.\nComparing this to Eq. (9) earlier, it appears that the a-th\ncolumn νa of the attention output is the mean prediction\non x1, x2, . . . , xn of a modeling the dataset {xr, [za]r}n\nr=1.\nAs such, one can assess the attention uncertainty or variance\nof νa (i.e., the a-th column of V+),\nV [νa]\n=\nK −K\n\u0000K + σ2I\n\u0001−1 K.\n(12)\nOverall, if the output dimension of the kernel attention unit\nis s, we can equivalently represent it using s independent\nGPs. Furthermore, we can extend the above formalism to-\nwards multi-head self-attention with GPs by concatenating\nthe equivalent GP inferences corresponding to each head\nand multiplying all with the weight matrix Wo.\nNote that this equivalence is only possible if the kernel\nmatrix above is symmetric, which requires Wq = Wk as\nexplained earlier. A more recent work by [Chen and Li,\n2023] has also extended the above to instead align with a\nsparse GP inference, which similarly cast the kernel atten-\ntion output in terms of the sparse GP inference. Nonetheless,\nlike the GP attention approach, the proposed sparse GP\nattention will still require the use of symmetric kernel to\nensure the modeling consistency of its underlying GP.\n3\nREVISITING KERNEL ATTENTION\nTo mitigate the restriction to symmetric kernel imposed on\nexisting GP-based transformers, we will instead explore an\nalternative perspective of correlated Gaussian process (CGP)\nmodeling [Titsias et al., 2013]. This will allow us to model\nthe kernel attention unit in terms of the cross-covariance\nbetween two correlated Gaussian processes, which naturally\npermits kernel asymmetries while preserving the GP’s built-\nin capability to calibrate uncertainty.\nThis consequently inspires a principled approach to cali-\nbrate a transformer model without compromising its atten-\ntion mechanism. To elaborate, Section 3.1 and Section 3.2\nwill provide background on the canonical representation\nof GP and CGP modeling. Section 3.3 will then derive a\nnew CGP-based attention structure that can accommodate\nboth attention asymmetries and uncertainty calibration. A\nsparse approximation of this CGP-based structure is further\nderived in Section 4 for better scalability.\n3.1\nCANONICAL REPRESENTATION OF GP\nOur correlated GP (CGP) modeling was inspired from a\nrepresentation of GP that parameterizes its kernel in terms\nof an affine input scaling applied to another parameter-free,\n\n\ncanonical GP [Titsias et al., 2013]. We review this represen-\ntation below and show how this can be extended towards\ncorrelated GP modeling.\nDefinition 1 (Canonical Gaussian process (GP)). A canoni-\ncal GP zo(x) ∼GP(mo(x), κo(x, x′)) is a Gaussian pro-\ncess specified with a zero mean function mo(x) = 0 and a\nparameter-free kernel function κo(x, x′).\nA canonical GP defined in Definition 1, for instance,\ncan attain a squared exponential kernel with the kernel\nlength-scales and global scale equal to 1, κo(x, x′) =\nexp(−0.5∥x −x′∥2). Another GP zλ(x) can then be repre-\nsented in terms of an affine scaling of zo(x),\nzλ(x)\n≜\nσλ · zo\n\u0010\nxW⊤\u0011\n,\n(13)\nwith mean m(x) = 0 and covariance function\nκλ(x, x′)\n=\nE\nh\nσ2\nλzo\n\u0010\nxW⊤\nλ\n\u0011\n· zo\n\u0010\nx′W⊤\nλ\n\u0011i\n=\nσ2\nλκo\n\u0010\nxW⊤\nλ , x′W⊤\nλ\n\u0011\n,\n(14)\nwhere the first equality follows from the definition of co-\nvariance and zλ(x) in Eq. (13), as well as the fact that zo(x)\nhas zero means. The second equality holds because of the\ncanonical kernel definition. The parameter of this kernel\nfunction is defined as a tuple (W, σλ) of parameters.\n3.2\nCGP MODELING\nInspired by the canonical representation in Definition 1, we\ncan now characterize two GPs zk(x) and zq(x), both of\nwhich are obtained via scaling the input of zo(x) using the\nabove mechanism with separate parameters (Wk, σk) and\n(Wq, σq). Following Eq. (14) above,\nκk(x, x′)\n=\nσ2\nkκo\n\u0010\nxW⊤\nk , x′W⊤\nk\n\u0011\nκq(x, x′)\n=\nσ2\nqκo\n\u0010\nxW⊤\nq , x′W⊤\nq\n\u0011\n,\n(15)\nwhere κo is a parameter-free kernel function of the canonical\nGP zo(x). Furthermore, it can be shown that this representa-\ntion also allows analytic derivation of the cross-covariance\nbetween zk(x) and zq(x) as follow,\nκkq(x, x′)\n=\nσkσq κo\n\u0010\nxW⊤\nk , x′W⊤\nq\n\u0011\nκqk(x, x′)\n=\nσqσk κo\n\u0010\nxW⊤\nq , x′W⊤\nk\n\u0011\n.\n(16)\nNote that, unlike the in-domain covariance functions\nκk and κq, the cross-domain covariance κkq and κqk\nare not symmetric, unless we force Wk\n=\nWq\n=\nW. This relaxes the restrictive Gaussian imposition on\nthe marginal of (zk(x), zq(x)) on a finite set of inputs\nX = {x1, x2, . . . , xn} while still enabling a closed-form\ncomputation of the cross-function prediction of zq\n=\n[zq(x1), . . . , zq(xn)]⊤given the perturbed observations\nzk = [zk1, zk2, . . . , zkn]⊤with zki ∼N(zk(xi), σ2). This\nis possible because (zq(x), zo(x)) and (zk(x), zo(x)) are\nboth Gaussian even though (zk(x), zq(x)) is not. This also\nenables a mathematical quantification of the prediction un-\ncertainty in terms of the conditional covariance of zq | zk.\nSuch modeling properties are desirable because (1) the\nclosed-form representation of the cross-function prediction\nwill help reproduce the kernel attention form in Eq. (6) with\nzq being the attention output and zk being the input to the\nattention unit; and (2) the mathematically induced form of\nits predictive covariance can be leveraged to calibrate the\nuncertainty output of the attention unit. To achieve this, we\nwill establish the closed-form prediction of zq | zk in the\nrest of this section, and then establish its correspondence to\n(asymmetric) kernel attention in Section 3.3.\nTo derive the closed form for E[zq | zk], note that for any\nset of outputs zo = [zo(xo1), zo(xo2), . . . , zo(xon)] of the\ncanonical GP-distributed function zo(x) at a set of latent\ninputs Xo = [xo1, xo2, . . . , xon],\np\n\u0000zq|zk\n\u0001\n=\nZ\nzo\np(zq | zo) · p(zo | zk) dzo .\n(17)\nThe mean of the above distribution is therefore:\nE\nh\nzq | zk\ni\n=\nZ\nzo\n Z\nzq\nzqp(zq | zo)dzq\n!\np(zo | zk) dzo .\nThe above can be rewritten concisely as\nE\nh\nzq | zk\ni\n=\nEzo∼p(zo|zk)\n\"\nE\nh\nzq | zo\ni\n| zk\n#\n,\n(18)\nwhere the inner expectation is over zq ∼p(zq | zo). Now,\nlet Ko denote the induced Gram matrix of κo(x, x′) on the\nset of n latent inputs Xo. As the marginals (zo(x), zq(x))\nand (zo(x), zk(x)) are both Gaussian, it follows that\nE[zq | zo]\n=\nKqo(Ko + σ2I)−1zo ,\n(19)\nE[zo | zk]\n=\nKok(Kk + σ2I)−1zk ,\n(20)\nwhere Kqo and Kok denote the cross covariance matrix be-\ntween zq(x) and zo(x); and between zk(x) and zo(x) on\nXo = [xo1, . . . , xon], respectively. This means the entry at\nrow a and column b of Kqo is κo(xaWq, xob) and likewise,\nthe entry at row a and column b of Kok is κo(xoa, xbWk).\nEq. (20) is the direct result of the Gaussian conditional iden-\ntity. Thus, plugging Eq. (20) into Eq. (18) gives\nE[zq | zk] = Ezo∼p(zo|zk)\nh\nE[zq | zo] | zk\ni\n(21)\n= Kqo(Ko + σ2I)−1Kok(Kk + σ2I)−1zk, (22)\nwhich establishes the closed form for the cross-function\nprediction of zq conditioned on zk.\n3.3\nKERNEL ATTENTION VIA CGP\nWith the above cross-function GP prediction recipe, we are\nnow ready to draw correspondence with (asymmetric) kernel\nattention. As usual, we have\nV+\n=\nKV = KXW⊤\nv ,\n(23)\n\n\nwhere K corresponds to a particular choice of kernel. To\ndraw the correspondence between this and the CGP predic-\ntion, we choose K = Kqo(Ko + σ2I)−1Kok. With this,\nV+ = KV = KXW⊤\nv\n= Kqo(Ko + σ2I)−1Kok(Kk + σ2I)−1Z ,\n(24)\nwhere Z ≜(Kk + σ2I) XW⊤\nv . Again, let νa denotes the\na-th column of V+ and za denote the a-column of Z,\nνa\n=\nKqo\n\u0010\nKo + σ2I\n\u0011−1\nKok\n\u0010\nKk + σ2I\n\u0011−1\nza . (25)\nThis implies the a-th column νa of the kernel atten-\ntion output in fact corresponds to the mean prediction of\nzq(x1), . . . , zq(xn) of the conditional distribution, p(zq|zk)\nwhich was fitted on the dataset {xr, [za]r}n\nr=1. Here, the tar-\nget fitting output {[za]r}n\nr=1 are treated as perturbed obser-\nvations of {zk(xr)}n\nr=1 such that [za]r ∼N(zk(xr), σ2).\nRemark 1 (CGP-based Attention can be Asymmetric.).\nSince the induced Gram matrices Kqk and Kkq do not need\nto be symmetric to guarantee a consistent CGP model, we\nare no longer constrained to set Wq = Wk to enforce\nsymmetric kernel. As a result, our CGP-based attention can\naccommodate attention asymmetries.\nRemark 2 (One CGP per Attention Dimension). Suppose\nthe attention output is s-dimensional, the kernel attention\nunit is equivalent to a collection of s CGPs modeling of s\ndatasets {xr, [za]r}n\nr=1 with a ∈[s].\n3.4\nCORRELATED GP TRANSFORMER\nOur proposed Correlated Gaussian Process Transformer\n(CGPT) framework is derived via replacing the conventional\nattention unit with the above CGP prediction mechanism\n(Section 3.4.1). Unlike a conventional transformer which\noptimizes for performance while neglecting uncertainty cali-\nbration for the attention output, our CGPT will optimize for\nboth to avoid making prediction with high uncertainty while\npreserving overall performance. This is achieved via aug-\nmenting the original transformer loss with a regularization\nloss per attention block, which is expressed in terms of its\nprediction’s uncertainty (Section 3.4.2). This highlights the\nimportance of CGP’s uncertainty quantification mechanism.\nOur overall CGPT workflow is depicted in Fig I.\n3.4.1\nCGP-based Attention\nGiven the above correspondence between the CGP model\nand kernel attention mechanism, we can replace the original\nkernel attention with the following CGP-based attention:\nCGP-based Attention Workflow. At each training iteration,\nupon receiving X = (x1, x2, . . . , xn) from the preceding\nneural block, we will run the following routine in Alg. 1.\nIn the above workflow, the output of each CGP-based atten-\ntion block will be forwarded to the classification block that\ncomputes the logit output of the transformer, which induces\nAlgorithm 1 CGP-based Attention\ninput: sequence of tokens X = (x1, x2, . . . , xn)\noutput: attention output V+ and uncertainty U\n1: compute Z = (Kk + σ2I) XW⊤\nv and initialize U ←0\n2: for a ←1 : s do\n3:\nbuild {xr, [za]r}n\nr=1 where za ←[Z]a\n4:\ncompute νa using Eq. (25).\n5:\nset zk ←([za]1, . . . , [za]n)\n6:\ncompute log p(zq = νa, zk = zk) using Eq. (27)\n7:\nupdate U ←U −log p(zq = νa, zk = zk)\n8: end for\n9: return V+ ←[ν1, . . . , νs] and U\nits training loss. As the CGP-based attention’s output is a\nfunction of the CGP’s modeling parameters, the transformer\nloss is also a function of these parameters. Hence, the CGP\ncan be fitted via minimizing this training loss. However,\nthe CGP parameters learned in this manner might induced\nbrittle prediction with high variance, especially in out-of-\ndistribution data regime. Step 4 of the above workflow is\ntherefore necessary to encode a preference for parameters\nthat induce output with low uncertainty. This is achieved via\naccumulating the CGP’s output uncertainty – Eq. (26) and\nEq. (27) – per attention block into a regularizer, which is\nadded to the main loss of the transformer prior to gradient\npropagation, as demonstrated in Eq. (28).\n3.4.2\nCGP Regularization Loss\nFor each attention output dimension a, the observations\nzk = [zk(x1), zk(x2), . . . , zk(xn)] are set to be za which\nis the a-th column of Z = (Kk + σ2I)XW⊤\nv .\nFollowing Eq. (25), the attention output for this dimension,\nνa = E[zq | zk], is the expected CGP prediction of zq =\n[zq(x1), zq(x2), . . . , zq(xn)] given the observation zk. We\nwould therefore want to maximize:\nlog p\n\u0010\nzq = νa, zk\n\u0011\n= log Ezo\nh\np\n\u0010\nzq = νa, zk\n\f\f\fzo\n\u0011i\n(26)\nbecause this would minimize the output uncertainty of our\nCGP-based attention mechanism, i.e. maximizing the fit\nbetween the input and output of the kernel attention unit. In\nother words, the output uncertainty of the attention output is\nthe negation of the above log probability term. To compute\nit, we note that p(zq, zk | zo) = p(zq | zo) · p(zk | zo)\nsince zk ⊥zq | zo which follows from the CGP definition.\nConsequently, we have\nlog Ezo\nh\np(zq, zk | zo)\ni\n= log Ezo\nh\np(zq | zo) · p(zk | zo)\ni\n= log Ezo\nh\np(zq | zo)\ni\n+ log Ezo\nh\np(zk | zo)\ni\n.\n(27)\nNow, let loss(νa) denote the original loss of the transformer\nwhich is a function of the attention output3 νa. To opt for\n3For simplicity, we narrate this part assuming there is a single\n\n\nAttention 1\nFeed forward 1\nAttention 2\nFeed forward 2\nCGP log term 2\n...\nAttention n\nFeed forward n\nCGP log term n\nTransformer loss\nCGP objective function\nCGP log term 1\nCGP log term all layers\n...\n(Algorithm 1)\n(Algorithm 1)\n(Algorithm 1)\nFigure I: Diagram of the training workflow of CGPT. Each attention block forwards the CGP’s prediction to the next block\nand caches the prediction uncertainty into a CGP regularizing term (see Algorithm 1). Once the attention output is propagated\nto the last classification block, the original transformer loss is computed and augmented with the CGP regularizing term.\nGradient propagation from this augmented loss will help optimize the CGP parameters to reduce prediction uncertainty\nwhile maximizing predictive performance.\nboth uncertainty minimization and performance maximiza-\ntion, we propose to minimize the following augmented loss\nθ∗= arg minθ L(θ) where\nL(θ) ≜loss(νa) −α · log p(zq = νa, zk)\n= loss(νa) −α · log Ezo\nh\np(zq = νa, zk | zo)\ni\n, (28)\nwhere α > 0 is a regularization coefficient while θ rep-\nresents the collection of all CGP parameters from which\nthe attention output νa = E[zq | zk] and the CGP density\np(zq = νa, zk | zo) are computed.\nFinally, plugging Eq. (27) in Eq. (28),\nL(θ)\n=\nloss(νa) −α ·\nh\nlog Ezo\nh\np(zq = νa | zo)\ni\n+\nlog Ezo\nh\np(zk | zo)\nii\n,\n(29)\nwhere p(zq | zo) and p(zk | zo) are both Gaussian whose\nspecific forms are detailed in Appendix A. We refer to the\nobjective function in (29) as the CGP objective and its full\nderivation as well as the uncertainty calibration of the in-\nduced attention output is detailed in Appendix A.\nRemark 3. The regularization coefficient α > 0 is a hyper-\nparameter balances between performance maximization and\nuncertainty minimization. In practice, it can be empirically\nselected using a validation set.\nattention block with one output dimension. Otherwise, it is straight-\nforward to extend the above to multiple attention blocks with\nmultiple output dimensions by including one uncertainty term per\nattention block and output dimension into the final loss.\n4\nSPARSE APPROXIMATION\nAs CGPT is developed based on the correlation structure\nbetween the two full-rank, correlated Gaussian processes, its\ncomplexity also scales cubically in the size of the number of\ninput tokens. This is evident in Eq. (25) which computes the\nCGP’s predictive mean via inverting Gram matrices of size n\nby n where n is the length of the input sequence. This incurs\na prohibitively expensive computation cost of O(n3). To\nmitigate this cubic dependence on the input length, we fur-\nther develop a sparse approximation to CGP. The resulting\nsparse approximation can thus replaced the aforementioned\nCGP-based attention, which gives rise to a new framework\nof sparse correlated Gaussian process transformer (SCGPT).\nTo derive this sparse approximation, we begin with the pre-\ndictive mean E[zq | zk], whose explicit form is essential to\ndraw correspondence to the output of kernel attention,\nE\nh\nzq | zk\ni\n=\nEzo∼p(zo|zk)\n\"\nE\nh\nzq | zo\ni\n| zk\n#\n.\n(30)\nFollowing the previous derivation in Section 3.2, we rec-\nognize that the main computational bottleneck stems from\nthe fact that we are computing the nested expectation above\nwith respect to the predictive distributions of two full-rank\nGaussian processes, p(zq | zo) and p(zo | zk). Thus, to mit-\nigate such bottleneck, we can instead adopt existing sparse\napproximations of Gaussian processes [Smola and Bartlett,\n2001, Tresp, 2000, Schwaighofer and Tresp, 2003, Seeger\net al., 2003, Quiñonero-Candela and Rasmussen, 2005, Snel-\nson and Gharahmani, 2005, Titsias, 2009, Lázaro-Gredilla\net al., 2010, Hensman et al., 2013, Hoang et al., 2015, 2016,\n\n\n2017, 2019]. In this work, we use the Deterministic Training\nConditional (DTC) approximation of [Seeger et al., 2003].\nSpecifically, we first replace the exact p(zq | zo) with its\nDTC approximation, which results in the following sparse\napproximation of E[zq | zo],\nE[zq | zo]= 1\nσ2 Kqm\n\u0010\nKmm + 1\nσ2 KmoKom\n\u0011−1\nKmozo, (31)\nwhere Kmm is the Gram matrix of a set of inducing points\n{s1, s2, . . . , sm} that lives on the input space of zo(x) while\nKqm and Kom denote the cross-covariance matrices be-\ntween the {zq(xi)}n\ni=1, {zo(xi)}n\ni=1 with {si}m\ni=1, respec-\ntively; and Kmo is the transposition of Kom.\nLikewise, we can do the same for p(zo | zk), which results\nin the following sparse approximation for E[zo | zq],\nE(zo | zk) =\n1\nσ2 Koℓ\n\u0010\nKℓℓ+ 1\nσ2 KℓkKkℓ\n\u0011−1\nKℓkzk,\n(32)\nwhich is based on another set of inducing points {s′\ni}ℓ\ni=1 that\nlives on the input space of zo(x) while Koℓand Kkℓdenote\nthe cross-covariance between the {zo(xi)}n\ni=1, {zk(xi)}n\ni=1\nwith {s′\ni}ℓ\ni=1, respectively; Kℓk is the transposition of Kkℓ\nand Kℓℓis the Gram matrix of {s′\ni}ℓ\ni=1. Plugging Eq. (31)\nand Eq. (32) into Eq. (30) leads to a closed form for the\nSCGP’s predictive mean,\nE[zq | zk]\n=\n1\nσ4 Kqm\n\u0010\nKmm + 1\nσ2 KmoKom\n\u0011−1\nKmo\n×\nKoℓ\n\u0010\nKℓℓ+ 1\nσ2 KℓkKkℓ\n\u0011−1\nKℓkzk,\n(33)\nwhich is a direct consequence of taking expectation of\nGaussian random variables. The readers are referred to\nAppendix C for a detailed step-by-step derivation. Using\nEq. (33), we can now draw a correspondence between the\npredictive mean of SCGP and the output of kernel attention,\nV+\n=\nKV = KXW⊤\nv .\n(34)\nvia setting Z = XW⊤\nv and the kernel attention matrix K as\nK\n≜\n1\nσ4 Kqm\n\u0010\nKmm + 1\nσ2 KmoKom\n\u0011−1\n×\nKmoKoℓ\n\u0010\nKℓℓ+ 1\nσ2 KℓkKkℓ\n\u0011−1\nKℓk .\n(35)\nThe cubic cost is now mitigated to the number m and ℓ\nof the two set of inducing inputs introduced above. Thus,\nto improve scalability, we can opt for small values of m\nand ℓsuch that κ = max(m, ℓ) ≪n. Hence, the overall\ncomplexity of computing the predictive mean of SCGP is\nnow O(κ3 + n2 · κ) which is more efficient than CGPT’s\nwhich is O(n3).\nRemark 4. Both sets of inducing inputs {s1, s2, . . . , sm}\nand {s′\n1, s′\n2, . . . , s′\nℓ} can be treated as part of the kernel\nparameters in our SCGP scheme and can be learned to-\ngether with other kernel parameters while we optimize the\ncorresponding augmented loss of SCGPT, whose details are\ndeferred to Appendix E due to limited space.\n5\nEXPERIMENTAL RESULTS\nIn this section, we empirically study the advantages of\nCGPT and SCGPT in calibrating transformers on a variety\nof tasks including the COLA linguistic acceptability predic-\ntion task [Warstadt et al., 2019] and CIFAR10 classification\nand out-of-distribution (OOD) evaluation [Krizhevsky et al.,\n2009, Hendrycks and Dietterich, 2019]. We aim to show\nthat both of our CGPT and SCGPT can attain comparable\nor better calibration ability than the SGPA [Chen and Li,\n2023] and kernel attention [Tsai et al., 2019] baselines due\nto the increased representation capacity, which is enabled\nvia the use of asymmetric kernel function. Moreover, we\nalso compare the complexity efficiency of SCGPT against\nthe SGPA [Chen and Li, 2023] baseline. We adopt similar\nexperiment settings as in [Chen and Li, 2023]. Additional\nexperimental results are provided in Appendix F.\n5.1\nEXPERIMENT SETTINGS\nFollowing the prior work of [Chen and Li, 2023], we will\nconduct experiments on image classification and the linguis-\ntic acceptability prediction with the following setup:\nTasks. We study the performance of CGPT and SCGPT\non image classification using the CIFAR10 [Krizhevsky\net al., 2009] dataset and linguistic acceptability prediction\nusing the CoLA dataset [Warstadt et al., 2019]. For the out-\nof-distribution (OOD) evaluations, we use the corrupted\nCIFAR10-C dataset [Hendrycks and Dietterich, 2019] for\nimage classification and the out-of-distribution data within\nthe CoLA dataset for linguistic acceptability prediction. We\nalso evaluate and compare the uncertainty calibration of\nour proposed models and other baselines in OOD detection\ntasks for image classification (see Section 5.2).\nGeneral settings for all tasks. For SGPA and kernel atten-\ntion, we use the ARD-RBF kernel [Rasmussen and Williams,\n2006] for the image classification tasks κ(x, x′)\n=\nσ2\ns exp(−0.5 Pd\ni=1(xi −x′\ni)2/σ2\ni ) , and an exponential of\nscaled dot product variant for the linguistic acceptability\ntask κ(x, x′) = σ2\ns exp(Pd\ni=1 xix′\ni/σ2\ni ) . Here, x and x′\nare d-dimensional inputs, σ2\ns denotes the output variance\nand {σ2\ni }d\ni=1 are the length scales. For CGPT and SCGPT,\nwe use the parameter-free squared exponential kernel func-\ntion for all tasks κo(x, x′) = exp(−0.5∥x −x′∥2) as the\ncanonical representation and model the latent inputs Xo by\nlinear projection of a finite set of inputs X for simplicity.\nThe regularization coefficient α in our objective function\nis chosen using its induced performance on a validation\ndataset. Our experiments are conducted on A100 40GB\nSMX NVIDIA GPUs.\nBaselines. We compare the performance of our proposed\nmodels against that of SGPA [Chen and Li, 2023], which\nleverages sparse GP to design (symmetric) attention and\nprovide uncertainty calibration for the Transformer. Our pro-\nposed models are also compared against standard non-GP\nbaselines with symmetric and asymmetric kernel attention\n\n\nTable I: Test MCC and other uncertainty calibration metrics achieved by SGPA and our CGPT/SCGPT on the CoLA dataset\nunder both in-distribution and out-of-distribution settings. The first rows report results on CoLA under the in-distribution\nsetting. The last row reports results on CoLA under the OOD setting. For each metric, we report the mean value and its\nstandard deviation obtained over multiple runs.\nDataset\nModel\nMCC ↑\nNLL ↓\nMCE ↓\nECE ↓\nCoLA\nSGPA\n28.826 ± 0.982\n0.842 ± 0.045\n0.713 ± 0.031\n0.257 ± 0.011\nCGPT (ours)\n26.471 ± 0,387\n0.774 ± 0.010\n0.725 ± 0.013\n0.236 ± 0.004\nSCGPT (ours)\n27.686 ± 1.425\n2.254 ± 0.204\n0.724 ± 0.004\n0.293 ± 0.003\nCoLA (OOD)\nSGPA\n22.500 ± 0.877\n0.876 ± 0.053\n0.740 ± 0.040\n0.271 ± 0.0192\nCGPT (ours)\n26.957 ± 0.748\n0.749 ± 0.038\n0.711 ± 0.002\n0.230 ± 0.004\nSCGPT (ours)\n25.369 ± 0.452\n2.243 ± 0.110\n0.700 ± 0.003\n0.306 ± 0.010\nTable II: Test accuracy and other calibration metrics achieved by our CGPT/SCGPT models on CIFAR10-C dataset under\nthe OOD setting. For each distortion category, we report the mean metrics over all distortion types. We observe that\nCGPT/SCGPT attains better accuracy and calibration metrics than SGPA across 14/16 cases, indicating that CGPT/SCGPT\nis more robust than SGPA under distribution shift.\nMetric\nModel\nNoise\nBlur\nWeather\nDigital\nAvg.\nAcc ↑\nSGPA\n50.803 ± 0.447\n59.264 ± 0.915\n64.148 ± 0.472\n63.028 ± 0.334\n59.722 ± 0.323\nKernel (asym)\n53.014 ± 0.040\n61.327 ± 1.511\n63.426 ± 0.930\n62.507 ± 0.847\n60.340 ± 0.816\nKernel (sym)\n52.675 ± 0.190\n60.093 ± 1.846\n62.643 ± 0.472\n62.710 ± 0.520\n59.884 ± 0.838\nCGPT (ours)\n55.177 ± 0.953\n56.412 ± 1.506\n61.515 ± 0.703\n60.373 ± 0.123\n58.591 ± 0.664\nSCGPT (ours)\n57.701 ± 0.870\n59.647 ± 0.925\n63.287 ± 0.849\n62.516 ± 0.252\n61.746 ± 0.438\nNLL ↓\nSGPA\n3.464 ± 0.423\n2.551 ± 0.091\n2.137 ± 0.162\n2.298 ± 0.045\n2.626 ± 0.202\nKernel (asym)\n3.779 ± 0.604\n2.690 ± 0.293\n2.462 ± 0.305\n2.673 ± 0.176\n2.875 ± 0.384\nKernel (sym)\n3.379 ± 0.448\n2.435 ± 0.177\n2.262 ± 0.283\n2.389 ± 0.303\n2.591 ± 0.331\nCGPT (ours)\n1.688 ± 0.033\n1.565 ± 0.068\n1.352 ± 0.049\n1.461 ± 0.027\n1.516 ± 0.029\nSCGPT (ours)\n2.060 ± 0.064\n1.835 ± 0.081\n1.663 ± 0.046\n1.796 ± 0.051\n1.787 ± 0.017\nMCE ↓\nSGPA\n0.668± 0.009\n0.592 ± 0.014\n0.576 ± 0.014\n0.575 ± 0.001\n0.593 ± 0.002\nKernel (asym)\n0.512± 0.021\n0.460 ± 0.016\n0.456 ± 0.010\n0.457 ± 0.020\n0.470 ± 0.018\nKernel (sym)\n0.498± 0.014\n0.449 ± 0.011\n0.443 ± 0.007\n0.437 ± 0.020\n0.456 ± 0.024\nCGPT (ours)\n0.360 ± 0.011\n0.334 ± 0.013\n0.284 ± 0.002\n0.314 ± 0.003\n0.324 ± 0.002\nSCGPT (ours)\n0.443 ± 0.018\n0.417 ± 0.016\n0.400 ± 0.003\n0.419 ± 0.003\n0.421 ± 0.004\nECE ↓\nSGPA\n0.532 ± 0.021\n0.488 ± 0.012\n0.469 ± 0.003\n0.472 ± 0.010\n0.487 ± 0.012\nKernel (asym)\n0.377 ± 0.015\n0.294 ± 0.004\n0.275 ± 0.008\n0.280 ± 0.011\n0.304 ± 0.009\nKernel (sym)\n0.363 ± 0.023\n0.285 ± 0.001\n0.266 ± 0.012\n0.267 ± 0.012\n0.292 ± 0.010\nCGPT (ours)\n0.226 ± 0.012\n0.202 ± 0.007\n0.159 ± 0.004\n0.183 ± 0.003\n0.192 ± 0.001\nSCGPT (ours)\n0.292 ± 0.010\n0.259 ± 0.004\n0.234 ± 0.005\n0.243 ± 0.004\n0.249 ± 0.002\nTable III: Averaged OOD detection performance achieved by SCGPT, SGPA and kernel attention over 4 datasets (Textures,\nLSUNCrop, LSUNResize and TinyImageNetCrop). For each method, the average OOD performance is reported for each\ndetector. SCGPT outperforms the baselines in most OOD detection metrics and has the best performance on average,\nsuggesting its advantage on OOD detection task.\nModel\nDetector\nAUROC ↑\nAUPR-IN ↑\nAUPR-OUT ↑\nFPR@95 ↓\nKernel (sym)\nKLMatching\n63.80\n60.61\n63.70\n87.08\nMaxSoftmax\n69.39\n61.00\n75.21\n71.68\nEntropy\n69.82\n62.08\n75.35\n71.68\nEnergy-Based\n72.83\n62.79\n76.85\n65.08\nAverage\n68.21\n61.97\n72.03\n73.38\nKernel (asym)\nKLMatching\n64.72\n60.62\n62.83\n92.47\nMaxSoftmax\n69.51\n61.40\n75.11\n71.30\nEntropy\n70.01\n62.54\n75.39\n70.78\nEnergy-Based\n76.15\n66.61\n80.96\n58.95\nAverage\n70.10\n62.78\n73.32\n73.37\nSGPA\nKLMatching\n64.82\n60.32\n63.75\n90.72\nMaxSoftmax\n68.63\n60.76\n74.50\n72.62\nEntropy\n69.16\n61.97\n74.74\n72.31\nEnergy-Based\n77.78\n62.66\n82.46\n58.21\nAverage\n70.09\n61.42\n73.86\n73.47\nSCGPT (ours)\nKLMatching\n67.31\n62.11\n66.78\n86.50\nMaxSoftmax\n69.18\n61.10\n75.13\n71.39\nEntropy\n69.91\n62.82\n75.47\n70.89\nEnergy-Based\n77.70\n68.56\n82.06\n60.09\nAverage\n70.27\n63.40\n74.12\n72.47\n[Tsai et al., 2019].\nArchitectures. We use Vision Transformer [Dosovitskiy\net al., 2020] for image classification and standard trans-\nformer architecture [Vaswani et al., 2017] for linguistic\nacceptability prediction. We use the parameter-free squared\nexponential kernel for CGPT and SCGPT for both of the\ntasks while in SGPA, we use the ARD kernel [Rasmussen\nand Williams, 2006] for image classification and the expo-\n\n\nnential kernel for linguistic acceptability prediction.\nEvaluation. We study the calibration capacity of the mod-\nels by evaluating the robustness of them under out-of-\ndistribution setting in section 5. We also compare the out-\nof-distribution detection capacity of our methods against\nother baselines in section 5.2. We report the accuracy (Acc)\nfor the image classification tasks and Matthew correlation\ncoefficient (MCC) for CoLA, as well as other test calibra-\ntion metrics, including negative log likelihood (NLL), ex-\npected calibration error (ECE) and maximum calibration\nerror (MCE).\nCGPT and SCGPT proprietary hyperparameters. The\nα value in our CGP objective function is linearly annealed\nfrom 0.0 to 1.0 during the training phase. For SCGPT, we\nset the inducing variable dimension m to be m = 16 in im-\nage classification tasks, which is smaller than the sequence\nlength n in order to be more memory and computationally\nefficient, as discussed in Section 4. The value of the noise σ\nin SCGPT is tuned from 0 to 1 and chosen to be σ = 0.1 as\nwe find that value gives the best performance for SCGPT.\n5.1.1\nImage Classification\nFor the OOD tasks on CIFAR10-C, we use the corrupted\ndatasets and the models trained on the clean datasets to eval-\nuate the OOD performances. The CIFAR10-C dataset con-\ntains 19 types of distortions covering 4 distortion categories:\nNoise, Blur, Weather and Digital. For each experiment on\neach type of corruption, we report the mean OOD results\nover multiple independent runs. The corresponding standard\ndeviations are also reported.\nDatasets. The original training partition of the CIFAR10\ndataset is randomly split into 45,000 instances for training\nand 5,000 instances for validation.\nImplementation details. The architecture of ViT for the\nCIFAR10 dataset contains 5 MHSA layers. Each layer has\n4 attention heads whose the hidden dimension is set to 128.\nBoth CGPT and SCGPT are trained with batch-size 100\nfor 600 epochs. Their loss functions are minimized us-\ning ADAM [Kingma and Ba, 2014] with an initial learn-\ning rate of 0.0005 which decays to 0.00001 linearly. We\nadopt the same training scheme of [Chen and Li, 2023]\nfor CGPT/SCGPT: ViT with asymmetric kernel attention is\ntrained for the first 200 epochs and its parameters are used\nto initialize parameters which are continued to be updated\nfor the next 400 epochs using the CGPT’s/SCGPT’s loss\nfunction. For SGPA, we use the same hyper-parameter con-\nfiguration as reported in [Chen and Li, 2023] for training.\nEvaluation. We choose the best model using the validation\naccuracy evaluated after each 10 epochs. The reported re-\nsults are averaged over multiple independent runs. Their cor-\nresponding mean and standard deviation are also reported.\n5.1.2\nLinguistic Acceptability\nFor the OOD task on the COLA dataset, we use the provided\nOOD set and the model trained on the corresponding clean\ndataset to evaluate the robustness of model’s performance.\nDatasets. The COLA dataset contains 516 OOD samples\nand the original (clean) training set, which is randomly split\ninto 7, 262 in-distribution training samples and 1, 816 in-\ndistribution testing samples.\nImplementation details. The architecture of Transformer\nfor the COLA dataset has 2 MHSA layers with each layer\ncontains 4 attention heads. The hidden dimension and em-\nbedding dimension are 256 and 128 respectively. We also\nuse ELMO-style representation [Peters et al., 2018] for the\ninput embeddings as in [Chen and Li, 2023].\nCGPT and SCGPT are trained with batch-size 32 for 50\nepochs. Their loss functions are minimized using the ADAM\noptimizer with an initial learning rate of 0.0005 which de-\ncays to 0.00001 linearly. For SGPA, we use the same hyper-\nparameter configuration of [Chen and Li, 2023]. We choose\nthe noise term to be σ = 0.5 for SCGPT.\nEvaluation. The performance of the model is evaluated after\n50 training epochs. The reported performance is averaged\nover multiple independent runs with different random seeds.\nThe corresponding standard deviations are also reported.\n5.2\nOUT-OF-DISTRIBUTION CALIBRATION\nWe perform out-of-distribution (OOD) prediction under dis-\ntribution perturbation on image classification (CIFAR10)\nand linguistic acceptability (CoLA) tasks. We use the OOD\ndata for CoLA provided in [Warstadt et al., 2019]. For\nclassification task, we use the corrupted CIFAR datasets\n(CIFAR10-C) [Hendrycks and Dietterich, 2019] as OOD\ndata, featuring images under different forms of distortion.\nOn the OOD CoLA dataset, Table I shows that CGPT and\nSCGPT achieve the best performance across all metrics eval-\nuated. SCGPT also outperforms SGPA on 2 out of 4 metrics\nincluding MCC on the OOD setting. For the vision task,\nwe observe that SCGPT achieves the best average out-of-\ndistribution accuracy over all forms of distortion. SCGPT\nalso has the second-highest performance across all calibra-\ntion metrics, falling slightly behind CGPT, while surpassing\nall other baseline models. Interestingly, SCGPT significantly\noutperforms SGPA while using a set of 16 inducing inputs,\nwhich is half the number of inducing inputs used by SGPA.\nAs a result, SCGPT uses less GPU memory than SGPA (see\nFigure II) while achieving better results. CGPT also out-\nperforms all baselines across all the calibration metrics and\ndistortion types introduced in CIFAR10-C while preserving\ncomparable accuracy as shown in Table II. These results jus-\ntify that our proposed methods are more robust than SGPA\nand kernel attention under distribution shift in terms of both\naccuracy and calibration ability.\n\n\nSCGPT\nSGPA\nNumber of inducing inputs\nNumber of epochs\nMemory (mb)\nProcessing time (s)\nFigure II: Runtime per training epoch (right) and GPU memory\nallocated during training (left) of SCGPT and SGPA on CIFAR10.\nSCGPT is more efficient than SGPA in terms of GPU memory\nusage while having a comparable runtime per epoch to SGPA.\n5.3\nOUT-OF-DISTRIBUTION DETECTION\nIn this experiment, we use the CIFAR10 dataset as the\nin-distribution dataset for OOD detection. We choose 4\ndifferent common image datasets for the OOD detection\ntask which includes Textures, LSUNCrop, LSUNResize and\nTinyImageNetCrop as our OOD datasets. For the detectors\nthat detect outliers, we choose 4 state-of-the-art detectors\nto be used in our experiments: KLMatching [Hendrycks\net al., 2019], Maximum Softmax Probability (MaxSoftmax)\n[Hendrycks and Gimpel, 2016], Entropy Maximization (En-\ntropy) [Chan et al., 2021] and Energy-Based OOD Detection\n(EnergyBased) [Liu et al., 2020b]. We use the following stan-\ndard OOD detection metrics for evaluation, which includes\n(1) the area under the Receiver Operating Characteristic\ncurve (AUROC), (2) the in-distribution and out-distribution\narea under the Precision-Recall curve (AUPR-IN and AUPR-\nOUT) and (3) the false positive rate when the true positive\nrate is equal to 95% (FPR@95). For each method, we evalu-\nate the OOD performance of SCGPT and SGPA measured\nusing the above metrics on the 4 OOD datasets and report\nthe mean metrics for each of the 4 detectors. All results are\nreported in Table III, which shows that SCGPT has the best\nperformance in 7/16 cases (4 metrics × 4 detectors) while\nthe rest of the baselines (including SGPA and the two vari-\nants of kernel attention) only has the best performance in\nno more than 4/16 cases. Furthermore, on average, SCGPT\nalso outperforms all other baselines across all metrics, show-\ning the best (averaged) quality of uncertainty estimates.\n5.4\nREDUCING OVERSMOOTHING\nOversmoothing [Shi et al., 2022] occurs when the output\nof transformers converge to a low-rank sub-space as the\nnumber of attention blocks increases, limiting the expres-\nsivity of the models. Fig. III in Appendix F.2 demonstrates\nthat our CGPT and SCGPT help alleviate oversmoothing in\ntransformers while the SGPA and kernel attention baselines\ndo not. This is demonstrated via comparing the cosine simi-\nlarities between the output of the attention blocks, i.e., the\ngreater the cosine similarities, the more oversmoothing.\n5.5\nEFFICIENCY ANALYSIS\nThis section compares the processing and memory costs\nincurred by SCGPT and SGPA during training. Since both\nmethods utilize sparse GPs, we report their average process-\ning time per training epoch and GPU memory usage with\nrespect to the number of inducing inputs used in their sparse\napproximation. For CIFAR10, the sequence length is 64. So,\nwe report the processing time and memory consumption of\nboth methods with respect to using 8, 16 and 32 inducing\ninputs. Figure II indicates that SCGPT incurs less memory\ncost than SGPA while still preserving a comparable pro-\ncessing time to SGPA. Particularly, the GPU memory cost\nincurred by SCGPT is less than that of SGPA over all the\nabove settings and the processing time per epoch of the two\nmethods are also comparable to each other. This implies\nthat SCGPT scales better to larger tasks than SGPA.\n6\nRELATED WORK\nRecent works have aimed to calibrate transformers using\nBayesian approaches. Fan et al. [2020] and Cinquin et al.\n[2021] apply variational inference to the attention matri-\nces. Liu et al. [2020a], Bradshaw et al. [2017] suggests\nfitting a GP on the output of the last attention layer. An-\nother work utilizing GP was proposed by [Chen and Li,\n2023] that fits a sparse variational GP to each attention\nlayer and propagates uncertainty across the layers. CGPT\nextends this research direction by fitting correlated GPs to\nthe attention outputs.Additionally, convolutional and recur-\nrent neural networks, have benefited from the application\nof Bayesian approaches [Mukhoti and Gal, 2018, Kendall\nand Gal, 2017, Gustafsson et al., 2020, Chien and Ku, 2015,\nRitter et al., 2021, Tran et al., 2019], and early efforts to\nemploy similar methods for transformers have attained ini-\ntial successes [Xue et al., 2021]. Another line of work by\n[Müller et al., 2021] make the connection between trans-\nformers and Bayesian inference, showing that transformers\ncan efficiently do Bayesian inference. Our proposed CGPT\nis complementary to those methods.\n7\nCONCLUDING REMARKS\nThis paper introduces the Correlated Gaussian Process\nTransformer (CGPT), a new framework to calibrate uncer-\ntainty for transformers. CGPT leverages a novel CGP rep-\nresentation, which allows us to draw connection between\nthe output of kernel attention often used in transformers and\ninference using cross-covariance between two correlated\nGPs defined through a latent canonical GP. With this for-\nmulation, our cross-covariance function does not have to\nbe a symmetric kernel, which is a condition imposed on\nexisting GP-based transformers in exchange for uncertainty\ncalibration. Therefore, our framework preserves the flexibil-\nity in the representation capacity of attention by allowing\nasymmetries in the attention unit while being able to fully\nutilize the uncertainty estimation ability of GPs. Improving\nthe efficiency of CGPT using random features or sparse GP\nis an interesting future work to explore.\nReferences\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo,\nand Llion Jones. Character-level language modeling with\n\n\ndeeper self-attention. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, volume 33, pages 3159–\n3166, 2019.\nAnurag Arnab, Mostafa Dehghani, Georg Heigold, Chen\nSun, Mario Luˇci´c, and Cordelia Schmid. Vivit: A video\nvision transformer.\nIn 2021 IEEE/CVF International\nConference on Computer Vision (ICCV), pages 6816–\n6826, 2021. doi: 10.1109/ICCV48922.2021.00676.\nAlexei Baevski and Michael Auli.\nAdaptive input rep-\nresentations for neural language modeling.\nIn In-\nternational Conference on Learning Representations,\n2019. URL https://openreview.net/forum?\nid=ByxZX20qFQ.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.\nNeural machine translation by jointly learning to align\nand translate. arXiv preprint arXiv:1409.0473, 2014.\nJohn Bradshaw, Alexander G de G Matthews, and Zoubin\nGhahramani.\nAdversarial examples, uncertainty, and\ntransfer testing robustness in gaussian process hybrid\ndeep networks. arXiv preprint arXiv:1707.02476, 2017.\nTom Brown and et al.\nLanguage models are few-shot\nlearners.\nIn H. Larochelle, M. Ranzato, R. Hadsell,\nM. F. Balcan, and H. Lin, editors, Advances in Neural\nInformation Processing Systems, volume 33, pages 1877–\n1901,\n2020.\nURL\nhttps://proceedings.\nneurips.cc/paper/2020/file/\n1457c0d6bfcb4967418bfb8ac142f64a-Paper.\npdf.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,\nJared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in neural\ninformation processing systems, 33:1877–1901, 2020.\nRobin Chan, Matthias Rottmann, and Hanno Gottschalk.\nEntropy maximization and meta classification for out-\nof-distribution detection in semantic segmentation. In\nProceedings of the ieee/cvf international conference on\ncomputer vision, pages 5128–5137, 2021.\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee,\nAditya Grover, Misha Laskin, Pieter Abbeel, Aravind\nSrinivas, and Igor Mordatch. Decision transformer: Rein-\nforcement learning via sequence modeling. Advances in\nneural information processing systems, 34:15084–15097,\n2021.\nWenlong Chen and Yingzhen Li. Calibrating transform-\ners via sparse gaussian processes.\nIn The Eleventh\nInternational Conference on Learning Representations,\n2023. URL https://openreview.net/forum?\nid=jPVAFXHlbL.\nYingyi Chen, Qinghua Tao, Francesco Tonin, and Johan\nSuykens. Primal-attention: Self-attention through asym-\nmetric kernel svd in primal representation. Advances in\nNeural Information Processing Systems, 36, 2024.\nJen-Tzung Chien and Yuan-Chu Ku. Bayesian recurrent\nneural network for language modeling. IEEE transactions\non neural networks and learning systems, 27(2):361–374,\n2015.\nKyunghyun Cho, Bart van Merriënboer, Caglar Gulcehre,\nDzmitry Bahdanau, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations\nusing RNN encoder–decoder for statistical machine\ntranslation.\nIn Proceedings of the 2014 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 1724–1734, Doha, Qatar, October\n2014. Association for Computational Linguistics. doi:\n10.3115/v1/D14-1179. URL https://www.aclweb.\norg/anthology/D14-1179.\nTristan Cinquin, Alexander Immer, Max Horn, and Vincent\nFortuin. Pathologies in priors and inference for bayesian\ntransformers. arXiv preprint arXiv:2110.04020, 2021.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell,\nQuoc V Le, and Ruslan Salakhutdinov. Transformer-xl:\nAttentive language models beyond a fixed-length context.\narXiv preprint arXiv:1901.02860, 2019.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob\nUszkoreit, and Lukasz Kaiser. Universal transformers. In\nInternational Conference on Learning Representations,\n2019. URL https://openreview.net/forum?\nid=HyzdRiR9Y7.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova.\nBERT: Pre-training of deep bidirectional\ntransformers for language understanding. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguis-\ntics: Human Language Technologies, Volume 1 (Long\nand Short Papers), pages 4171–4186, Minneapolis, Min-\nnesota, June 2019. Association for Computational Lin-\nguistics. doi: 10.18653/v1/N19-1423. URL https:\n//aclanthology.org/N19-1423.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, et al. An image is worth 16x16 words:\nTransformers for image recognition at scale.\narXiv\npreprint arXiv:2010.11929, 2020.\nXinjie Fan, Shujian Zhang, Bo Chen, and Mingyuan Zhou.\nBayesian attention modules. Advances in Neural Infor-\nmation Processing Systems, 33:16362–16376, 2020.\n\n\nMeng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang\nMu, Ralph R Martin, and Shi-Min Hu. Pct: Point cloud\ntransformer. Computational Visual Media, 7(2):187–199,\n2021.\nFredrik K Gustafsson, Martin Danelljan, and Thomas B\nSchon. Evaluating scalable bayesian deep learning meth-\nods for robust computer vision. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern\nrecognition workshops, pages 318–319, 2020.\nOliver Hamelijnck, William Wilkinson, Niki Loppi, Arno\nSolin, and Theodoros Damoulas. Spatio-temporal varia-\ntional gaussian processes. Advances in Neural Informa-\ntion Processing Systems, 34:23621–23633, 2021.\nDan Hendrycks and Thomas Dietterich. Benchmarking\nneural network robustness to common corruptions and\nperturbations. arXiv preprint arXiv:1903.12261, 2019.\nDan Hendrycks and Kevin Gimpel. A baseline for detecting\nmisclassified and out-of-distribution examples in neural\nnetworks. arXiv preprint arXiv:1610.02136, 2016.\nDan Hendrycks, Steven Basart, Mantas Mazeika, Andy Zou,\nJoe Kwon, Mohammadreza Mostajabi, Jacob Steinhardt,\nand Dawn Song. Scaling out-of-distribution detection\nfor real-world settings. arXiv preprint arXiv:1911.11132,\n2019.\nJ. Hensman, N. Fusi, and N. D. Lawrence. Gaussian pro-\ncesses for big data. In Proc. UAI, pages 282–290, 2013.\nQ. M. Hoang, T. N. Hoang, and K. H. Low. A generalized\nstochastic variational Bayesian hyperparameter learning\nframework for sparse spectrum Gaussian process regres-\nsion. In Proc. AAAI, pages 2007–2014, 2017.\nT. N. Hoang, Q. M. Hoang, and K. H. Low. A unifying\nframework of anytime sparse Gaussian process regression\nmodels with stochastic variational inference for big data.\nIn Proc. ICML, pages 569–578, 2015.\nT. N. Hoang, Q. M. Hoang, and K. H. Low. A distributed\nvariational inference framework for unifying parallel\nsparse Gaussian process regression models.\nIn Proc.\nICML, pages 382–391, 2016.\nT. N. Hoang, Q. M. Hoang, K. H. Low, and J. P. How. Col-\nlective online learning of Gaussian processes in massive\nmulti-agent systems. In Proc. AAAI, 2019.\nSepp Hochreiter and Jürgen Schmidhuber. Long short-term\nmemory. Neural computation, 9(8):1735–1780, 1997.\nMichael Janner, Qiyang Li, and Sergey Levine. Offline\nreinforcement learning as one big sequence modeling\nproblem. Advances in neural information processing\nsystems, 34:1273–1286, 2021.\nAlex Kendall and Yarin Gal. What uncertainties do we need\nin bayesian deep learning for computer vision? Advances\nin neural information processing systems, 30, 2017.\nYoon Kim, Carl Denton, Luong Hoang, and Alexander M\nRush. Structured attention networks. arXiv preprint\narXiv:1702.00887, 2017.\nDiederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980,\n2014.\nAlex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-\n10 and cifar-100 datasets. URl: https://www. cs. toronto.\nedu/kriz/cifar. html, 6(1):1, 2009.\nM. Lázaro-Gredilla, J. Quiñonero-Candela, C. E. Ras-\nmussen, and A. R. Figueiras-Vidal. Sparse spectrum\nGaussian process regression. Journal of Machine Learn-\ning Research, pages 1865–1881, 2010.\nTianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng\nQiu. A survey of transformers. AI Open, 2022.\nZhouhan Lin, Minwei Feng, Cícero Nogueira dos Santos,\nMo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio.\nA structured self-attentive sentence embedding. CoRR,\nabs/1703.03130, 2017. URL http://arxiv.org/\nabs/1703.03130.\nJeremiah Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania\nBedrax Weiss, and Balaji Lakshminarayanan. Simple\nand principled uncertainty estimation with deterministic\ndeep learning via distance awareness. Advances in Neural\nInformation Processing Systems, 33:7498–7512, 2020a.\nWeitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li.\nEnergy-based out-of-distribution detection. Advances in\nneural information processing systems, 33:21464–21475,\n2020b.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. Roberta: A robustly\noptimized bert pretraining approach.\narXiv preprint\narXiv:1907.11692, 2019.\nZe Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang,\nStephen Lin, and Han Hu. Video swin transformer. In\nIEEE Conference on Computer Vision and Pattern Recog-\nnition (CVPR), 2022.\nJaakko Luttinen and Alexander Ilin. Efficient gaussian pro-\ncess inference for short-scale spatio-temporal modeling.\nIn Artificial Intelligence and Statistics, pages 741–750.\nPMLR, 2012.\nLarry R Medsker and LC Jain. Recurrent neural networks.\nDesign and Applications, 5:64–67, 2001.\n\n\nJishnu Mukhoti and Yarin Gal. Evaluating bayesian deep\nlearning methods for semantic segmentation.\narXiv\npreprint arXiv:1811.12709, 2018.\nSamuel Müller, Noah Hollmann, Sebastian Pineda Arango,\nJosif Grabocka, and Frank Hutter. Transformers can do\nbayesian inference. arXiv preprint arXiv:2112.10510,\n2021.\nAnkur Parikh, Oscar Täckström, Dipanjan Das, and Jakob\nUszkoreit.\nA decomposable attention model for nat-\nural language inference.\nIn Proceedings of the 2016\nConference on Empirical Methods in Natural Language\nProcessing, pages 2249–2255, Austin, Texas, Novem-\nber 2016. Association for Computational Linguistics.\ndoi: 10.18653/v1/D16-1244.\nURL https://www.\naclweb.org/anthology/D16-1244.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gard-\nner, Christopher Clark, Kenton Lee, and Luke Zettle-\nmoyer. Deep contextualized word representations. In\nMarilyn A. Walker, Heng Ji, and Amanda Stent, edi-\ntors, Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, NAACL-\nHLT 2018, New Orleans, Louisiana, USA, June 1-6,\n2018, Volume 1 (Long Papers), pages 2227–2237. As-\nsociation for Computational Linguistics, 2018.\ndoi:\n10.18653/v1/n18-1202.\nURL https://doi.org/\n10.18653/v1/n18-1202.\nJ. Quiñonero-Candela and C. E. Rasmussen. A unifying\nview of sparse approximate Gaussian process regression.\nJournal of Machine Learning Research, 6:1939–1959,\n2005.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya\nSutskever. Improving language understanding by genera-\ntive pre-training. OpenAI report, 2018.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, and Ilya Sutskever. Language models are unsu-\npervised multitask learners. OpenAI blog, 1(8):9, 2019.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.\nLearning transferable visual models from natural lan-\nguage supervision. In International Conference on Ma-\nchine Learning, pages 8748–8763. PMLR, 2021.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya\nSutskever. Zero-shot text-to-image generation. In Inter-\nnational Conference on Machine Learning, pages 8821–\n8831. PMLR, 2021.\nC. E. Rasmussen and C. K. I. Williams. Gaussian Processes\nfor Machine Learning. MIT Press, 2006.\nHippolyt Ritter, Martin Kukla, Cheng Zhang, and Yingzhen\nLi. Sparse uncertainty representation in deep learning\nwith inducing weights. Advances in Neural Information\nProcessing Systems, 34:6515–6528, 2021.\nA. Schwaighofer and V. Tresp. Transductive and inductive\nmethods for approximate Gaussian process regression. In\nProc. NIPS, pages 953–960, 2003.\nMatthias W Seeger, Christopher KI Williams, and Neil D\nLawrence. Fast forward selection to speed up sparse\ngaussian process regression. In International Workshop\non Artificial Intelligence and Statistics, pages 254–261.\nPMLR, 2003.\nHan Shi, Jiahui Gao, Hang Xu, Xiaodan Liang, Zhenguo\nLi, Lingpeng Kong, Stephen Lee, and James T Kwok.\nRevisiting over-smoothing in bert from the perspective of\ngraph. arXiv preprint arXiv:2202.08625, 2022.\nA. J. Smola and P. L. Bartlett. Sparse greedy Gaussian\nprocess regression. In Proc. NIPS, pages 619–625, 2001.\nE. Snelson and Z. Gharahmani. Sparse Gaussian processes\nusing pseudo-inputs. In Proc. NIPS, pages 1259–1266,\n2005.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald Met-\nzler. Efficient transformers: A survey. ACM Computing\nSurveys, 55(6):1–28, 2022.\nM. K. Titsias. Variational learning of inducing variables in\nsparse Gaussian processes. In Proc. AISTATS, 2009.\nMichalis Titsias, Miguel Lázaro-Gredilla, et al. Variational\ninference for mahalanobis distance metrics in gaussian\nprocess regression. Advances in Neural Information Pro-\ncessing Systems, 26, 2013.\nDustin Tran, Mike Dusenberry, Mark Van Der Wilk, and\nDanijar Hafner. Bayesian layers: A module for neural\nnetwork uncertainty. Advances in neural information\nprocessing systems, 32, 2019.\nV. Tresp. A Bayesian committee machine. Neural Compu-\ntation, 12:2719–2741, 2000.\nYao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-\nPhilippe Morency, and Ruslan Salakhutdinov.\nTrans-\nformer dissection: A unified understanding of trans-\nformer’s attention via the lens of kernel. arXiv preprint\narXiv:1908.11775, 2019.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkor-\neit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in\nneural information processing systems, pages 5998–6008,\n2017.\n\n\nAlex Warstadt, Amanpreet Singh, and Samuel R Bowman.\nNeural network acceptability judgments. Transactions\nof the Association for Computational Linguistics, 7:625–\n641, 2019.\nBoyang Xue, Jianwei Yu, Junhao Xu, Shansong Liu,\nShoukang Hu, Zi Ye, Mengzhe Geng, Xunying Liu, and\nHelen Meng. Bayesian transformer language models for\nspeech recognition. In ICASSP 2021-2021 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 7378–7382. IEEE, 2021.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell,\nRuslan Salakhutdinov, and Quoc V Le. Xlnet: General-\nized autoregressive pretraining for language understand-\ning. arXiv preprint arXiv:1906.08237, 2019.\nHengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and\nVladlen Koltun. Point transformer. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 16259–16268, 2021.\n\n\nSupplementary Materials for\nRevisiting Kernel Attention with Correlated Gaussian Process Representation\nTable of Contents\nA Derivation of CGP Objective Function in Eq. 29\n16\nB Analytic Form of CGPT’s Predictive Variance from Section 3.4\n16\nC Sparse CGPT Predictive Mean\n17\nD Sparse CGPT Predictive Variance\n18\nE\nDerivation of Sparse CGP Loss Function\n19\nF\nAdditional Experiment Results\n19\nF.1\nOut-of-Distribution Calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\nF.2\nCGPT Helps Reduce Oversmoothing in Transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n\n\nA\nDERIVATION OF CGP OBJECTIVE FUNCTION IN EQ. 29\nThis section derives a more specific expression for our objective function in Eq. (29), which is quoted below\nmin\nθ\nn\nL(θ)\n≜\nloss(νa) −α ·\n\u0010\nlog Ezo\n\u0002\np(zq = νa | zo)\n\u0003\n+ log Ezo\n\u0002\np(zk | zo)\n\u0003\u0011o\n.\n(36)\nwhere νa and za is defined previously in Eq. (25).\nTo sidestep the intractability of log E[p(zq = νa | zo)] and log E[p(zk | z0)], we will instead optimize their\nlower bounds as follow. First, recall that\np\n\u0010\nzq = νa | zo\n\u0011\n=\nN\n\u0010\nνa ; Kqo\n\u0000Ko + σ2I\n\u0001−1 zo, Kq −Kqo\n\u0000Ko + σ2I\n\u0001−1 Koq\n\u0011\n,\n(37)\nwhich follows from the CGP definition. Next, let Σq ≜Kq −Kqo\n\u0000Ko + σ2I\n\u0001−1 Koq and mq ≜Kqo\n\u0000Ko + σ2I\n\u0001−1 zo.\nUsing Jensen inequality,\nlog E\n\u0002\np(zq = νa | zo)\n\u0003\n≥\nE\n\u0002\nlog p(zq = νa | zo)\n\u0003\n(38)\n= 0.5 · Ezo\nh\n−(νa −mq)⊤Σ−1\nq (νa −mq) −log det\n\u0000Σq\n\u0001\n−n log 2π\ni\n(39)\n= −0.5 ·\nZ\nzo\np(z0)\nh\n(νa −mq)⊤Σ−1\nq (νa −mq) + log det(Σq) + n log 2π\ni\ndz0\n(40)\n= −0.5 ·\nZ\nz0\np(z0)\nh\n(νa −mq)⊤Σ−1\nq (νa −mq)\ni\ndzo −0.5 ·\n\u0010\nlog det(Σq) −log 2π\n\u0011\n.\n(41)\nFinally, the integral in the above lower-bound can be approximated arbitrarily closely via an empirical average based on a\nsufficiently large number of samples zi\no ∼p(zo) = N(0, Ko). Thus, approximately, we have the following lower-bound\nlog E\nh\np(zq = νa | zo)\ni\n≥−0.5 · 1\nn\nn\nX\ni=1\nh \u0000νa −mi\nq\n\u0001⊤Σ−1\nq\n\u0000νa −mi\nq\n\u0001 i\n−0.5 · log det(Σq) −0.5 · n log 2π,\n(42)\nwhere mi\nq ≜Kqo\n\u0000Ko + σ2I\n\u0001−1 zi\no. Likewise, we can also lower bound log E[p(zk | zo)]:\nlog E\nh\np(zk | zo)\ni\n≥−0.5 · 1\nn\nn\nX\ni=1\nh \u0000zk −mi\nk\n\u0001⊤Σ−1\nk\n\u0000zk −mi\nk\n\u0001 i\n−0.5 · log det(Σk) −0.5 · n log 2π,\n(43)\nwhere mi\nk ≜Kko\n\u0000Ko + σ2I\n\u0001−1 zi\no and Σk ≜Kk −Kko\n\u0000Ko + σ2I\n\u0001−1 Kok. Therefore, our CGP objective becomes\nmax\nθ\n(\nˆL(θ) ≜α\n\u0010\n−1\nn\nn\nX\ni=1\nh \u0000νa −mi\nq\n\u0001⊤Σ−1\nq\n\u0000νa −mi\nq\n\u0001 i\n−log det(Σq)\n−1\nn\nn\nX\ni=1\nh \u0000zk −mi\nk\n\u0001⊤Σ−1\nk\n\u0000zk −mi\nk\n\u0001 i\n−log det(Σk)\n\u0011\n−loss(νa)\n)\n.\n(44)\nB\nANALYTIC FORM OF CGPT’S PREDICTIVE VARIANCE FROM SECTION 3.4\nIn Eq. (22), we have derived the expectation E[zq | zk] of the CGP model, which then can be modeled as the predictive\nmean of CGPT in equation (25). To perform uncertainty calibration, we need to further derive the predictive variance of\nzq | zk. We have the following identity:\nV[zq | zk]\n=\nE\n\u0002\nzqz⊤\nq | zk\n\u0003\n−E [zq|zk] · E[zq|zk]⊤,\n(45)\nwhere E[zq|zk] is the predictive mean given in (22) and E\n\u0002\nzqz⊤\nq | zk\n\u0003\nis given by the following integral,\nE\nh\nzqz⊤\nq | zk\ni\n=\nZ\nzq\nzqz⊤\nq\n Z\nzo\np\n\u0000zqz⊤\nq | zo\n\u0001\np(zo | zk)dzo\n!\ndzq\n(46)\n=\nZ\nz0\nZ\nzq\nzqz⊤\nq p\n\u0000zqz⊤\nq | zo\n\u0001\np(zo | zk)dzodzq\n(47)\n=\nZ\nzo\nE\nh\nzqz⊤\nq | zo\ni\np(zo | zk)dzo = E\nh\nE\nh\nzqz⊤\nq | zo\ni\n| zk\ni\n.\n(48)\n\n\nBy the canonical representation of GP, we have\nzq | zo\n∼\nN\n\u0010\nKqo(Ko + σ2I)−1zo, Kq −Kqo(Ko + σ2I)−1Koq\n\u0011\n.\n(49)\nThus, using the identity E(xx⊤) = Σ + mm⊤for x ∼N(m, Σ) we have,\nE\nh\nzqz⊤\nq | zo\ni\n=\nKq −Kqo(Ko + σ2I)−1Koq + Kqo(Ko + σ2I)−1zoz⊤\no (Ko + σ2I)−1Koq .\n(50)\nNext, taking the expectation of E[zqz⊤\nq |zo] with respect to zo | zk, gives\nEzo|zk\nh\nE\n\u0002\nzqz⊤\nq | zo\n\u0003 i\n=\nKq −Kqo(Ko + σ2I)−1Koq + Kqo(Ko + σ2I)−1E\n\u0002\nzoz⊤\no | zk\n\u0003\n(Ko + σ2I)−1Koq.\n(51)\nNote that zo | zk ∼N(Kok(Kk + σ2I)−1zk, Ko −Kok(Kk + σ2I)−1Kko) due to the canonical GP representation. Thus,\nwe have\nE\nh\nzoz⊤\no | zk\ni\n=\nKo −Kok(Kk + σ2I)−1Kok + Kok(Kk + σ2I)−1zkz⊤\nk (Kk + σ2I)−1Kko .\n(52)\nHence, we can obtain the closed form of the predictive variance V[zq | zk] by putting together Eq. (45), Eq. (22), Eq. (46),\nEq. (51) and Eq. (52). This consequently allows us to perform uncertainty calibration for the CGP-based attention unit’s\noutput analytically.\nC\nSPARSE CGPT PREDICTIVE MEAN\nWe need to find the predictive mean,\nE\nh\nzq | zk\ni\n=\nEzo∼p(zo|zk)\n\"\nE\nh\nzq | zo\ni\n| zk\n#\n.\n(53)\nThe distribution zq | zo can be approximated using sparse GP techniques, such as DTC,\np(zq | zo)\n=\nZ\nzm\np(zq | zm)p(zm | zo)dzm,\n(54)\nwhere p(zm | zo) is the inducing posterior and has the form\np(zm | zo) =\nN\n\u0010\nzm | 1\nσ2 Kmm\n\u0010\nKmm + 1\nσ2 KmoKom\n\u0011−1\nKmozo,Kmm\n\u0010\nKmm + 1\nσ2 KmoKom\n\u0011−1\nKmm\n\u0011\n.\n(55)\nThe distribution p(zq | zm) has the form N(zq | KqmK−1\nmmzm, σ2I). Therefore, with some algebras, we can calculate the\nexpectation of zq | zo,\nE(zq | zo) =\nZ\nzq\nzq\nZ\nzm\np(zq | zm)p(zm | zo)dzmdzq\n=\nZ\nzm\n\u0010 Z\nzq\nzqp(zq | zm)dzq\n\u0011\np(zm | zo)dzm\n=\nZ\nzm\nzqKqmK−1\nmmzmp(zm | zo)dzm\n= 1\nσ2 KqmK−1\nmmKmm\n\u0010\nKmm + 1\nσ2 KmoKom\n\u0011−1\nKmozo\n= 1\nσ2 Kqm\n\u0010\nKmm + 1\nσ2 KmoKom\n\u0011−1\nKmozo.\n(56)\nIn a similar fashion, we can find the expectation of zo | zk\nE(zo | zk)\n=\n1\nσ2 Kol\n\u0010\nKll + 1\nσ2 KlkKkl\n\u0011−1\nKlkzk.\n(57)\nSince zq | zo and zo | zk are Gaussians, we can analytically calculate\nE(zq | zk) = 1\nσ2 Kqm\n\u0010\nKmm + 1\nσ2 KmoKom\n\u0011−1\nKmo · 1\nσ2 Kol\n\u0010\nKll + 1\nσ2 KlkKkl\n\u0011−1\nKlkzk\n= 1\nσ4 Kqm\n\u0010\nKmm + 1\nσ2 KmoKom\n\u0011−1\nKmoKol\n\u0010\nKll + 1\nσ2 KlkKkl\n\u0011−1\nKlkzk.\n(58)\n\n\nD\nSPARSE CGPT PREDICTIVE VARIANCE\nThe variance of zq | zk is given by\nV[zq | zk] = E[zqz⊤\nq | zk] −E[zq | zk]E[zq | zk]⊤.\n(59)\nwhere E[zq | zk] is the predictive mean in Section C. The expectation of zqzq⊤|zk is given by\nE[zqz⊤\nq |zk] = Ezo∼p(zo|zk)\n\"\nE\nh\nzqz⊤\nq | zo\ni\n| zk\n#\n.\n(60)\nConsider,\nE\nh\nzqz⊤\nq | zo\ni\n=\nZ\nzq\nzqz⊤\nq p(zq|zo)dzq =\nZ\nzq\nzqz⊤\nq\nZ\nzm\np(zq|zm)p(zm|zo)dzmdzq\n(61)\n=\nZ\nzm\n\u0010 Z\nzq\nzqz⊤\nq p(zq|zm)dzq\n\u0011\np(zm|zo)dzm =\nZ\nzm\nE[zqz⊤\nq | zm]p(zm|zo)dzm.\n(62)\nSince p(E[zq | zm]) = N(zq | KqmK−1\nmmzm, σ2I), we have,\nE[zqz⊤\nq | zm] = σ2I + KqmK−1\nmmzmz⊤\nmK−1\nmmKmq.\nTherefore,\nE\nh\nzqz⊤\nq | zo\ni\n=\nZ\nzm\n(σ2I + KqmK−1\nmmzmz⊤\nmK−1\nmmKmq)p(zm|zo)dzm\n= σ2I + KqmK−1\nmm\nZ\nzm\nzmz⊤\nmp(zm|zo)dzmK−1\nmmKmq\n= σ2I + KqmK−1\nmmE[zmz⊤\nm | zo]K−1\nmmKmq.\n(63)\nSince p(zm | zo) = N\n\u0010\nzm |\n1\nσ2 Kmm\n\u0010\nKmm +\n1\nσ2 KmoKom\n\u0011−1\nKmozo, Kmm\n\u0010\nKmm +\n1\nσ2 KmoKom\n\u0011−1\nKmm\n\u0011\n, we have\nthe following\nE[zmz⊤\nm | zo] = Kmm\n\u0010\nKmm + 1\nσ2 KmoKom\n\u0011−1\nKmm + 1\nσ4 Kmm\n\u0010\nKmm + 1\nσ2 KmoKom\n\u0011−1\nKmozo×\nz⊤\no Kom\n\u0010\nKmm + 1\nσ2 KmoKom\n\u0011−1\nKmm.\n(64)\nCombining Eq (63) and (64), we have\nE\nh\nzqz⊤\nq | zo\ni\n= σ2I + Kqm\nh\u0010\nKmm + 1\nσ2 KmoKom\n\u0011−1\n+ 1\nσ4\n\u0010\nKmm + 1\nσ2 KmoKom\n\u0011−1\nKmozo×\nz⊤\no Kom\n\u0010\nKmm + 1\nσ2 KmoKom\n\u0011−1i\nKmq.\n(65)\nPlug Eq (65) to Eq (60), we have\nE[zqz⊤\nq |zk] = σ2I + Kqm\nh\u0010\nKmm + 1\nσ2 KmoKom\n\u0011−1\n+ 1\nσ4\n\u0010\nKmm + 1\nσ2 KmoKom\n\u0011−1\nKmo×\n\u0010 Z\nzo\nzoz⊤\no p(zo | zk)dzo\n\u0011\nKom\n\u0010\nKmm + 1\nσ2 KmoKom\n\u0011−1i\nKmq.\n(66)\nIn a similar manner, we can calculate the integral\nZ\nzo\nzoz⊤\no p(zo | zk)dzo = E[zoz⊤\no |zk]\n= σ2I + Kol\nh\u0010\nKll + 1\nσ2 KlkKkl\n\u0011−1\n+ 1\nσ4\n\u0010\nKll + 1\nσ2 KlkKkl\n\u0011−1\nKlkzkz⊤\nk Kkl\n\u0010\nKll + 1\nσ2 KlkKkl\n\u0011−1i\nKlo.\n(67)\nFrom equation (67), (66) and (59), we have the full predictive varicance of sparse CGPT.\n\n\nE\nDERIVATION OF SPARSE CGP LOSS FUNCTION\nThe objective function of Sparse CGP is given by\nmin\nθ\nn\nL(θ) ≜loss(νa) −α ·\n\u0010\nlog Ezo\n\u0002\np(zq = νa | zo)\n\u0003\n+ log Ezo\n\u0002\np(zk | zo)\n\u0003\u0011o\n.\n(68)\nWe will optimize the lower bound of log p(zq = νa | zo) and log p(zk | zo). Consider p(zq = νa | zo), we have\nlog Ezo\nh\np(zq = νa | zo)\ni\n≥Ezo log\nh\np(zq = νa | zo)\ni\n= Ezo\nh\nlog Ezm|zo[p(zq = νa | zm)]\ni\n≥Ezo\nh\nEzm|zo\nh\nlog[p(zq = νa | zm)]\nii\n,\n(69)\nwhere we use Jensen’s inequality to lower bound the log expectation. Since zq | zm ∼N(KqmK−1\nmmzm, σ2I), we have the\nfollowing\nEzm|zo\nh\nlog p(zq = νa | zm)\ni\n= −1\n2σ2 Ezm|zo\nh\n||νa −KqmK−1\nmmzm||2i\n−n log 2π −\n1\n2σ.\n(70)\nWe have the following identity: If X ∼N(µ, Σ) and A is a symmetric matrix, then E[X⊤AX] = µ⊤Aµ + trace(AΣ).\nSince zm | zo ∼N\n\u0010\n1\nσ2 Kmm\n\u0010\nKmm +\n1\nσ2 I\n\u0011−1\nKmozo, Kmm\n\u0010\nKmm +\n1\nσ2 I\n\u0011−1\nKmm\n\u0011\n, following the above identity with\nX = νa −KqmK−1\nmmzm and A = I, we have\nEzm|zo\nh\n||νa −KqmK−1\nmmzm||2i\n=\n\f\f\f\n\f\f\fνa −1\nσ2 Kqm\n\u0010\nKmm + 1\nσ2 I\n\u0011−1\nKmozo\n\f\f\f\n\f\f\f\n2\n+ trace\nh\nKqm\n\u0010\nKmm + 1\nσ2 I\n\u0011−1\nKmq\ni\n.\n(71)\nTaking the expectation over zo ∼N(0, Koo), we have\nEzo\nh\nEzm|zo\nh\n||νa −KqmK−1\nmmzm||2ii\n= ||νa||2 + 1\nσ4 trace\nh\nKom\n\u0010\nKmm + 1\nσ2 I\n\u0011−1\nKmqKqm\n\u0010\nKmm + 1\nσ2 I\n\u0011−1\nKmoKoo\ni\n+\ntrace\nh\nKqm\n\u0010\nKmm + 1\nσ2 I\n\u0011−1\nKmq\ni\n.\n(72)\nCombining (69), (70), (71) and (72), we have the closed form lower bound for log p(zq | zo). Using similar argument, we\nalso obtain the lower bound for log p(zk | zo).\nF\nADDITIONAL EXPERIMENT RESULTS\nF.1\nOUT-OF-DISTRIBUTION CALIBRATION\nCIFAR100-C. This section expands on our previous empirical comparison between SGPA and CGPT. Previously, we have\nshown that CGPT has comparable performances with SGPA in terms of accuracy (MCC) and has much better uncertainty\ncalibration on the CIFAR10 dataset (see Table I and Table II). In addition, to compare the robust performance of CGPT and\nSGPA on larger scale OOD learning scenarios, we also use the corrupted CIFAR100-C dataset. Similar to the CIFAR10-C\ndataset, the CIFAR100-C dataset also contains corrupted images from CIFAR100, which can be divided into 19 types of\ndistortion belonging to 4 distortion categories: Noise, Blur, Weather and Digital. For each method, we calculate the mean\nperformance metrics over the distortion types in each distortion category. The results in Table V shows that while CGPT has\ncomparable accuracy with the SGPA baseline, the calibration capacity of CGPT is much better than SGPA with lower NLL,\nMCE and ECE across all types of distortion.\nCIFAR10-C. We provide additional results for CGPT on CIFAR10-C. Beside the results for CGPT with the value α\ngradually increases from 0.5 to 1.0 during training as in Table II, we also train another CGPT with fixed value of α = 0.7.\nWe found that CGPT in this setting can help achieve better accuracy and calibration results, which are shown in Table IV.\nF.2\nCGPT HELPS REDUCE OVERSMOOTHING IN TRANSFORMERS\nIn this section, we conduct additional oversmoothing analysis similar to that in section 5 on the larger dataset CIFAR100.\nWe compare the oversmoothing effect of SGPA and CGPT and use the settings for CIFAR100 detailed in Section 5.1.1.\nFor CGPT, we fix α = 0.7 in the CGP objective function in the training phase. After training both CGPT and SGPA, we\nmeasured the cosine similarity between the outputs of the attention block in each layer to depict the oversmoothing effect.\nThis is visually demonstrated in Fig. IV, which shows that as the number of attention blocks increases, the cosine similarities\nbetween the representations learned with SGPA become gradually higher. This implies that these representations will become\nmore similar with each other as the models get deeper. On the contrary, the learned representations of CGPT have much\nlower cosine similarity as the model depth increases, which implies that CGPT will suffer less from oversmoothing than the\nSGPA.\n\n\nTable IV: Test Accuracy and other calibration metrics achieved by our CGPT model with 2 different settings of α on\nCIFAR10-C dataset. For each of the 4 distortion categories, we report the mean metrics over all distortion types in the\ncategory. And for each reported result, we run with 3 random seeds and report mean and standard deviation.\nMetric\nModel\nNoise\nBlur\nWeather\nDigital\nAvg.\nAcc ↑\nSGPA\n50.803 ± 0.447\n59.264 ± 0.915\n64.148 ± 0.472\n63.028 ± 0.334\n59.722 ± 0.323\nCGPT (α = 0.5 →1.0)\n55.177 ± 0.953\n56.412 ± 1.506\n61.515 ± 0.703\n60.373 ± 0.123\n58.591 ± 0.664\nCGPT (α = 0.7)\n54.110 ± 0.298\n58.056 ± 0.233\n61.655 ± 0.348\n61.029 ± 0.258\n58.971 ± 0.111\nNLL ↓\nSGPA\n3.464 ± 0.423\n2.551 ± 0.091\n2.137 ± 0.162\n2.298 ± 0.045\n2.626 ± 0.202\nCGPT (α = 0.5 →1.0)\n1.688 ± 0.033\n1.565 ± 0.068\n1.352 ± 0.049\n1.461 ± 0.027\n1.516 ± 0.029\nCGPT (α = 0.7)\n1.670 ± 0.180\n1.403 ± 0.131\n1.281 ± 0.132\n1.341 ± 0.099\n1.414 ± 0.131\nMCE ↓\nSGPA\n0.668± 0.009\n0.592 ± 0.014\n0.576 ± 0.014\n0.575 ± 0.001\n0.593 ± 0.002\nCGPT (α = 0.5 →1.0)\n0.360 ± 0.011\n0.334 ± 0.013\n0.284 ± 0.002\n0.314 ± 0.003\n0.324 ± 0.002\nCGPT (α = 0.7)\n0.379 ± 0.025\n0.330 ± 0.009\n0.299 ± 0.017\n0.318 ± 0.000\n0.330 ± 0.011\nECE ↓\nSGPA\n0.532 ± 0.021\n0.488 ± 0.012\n0.469 ± 0.003\n0.472 ± 0.010\n0.487 ± 0.012\nCGPT (α = 0.5 →1.0)\n0.226 ± 0.012\n0.202 ± 0.007\n0.159 ± 0.004\n0.183 ± 0.003\n0.192 ± 0.001\nCGPT (α = 0.7)\n0.241 ± 0.021\n0.199 ± 0.001\n0.169 ± 0.013\n0.180 ± 0.003\n0.195 ± 0.007\nTable V: Test Accuracy and other calibration metrics achieved by our CGPT model on CIFAR100-C dataset under the OOD\nsetting. For each of the 4 distortion categories, we report the mean metrics over all distortion types in the category. And for\neach reported result, we run with 3 random seeds and report mean and standard deviation. We again observe that CGPT\nattains better calibration metrics than SGPA across all cases.\nMetric\nModel\nNoise\nBlur\nWeather\nDigital\nAvg.\nAcc ↑\nSGPA\n23.383 ± 0.308\n36.405 ± 0.263\n35.940 ± 0.120\n35.533 ± 0.084\n33.117 ± 0.126\nCGPT (α = 0.7)\n22.664 ± 0.007\n34.488 ± 0.949\n35.341 ± 0.375\n34.259 ± 0.059\n31.973 ± 0.313\nNLL ↓\nSGPA\n10.163 ± 0.583\n6.987 ± 0.033\n6.856 ± 0.050\n7.284 ± 0.039\n7.763 ± 0.161\nCGPT (α = 0.7)\n5.600 ± 0.527\n3.270 ± 0.360\n3.197 ± 0.303\n3.348 ± 0.272\n3.797 ± 0.355\nMCE ↓\nSGPA\n0.723± 0.008\n0.628 ± 0.003\n0.626 ± 0.004\n0.637 ± 0.001\n0.652 ± 0.004\nCGPT (α = 0.7)\n0.633 ± 0.030\n0.456 ± 0.068\n0.459 ± 0.049\n0.459 ± 0.057\n0.497 ± 0.052\nECE ↓\nSGPA\n0.597 ± 0.015\n0.491 ± 0.004\n0.492 ± 0.002\n0.495 ± 0.001\n0.521 ± 0.001\nCGPT (α = 0.7)\n0.454 ± 0.038\n0.294 ± 0.060\n0.295 ± 0.055\n0.289 ± 0.050\n0.328 ± 0.050\n1\n2\n3\n4\n5\nLayer\n0.2\n0.4\n0.6\n0.8\nCosine similarity\nCGPT\nSCGPT\nSGPA\nMLE SYM\nMLE ASYM\nFigure III: The cosine similarity between the token representations vs. the layer index of CGPT and SGPA on CIFAR10. CGPT is much\nless vulnerable to oversmoothing compared to SGPA.\n\n\nFigure IV: The cosine similarity between the token representations after the attention calculation vs. the layer index of\nCGPT and SGPA on CIFAR100. CGPT is much less vulnerable to oversmoothing compared to SGPA.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20525v1.pdf",
    "total_pages": 21,
    "title": "Revisiting Kernel Attention with Correlated Gaussian Process Representation",
    "authors": [
      "Long Minh Bui",
      "Tho Tran Huu",
      "Duy Dinh",
      "Tan Minh Nguyen",
      "Trong Nghia Hoang"
    ],
    "abstract": "Transformers have increasingly become the de facto method to model sequential\ndata with state-of-the-art performance. Due to its widespread use, being able\nto estimate and calibrate its modeling uncertainty is important to understand\nand design robust transformer models. To achieve this, previous works have used\nGaussian processes (GPs) to perform uncertainty calibration for the attention\nunits of transformers and attained notable successes. However, such approaches\nhave to confine the transformers to the space of symmetric attention to ensure\nthe necessary symmetric requirement of their GP's kernel specification, which\nreduces the representation capacity of the model. To mitigate this restriction,\nwe propose the Correlated Gaussian Process Transformer (CGPT), a new class of\ntransformers whose self-attention units are modeled as cross-covariance between\ntwo correlated GPs (CGPs). This allows asymmetries in attention and can enhance\nthe representation capacity of GP-based transformers. We also derive a sparse\napproximation for CGP to make it scale better. Our empirical studies show that\nboth CGP-based and sparse CGP-based transformers achieve better performance\nthan state-of-the-art GP-based transformers on a variety of benchmark tasks.\nThe code for our experiments is available at\nhttps://github.com/MinhLong210/CGP-Transformers.",
    "published_date": "2025-02-27",
    "source": "arxiv"
  }
}