{
  "id": "arxiv_2502.21183v1",
  "text": "HQColon: A Hybrid Interactive Machine\nLearning Pipeline for High Quality Colon\nLabeling and Segmentation\nMartina Finocchiaro1, Ronja Stern1, Abraham George Smith1,2, Jens\nPetersen1,2, Kenny Erleben1, and Melanie Ganz1,3\n1 Department of Computer Science, University of Copenhagen, Copenhagen,\nDenmark\n2 Department of Clinical Oncology, Center for Cancer and Organ Diseases,\nCopenhagen University Hospital, Copenhagen, Denmark\n3 Neurobiology Research Unit, Copenhagen University Hospital, Copenhagen,\nDenmark\nAbstract. High-resolution colon segmentation is crucial for clinical and\nresearch applications, such as digital twins and personalized medicine.\nHowever, the leading open-source abdominal segmentation tool, TotalSeg-\nmentator, struggles with accuracy for the colon, which has a complex and\nvariable shape, requiring time-intensive labeling. Here, we present the\nfirst fully automatic high-resolution colon segmentation method. To de-\nvelop it, we first created a high resolution colon dataset using a pipeline\nthat combines region growing with interactive machine learning to effi-\nciently and accurately label the colon on CT colonography (CTC) im-\nages. Based on the generated dataset consisting of 435 labeled CTC\nimages we trained an nnU-Net model for fully automatic colon segmen-\ntation. Our fully automatic model achieved an average symmetric sur-\nface distance of 0.2 mm (vs. 4.0 mm from TotalSegmentator) and a 95th\npercentile Hausdorff distance of 1.0 mm (vs. 18 mm from TotalSegmenta-\ntor). Our segmentation accuracy substantially surpasses TotalSegmenta-\ntor. We share our trained model and pipeline code, providing the first and\nonly open-source tool for high-resolution colon segmentation. Addition-\nally, we created a large-scale dataset of publicly available high-resolution\ncolon labels.\nKeywords: Colon Segmentation · Interactive Machine Learning · Au-\ntomated Segmentation\n1\nIntroduction\nCT colonography (CTC) is a non-invasive imaging technique to detect and mon-\nitor colorectal abnormalities. The procedure involves inflating the colon with air\nand capturing CT images [11]. Its effectiveness in both clinical applications and\nlarge-scale studies is dependent on accurate segmentation [14],[3]. Automating\nthe segmentation process is essential to generate extensive organ datasets sup-\nporting the advancements of digital twins, in-vitro clinical trials, and AI-driven\narXiv:2502.21183v1  [cs.CV]  28 Feb 2025\n\n\n2\nM. Finocchiaro et al.\nFig. 1: Example of axial, sagittal, and coronal CT colonography slices (top) with\ncorresponding air- and fluid-filled colon annotations (bottom). On the right, 3D\nreconstructions show the colon alone (bottom) and with the small bowel (top).\npersonalized medicine [18], [13] [9], [19].\nColon segmentation is challenging due to its flexible anatomy, surrounding air-\nfilled structures, and variability in size, shape, and position. Collapsed sections\nand residual fluids add further complexity (Fig. 1). Traditional segmentation\nmethods, using thresholding, region growing, and morphological operations, rely\non heuristic rules that require manual feature selection and lack generalizability\n[12], [2], [1], [5], [7]. Deep learning approaches, particularly CNNs and U-Nets,\nenhance segmentation by reducing manual effort and handling anatomical vari-\nability [21], [4]. However, their effectiveness depends on large annotated datasets,\nwhich are time-consuming to produce for complex 3D organs like the colon. For\ninstance, expert colon annotation with software like 3D Slicer [8] takes 10–30\nminutes per case, in our experience. Models like TotalSegmentator show promise\nbut generate coarse segmentations that fail when encountering colon-specific\nchallenges [20].\nWe developed an open-source tool, HQColon, for fully automatic high-resolution\ncolon segmentation from CTC images. Traditional segmentation methods strug-\ngle with highly variable datasets, and manual annotation is too time-intensive.\nTo address these issues, we: 1) developed a semi-automatic pipeline for colon\nannotation with minimal user input; 2) used the above approach to generate a\ntraining dataset with expert-validated labels (Fig. 2, step 1) and 3) trained and\ntested a neural network for fully automatic colon segmentation (Fig. 2, step 2).\nThe trained model and expert validated annotated dataset are publicly available\n[anonymous]. To the best of our knowledge, our presented method is the first\nopen-source tool for high-resolution colon segmentation from CTC images.\n\n\nHQColon\n3\nFig. 2: Two steps pipeline: 1) generation of high-resolution colon-labeled dataset\n2) training and testing for fully automated colon segmentation.\n2\nMethods\n2.1\nInitial Dataset\nWe used the publicly accessible CTC dataset from The Cancer Imaging Archive\n(TCIA) [17]. The dataset includes 825 outpatients aged 50 and older, scheduled\nfor colonoscopy screening with no procedures in the past five years. It consists\nof 3,451 CT scans with a spatial resolution around 0.8 mm. Scans were excluded\nif they exhibited dimensional inconsistencies, including (1) fewer than 350 or\nmore than 700 axial slices, (2) axial slices smaller than 512×512 pixels, or (3) a\ndisrupted format.\n2.2\nSemi-automatic annotation of the air-filled colon segments\nFilled with air, the colon appears darker in CT colonography compared to most\nof the surrounding organs (Fig. 1). To quickly generate high-quality colon an-\nnotations for training a fully automated segmentation model, we first applied\ntraditional segmentation methods based on simple rules. The images were con-\nverted to binary format using a threshold of -800 HU. Fast annotation of air-\nfilled colon regions was achieved by applying a 3D 26-neighbor region growing\nalgorithm. The seed was placed by automatically extracting a region along the\nleft-right midline, spanning ±50 pixels around the anterior-posterior midpoint,\nand selecting slices from index 50 to 250 along the inferior-superior axis. The\nfirst air-filled pixel encountered when scanning upward was chosen as the seed,\nensuring placement in the rectum in most cases. This selection is based on three\nkey observations: (1) patients are well-centered along the left-right axis but vary\nalong the anterior-posterior axis, (2) the first air-filled region from an inferior-to-\nsuperior scan is likely the colon, and (3) some upper-body scans may include the\nproximal legs. Cases where automatic seed placement failed were excluded, as\nthis phase aimed for fast colon annotation rather than a universally applicable\nmethod. To account for potential colon collapse or connections to other organs\nlike the small bowel, segmentations with volumes exceeding 27 cm3 or below 3.5\ncm3 were discarded.\nThe final colon annotations were validated by an expert, and incorrect ones were\nremoved (e.g., with collapsed segments or incorrectly connected to other organs).\nThe resulting annotated and validated images represent the dataset used in the\nsecond part of the study to train and test nnU-Net for fully automatic colon\nsegmentation.\n\n\n4\nM. Finocchiaro et al.\nFig. 3: Examples of fluid visualization on axial slices: (A) supine and (B) prone\npatient. C, D show RootPainter fluid segmentation.\n2.3\nInteractive Machine Learning annotation of fluid-pockets\nIn CTC, the colon contains fecal residues along with air, which appear as fluid\npockets in the images. This fluid varies in color, covering the full range of CTC\npixel intensities, making traditional segmentation methods ineffective (Fig. 3).\nTo address this issue, we used an interactive machine learning approach with\nRootPainter [16].\nThe dataset was prepared by generating colon masks using TotalSegmentator on\nthe raw images from a subset of the dataset (specifically, the nnU-Net training\nset) to simplify the segmentation task for the network. Since TotalSegmentator\noften produces coarse segmentations that may exclude parts of the colon, the\nmasks were enlarged with 35 voxels dilation to ensure better coverage.\nTo create a dataset for training a fluid segmentation model in RootPainter, we\nrandomly selected seven axial slices per scan from regions containing the pre-\nviously segmented air-filled colon. The scans selection matched the training set\nfor the nnU-Net. The created dataset ensures diverse patient representation and\nsufficient slices for interactive training. This step resulted in 2,030 slices, which\nwere converted to 1000×1000 PNGs for detailed annotation and segmentation.\nWe followed the corrective-annotation protocol described in [16], which involves\nsimultaneous annotation and model training. An expert inspected model predic-\ntions and assigned corrections, which were then incorporated into the training\nset to refine subsequent predictions. We evaluated the accuracy of the fluid seg-\nmentation model with the Dice score, i.e., the difference between the initial\npredicted segmentation and the corrected segmentation after user annotation is\nassigned.\nWe applied the resulting model to predict the fluid across the entire dataset, fol-\nlowed by post-processing to refine the results. Components less than 2000 voxels\n(≈0.002cm3) and less than 2 mm from the surface of the colon were removed.\nSince CTC is typically performed in prone or supine positions, fluid accumu-\nlates below (supine) or above (prone) the colon due to gravity (see Fig. 3). This\ninformation was used to refine segmentation by discarding fluid pixels without\nair-filled colon pixels within ±2 axial slices in the axial plane. Additional post-\nprocessing steps included hole filling, Gaussian smoothing for surface refinement,\nand connecting fluid regions to the nearest air-filled colon by filling empty pixels,\n\n\nHQColon\n5\nin the sagittal plane. The final fluid annotations were validated by an expert,\nwith additional manual post-processing applied to correct errors or fill in missing\nregions.\n2.4\nFully automated colon segmentation with nnU-Net\nTo develop the fully automated colon segmentation, we utilized the high-resolution\ncolon annotation dataset generated above. The dataset was split into training\nset (66.6 %) and test set (33.3 %), ensuring an even distribution of gender and\npatient position (supine and prone). The split was not subject-based, meaning\nthe same subject could appear in both training and test sets. This approach\nwas chosen because the colon exhibits high anatomical variability and flexibility,\nresulting in significantly different shapes when scanned in different positions.\nWe trained four 3D full-resolution nnU-Net v2 [6] using two input dataset: raw\nimages and masked images. The masks were produced as described in section 2.3\nfrom TotalSegmentator colon predictions, dilated by 35 voxels. For each of the\ntwo datasets, we trained two models. One model that segmented the full colon\nincluding air and fluid, and one model that segmented only the air filled region.\nTo refine network predictions, small artifacts were removed by filtering out is-\nlands smaller than 2,000 voxels. The models were evaluated on the test set using\nthree boundary-based metrics: (1) mean absolute surface distance (MASD), (2)\naverage symmetric surface distance (ASSD), and (3) 95th percentile Hausdorff\ndistance (HD 95%). These metrics, selected based on the Metrics Reloader frame-\nwork [10], emphasize boundary precision due to the colon tubular shape and high\nvolume-to-surface ratio. We used the MONAI implementation of the metrics [15]\nand computed them on both raw model predictions and refined predictions (after\nsmall island filtering). Since TotalSegmentator is the state-of-the-art open-source\ntool for colon segmentation, we compared our results against its predictions on\nthe test set, evaluating the same metrics for both raw and refined outputs.\n3\nResults\n3.1\nCharacteristics of the high-resolution annotated colon dataset\nOf the 3,451 scans in the TCIA CT colonography dataset, 435 were used to\ntrain and test an nnU-Net for automatic colon segmentation. Fig. 4 illustrates\nthe reasons for exclusion. The final dataset, containing high-resolution validated\ncolon labels, was split into 290 training scans and 145 test scans, stratified by\ngender and patient position.\n3.2\nAnnotations of the colon fluid-filled segments\nThe corrective annotation process continued until 1,134 images were evaluated,\nwith annotations assigned to 390 images over 215 minutes. Fig. 5 illustrates\nhow Dice scores improved and fluctuated over time. Annotation stopped at im-\nage 1,134 when RootPainter’s colon segmentation was deemed satisfactory upon\n\n\n6\nM. Finocchiaro et al.\nDataflow \nTCIA CT COLONOGRAPHY: \n3451 Scans / 825 Subjects\n1714 Scans / 824 Subjects\n1708 Scans /  824 Subjects\nValidated colon labels \n445 Scans / 315 Subjects\n• number of axial slices <350 or >700 (n=1714) \n• disrupted format (n=23)\nAutomatic region growing seed placement failed \n(n=6)\nTrain Split \n290 Scans / 237 Subjects \nTest Split \n145 Scans / 128 Subjects \n• segmentation volume <3.5cm^3 or > 27cm^3  \n(n=1212) \n• expert validation failed (n=51)\nSemi-automatic high-resolution \ncolon annotation generation\nFully automatic \ncolon \nsegmentation\nFig. 4: Dataflow for creating the high-resolution annotated dataset to train and\ntest the fully automatic model for colon segmentation. The gray box indicates\nreasons for scans exclusion. The plots on the left display the gender and age\ndistribution in the final annotated dataset.\n1\n.6\n.2\nCorrectively Annotated Images\nTotal Ann. Duration [min]\n0\n1000\n0\n200\n.2\n.6\n1\nFig. 5: Dice as function of number of annotated images and annotation time.\nvisual inspection. The Dice also showed diminishing returns, justifying the deci-\nsion. Performance improved most in the first 80 minutes, reaching a rolling Dice\nabove 0.95 by image 210 (Fig. 5). The final trained RootPainter model was then\nused to segment the full dataset.\n3.3\nEvaluation of the fully automatic model\nTable 1 and Fig. 7 present the median values of computed metrics across the\ndifferent trained models, comparing our approach to TotalSegmentator. Medians\nwere used due to the non-normal distribution of most metrics. Since ASSD and\nMASD showed nearly identical results (differences <0.01 mm), only ASSD is\nreported.\nResults show a high level of segmentation accuracy across all our models, with no\nsignificant differences, regardless of using a mask as input or applying refinement\non the prediction. However, compared to TotalSegmentator, all models signifi-\ncantly improved performance in both HD 95% and ASSD. This improvement\n\n\nHQColon\n7\nFig. 6: Examples of colon segmentation generated by HQColon compared to To-\ntalSegmentator. HQColon provides high-resolution segmentation, revealing de-\ntails such as haustral folds, features not captured by TotalSegmentator.\nis also evident in Fig. 6, where our method consistently delivers high-resolution\ncolon segmentation, including fluid and haustral folds—features not captured\nby TotalSegmentator. Additionally, our approach correctly segments cases where\nTotalSegmentator either misses segments or erroneously merges separate colon\nsegments. One complete high quality colon segmentation with our method, HQ-\nColon, of a raw CTC image takes approximately 69 sec using an NVIDIA RTX\n4090 GPU.\nFig. 7: Distributions of the Average Symmetric Surface Distance and the Haus-\ndorff 95th percentile for the nnU-Net (NN) using raw image as input predicting\nair-filled colon segments (Air) or the full colon (Full) vs. TotalSegmentator (TS).\n\n\n8\nM. Finocchiaro et al.\nTable 1: Medians with 95% confidence intervals for the Hausdorff 95th percentile\nand Average Symmetric Surface Distance across different models: nnU-Net (NN)\nusing raw and masked input images (Mask-), and TotalSegmentator (TS), pre-\ndicting either air-filled colon segments (-Air) or the full colon (-Full), both before\nand after prediction refinement\nRaw prediction\nRefined prediction\nMethod\nHD 95%\nASSD\nHD 95%\nASSD\nNN-Air\n1.00 [1.00, 1.00]\n0.14 [0.12, 0.17]\n1.00 [1.00, 1.00]\n0.13 [0.12, 0.17]\nMask-NN-Air\n1.00 [1.00, 1.00]\n0.12 [0.11, 0.14]\n1.00 [1.00, 1.00]\n0.12 [0.11,0.14]\nTS-Air\n17.97 [16.58,19.65] 4.03 [3.88,4.23] 17.97 [16.34,19.24] 4.03 [3.83,4.29]\nNN-Full\n1.00 [1.00, 1.00]\n0.36 [0.19,0.25]\n1.00 [1.00, 1.00]\n0.22 [0.19,0.25]\nMask-NN-Full\n1.00 [1.00, 1.00]\n0.18 [0.16,0.21]\n1.00 [1.00, 1.00]\n0.18 [0.16,0.22]\nTS-Full\n17.72 [16.58,19.90] 3.94 [3.83,4.18] 17.72 [16.58,20.10] 3.94 [3.78,4.18]\n4\nDiscussion and Conclusion\nOur results demonstrate that HQColon significantly outperforms TotalSegmen-\ntator, the only other open-source tool for colon segmentation, achieving an order\nof magnitude higher accuracy (Table 1). Regardless of input type, segmenta-\ntion target, or refinement, we observed minimal performance differences in the\nmodels we trained (Table 1). This outcome suggests that nnU-Net can achieve\nhigh-accuracy colon segmentation even when trained on raw images without pre-\nprocessing or post-processing. This approach eliminates the need for additional\nmask computation, which would otherwise rely on TotalSegmentator and risk\nmissing parts of the colon due to incomplete masks.\nThe fast annotation pipeline we developed addresses a key challenge in medi-\ncal image analysis: generating high-resolution segmentation models with limited\nlabeled data. Labeling is often time-intensive, even for experts, especially for\ncomplex 3D structures like the colon. Our approach reduces user involvement by\nautomatically discarding images where fast annotation fails (e.g., due to incor-\nrect automatic seed placement of the regional growing algorithm). This solution\nwas feasible due to the availability of a large dataset, ensuring that the final sub-\nset was sufficient for training nnU-Net. In addition, we leveraged an interactive\nmachine learning approach for the colon fluid segmentation, that allowed us to\nrapidly annotate, train and review a fluid segmentation model (Fig. 5).\nOur method has been tested only on fully reconstructed colons, excluding col-\nlapsed cases. Future work will focus on extending its applicability to collapsed\ncolons to enhance robustness across different clinical scenarios.\nThe overarching goal of this study was to develop a fully automatic, high-\nresolution colon segmentation method. Our approach, HQColon, is the first\nopen-source tool for high-resolution colon segmentation, empowering researchers\nand clinicians to generate their own segmentations and expand colon annotation\n\n\nHQColon\n9\ndatasets. The resulting anatomical models and 3D reconstructions have broad\npotential applications, including population studies, digital twins, personalized\nmedicine, and AI-driven diagnostic tools.\nReferences\n1. Bert, A., Dmitriev, I., Agliozzo, S., Pietrosemoli, N., Mandelkern, M., Gallo, T.,\nRegge, D.: An automatic method for colon segmentation in ct colonography. Com-\nputerized Medical Imaging and Graphics 33(4), 325–331 (2009)\n2. Chowdhury, T.A., Whelan, P.F.: A fast and accurate method for automatic seg-\nmentation of colons at ct colonography based on colon geometrical features. In:\n2011 Irish Machine Vision and Image Processing Conference. pp. 94–100. IEEE\n(2011)\n3. Cirillo, D., Valencia, A.: Big data analytics for personalized medicine. Current\nopinion in biotechnology 58, 161–167 (2019)\n4. Guachi, L., Guachi, R., Bini, F., Marinozzi, F., et al.: Automatic colorectal seg-\nmentation with convolutional neural network. Computer-Aided Design and Appli-\ncations 16(5), 836–845 (2019)\n5. Iordanescu, G., Pickhardt, P.J., Choi, J.R., Summers, R.M.: Automated seed place-\nment for colon segmentation in computed tomography colonography1. Academic\nradiology 12(2), 182–190 (2005)\n6. Isensee, F., Jaeger, P.F., Kohl, S.A., Petersen, J., Maier-Hein, K.H.: nnu-net: a self-\nconfiguring method for deep learning-based biomedical image segmentation. Nature\nMethods 18(2), 203–211 (2021). https://doi.org/10.1038/s41592-020-01008-z\n7. Ismail, M., Elhabian, S., Farag, A., Dryden, G., Seow, A.: Fully automated 3d\ncolon segmentation for early detection of colorectal cancer based on convex formu-\nlation of the active contour model. In: 2012 IEEE Computer Society Conference\non Computer Vision and Pattern Recognition Workshops. pp. 58–63. IEEE (2012)\n8. Kikinis, R., Pieper, S.D., Vosburgh, K.G.: 3d slicer: a platform for subject-specific\nimage analysis, visualization, and clinical support. In: Intraoperative imaging and\nimage-guided therapy, pp. 277–289. Springer (2013)\n9. Liu, X., Qu, L., Xie, Z., Zhao, J., Shi, Y., Song, Z.: Towards more precise automatic\nanalysis: a systematic review of deep learning-based multi-organ segmentation.\nBioMedical Engineering OnLine 23(1), 52 (2024)\n10. Maier-Hein, L., Reinke, A., Godau, P., Tizabi, M.D., Buettner, F., Christodoulou,\nE., Glocker, B., Isensee, F., Kleesiek, J., Kozubek, M., et al.: Metrics reloaded:\nrecommendations for image analysis validation. Nature methods 21(2), 195–212\n(2024)\n11. Mang, T., Graser, A., Schima, W., Maier, A.: Ct colonography: techniques, indi-\ncations, findings. European journal of radiology 61(3), 388–399 (2007)\n12. Masutani, Y., Yoshida, H., MacEneaney, P.M., Dachman, A.H.: Automated seg-\nmentation of colonic walls for computerized detection of polyps in ct colonography.\nJournal of Computer Assisted Tomography 25(4), 629–638 (2001)\n13. Meijer, C., Uh, H.W., El Bouhaddani, S.: Digital twins in healthcare: Methodolog-\nical challenges and opportunities. Journal of Personalized Medicine 13(10), 1522\n(2023)\n14. Park, H.S., Kim, S.H., Kim, J.H., Lee, J.G., Kim, S.G., Lee, J.M., Lee, J.Y., Han,\nJ.K., Choi, B.I.: Computer-aided polyp detection on ct colonography: comparison\nof three systems in a high-risk human population. European Journal of Radiology\n75(2), e147–e157 (2010)\n\n\n10\nM. Finocchiaro et al.\n15. Project-MONAI: Metrics Reloaded: A Metrics Framework for Medical Image Seg-\nmentation (2024), https://github.com/Project-MONAI/MetricsReloaded, ac-\ncessed: [Insert Date Here]\n16. Smith, A.G., Han, E., Petersen, J., Olsen, N.A.F., Giese, C., Athmann, M., Dres-\nbøll, D.B., Thorup-Kristensen, K.: Rootpainter: deep learning segmentation of bio-\nlogical images with corrective annotation. New Phytologist 236(2), 774–791 (2022)\n17. Smith,\nK.,\nClark,\nK.,\nBennett,\nW.,\nNolan,\nT.,\nKirby,\nJ.,\nWolfsberger,\nM.,\nMoulton,\nJ.,\nVendt,\nB.,\nFreymann,\nJ.:\nData\nfrom\nct\ncolonography\n(2015). https://doi.org/10.7937/K9/TCIA.2015.NWTESAY1, https://doi.org/\n10.7937/K9/TCIA.2015.NWTESAY1\n18. Tang, C., Yi, W., Occhipinti, E., Dai, Y., Gao, S., Occhipinti, L.G.: A roadmap\nfor the development of human body digital twins. Nature Reviews Electrical En-\ngineering 1(3), 199–207 (2024)\n19. Vallée, A.: Envisioning the future of personalized medicine: Role and realities of\ndigital twins. Journal of Medical Internet Research 26, e50204 (2024)\n20. Wasserthal, J., Breit, H.C., Meyer, M.T., Pradella, M., Hinck, D., Sauter, A.W.,\nHeye, T., Boll, D.T., Cyriac, J., Yang, S., et al.: Totalsegmentator: robust segmen-\ntation of 104 anatomic structures in ct images. Radiology: Artificial Intelligence\n5(5) (2023)\n21. Yu, C., Anakwenze, C.P., Zhao, Y., Martin, R.M., Ludmir, E.B., S. Niedzielski, J.,\nQureshi, A., Das, P., Holliday, E.B., Raldow, A.C., et al.: Multi-organ segmenta-\ntion of abdominal structures from non-contrast and contrast enhanced ct images.\nScientific reports 12(1), 19093 (2022)\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21183v1.pdf",
    "total_pages": 10,
    "title": "HQColon: A Hybrid Interactive Machine Learning Pipeline for High Quality Colon Labeling and Segmentation",
    "authors": [
      "Martina Finocchiaro",
      "Ronja Stern",
      "Abraham George Smith",
      "Jens Petersen",
      "Kenny Erleben",
      "Melanie Ganz"
    ],
    "abstract": "High-resolution colon segmentation is crucial for clinical and research\napplications, such as digital twins and personalized medicine. However, the\nleading open-source abdominal segmentation tool, TotalSegmentator, struggles\nwith accuracy for the colon, which has a complex and variable shape, requiring\ntime-intensive labeling. Here, we present the first fully automatic\nhigh-resolution colon segmentation method. To develop it, we first created a\nhigh resolution colon dataset using a pipeline that combines region growing\nwith interactive machine learning to efficiently and accurately label the colon\non CT colonography (CTC) images. Based on the generated dataset consisting of\n435 labeled CTC images we trained an nnU-Net model for fully automatic colon\nsegmentation. Our fully automatic model achieved an average symmetric surface\ndistance of 0.2 mm (vs. 4.0 mm from TotalSegmentator) and a 95th percentile\nHausdorff distance of 1.0 mm (vs. 18 mm from TotalSegmentator). Our\nsegmentation accuracy substantially surpasses TotalSegmentator. We share our\ntrained model and pipeline code, providing the first and only open-source tool\nfor high-resolution colon segmentation. Additionally, we created a large-scale\ndataset of publicly available high-resolution colon labels.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}