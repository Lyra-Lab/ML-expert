{
  "id": "arxiv_2502.21201v2",
  "text": "The PanAf-FGBG Dataset:\nUnderstanding the Impact of Backgrounds in Wildlife Behaviour Recognition\nOtto Brookes1,2†∗, Maksim Kukushkin3,4∗, Majid Mirmehdi1, Colleen Stephens5, Paula Dieguez5,\nThurston C. Hicks6, Sorrel Jones5, Kevin Lee5, Maureen S. McCarthy5, Amelia Meier5,\nEmmanuelle Normand2, Erin G. Wessling7, Roman M.Wittig5,8, Kevin Langergraber9,\nKlaus Zuberb¨uhler10, Lukas Boesch2, Thomas Schmid4,11, Mimi Arandjelovic5,\nHjalmar K¨uhl5,12, Tilo Burghardt1\n1University of Bristol, 2Wild Chimpanzee Foundation, 3Leipzig University,\n4Martin Luther University Halle-Wittenberg, 5Max Planck Institute for Evolutionary Anthropology,\n6University of Warsaw, 7Harvard University, 8University of Lyon, 9Arizona State University,\n10University of St Andrews, 11Lancaster University in Leipzig, 12Senckenberg Museum of Natural History\nAbstract\nComputer vision analysis of camera trap video footage\nis essential for wildlife conservation, as captured be-\nhaviours offer some of the earliest indicators of changes\nin population health.\nRecently, several high-impact ani-\nmal behaviour datasets and methods have been introduced\nto encourage their use; however, the role of behaviour-\ncorrelated background information and its significant effect\non out-of-distribution generalisation remain unexplored. In\nresponse, we present the PanAf-FGBG dataset, featuring 20\nhours of wild chimpanzee behaviours, recorded at over 350\nindividual camera locations. Uniquely, it pairs every video\nwith a chimpanzee (referred to as a foreground video) with\na corresponding background video (with no chimpanzee)\nfrom the same camera location. We present two views of\nthe dataset: one with overlapping camera locations and one\nwith disjoint locations. This setup enables, for the first time,\ndirect evaluation of in-distribution and out-of-distribution\nconditions, and for the impact of backgrounds on behaviour\nrecognition models to be quantified. All clips come with rich\nbehavioural annotations and metadata including unique\ncamera IDs and detailed textual scene descriptions. Addi-\ntionally, we establish several baselines and present a highly\neffective latent-space normalisation technique that boosts\nout-of-distribution performance by +5.42% mAP for con-\nvolutional and +3.75% mAP for transformer-based mod-\nels. Finally, we provide an in-depth analysis on the role of\nbackgrounds in out-of-distribution behaviour recognition,\nincluding the so far unexplored impact of background du-\nrations (i.e., the count of background frames within fore-\nground videos).\n†Corresponding author, *Equal technical contribution\n1. Introduction\nDeep learning models have shown impressive results on ac-\ntion recognition tasks [47], particularly when both train-\ning and test data share the same distribution.\nHow-\never, their performance declines significantly on out-of-\ndistribution (OOD) data [11, 23]. In practice, distribution\nshifts are common in nearly all settings [36], but are partic-\nularly unforgiving when computer vision is applied to un-\ncontrolled, complex or natural environments [23].\nIn the human action recognition domain, poor OOD per-\nformance is well-known to originate from shortcut learn-\ning [14] of static cues or backgrounds [20, 25, 26]. This\nleads models to rely heavily on background information\ninstead of the behaviour of interest, and several studies\nhave shown that the majority of action recognition perfor-\nmance in this domain can be attributed to background learn-\ning [8, 18, 41]. Despite the known impact of backgrounds\non action recognition and a critical need to advance model\nperformance in challenging wildlife applications [24, 40],\nreliance on static cues for animal behaviour recognition has\nreceived only limited attention. This is largely due to a lack\nof datasets explicitly designed to study its effects.\nIn response, we introduce the PanAf-FGBG dataset,\nwhich comprises real-world foreground-background video\npairs (see Fig. 1) extracted from six African countries, 14\nnational parks, and over 350 individual camera locations\nacross many habitats and location types (see Fig. 3). Here,\nwe define foreground videos as those containing a chim-\npanzee, and background videos as those that do not. This\nallows us to address a key limitation in existing studies,\nwhere the background’s role is typically assessed by creat-\ning background-only versions of datasets through synthetic\nmodifications (e.g., masking or in-painting) that remove the\nactor [7, 8, 19]. While such synthetic inputs often serve as\nuseful proxies, the actual effect of synthesis on training is\narXiv:2502.21201v2  [cs.CV]  3 Mar 2025\n\n\nFigure 1.\nConceptual Overview.\nThe PanAf-FGBG dataset\ncomprises 20+ hours of paired and richly annotated foreground-\nbackground videos from camera traps in wild chimpanzee habi-\ntats (see example frames at the top). The dataset unlocks system-\natic analyses of the impact of background information on wildlife\nbehaviour recognition. We provide baselines, quantify the back-\nground impact in relation to other sources of information, and\ndemonstrate that utilising background information in latent space\n(see blue data point)) can significantly improve recognition perfor-\nmance in this challenging domain.\nlargely unknown [34, 39].\nIn contrast, our dataset introduces paired camera loca-\ntions and enables a truly realistic disentangling of back-\nground effects under in- and out-of-distribution conditions:\ndisjoint experiments with mutually exclusive camera loca-\ntions for training and test splits, and overlapping experi-\nments with shared locations. This focus is particularly im-\nportant in the wildlife domain, where the transferability and\ngeneralisation of camera trap analysis to new locations is\ncritical for effective operation. While studies have shown\nthe over-reliance of action recognition models on back-\ngrounds [18, 32, 41], we find that the effect of background\nduration (i.e., the count of background frames within fore-\nground videos with no actor) is largely unexplored. In cam-\nera trap video footage like in many other CCTV or triggered\ncamera settings, actors may only appear momentarily, leav-\ning behind footage of only the background [10, 33] within\nforeground videos. Since this effect alters learning given\nvideo-level behaviour annotations, we quantify its effect on\nseveral action recognition architectures.\nOur contributions are as follows:\n(i) we release\nPanAf-FGBG with its rich annotations and meta-data, pro-\nviding a large wildlife dataset suitable for explicitly investi-\ngating the impact of backgrounds on behaviour recognition;\n(ii) we show models trained on background-only videos\nachieve strong animal behaviour recognition performance\n(∼65% of baseline performance), corroborating findings\nin the human domain; (iii) we conduct the first evalua-\ntion of background duration in videos, revealing its impact\non action recognition models and highlighting key differ-\nences between convolutional and attention-based networks;\nand (iv) we introduce a simple yet powerful latent-space\nbackground neutralisation that yields notable performance\ngains, particularly in OOD scenarios for both convolutional\n(+5.42% mAP) and transformer-based approaches (+3.75%\nmAP).\n2. Related Work\nAnimal Behaviour Datasets. In recent years, several high-\nimpact animal behaviour datasets have been introduced to\nthe CV community [4, 6, 22, 29, 31, 35]. Although they all\ncentre on animal behaviour understanding, they each have\na different focus. The large-scale datasets Animal King-\ndom [35] and MammalNet [6] focus on behavioural under-\nstanding across species, each including videos of ∼800\ndifferent species.\nThey provide atomic action and high-\nlevel behaviour annotations, respectively, and support, in\naddition to action recognition, the related tasks of action\nlocalisation [6, 35] and pose estimation [35]. The smaller\nChimpAct dataset [31] provides longitudinal data that fol-\nlows a zoo-housed group of 20 chimpanzees with a focus on\na single male juvenile. It also contains key-point pose infor-\nmation, and fine-grained spatio-temporal behaviour labels.\nWhile valuable, these datasets lack in-situ footage from eco-\nlogical sensors like camera traps and drones, meaning ani-\nmals are not observed in their natural environment. This\nmay result in unrepresentative behaviour distributions and\nlimit their suitability for studying wild behaviour [5, 9].\nIn contrast, KABR [22] and BaboonLand [13] comprise\ndrone footage of free-living Kenyan wildlife, with fine-\ngrained spatio-temporal behaviour annotations, focussing\non individual and group-level behaviour, respectively. The\nLoTE dataset [29] provides camera trap video footage of 11\nendangered species from a 12-year longitudinal study; it is\naccompanied by environmental information (i.e., weather\nconditions and habitat) and highlights the importance of\ntemporal and spatial continuity for conservation efforts.\nThe dataset most comparable to our own is the PanAf20k\n\n\ndataset [4], which comprises camera trap videos of great\napes accompanied by multi-label behaviour annotations.\nOur dataset orthogonally complements the growing body\nof work dedicated to understanding animal behaviour by\nproviding the first dataset specifically designed to evaluate\nbackgrounds and out-of-distribution generalisation for be-\nhaviour recognition. It is unique in providing foreground-\nbackground pairs, and its configurations for in- and out-of-\ndistribution evaluation are defined identically to the well-\nknown iWildCam [3] and Wilds dataset [23] for species\nclassification. Similar to LotE [29], we provide a detailed\ndescription of the background, however we also include a\nunique camera ID (i.e., geospatial location) to allow footage\ntaken from the same camera to be linked. Similar to [4], our\ndata is gathered from 14 national parks, spanning 6 different\ncountries, although unlike ours they do not include metadata\nto allow videos to be linked to specific countries, research\nsites, locations, or habitats. Similarly, it is more diverse\nthan [29] with its footage from a single national park.\nBackground Impact.\nThe effect of backgrounds or\nstatic bias is well-studied within computer vision [25, 44],\nwhere it has shown to be necessary for models to per-\nform well [43].\nAction recognition models are particu-\nlarly vulnerable to shortcut learning. These models often\nrely on correlated, but distinct information beyond the in-\ntended action, such as object [16, 46], scene [32], and ac-\ntor biases [27]. For example, several studies have reported\nthat strong performance on action recognition tasks can be\nachieved without the actor [18, 32, 41] and that approxi-\nmately 70% of model performance is attributable to static\ncues (i.e., scene or background) [8]. This results in biased\nrepresentations and ultimately impairs out-of-distribution\ngeneralization performance [26].\nBackground Bias Datasets.\nThe effect of the back-\nground has often been modelled using synthetically gener-\nated data that removes either the foreground or the back-\nground. [7, 8, 15, 25]. For instance, Ilic et al. [20] present\nan synthetic version of the UCF101 dataset, animating spa-\ntial noise using image motion extracted by an optical flow\nestimator, Li et al. [25] replace the backgrounds of IID\nHMDB51 with sinusoidal stripe images, and Chung et al [8]\nadopt an in-painting model to remove actors from the video.\nDatasets that include fine-grained spatio-temporal action\nannotations also remove static cues implicitly by isolating\nthe region of the video where the action is occurring.\nMitigating Background Bias.\nProminent attempts\nto mitigate this bias include dataset resampling methods,\nwhich down-weight biased samples [26, 27], adversar-\nial training regimes that rely on scene labels and human\nmasks [7], and augmentation techniques that mix or swap\nthe backgrounds of different videos during training [12, 25,\n42]. There are also several implicit approaches, such as\nframe selection, that aim to utilise only frames where ac-\ntivity is occurring [45]. Our datatset, introduced next, in-\nstead introduces foreground-background pairing informa-\ntion with close proximity in time of capture, which de-\nscribes the environment truly independently and in relation\nto the presence of animals showcasing behaviour.\n3. Dataset\nDataset Overview and Statistics. PanAf-FGBG comprises\nfootage gathered under ethical oversight as part of the The\nPanAf Programme: The Cultured Chimpanzee [1]. It con-\ntains 21 hours of camera trap footage of individual chim-\npanzees in tropical Africa. Its footage is collected from 389\nindividual camera locations across 14 national parks in 6\nAfrican countries. In total, we provide 5,070 video pairs,\neach 15-seconds in duration. All videos are accompanied by\na multi-label behaviour annotation and scene metadata, plus\nthe unique feature of foreground-background video pair-\ning (see Fig. 1). Class and habitat distributions of the data\nare given in Fig. 2 and Fig. 3, respectively. Our dataset fol-\nlows a long-tailed distribution common in animal behaviour\ndatasets [4, 6, 29, 31, 35]. The three of the most commonly\noccurring classes are observed in >60% of videos, while\nthe rarest are observed in fewer than < 3%.\nData Collection. The data collection strategy was de-\nsigned specifically to maximise the chance of filming the\nterrestrial activity of apes and we provide only a brief de-\nscription here (for full details we refer the reader to [1]).\nData was collected from 14 different research sites, where\nan average of 29 movement-triggered Bushnell cameras\nwere installed per site. Grids comprising 20 to 96 1 × 1 km\ncells were established and one camera was installed per grid\ncell. Footage was recorded at 30 frames-per-second and a\nresolution of 1280 × 720. The cameras were visited every\n1-3 months for maintenance and to download the recorded\nfootage throughout the study periods.\nEthical Considerations. Both GPS location and habitat\ncamera reaction - 6.72%\nclimbing - 6.38%\nobject carrying - 4.44%\ntool use - 3.76%\ngrooming - 3.48%\nvocalisation - 2.91%\nbipedal - 2.52%\ndisplay - 2.26%\naggression - 1.32%\npiloerection - 0.70%\nplaying - 0.65%\nresting\n23.76%\nfeeding\n13.54%\ntravel \n27.57%\nFigure 2. Distribution of Behaviour. Proportion of behaviours in\nthe dataset where smaller segments are colour coordinated.\n\n\n(       )\nLocation Type\nVideo Count\nFigure 3. Distribution of Habitat and Location Type. The pie\nchart gives the proportion of videos taken at each of the three main\nhabitats: forest, marshes and savannah.The histogram shows the\ntotal number of videos extracted from each location type.\ntype for each camera was noted during this process. To pro-\ntect the location of the apes from threats, such as poaching,\nwe use randomly generated monikers for the names of the\nresearch sites and assign a unique camera ID to each video,\ninstead of revealing the geospatial location of the cameras.\nData Annotation.\nThe behavioural annotations were\nprovided by users on the community science platform\nChimp&See [2]. Annotators were presented with a choice\nof behaviour classification categories, chosen specifically\nby experts for their ecological importance. Behaviours not\nlisted in the classification categories were also permitted.\nThese annotations were then extracted and expertly grouped\ninto 14 co-occurring classes, which form the multi-label be-\nhavioural annotations presented here. The annotations fol-\nlow a multi-hot binary format that indicate the presence of\none or more behaviours. It should also be noted that be-\nhaviours are not assigned to individual apes or temporally\nlocalised within each video. To ensure annotation quality\nand consistency a video was only deemed to be analysed\nwhen either three volunteers marked the video as blank,\nor unanimous agreement between seven volunteers was ob-\nserved, or 15 volunteers annotated the video.\nOverlapping & Disjoint Configurations. We present\nboth a disjoint view of dataset, where camera locations in\nthe training and test splits are mutually exclusive, denoted\nas Ddisjoint = {di | di ∩dj = ∅, ∀i ̸= j} following [23].\nA second, overlapping view is also provided, where camera\nlocations are shared between training and test splits, repre-\nsented as Doverlap = {di | di ∩dj ̸= ∅for some i ̸= j}.\nNote that di refer to sets of camera locations associated with\nvideos in each split. Fig. 4 provides an overview of both\nconfigurations. We ensure that the class distribution in each\nconfiguration remains approximately the same for full com-\nparability. Unique camera identifiers are available for each\nvideo to allow researchers interested in this problem to cre-\nate additional configurations, should they wish to.\n4. Preliminaries\nWe train multi-label classification models on both views\nof the dataset: Doverlap and Ddisjoint using various combi-\nnations of foreground (F), background ( B) and synthetic\nbackground ( ˜B) videos. The latter were created by gen-\nerating a segmentation mask using SAM2 [37] followed\nby mean pixel value filling, similar to [7, 8, 19].\nBoth\nviews are comprised of foreground-background video pairs\n{(vF\ni , vB\ni )}N\ni=1, where vF\ni\n∈F and vB\ni\n∈B, with N\nrepresenting the total number of video pairs. Each fore-\nground video vF\ni is associated with a video-level label vec-\ntor yi = (yi1, yi2, . . . , yiC), where each yij ∈{0, 1} in-\ndicates the presence (1) or absence (0) of the j-th class in\nvideo i, and C is the total number of classes.\n5. Experiments\n5.1. Setup\nAll experiments were performed using the ResNet-50 [17]\nand MViT-V2 [28] architectures, which were initialised\nwith feature extractors pre-trained on Kinetics-400 [21].\nFor ResNet-50, both 2D and 3D convolution versions\nare tested.\nAfter initial experiments to ensure optimal\noptimiser-network pairings for the task, ResNet and MViT-\nV2 models were fine-tuned for 300 epochs using the\nSGD [38] and AdamW [30] optimisers, respectively. Both\nmodels followed a linear warm-up schedule followed by\ncosine annealing.\nFor ResNet models, the learning rate\nwas increased from 1 × 10−5 to 1 × 10−4 over 20 epochs,\nwhile it was increased from 1 × 10−6 to 1 × 10−4 over\n30 epochs for MViT-V2 models. In both cases, momen-\ntum of 0.9 was utilised. All computation was performed on\n4×NVIDIA H200 GPUs using a distributed batch size of\n336. During training, video tensors with a spatio-temporal\nresolution of 16 × 256 × 256 were employed.\nFrames\nwere sampled uniformly at a rate of 1/24, avoiding dense\nsampling strategies which are shown to exacerbate shortcut\nlearning in action recognition models [8]. For MViT-V2\nmodels, a patch size of 16 × 16 pixels was utilised. Evalu-\nation was performed using micro and mean Average Preci-\nsion and are referred to as uAP and mAP, respectively.\n\n\ndisjoint\noverlap\ntrain\ntest\nFigure 4. Overlapping & Disjoint Dataset Configurations. Six foreground-background video pairs are shown to visualise overlapping\n(left) and disjoint (right) dataset configurations. On the left, train and test videos taken from the same camera are selected, emphasizing that\nthe locations are not mutually exclusive, with backgrounds containing several cues that are highly correlated with the displayed behaviours\n(i.e., algae fishing, a form of tool use). Train and test videos on the right are extracted from mutually exclusive or disjoint camera locations.\nThey are extracted from forest and savannah, respectively, and highlight the challenge of generalising to new locations.\n5.2. Background Reliance\nExperimental Overview. First, we examined the reliance\nof animal behaviour recognition on the background by\ntraining model architectures on the foreground F, back-\nground B, and synthetic background videos ˜B only. We\nrefer to models trained on F and B (or ˜B) as FG-only and\nBG-only models, respectively, as shown earlier in Fig. 1.\nTo separate background information from class-distribution\nintrinsic information, a dummy classifier with no access to\nvisual information was also evaluated. It would effectively\nreturn predictions based on the class distribution informa-\ntion alone. Since our dataset consists of unique foreground-\nbackground pairs, it was possible to train BG-only mod-\nels by retaining the original labels associated with the fore-\nground video of the pairing. In all cases, models were tested\non F for comparability. Note that this makes the task more\ndifficult for BG-only models, since animals are only ob-\nserved at test time and not during training. All experiments\nare conducted on both Doverlap as well as Ddisjoint.\nDual-Stream Fusion Model (F+B). To clarify whether\nBG-only models and FG-only models, each trained sepa-\nrately, can be fused late to orthogonally contribute towards\nperformance we also implemented a dual-stream model. To\ndo this, the independently optimised weights of the FG-only\nand BG-only models trained for 300 epochs were loaded.\nDuring training the weights were frozen and video pairs\nwere processed to extract late features zF and zB.\nFor\nResNet models, features were extracted from the last con-\nvolutional layer and in MViT-V2 models an average of the\ntokens from the last hidden state were taken. Late features\nwere then concatenated ˜z = [zF , zB] and final class predic-\ntions were produced by a trainable multi-layer perceptron.\nResult 1 – Backgrounds are a Predictor of Animal\nBehaviour. Table 1 shows that BG-only models achieve\nstrong performance when compared to their respective FG-\nonly counterparts. To most clearly illustrate this, the ratio\nof background-to-foreground performance AP B\nAP F published\nin [8] is reported for each architecture in rows 6, 11, and\n16. Background-to-foreground performance ratios are con-\nsistently high across all models, never dropping below 70%\nand 65% for uAP and mAP, respectively, on either dataset.\nThis indicates that much of the behaviour recognition per-\nformance is indeed achievable by just utilisation of back-\nground information - the environment is a strong predic-\ntor of behaviour, even out-of-distribution. Thus, features of\nthe environment are already behaviour predictors (trees for\nclimbing, paths for travel, termite mound for feeding etc.).\nThis result is further confounded by the fact that BG-only\nmodels outperform the non-visual classifier (row 1) by a\nsignificant margin, particularly on Doverlap. On Ddisjoint\nthis observation still holds, although by somewhat smaller\nmargins. Note that for the weakest model (2D R50) back-\n\n\nModel\nData\nDoverlap\nDdisjoint\nuAP\nmAP\nuAP\nmAP\n1\nDummy\n-\n19.15\n14.73\n19.61\n14.87\n2\n2D R50\nF\n64.15\n41.28\n44.67\n18.08\n3\nB\n57.13\n30.03\n45.80\n18.27\n4\n˜B\n57.87\n30.82\n48.28\n20.39\n5\nF + B\n67.74\n44.99\n45.28\n18.84\n6\n89.06\n74.67\n102.53\n98.94\n7\n3D R50\nF\n72.79\n49.32\n57.74\n30.89\n8\nB\n58.92\n31.72\n48.40\n21.38\n9\n˜B\n59.60\n32.86\n49.29\n21.64\n10\nF + B\n76.41\n53.57\n65.66\n39.17\n11\n80.95\n66.63\n83.82\n69.21\n12\nMViT-V2\nF\n75.44\n51.54\n70.50\n35.29\n13\nB\n61.90\n32.74\n50.03\n24.00\n14\n˜B\n62.15\n33.58\n52.13\n24.77\n15\nF + B\n72.82\n53.49\n68.00\n38.09\n16\n82.05\n65.15\n70.96\n68.00\nTable 1. Quantifying Background Reliance. Performance com-\nparison of models trained on F, B, and ˜B videos. Dual-stream\nmodels are indicated by F + B.\nThe absolute highest perfor-\nmance is indicated in bold whereas best performance using only\na single-stream is underlined. The performance based on class\ndistribution only is given by a dummy classifier ( see row 1 ).\nThe background-to-foreground performance ratio is also provided\n(see rows 6, 11, and 16) . Performance is reported on Doverlap\nand Ddisjoint where Fig. 1 visualises an mAP scatter plot between\nthe two for 3D R50.\nground mAP is even on-par or above the foreground base-\nline (see rows 2-3 in the final column).\nTogether, these\nresults illustrate powerfully, that across architectures back-\nground information alone is employable as a strong animal\nbehaviour predictor.\nResult 2 – Transformers rely less on Backgrounds. On\nthe overlapping dataset, the 3D-R50 and MViT-V2 mod-\nels (rows 7-16) display significantly better performance\nwhen compared with the 2D-R50 (rows 2-6), and perform\nsimilarly to each other with respect to uAP and mAP. How-\never, the MViT-V2 model widely outperforms the 3D R50\non the disjoint dataset, suggesting that it does not rely as\nheavily on background features for performance. In sup-\nport of this, while all models experience a large decrease\nin performance when moving from the overlapping to the\ndisjoint dataset (generalisation gap), the smallest impact\nis on the MViT-V2. These differences between architec-\ntures are highlighted most clearly by observing uAP on the\ndisjoint dataset. Here, the background only ratio for the\nMViT-V2 model is ∼30% and ∼13% lower than the 2D-\nR50 and 3D-R50, respectively. MViT-V2 achieves the best\noverall performance and exhibits the lowest background-\nto-foreground performance ratio on both dataset configura-\ntions, corroborating similar findings from the still image do-\nmain that show transformers are more invariant to changes\nin the background [34].\nAnalysing Background Information Contributions.\nThe dual 3D-R50 model achieves significant performance\ngains over its FG-only baseline, particularly on Ddisjoint,\nwhere increases of 7.92% and 8.28% in uAP and mAP, re-\nspectively, can be observed. Smaller, but still significant\nimprovements are also observed on Doverlap (+3.62% uAP\nand +4.25% mAP). Interestingly, the performance of the\ndual-stream 3D-R50 exceeds that of MViT-v2, including\nits dual-stream counterpart, on nearly all metrics except for\ndisjoint uAP. The dual-stream MViT-V2 model, on the other\nhand, shows improved mAP but a lower uAP when com-\npared with the baseline model, a trend consistent across\nboth dataset configurations.\nThis analysis indicates that\nBG-only performance in the wildlife domain is not a sub-\nset of the FG-only model – which of course also has access\nto background information. Instead, explicitly learned (and\nfrozen) background features can aid the FG-only baseline in\nmodels such as ResNet via late fusion. Finally, all BG-only\nmodels trained on ˜B seem to outperform those trained on B.\nHowever, we will show later in Sec. 5.4 that non-synthetic\nbackground videos dominate synthetic counterparts when\nexplicitly utilised for performance improvements above the\nforeground baselines.\nFigure 5. The Effect of increasing Background Duration on\nMicro- and Mean- Average Precision. Comparison of 2D R50,\n3D R50 and MViT-V2 model performance as a function of the\nsimulated background duration (λ). The 3D-R50 is more sensitive\nto increasing background durations than MViT-V2. MViT-V2 is\nrelatively robust to increasing background durations on the over-\nlapping dataset, although it becomes highly sensitive when evalu-\nated on OOD data.\n\n\n5.3. Background Duration\nExperimental Overview. Noting that foreground videos\nmay contain some frames with no animals, effectively\nsupplying background only information for these frames,\nwe aim at quantifying the effect of this on model per-\nformance (see Fig. 5).\nPractically, this is achieved by\nconcatenating increasing numbers of background frames\nto our foreground videos.\nTo do this, we sampled a\nset of λT frames from vB, where λ is a predefined\nfactor controlling the proportion of background frames\nused.\nThe sampled background frames are denoted as\nV B\n= {V B\ni0 , V B\ni1 , . . . , V B\niλT −1}, with each index ij se-\nlected from {0, . . . , T −1}.\nThe foreground frames\nV F = {F F\n0 , F F\n1 , . . . , F F\nT −1} were then concatenated with\nV B, resulting in a combined video sequence V concat =\n{F F\n0 , F F\n1 , . . . , F F\nT −1, F B\ni0, F B\ni1, . . . , F B\niλT −1} of length T +\nλT. From this concatenated sequence, a starting frame Fstart\nwas sampled at random from the indices {0, . . . , λT}, en-\nsuring that T consecutive frames could be sampled without\nexceeding the sequence bounds. The final sampled frame\nsequence S = {Fstart, Fstart+1, . . . , Fstart+T −1} was then\nused as input for training while retaining the original label\ny from the foreground video vF.\nAnalysing the Continuum Between Foreground and\nBackground Videos. Figure 5 shows the full performance\ncomparison between the 2D R50, 3D R50 and MViT-V2\nmodels as background duration increases in videos (λ). As\nexpected, Figure 5 confirms that as λ increases model per-\nformance declines towards background model performance\n(see Tab 1). On Doverlap, the 3D-R50 and MViT-V2 mod-\nels exhibit a sharp decline before beginning to converge.\nThis occurs at λ = 32 and λ = 64 for the 3D-R50 and\nMViT-V2 models, respectively. Additionally, as shown by\nthe interval between λ = 0 and λ = 32, the uAP and\nmAP of 3D-R50 falls by 16.88% and 14.32%, respectively,\nwhile falling by only 7.23% and 9.13% for the MViT-V2\nmodel. Earlier convergence to near BG-only performance\nand a more rapid rate of decline, show that 3D-R50 is\nmore sensitive to increases in background durations than\nMViT-V2. Despite starting from lower performance, the\n2D R50 model exhibits a more gradual, near-linear decline,\nindicating less sensitivity to changes in λ. While the be-\nhaviour of the 2D-R50 and 3D-R50 models remain rela-\ntively consistent on Ddisjoint, the behaviour of MViT-V2\nchanges significantly. Its uAP declines more steeply, falling\nfrom 70.05% to 48.29% (a decrease of 22.21%) between\nλ = 0 and λ = 32, where its performance converges. It\nis also more sensitive to smaller increases in background\ndurations, falling from 70.50% to 64.08% (a decrease of\n6.42%) between λ = 0 and λ = 1.0, where a decrease\nof only 0.38% is experienced on Doverlap over the same\ninterval.\nAt higher values of λ (i.e., > 40) the perfor-\nmance drops below that of the 3D-R50 (although only once\nthe background-only performance has been surpassed). Al-\nthough uAP falls more steeply, mAP declines more slowly\nand performance above that of the BG-only model is main-\ntained even at the highest values of λ. This suggests that, al-\nthough the MViT-V2 achieves higher baseline performance\non the disjoint dataset, it is more sensitive to increases in\nbackground frames when evaluating uAP – a specific data\nscenario drowning the advantages of the transformer model\nin background information.\n5.4. Background Bias Mitigation\nAugmenting Performance via Explicit Background Neu-\ntralisation. In this final experimental section, we justify\nthe need for paired, real-world background samples, as\nuniquely provided by our dataset. We show that these sam-\nple pairings are demonstrably qualified to significantly im-\nprove model performance. To show this, we analyse the\nimpact of background subtraction operations in both in-\nput and embedding spaces across each of the model ar-\nchitectures and dataset configurations. Two background-\nsubtracted views of our dataset were created for input space\nexperiments. This was done by performing pixel-wise sub-\ntraction of B and ˜B from F. To perform algebraic back-\nground subtraction in latent space, we extracted the fea-\nture vectors of the foreground video pairs. As described\nin Sec. 5.2, we achieve this by processing the video tensors\nup to the last convolutional layer and averaging the tokens\nfrom the last hidden state in the ResNet and MViT-V2 mod-\nels, respectively. The background-subtracted embedding is\nthen computed as zF−B = zF −(1 −α) · zB where α is\na coefficient used to control the relative magnitude of the\nalgebraic subtraction operation as schematically shown in\nFig. 6. In practice, the modulation parameter α ∈[0, 1] was\nablated to increase linearly or exponentially over the course\nof training and results are reported for both. In all cases α\nwas initialized at zero and increased to one by the end of\ntraining.\nResults and Performance Improvements. For the ba-\nsic 2D-R50 model, background subtraction in input space\nis highly effective. When using B, we observe substan-\ntial performance improvements over the baseline, with per-\ncentage increases of +8.06% and 5.84% in uAP and mAP\non Doverlap, and +9.06% and +7.76% in uAP and mAP\non Ddisjoint. Using ˜B also yields improvements, though\nthese gains are notably smaller. Background subtraction in\nembedding space does not provide any benefit to the 2D-\nR50. Although input space background subtraction is ef-\nfective for the 2D-R50, it does not improve performance\nfor the 3D-R50 and MViT-V2 models. Neither original nor\nsynthetic backgrounds provide an advantage over their re-\nspective baselines, indicating that input space subtraction\nmay not be suitable for architectures that process spatio-\ntemporal features.\n\n\nModel\nSubtraction\nDoverlap\nDdisjoint\nu\nm\nu\nm\nBaseline\n64.15\n41.28\n44.67\n18.08\nF −˜B\n65.96\n42.12\n54.53\n26.65\nF −B\n72.21\n47.12\n53.73\n25.84\nNone\n56.21\n38.84\n27.24\n17.94\nLinear\n53.78\n39.01\n29.75\n17.17\n2D R50\nExponential\n54.05\n37.11\n25.80\n17.73\nBaseline\n72.79\n49.32\n57.74\n30.89\nF −˜B\n66.51\n42.56\n53.58\n27.98\nF −B\n71.93\n46.36\n53.92\n25.05\nNone\n72.30\n51.39\n63.14\n34.85\nLinear\n77.31\n52.82\n66.69\n36.31\n3D R50\nExponential\n73.52\n51.24\n64.58\n36.36\nBaseline\n75.44\n51.54\n70.50\n35.29\nF −˜B\n66.84\n42.27\n57.21\n28.80\nF −B\n74.90\n49.40\n61.22\n31.51\nNone\n74.23\n44.11\n68.56\n35.14\nLinear\n79.32\n53.83\n71.64\n39.04\nMViT-V2\nExponential\n77.70\n50.93\n71.28\n39.25\nTable 2.\nQuantifying Performance Improvements of Back-\nground Subtraction. Performance comparison of the 2D R50,\n3D R50, and MViT-V2 models on both dataset configurations: DO\nand DD. Each model’s performance is reported when training on\nF −B or F −˜B videos in input ( light green ) and embedding\nspace ( light blue ). The highest scores are indicated in bold. Re-\nsults confirm superiority of the availability of paired, real-world\nbackground videos and algebraic operation in embedding space,\nthat is over synthetic data or input space operations.\nforeground\n(b x f x 3 x 224 x 224)\nVision Encoder\nVision Encoder\nbackground\n(b x f x 3 x 224 x 224)\nx0 ......... xn\nα\n(1 -  )\n-\n.\nx0 ......... xn\nFigure 6. Latent Space Background Compensation. The pro-\nposed operation uses α to perform a weighted subtraction of back-\nground features (computed via the dataset pairing of a real-world,\nbackground video) from foreground features in latent space. The\ndotted line between the encoders indicates shared weights (i.e.,\nthey are the same model).\nConversely, background subtraction in embedding space\nprovides significant performance improvements for the\n3D-R50 and MViT-V2 models, particularly on Ddisjoint.\nFor the 3D-R50, embedding space subtraction with linear\nscheduling results in a +8.95% and +5.42% increase in uAP\nand mAP, respectively. Exponential scheduling also yields\nimprovements over the baseline.\nThe improvements are\nlower than linear scheduling with respect to uAP (+6.84%)\nbut marginally better for mAP (+5.47%). MViT-V2 also\nbenefits from embedding space subtraction on Ddisjoint:\nlinear scheduling improves uAP and mAP by +1.14% and\n+3.75%, respectively, whereas exponential scheduling pro-\nvides a +0.78% increase in uAP and a +3.96% increase in\nmAP. These findings suggest that embedding space subtrac-\ntion is effective for both 3D-R50 and MViT-V2 models on\nDdisjoint, with linear and exponential scheduling favouring\nuAP and mAP performance, respectively. Embedding space\nsubtraction also enhances performance on Doverlap, though\nto a lesser extent than on Ddisjoint. For the 3D-R50 model,\napplying embedding space subtraction with linear schedul-\ning yields an improvement of 4.52% and 3.50% in uAP and\nmAP, respectively, while exponential scheduling results in\nsmaller gains of 0.73% and 1.92%. The MViT-V2 model\nfollows a similar trend: linear scheduling increases uAP\nand mAP by 3.88% and 2.29%, respecitvely, whereas expo-\nnential scheduling improves uAP by 2.26% but slightly re-\nduces mAP by -0.61%. These results suggest that algebraic\nembedding space subtraction is beneficial on Doverlap, al-\nthough the performance gains are relatively small compared\nto Ddisjoint, and that linear scheduling generally provides\nbetter results than exponential scheduling.\n6. Limitations\nKey limitations include that; (1) our experiments and\ndataset relate, due to the nature of camera traps, to statically\ninstalled sensors only; (2) the latent space background sub-\ntraction requires additional data (in the form of background\nframes) and has computational implications of two forward\npasses of the model (instead of one) for each training step;\n(3) the background concatenation experiments may have\nsynthesised videos that are unnatural (e.g., a chimpanzee\nsitting could suddenly disappear without moving out of\nframe) and may affect learning performance.\n7. Conclusion\nIn this work, we introduced PanAf-FGBG – the first wildlife\ndataset specifically designed to evaluate backgrounds and\nout-of-distribution generalisation for behaviour recognition.\nIt is a large, richly annotated wild chimpanzee behaviour\ndataset which defines explicit data configurations for exper-\niments with overlapping and disjoint camera locations. Per-\nforming in-depth experimentation with this unique dataset,\n\n\nwe demonstrated that real-world background information\nfrom wild habitats is not only a powerful predictor of an-\nimal behaviour, but can be utilised to significantly improve\nrecognition performance across models.\nBeyond these results, the release of this dataset will en-\nable the computer vision community to run direct evaluative\nstudies under in-distribution and out-of-distribution condi-\ntions to improve the utilisation of backgrounds for deep ac-\ntion recognition in the wildlife domain. The rich metadata\nwill allow further tasks and experimental configurations to\nbe created. We hope that this will aid model transferability\nand performance in this challenging domain and ultimately\nbenefit endangered and charismatic species like the one fea-\ntured in this paper.\n\n\nSupplementary Material\nIn the supplementary material, we provide: (i) addi-\ntional dataset statistics in Sec. 1; (ii) a description of\nhow synthetic background videos are created in Sec. 2;\n(iii) foreground-background video pair visualisations (see\nFig 5) and; (iv) a visualisation showcasing a small fraction\n(∼0.05%) of the total available frames (see Fig. 6)\n1. Dataset Statistics\nWe present additional dataset statistics for the country, re-\nsearch site, camera locations and behaviours. Specifically,\n(i) the distribution of countries and the corresponding re-\nsearch sites are displayed in Fig. 1 (ii) the accumulative\nproportion of videos contributed by each camera is shown\nin Fig. 2 and; (iii) a comparison of the behaviour distribu-\ntion for the overlapping and disjoint datasets is displayed in\nFig. 3\nA (10.46)\nB (11.63)\nC (12.67)\nD (5.15)\nE (3.40)\nF (3.40)\nG (5.87)\nH (6.88)\nI (3.17)\nJ (8.00)\nK (3.77)\nL (7.45)\nM (8.85)\nN (9.30)\nUganda (34.76)\nDRC (9.30)\nLiberia (20.07)\nGuinea (11.17)\nCotedivoire (19.55)\nSenegal (5.15)\nCountries\nFigure 1. Proportion of videos from each country and research\nsite. The inner ring displays the proportion of videos extracted\nfrom each country, while the outer ring represents individual re-\nsearch sites. Each research site segment is a unique shade de-\nrived from its corresponding country’s colour. All proportions are\nshown in brackets. Note that research site names are replaced with\nletters to protect the location of the chimps.\n2. Synthetic Background Generation\nWe\ngenerated\nsynthetic\nbackground\nvideos\nusing\nSAM2 [37] and mean pixel value filling.\nSpecifically,\nwe prompted the SAM2.1-Large model using a single\nspatial coordinate indicating the location of the chimpanzee\nto produce an initial segmentation mask. We then leveraged\nthe automatic mask propagation functionality of SAM2 to\ncreate spatio-temporal masklets for the full video.\nNote\nthat spatial coordinates were produced manually. Then, we\nCamera ID\nAccumulative Proportion (%)\n100\n80\n60\n40\n20\n0.0\nBelow 50%\nAbove 50%\n50%\n48\nFigure 2. Accumulative proportion of videos contributed by\neach camera.. The y-axis represents the accumulative proportion\nof videos, with individual cameras arranged by their contribution\non the x-axis. The red dashed line divides the cameras into two\ngroups: those contributing the first 50% of the data (left) and the\nremaining 50% (right). Many videos are contributed by a rela-\ntively small number of cameras: 48 cameras contribute 50% of the\nfootage.\nProportion of Videos (%)\nBehaviours\nFigure 3.\nComparison of the proportion of videos contain-\ning each behaviour between overlapping and disjoint datasets.\nProportions for the overlapping dataset are shown in blue and the\ndisjoint dataset in red. Behaviours are ordered from highest to\nlowest proportion, with exact values displayed above each bar for\neasy comparison.\nfilled the area indicated by the segmentation mask with the\nmean pixel value for the frame (see Fig. 4 for examples).\nSynthetic background videos, spatial coordinates, and\nsegmentation masks will be published with the dataset.\n\n\nFigure 4. Synthetic Background Video Examples. Three example video clips with the original segmentation mask generated by one-shot\nprompting of SAM2 overlaid and the corresponding mean pixel value filled frame.\n\n\nFigure 5. Foreground-Background Video Pair Examples. Shown are 18 pairs of still frames (two pairs per row) extracted from\nforeground-background video pairs.\n\n\nFigure 6. Dataset Overview. A small fraction (∼0.05%) of the 1.8 million frames in the dataset are shown, highlighting its diversity with\nrespect to exhibited behaviours, habitat, weather conditions, time of day, and more.\n\n\nReferences\n[1] Pan African Programme: The Cultured Chimpanzee, 1999.\nOnline; accessed 29 January 2014. 3\n[2] Mimi Arandjelovic, Colleen R Stephens, Maureen S Mc-\nCarthy, Paula Dieguez, Ammie K Kalan, Nuria Maldonado,\nChristophe Boesch, and Hjalmar S Kuehl. Chimp&see: An\nonline citizen science platform for large-scale, remote video\ncamera trap annotation of chimpanzee behaviour, demogra-\nphy and individual identification. PeerJ Preprints, 2016. 4\n[3] Sara Beery, Arushi Agarwal, Elijah Cole, and Vighnesh\nBirodkar. The iwildcam 2021 competition dataset. arXiv\npreprint arXiv:2105.03494, 2021. 3\n[4] Otto Brookes, Majid Mirmehdi, Colleen Stephens, Samuel\nAngedakin, Katherine Corogenes, Dervla Dowd, Paula\nDieguez, Thurston C Hicks, Sorrel Jones, Kevin Lee, et al.\nPanaf20k: a large video dataset for wild ape detection and\nbehaviour recognition. International Journal of Computer\nVision, pages 1–17, 2024. 2, 3\n[5] Jackie Chappell and Susannah KS Thorpe. The role of great\nape behavioral ecology in one health: Implications for cap-\ntive welfare and re-habilitation success. American journal of\nprimatology, 84(4-5):e23328, 2022. 2\n[6] Jun Chen, Ming Hu, Darren J Coker, Michael L Berumen,\nBlair Costelloe, Sara Beery, Anna Rohrbach, and Mohamed\nElhoseiny. Mammalnet: A large-scale video benchmark for\nmammal recognition and behavior understanding. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 13052–13061, 2023. 2, 3\n[7] Jinwoo Choi, Chen Gao, Joseph CE Messou, and Jia-Bin\nHuang. Why can’t i dance in the mall? learning to miti-\ngate scene bias in action recognition. Advances in Neural\nInformation Processing Systems, 32, 2019. 1, 3, 4\n[8] Jihoon Chung, Yu Wu, and Olga Russakovsky.\nEnabling\ndetailed action recognition evaluation through video dataset\naugmentation. Advances in Neural Information Processing\nSystems, 35:39020–39033, 2022. 1, 3, 4, 5\n[9] Fay E Clark. Great ape cognition and captive care: Can cog-\nnitive challenges enhance well-being? Applied Animal Be-\nhaviour Science, 135(1-2):1–12, 2011. 2\n[10] Fagner Cunha, Eulanda M dos Santos, Raimundo Barreto,\nand Juan G Colonna. Filtering empty camera trap images in\nembedded systems. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n2438–2446, 2021. 2\n[11] Alexander D’Amour, Katherine Heller, Dan Moldovan, Ben\nAdlam, Babak Alipanahi, Alex Beutel, Christina Chen,\nJonathan Deaton, Jacob Eisenstein, Matthew D Hoffman,\net al. Underspecification presents challenges for credibility\nin modern machine learning. Journal of Machine Learning\nResearch, 23(226):1–61, 2022. 1\n[12] Shuangrui Ding, Maomao Li, Tianyu Yang, Rui Qian, Hao-\nhang Xu, Qingyi Chen, Jue Wang, and Hongkai Xiong.\nMotion-aware contrastive video representation learning via\nforeground-background merging.\nIn Proceedings of the\nIEEE/CVF conference on computer vision and pattern\nrecognition, pages 9716–9726, 2022. 3\n[13] Isla Duporge, Maksim Kholiavchenko, Roi Harel, Dan\nRubenstein, Meg Crofoot, Tanya Berger-Wolf, Stephen Lee,\nScott Wolf, Julie Barreau, Jenna Kline, et al.\nBaboon-\nland dataset: Tracking primates in the wild and automat-\ning behaviour recognition from drone videos. arXiv preprint\narXiv:2405.17698, 2024. 2\n[14] Robert Geirhos, J¨orn-Henrik Jacobsen, Claudio Michaelis,\nRichard Zemel, Wieland Brendel, Matthias Bethge, and Fe-\nlix A Wichmann. Shortcut learning in deep neural networks.\nNature Machine Intelligence, 2(11):665–673, 2020. 1\n[15] Rohit Girdhar and Deva Ramanan.\nCater: A diagnostic\ndataset for compositional actions and temporal reasoning.\narXiv preprint arXiv:1910.04744, 2019. 3\n[16] Abhinav Gupta and Larry S Davis. Objects in action: An ap-\nproach for combining action understanding and object per-\nception. pages 1–8. IEEE, 2007. 3\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016. 4\n[18] Yun He, Soma Shirakabe, Yutaka Satoh, and Hirokatsu\nKataoka.\nHuman action recognition without human.\nIn\nComputer Vision–ECCV 2016 Workshops: Amsterdam, The\nNetherlands, October 8-10 and 15-16, 2016, Proceedings,\nPart III 14, pages 11–17. Springer, 2016. 1, 2, 3\n[19] Lisa Anne Hendricks, Kaylee Burns, Kate Saenko, Trevor\nDarrell, and Anna Rohrbach. Women also snowboard: Over-\ncoming bias in captioning models. In Proceedings of the Eu-\nropean conference on computer vision (ECCV), pages 771–\n787, 2018. 1, 4\n[20] Filip Ilic, Thomas Pock, and Richard P Wildes. Is appear-\nance free action recognition possible? In European Confer-\nence on Computer Vision, pages 156–173. Springer, 2022. 1,\n3\n[21] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,\nChloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,\nTim Green, Trevor Back, Paul Natsev, et al. The kinetics hu-\nman action video dataset. arXiv preprint arXiv:1705.06950,\n2017. 4\n[22] Maksim Kholiavchenko, Jenna Kline, Michelle Ramirez,\nSam Stevens, Alec Sheets, Reshma Babu, Namrata Banerji,\nElizabeth Campolongo, Matthew Thompson, Nina Van Tiel,\net al.\nKabr:\nIn-situ dataset for kenyan animal behav-\nior recognition from drone videos.\nIn Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision, pages 31–40, 2024. 2\n[23] Pang\nWei\nKoh,\nShiori\nSagawa,\nHenrik\nMarklund,\nSang Michael Xie,\nMarvin Zhang,\nAkshay Balsubra-\nmani, Weihua Hu, Michihiro Yasunaga, Richard Lanas\nPhillips, Irena Gao, et al. Wilds: A benchmark of in-the-\nwild distribution shifts.\nIn International conference on\nmachine learning, pages 5637–5664. PMLR, 2021. 1, 3, 4\n[24] Hjalmar S K¨uhl and Tilo Burghardt.\nAnimal biometrics:\nquantifying and detecting phenotypic appearance. Trends in\necology & evolution, 28(7):432–441, 2013. 1\n[25] Haoxin Li, Yuan Liu, Hanwang Zhang, and Boyang Li. Mit-\nigating and evaluating static bias of action representations in\n\n\nthe background and the foreground. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 19911–19923, 2023. 1, 3\n[26] Yi Li and Nuno Vasconcelos.\nRepair: Removing repre-\nsentation bias by dataset resampling.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 9572–9581, 2019. 1, 3\n[27] Yingwei Li, Yi Li, and Nuno Vasconcelos. Resound: To-\nwards action recognition without representation bias. In Pro-\nceedings of the European Conference on Computer Vision\n(ECCV), pages 513–528, 2018. 3\n[28] Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Man-\ngalam, Bo Xiong, Jitendra Malik, and Christoph Feicht-\nenhofer.\nMvitv2: Improved multiscale vision transform-\ners for classification and detection.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 4804–4814, 2022. 4\n[29] Dan Liu, Jin Hou, Shaoli Huang, Jing Liu, Yuxin He,\nBochuan Zheng, Jifeng Ning, and Jingdong Zhang. Lote-\nanimal: A long time-span dataset for endangered animal be-\nhavior understanding. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages 20064–\n20075, 2023. 2, 3\n[30] I Loshchilov. Decoupled weight decay regularization. arXiv\npreprint arXiv:1711.05101, 2017. 4\n[31] Xiaoxuan Ma,\nStephan Kaufhold,\nJiajun Su,\nWentao\nZhu, Jack Terwilliger, Andres Meza, Yixin Zhu, Federico\nRossano, and Yizhou Wang.\nChimpact: A longitudinal\ndataset for understanding chimpanzee behaviors. Advances\nin Neural Information Processing Systems, 36:27501–27531,\n2023. 2, 3\n[32] Marcin Marszalek, Ivan Laptev, and Cordelia Schmid. Ac-\ntions in context.\nIn 2009 IEEE Conference on Computer\nVision and Pattern Recognition, pages 2929–2936. IEEE,\n2009. 2, 3\n[33] Zhongqi Miao, Ziwei Liu, Kaitlyn M Gaynor, Meredith S\nPalmer, Stella X Yu, and Wayne M Getz. Iterative human and\nautomated identification of wildlife images. Nature Machine\nIntelligence, 3(10):885–895, 2021. 2\n[34] Mazda Moayeri, Phillip Pope, Yogesh Balaji, and So-\nheil Feizi. A comprehensive study of image classification\nmodel sensitivity to foregrounds, backgrounds, and visual\nattributes. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 19087–\n19097, 2022. 2, 6\n[35] Xun Long Ng, Kian Eng Ong, Qichen Zheng, Yun Ni,\nSi Yong Yeo, and Jun Liu. Animal kingdom: A large and\ndiverse dataset for animal behavior understanding. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 19023–19034, 2022. 2, 3\n[36] Joaquin Qui˜nonero-Candela, Masashi Sugiyama, Anton\nSchwaighofer, and Neil D Lawrence. Dataset shift in ma-\nchine learning. Mit Press, 2022. 1\n[37] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang\nHu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman\nR¨adle, Chloe Rolland, Laura Gustafson, et al.\nSam 2:\nSegment anything in images and videos.\narXiv preprint\narXiv:2408.00714, 2024. 4, 1\n[38] Herbert Robbins and Sutton Monro. A stochastic approxi-\nmation method. The annals of mathematical statistics, pages\n400–407, 1951. 4\n[39] Vaishaal Shankar, Achal Dave, Rebecca Roelofs, Deva Ra-\nmanan, Benjamin Recht, and Ludwig Schmidt. Do image\nclassifiers generalize across time?\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 9661–9669, 2021. 2\n[40] Devis Tuia, Benjamin Kellenberger, Sara Beery, Blair R\nCostelloe, Silvia Zuffi, Benjamin Risse, Alexander Mathis,\nMackenzie\nW\nMathis,\nFrank\nVan\nLangevelde,\nTilo\nBurghardt, et al.\nPerspectives in machine learning for\nwildlife conservation. Nature communications, 13(1):1–15,\n2022. 1\n[41] Tuan-Hung Vu, Catherine Olsson, Ivan Laptev, Aude Oliva,\nand Josef Sivic. Predicting actions from static scenes. In\nComputer Vision–ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 421–436. Springer, 2014. 1, 2, 3\n[42] Jinpeng Wang, Yuting Gao, Ke Li, Yiqi Lin, Andy J Ma,\nHao Cheng, Pai Peng, Feiyue Huang, Rongrong Ji, and Xing\nSun. Removing the background by adding the background:\nTowards background robust self-supervised video represen-\ntation learning. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 11804–\n11813, 2021. 3\n[43] Kai Xiao, Logan Engstrom, Andrew Ilyas, and Aleksander\nMadry. Noise or signal: The role of image backgrounds in\nobject recognition. arXiv preprint arXiv:2006.09994, 2020.\n3\n[44] Manlin Zhang, Jinpeng Wang, and Andy J Ma.\nSup-\npressing static visual cues via normalizing flows for self-\nsupervised video representation learning. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, pages 3300–\n3308, 2022. 3\n[45] Mingjun Zhao, Yakun Yu, Xiaoli Wang, Lei Yang, and Di\nNiu.\nSearch-map-search: a frame selection paradigm for\naction recognition. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n10627–10636, 2023. 3\n[46] Xingyi Zhou, Anurag Arnab, Chen Sun, and Cordelia\nSchmid. How can objects help action recognition? In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 2353–2362, 2023. 3\n[47] Yi Zhu, Xinyu Li, Chunhui Liu, Mohammadreza Zolfaghari,\nYuanjun Xiong, Chongruo Wu, Zhi Zhang, Joseph Tighe, R\nManmatha, and Mu Li. A comprehensive study of deep video\naction recognition. arXiv preprint arXiv:2012.06567, 2020.\n1\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21201v2.pdf",
    "total_pages": 15,
    "title": "The PanAf-FGBG Dataset: Understanding the Impact of Backgrounds in Wildlife Behaviour Recognition",
    "authors": [
      "Otto Brookes",
      "Maksim Kukushkin",
      "Majid Mirmehdi",
      "Colleen Stephens",
      "Paula Dieguez",
      "Thurston C. Hicks",
      "Sorrel Jones",
      "Kevin Lee",
      "Maureen S. McCarthy",
      "Amelia Meier",
      "Emmanuelle Normand",
      "Erin G. Wessling",
      "Roman M. Wittig",
      "Kevin Langergraber",
      "Klaus Zuberbühler",
      "Lukas Boesch",
      "Thomas Schmid",
      "Mimi Arandjelovic",
      "Hjalmar Kühl",
      "Tilo Burghardt"
    ],
    "abstract": "Computer vision analysis of camera trap video footage is essential for\nwildlife conservation, as captured behaviours offer some of the earliest\nindicators of changes in population health. Recently, several high-impact\nanimal behaviour datasets and methods have been introduced to encourage their\nuse; however, the role of behaviour-correlated background information and its\nsignificant effect on out-of-distribution generalisation remain unexplored. In\nresponse, we present the PanAf-FGBG dataset, featuring 20 hours of wild\nchimpanzee behaviours, recorded at over 350 individual camera locations.\nUniquely, it pairs every video with a chimpanzee (referred to as a foreground\nvideo) with a corresponding background video (with no chimpanzee) from the same\ncamera location. We present two views of the dataset: one with overlapping\ncamera locations and one with disjoint locations. This setup enables, for the\nfirst time, direct evaluation of in-distribution and out-of-distribution\nconditions, and for the impact of backgrounds on behaviour recognition models\nto be quantified. All clips come with rich behavioural annotations and metadata\nincluding unique camera IDs and detailed textual scene descriptions.\nAdditionally, we establish several baselines and present a highly effective\nlatent-space normalisation technique that boosts out-of-distribution\nperformance by +5.42% mAP for convolutional and +3.75% mAP for\ntransformer-based models. Finally, we provide an in-depth analysis on the role\nof backgrounds in out-of-distribution behaviour recognition, including the so\nfar unexplored impact of background durations (i.e., the count of background\nframes within foreground videos).",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}