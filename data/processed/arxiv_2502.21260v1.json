{
  "id": "arxiv_2502.21260v1",
  "text": "PET Image Denoising via Text-Guided Diffusion:\nIntegrating Anatomical Priors through Text\nPrompts\nBoxiao Yu1, Savas Ozdemir2, Jiong Wu1, Yizhou Chen3, Ruogu Fang1, Kuangyu\nShi3, and Kuang Gong1\n1 J. Crayton Pruitt Family Department of Biomedical Engineering, University of\nFlorida, Gainesville, FL, USA\n2 Department of Radiology, University of Florida, Jacksonville, FL, USA\n3 Department of Nuclear Medicine, University of Bern, Bern, Switzerland\nAbstract. Low-dose Positron Emission Tomography (PET) imaging\npresents a significant challenge due to increased noise and reduced im-\nage quality, which can compromise its diagnostic accuracy and clinical\nutility. Denoising diffusion probabilistic models (DDPMs) have demon-\nstrated promising performance for PET image denoising. However, exist-\ning DDPM-based methods typically overlook valuable metadata such as\npatient demographics, anatomical information, and scanning parameters,\nwhich should further enhance the denoising performance if considered.\nRecent advances in vision-language models (VLMs), particularly the pre-\ntrained Contrastive Language–Image Pre-training (CLIP) model, have\nhighlighted the potential of incorporating text-based information into\nvisual tasks to improve downstream performance. In this preliminary\nstudy, we proposed a novel text-guided DDPM for PET image denoising\nthat integrated anatomical priors through text prompts. Anatomical\ntext descriptions were encoded using a pre-trained CLIP text encoder to\nextract semantic guidance, which was then incorporated into the diffusion\nprocess via the cross-attention mechanism. Evaluations based on paired\n1/20 low-dose and normal-dose 18F-FDG PET datasets demonstrated\nthat the proposed method achieved better quantitative performance than\nconventional UNet and standard DDPM methods at both the whole-body\nand organ levels. These results underscored the potential of leveraging\nVLMs to integrate rich metadata into the diffusion framework to enhance\nthe image quality of low-dose PET scans.\nKeywords: PET image denoising · Diffusion models · Vision-language\nmodels.\n1\nIntroduction\nPositron Emission Tomography (PET) is widely utilized in clinical and research\nsettings for its high sensitivity and quantitative accuracy, making it indispens-\nable in tumor detection, neurological diagnosis, and metabolic function evalua-\ntion [2,19]. Considering the radiation risks caused by radiotracer injections, faster\narXiv:2502.21260v1  [eess.IV]  28 Feb 2025\n\n\n2\nB. Yu et al.\nFig. 1. Grad-CAM [22] results based on CLIP encoder, generated using axial PET\nslices and anatomical structures text prompts. The top figures show normal-dose PET\nimages, while the bottom figures present heatmaps highlighting regions of interest, along\nwith the top three anatomical text prompts that best match the PET image features.\nThe highlighted regions correspond to critical anatomical structures of PET images,\ndemonstrating that the CLIP text embeddings effectively capture relevant anatomical\ninformation.\nor low-dose PET imaging is always desired. This trade-off results in limited counts\nreceived during acquisition, leading to higher noise and lower signal-to-noise ra-\ntio (SNR), potentially compromising its diagnostic and quantitative accuracy.\nObtaining high-quality PET images from low-dose scanning protocols remains a\nkey clinical challenge.\nIn recent years, deep learning methods have revolutionized the image pro-\ncessing field. The diffusion model [3,8], an advanced distribution learning-based\nmethod, has shown promising performance in various computer vision tasks [11,12,18].\nCompared with traditional methods such as Convolutional Neural Networks\n(CNNs) [5,30,7] and Generative Adversarial Networks (GANs) [24,24,4] that\ndirectly map from a source to a target domain, diffusion models iteratively\nreconstruct the target distribution from Gaussian distribution via a two-stage\ndiffusion process. This approach has demonstrated greater stability and improved\nimage fidelity. Existing literatures[29,28,10,23] have shown its effectiveness in\nPET image denoising.\nNevertheless, existing diffusion-based PET enhancement methods typically\noverlook valuable clinical metadata, such as patient demographics, slice infor-\nmation, and scanning protocols, which contain patient-specific details and other\nfactors that potentially affect the image quality. Incorporating such metadata\ninformation might further improve image quality. Some previous CNN-based\nstudies [13,27,26] have attempted to integrate complementary features from CT\nor MR images into the PET reconstruction framework; however, the limited\navailability of paired PET/CT or PET/MR data restricts their application. Al-\n\n\nPET Image Denoising via Text-Guided Diffusion\n3\nternatively, Luo et al. [17] proposed transforming clinical anatomical information\ninto structural attributes via one-hot encoding to adjust the weights of different\nbody regions during PET image generation. However, one-hot encoding fails\nto capture the semantic relationships between structures, thereby limiting its\ncapacity to effectively align with the complex features present in PET images.\nRecent advances in large-scale vision-language models (VLMs) [16,14,15] have\nrevealed great potential in extracting robust visual and textual representations\nfor downstream tasks. The Contrastive Language–Image Pre-training (CLIP)\nmodel [20], for example, learns aligned multimodal features from noisy image-\ntext pairs via contrastive learning. As shown in Fig. 1, CLIP-generated text\nembeddings based on anatomical descriptions (e.g., \"brain\", \"lung\", \"liver\")\ncan effectively match corresponding PET image features, demonstrating their\nability to identify and focus on critical anatomical regions in PET images. In\nthis work, we integrated anatomical text prompts into a diffusion-based PET\nimage denoising framework. By leveraging semantic guidance from pre-trained\nCLIP through a cross-attention mechanism, our approach explicitly guided the\nmodel to focus on key anatomical structures, thus enhancing the accuracy of\norgan-intensity restoration during the denoising process. Experimental results\nbased on clinical total-body PET datasets demonstrated that the proposed\nmethod outperformed traditional UNet and DDPM methods in both global and\norgan-based quantitative accuracy.\n2\nMethodology\n2.1\nConditional DDPM for PET Image Denoising\nDenoising Diffusion Probabilistic Model (DDPM) [8] provides a powerful frame-\nwork for image generation by modeling a gradual denoising process. In this work,\nwe adopted DDPM for PET image denoising as a baseline method, leveraging\ntheir ability to iteratively refine noise inputs into high-quality outputs. The\nprocess consisted of two phases: a forward diffusion process that gradually cor-\nrupted the input image with Gaussian noise, and a reverse diffusion process that\nreconstructed the clean image from pure Gaussian noise.\nThe forward diffusion process transformed a high-quality normal-dose PET\nimage x0 into a noisy version xT over T time steps, following a predefined variance\nschedule {βt}T\nt=1, which is defined as a Markov chain:\nq(x1:T |x0) =\nT\nY\nt=1\nq(xt|xt−1),\nq(xt|xt−1) = N(xt;\np\n1 −βtxt−1, βtI).\n(1)\nBy introducing αt = 1 −βt and ¯αt = Qt\ns=1 αs, the intermediate noisy image xt\nat any time step can be directly sampled from x0:\nq(xt|x0) = N(xt; √¯αtx0, (1 −¯αt)I).\n(2)\n\n\n4\nB. Yu et al.\nThe reverse diffusion process aimed to recover the clean image x0 from the\nnoisy observation xT ∼N(0, I), which was achieved by learning a neural network\npθ that approximated the conditional distribution q(xt−1|xt):\npθ(xt−1|xt, y) = N(xt−1; µθ(xt, y, t), σ2\nt I),\n(3)\nwhere y represents the observed low-quality PET image used as a conditional\ninput to guide the denoising process. Instead of directly predicting the mean µθ,\nthe model was trained to estimate the noise ϵθ added during the forward process\nbased on the UNet [21] backbone. This simplified the learning objective, which\nminimized the difference between the predicted and true noise as\nL(θ) = Ex0,y,t,ϵ\nh\n∥ϵ −ϵθ (xt, y, t)∥2i\n.\n(4)\nConsequently, the iterative denoising process was governed by:\nxt−1 =\n1\n√αt\n\u0012\nxt −\nβt\n√1 −¯αt\nϵθ(xt, y, t)\n\u0013\n+ σtz,\nwhere z ∼N(0, I).\n(5)\nThis framework established the baseline for our PET image denoising method.\nIn the following section, we further extended this framework by incorporating\nanatomical information as text prompts.\n2.2\nIntegrating Anatomical Priors as Text Prompts in DDPM\nInput image \nfeatures\nForward Diffusion Process\n…\nUNet\ntth Step\nNormal-dose PET\nAdd Gaussian noise\n…\nReverse Denoising Process\nCLIP\nText\nEncoder\n1/20 Low-dose PET\nAnatomical text \nprompt\nResidual\nBlock\nFlatten\n×\nReshape\nK\nQ\nCross-attention\n+\nAttention\nScore\nV\nOutput image \nfeatures\n: Text embeddings\n: Image embeddings\nUNet Bottom Block\n…\nFig. 2. Diagram of the proposed text-guided DDPM framework.\nTo further improve the quality of denoised PET images, we proposed a novel\ntext-guided DDPM that incorporated domain-specific anatomical priors into the\n\n\nPET Image Denoising via Text-Guided Diffusion\n5\ndenoising process. Overall, textual prompts describing the major anatomical\nstructures of each axial PET slice (e.g., “Axial slice including spleen, liver,\nstomach, aorta, vertebrae T10, ...”) were injected into the diffusion process, and\na cross-attention mechanism was introduced to learn semantic guidance from the\npre-trained CLIP encoder.\nFig. 2 illustrates the diagram of the proposed text-guided DDPM. Specifically,\nthe anatomical textual prompts were first encoded by the CLIP text encoder\ninto high-dimensional anatomical text embeddings et, which captured organ- and\nstructure-level semantics for each axial slice. During training, these embeddings\nwere then fed into the bottom blocks of the UNet via cross-attention, where\nthe query features extracted from PET images and text-derived keys computed\nthe attention score to determine how the anatomical information modulated the\nimage. Consequently, the score function of the diffusion network was modified to\nϵθ(xt, y, t, et), making the denoising process explicitly aware of the underlying\nanatomical context. Unlike conventional DDPMs that rely solely on paired\nimages, the proposed model leveraged CLIP’s ability to learn robust text-image\nalignments. By embedding high-level anatomical cues into the feature space, the\nnetwork was prompted to focus on the critical anatomical regions, enabling the\nUNet to adaptively refine its predictions based on the specific anatomical context\nof each PET slice.\n3\nExperiments and Results\nTotalSegmentator\nText-guided PET image denoising data pair\nAxial slice including lung, \nesophagus, vertebrae T7, heart, \naorta, pulmonary vein, ...\nAnatomical Text Prompt\nSegmentation Mask\nNormal-dose PET Image\n1/20 Low-dose PET Image\nCT Image\nFig. 3. Overview of the dataset construction for text-guided PET image denoising.\n3.1\nDatasets\nWe constructed a text-guided PET image denoising dataset that integrated\nanatomical information. It comprised paired 1/20 low-dose and normal-dose\n\n\n6\nB. Yu et al.\nPET images together with corresponding anatomical text descriptions for each\naxial slice. The 18F-FDG PET images were obtained from the Siemens Biograph\nVision Quadra PET/CT scanner and reconstructed using OSEM (4 iterations, 5\nsubsets) with time-of-flight (TOF) and point spread function (PSF) modeling. A\n2 mm Gaussian filter was applied [1]. The resulting voxel size was 1.65 × 1.65 ×\n1.65 mm3. All data were acquired in list mode, enabling rebinding to simulate\nvarious dose levels. The 1/20 low-dose PET images were generated through\nsubsampling at 1/20 of the original dose. The image intensity was expressed in\nStandardized Uptake Value (SUV). A total of 115 whole-body PET scans were\nutilized, randomly divided into 90 for training, 5 for validation, and 20 for testing.\nTo improve computational efficiency, parts of background regions were cropped,\nyielding image matrices of 192 × 288 × 520 corresponding to the coronal, sagittal,\nand axial dimensions, respectively.\nAnatomical text prompts were derived from the corresponding CT images.\nUsing the TotalSegmentator [25] toolkit, which was based upon nnUNet [9] and\ntrained on CT and MR images from diverse scanners, institutions, and protocols,\nwere adopted to obtain segmentation masks for 117 major anatomical categories.\nThese segmentation maps were processed to generate a mask for each axial PET\nslice. Unique anatomical categories present in each mask were identified to create\ntext prompts listing the key anatomical structures for that slice. These text\nprompts were paired with the 1/20 low-dose and normal-dose PET images and\nserved as guidance in both training and testing of the text-guided DDPM. Fig. 3\noutlines the dataset preparation process.\n3.2\nImplementation Details\nTextual anatomical embeddings were obtained by feeding the generated text\nprompts into the CLIP text encoder, with token sequences truncated to a maxi-\nmum length of 77 to align with CLIP’s input dimension. The utilized pre-trained\nCLIP model employs a ViT-B/32 Transformer as its image encoder and a masked\nself-attention Transformer as its text encoder.\nInspired by the work of Gong et al. [6], two adjacent axial slices were jointly\nfed into the network during training and testing to mitigate axial artifacts. All\nimplementations were developed in PyTorch, with a learning rate of 1 × 10−4 and\nthe mixed-precision training strategy was employed to enhance computational\nefficiency. In both the forward and reverse diffusion processes, the time step T\nwas set to 1000, with the variance schedule βt linearly increasing from 1 × 10−4\nto 0.02. The total training batch size was set to 48, and distributed training\nwas performed on 6 NVIDIA A100 GPUs. The entire training process required\napproximately two days, with an average testing time of 24 minutes per dataset.\nFor comparison, reference models including UNet and the original DDPM were\ntrained on the same dataset under identical conditions.\n\n\nPET Image Denoising via Text-Guided Diffusion\n7\nFig. 4. Qualitative comparison of PET images in sagittal (top), coronal (middle), and\naxial (bottom) views. From left to right: normal-dose PET image, 1/20 low-dose PET\nimage (as input), and generated denoised results from UNet, DDPM, and our proposed\nmethod. The intensity scale on the right indicates SUV values.\n3.3\nEvaluation Metrics\nTo assess the quantitative performance of different denoising methods, two widely-\nused quantitative metrics: Peak Signal-to-Noise Ratio (PSNR) and Structural\nSimilarity Index Measure (SSIM) were adopted, using the normal-dose PET\nimages as the ground truth. All evaluations were initially conducted over the entire\nimage, and were further computed for individual organs based on segmentation\nmasks to assess the local denoising quality. Finally, Wilcoxon signed-rank tests\nwere performed on the PSNR and SSIM scores to statistically validate the\nperformance differences of different methods.\n3.4\nResults\nFig. 4 shows a qualitative comparison of different methods. The 1/20 dose images\nexhibited notable noise and low SNR, while all three denoising methods restored\nmore anatomical details. However, the UNet outputs appeared over-smoothed,\n\n\n8\nB. Yu et al.\nFig. 5. Quantitative comparison of UNet, DDPM, and the proposed method in terms of\nSSIM and PSNR based on 20 1/20 low-dose test datasets. (A) Overall performance across\nthe entire image. (B) Organ-specific results for the heart, liver, lung, pancreas, spleen,\nand vessels. ***, **, *, ns located at the top of the bar plot represents p-value<0.001,\np-value<0.01, p-value<0.05, p-value≥0.05, respectively.\nwhile both DDPM and the proposed method produced more realistic images\ncloser to the normal-dose PET reference. Notably, the proposed method reduced\nartifacts around organs and provided sharper edge contours.\nFig. 5 shows the quantitative comparison between UNet, DDPM, and our\nmethod on the entire body and individual organs. Overall, the proposed model\nachieved higher SSIM and PSNR than the other methods, indicating better\ndenoising performance. Organ-specific metrics demonstrated consistent improve-\nments in regions such as the heart, liver, and spleen, where anatomical clarity\nand quantitative accuracy were crucial for diagnostic assessments. Although the\nmagnitude of improvement varied across different organs, performance gains are\nstatistically significant, highlighting the robustness of the proposed text-guided\napproach.\n4\nConclusion\nIn this work, we proposed a novel text-guided diffusion model for PET image\ndenoising that integrated anatomical priors through text prompts. By leveraging\nsemantic information encoded via the pre-trained CLIP text encoder, the proposed\nmethod achieved better performance on both whole-body and organ-specific\nevaluations. These preliminary results underscored the potential of incorporating\nmulti-modal semantic guidance through vision-language models to enhance PET\n\n\nPET Image Denoising via Text-Guided Diffusion\n9\nimage quality. Our future work will focus on fine-tuning the CLIP text encoder\nfor PET domain and integrating additional metadata information.\n5\nAcknowledgments\nThis work was supported by NIH grants R01EB034692 and R01AG078250.\nReferences\n1. Alberts, I., Hünermund, J.N., Prenosil, G., Mingels, C., Bohn, K.P., Viscione, M.,\nSari, H., Vollnberg, B., Shi, K., Afshar-Oromieh, A., et al.: Clinical performance\nof long axial field of view pet/ct: a head-to-head intra-individual comparison of\nthe biograph vision quadra with the biograph vision pet/ct. European journal of\nnuclear medicine and molecular imaging 48, 2395–2404 (2021)\n2. Barthel, H., Schroeter, M.L., Hoffmann, K.T., Sabri, O.: Pet/mr in dementia and\nother neurodegenerative diseases. In: Seminars in nuclear medicine. vol. 45, pp.\n224–233. Elsevier (2015)\n3. Dhariwal, P., Nichol, A.: Diffusion models beat gans on image synthesis. Advances\nin neural information processing systems 34, 8780–8794 (2021)\n4. Fu, Y., Dong, S., Niu, M., Xue, L., Guo, H., Huang, Y., Xu, Y., Yu, T., Shi,\nK., Yang, Q., et al.: Aigan: Attention–encoding integrated generative adversarial\nnetwork for the reconstruction of low-dose ct and low-dose pet images. Medical\nImage Analysis 86, 102787 (2023)\n5. Gong, K., Guan, J., Liu, C.C., Qi, J.: Pet image denoising using a deep neural\nnetwork through fine tuning. IEEE Transactions on Radiation and Plasma Medical\nSciences 3(2), 153–161 (2018)\n6. Gong, K., Johnson, K., El Fakhri, G., Li, Q., Pan, T.: Pet image denoising based\non denoising diffusion probabilistic model. European Journal of Nuclear Medicine\nand Molecular Imaging pp. 1–11 (2023)\n7. Hashimoto, F., Ohba, H., Ote, K., Teramoto, A., Tsukada, H.: Dynamic pet image\ndenoising using deep convolutional neural networks without prior training datasets.\nIEEE access 7, 96594–96603 (2019)\n8. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Advances in\nneural information processing systems 33, 6840–6851 (2020)\n9. Isensee, F., Jaeger, P.F., Kohl, S.A., Petersen, J., Maier-Hein, K.H.: nnu-net: a\nself-configuring method for deep learning-based biomedical image segmentation.\nNature methods 18(2), 203–211 (2021)\n10. Jiang, C., Pan, Y., Liu, M., Ma, L., Zhang, X., Liu, J., Xiong, X., Shen, D.: Pet-\ndiffusion: Unsupervised pet enhancement based on the latent diffusion model. In:\nInternational Conference on Medical Image Computing and Computer-Assisted\nIntervention. pp. 3–12. Springer (2023)\n11. Jiang, H., Imran, M., Ma, L., Zhang, T., Zhou, Y., Liang, M., Gong, K., Shao, W.:\nFast-ddpm: Fast denoising diffusion probabilistic models for medical image-to-image\ngeneration. arXiv preprint arXiv:2405.14802 (2024)\n12. Lee, J., Li, X., Zhu, L., Ranka, S., Rangarajan, A.: Guaranteed conditional dif-\nfusion: 3d block-based models for scientific data compression. arXiv preprint\narXiv:2502.12951 (2025)\n\n\n10\nB. Yu et al.\n13. Lesonen, P., Wettenhovi, V., Kolehmainen, V., Pulkkinen, A., Vauhkonen, M.:\nAnatomy-guided multi-resolution image reconstruction in pet. Physics in Medicine\n& Biology 69(10), 105023 (2024)\n14. Li, C., Wong, C., Zhang, S., Usuyama, N., Liu, H., Yang, J., Naumann, T., Poon, H.,\nGao, J.: Llava-med: Training a large language-and-vision assistant for biomedicine\nin one day. Advances in Neural Information Processing Systems 36, 28541–28564\n(2023)\n15. Li, J., Li, D., Xiong, C., Hoi, S.: Blip: Bootstrapping language-image pre-training for\nunified vision-language understanding and generation. In: International conference\non machine learning. pp. 12888–12900. PMLR (2022)\n16. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. Advances in neural\ninformation processing systems 36, 34892–34916 (2023)\n17. Luo, F., Tang, H., Li, W., Wang, H., Chen, R., Liu, J., Zhou, C., Zhang, X.,\nFan, W., Zhao, Y., et al.: Weight-adaptive network with ct enhancement for short-\nduration pet imaging utilizing the uexplorer total-body system. IEEE Transactions\non Radiation and Plasma Medical Sciences (2025)\n18. Luo, Z., Gustafsson, F.K., Zhao, Z., Sjölund, J., Schön, T.B.: Controlling vision-\nlanguage models for multi-task image restoration. arXiv preprint arXiv:2310.01018\n(2023)\n19. Ming, Y., Wu, N., Qian, T., Li, X., Wan, D.Q., Li, C., Li, Y., Wu, Z., Wang, X.,\nLiu, J., et al.: Progress and future trends in pet/ct and pet/mri molecular imaging\napproaches for breast cancer. Frontiers in oncology 10, 1301 (2020)\n20. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G.,\nAskell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from\nnatural language supervision. In: International conference on machine learning. pp.\n8748–8763. PmLR (2021)\n21. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomed-\nical image segmentation. In: Medical image computing and computer-assisted\nintervention–MICCAI 2015: 18th international conference, Munich, Germany, Oc-\ntober 5-9, 2015, proceedings, part III 18. pp. 234–241. Springer (2015)\n22. Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra, D.: Grad-\ncam: Visual explanations from deep networks via gradient-based localization. In:\nProceedings of the IEEE international conference on computer vision. pp. 618–626\n(2017)\n23. Shen, C., Yang, Z., Zhang, Y.: Pet image denoising with score-based diffusion\nprobabilistic models. In: International Conference on Medical Image Computing\nand Computer-Assisted Intervention. pp. 270–278. Springer (2023)\n24. Wang, Y., Yu, B., Wang, L., Zu, C., Lalush, D.S., Lin, W., Wu, X., Zhou, J., Shen,\nD., Zhou, L.: 3d conditional generative adversarial networks for high-quality pet\nimage estimation at low dose. Neuroimage 174, 550–562 (2018)\n25. Wasserthal, J., Breit, H.C., Meyer, M.T., Pradella, M., Hinck, D., Sauter, A.W.,\nHeye, T., Boll, D.T., Cyriac, J., Yang, S., et al.: Totalsegmentator: robust segmen-\ntation of 104 anatomic structures in ct images. Radiology: Artificial Intelligence\n5(5), e230024 (2023)\n26. Xie, Z., Qi, J.: Anatomy-guided pet image reconstruction with deep neural network\n(2020)\n27. Yang, B., Gong, K., Liu, H., Li, Q., Zhu, W.: Anatomically guided pet image\nreconstruction using conditional weakly-supervised multi-task learning integrating\nself-attention. IEEE Transactions on Medical Imaging 43(6), 2098–2112 (2024)\n\n\nPET Image Denoising via Text-Guided Diffusion\n11\n28. Yu, B., Ozdemir, S., Dong, Y., Shao, W., Pan, T., Shi, K., Gong, K.: Robust\nwhole-body pet image denoising using 3d diffusion models: evaluation across various\nscanners, tracers, and dose levels. European Journal of Nuclear Medicine and\nMolecular Imaging pp. 1–14 (2025)\n29. Yu, B., Ozdemir, S., Dong, Y., Shao, W., Shi, K., Gong, K.: Pet image denoising\nbased on 3d denoising diffusion probabilistic model: Evaluations on total-body\ndatasets. In: International Conference on Medical Image Computing and Computer-\nAssisted Intervention. pp. 541–550. Springer (2024)\n30. Zhang, J., Contreras, M., Bandyopadhyay, S., Davidson, A., Sena, J., Ren, Y.,\nGuan, Z., Ozrazgat-Baslanti, T., Loftus, T.J., Nerella, S., et al.: Mango: Multimodal\nacuity transformer for intelligent icu outcomes. arXiv preprint arXiv:2412.17832\n(2024)\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21260v1.pdf",
    "total_pages": 11,
    "title": "PET Image Denoising via Text-Guided Diffusion: Integrating Anatomical Priors through Text Prompts",
    "authors": [
      "Boxiao Yu",
      "Savas Ozdemir",
      "Jiong Wu",
      "Yizhou Chen",
      "Ruogu Fang",
      "Kuangyu Shi",
      "Kuang Gong"
    ],
    "abstract": "Low-dose Positron Emission Tomography (PET) imaging presents a significant\nchallenge due to increased noise and reduced image quality, which can\ncompromise its diagnostic accuracy and clinical utility. Denoising diffusion\nprobabilistic models (DDPMs) have demonstrated promising performance for PET\nimage denoising. However, existing DDPM-based methods typically overlook\nvaluable metadata such as patient demographics, anatomical information, and\nscanning parameters, which should further enhance the denoising performance if\nconsidered. Recent advances in vision-language models (VLMs), particularly the\npre-trained Contrastive Language-Image Pre-training (CLIP) model, have\nhighlighted the potential of incorporating text-based information into visual\ntasks to improve downstream performance. In this preliminary study, we proposed\na novel text-guided DDPM for PET image denoising that integrated anatomical\npriors through text prompts. Anatomical text descriptions were encoded using a\npre-trained CLIP text encoder to extract semantic guidance, which was then\nincorporated into the diffusion process via the cross-attention mechanism.\nEvaluations based on paired 1/20 low-dose and normal-dose 18F-FDG PET datasets\ndemonstrated that the proposed method achieved better quantitative performance\nthan conventional UNet and standard DDPM methods at both the whole-body and\norgan levels. These results underscored the potential of leveraging VLMs to\nintegrate rich metadata into the diffusion framework to enhance the image\nquality of low-dose PET scans.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}