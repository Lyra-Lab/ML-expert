{
  "id": "arxiv_2502.21123v2",
  "text": "Causality Is Key to Understand and Balance Multiple Goals in Trustworthy\nML and Foundation Models\nRuta Binkyte * 1 Ivaxi Sheth * 1 Zhijing Jin 2 3 4 Mohammad Havaei 5 Bernhard Sch¨olkopf 2 Mario Fritz 1\nAbstract\nEnsuring trustworthiness in machine learning\n(ML) systems is crucial as they become increas-\ningly embedded in high-stakes domains.\nThis\npaper advocates for integrating causal methods\ninto machine learning to navigate the trade-offs\namong key principles of trustworthy ML, includ-\ning fairness, privacy, robustness, accuracy, and\nexplainability. While these objectives should ide-\nally be satisfied simultaneously, they are often\naddressed in isolation, leading to conflicts and\nsuboptimal solutions. Drawing on existing appli-\ncations of causality in ML that successfully align\ngoals such as fairness and accuracy or privacy\nand robustness, this paper argues that a causal\napproach is essential for balancing multiple\ncompeting objectives in both trustworthy ML\nand foundation models. Beyond highlighting\nthese trade-offs, we examine how causality can\nbe practically integrated into ML and foundation\nmodels, offering solutions to enhance their reli-\nability and interpretability. Finally, we discuss\nthe challenges, limitations, and opportunities in\nadopting causal frameworks, paving the way for\nmore accountable and ethically sound AI sys-\ntems.\n1. Introduction\nIn recent years, machine learning (ML) has made remark-\nable strides, driving breakthroughs in natural language pro-\ncessing (Achiam et al., 2023), computer vision (Brooks\net al., 2024), and decision-making systems (Jia et al.,\n2024). These advancements have led to widespread adop-\ntion across diverse domains, including healthcare (Singhal\net al., 2025), finance (Lee et al., 2024), education (Team\net al., 2024), and social media (Bashiri & Kowsari, 2024),\n*Equal contribution 1CISPA Helmholtz Center for Information\nSecurity 2Max Planck Institute for Intelligent, T¨ubingen 3ETH\nZ¨urich 4University of Toronto 5Google Research.\nCorrespon-\ndence to: Ruta Binkyte <ruta.binkyte@gmail.com>.\nPreprint\nCAUSAL\nDISCOVERY\nPRIOR CAUSAL\nKNOWLEDGE\nCAUSAL\nAUDIT\nFAIR\nPRIVATE\nROBUST\nEXPLAINABLE\nACCURATE\nFigure 1: Causal Trustworthy ML Cycle: Causal ML can\nleverage existing knowledge and causal auditing to en-\nhance different components of trustworthiness: explain-\nability, fairness, privacy, and accuracy while simultane-\nously advancing understanding through causal discovery.\nwhere ML models now play a crucial role in diagnostics,\nalgorithmic trading, personalized learning, and content rec-\nommendation.\nGiven their soaring influence, ensuring ethical and trust-\nworthy ML systems has become a global priority (Lewis\net al., 2020). Many international regulations and frame-\nworks (European Commission, 2021; OECD, 2019; Group\nof Twenty (G20), 2019; Infocomm Media Development\nAuthority, 2020) seek to establish guidelines for fairness,\nexplainability, robustness, and privacy protection. For the\nscope of our paper, we are aware of different definitions of\ntrustworthiness, but will focus on five core dimensions that\nare both widely recognized and directly relevant to causal\nreasoning: fairness, privacy, robustness, explainability, and\naccuracy. We will introduce these dimensions and highlight\ntheir trade-offs and intersections below.\nFairness.\nFairness in ML refers to the principle that\nsystems should make unbiased decisions that do not dis-\ncriminate against individuals or groups based on sensi-\ntive attributes such as race, gender, or socioeconomic sta-\ntus. ML systems have been shown to rely heavily on bi-\nased data, amplifying existing biases and leading to un-\nequal outcomes (COMPAS, 2020). These systems often\nexhibit reduced accuracy for minority or underrepresented\ngroups, further exacerbating disparities (Buolamwini &\n1\narXiv:2502.21123v2  [cs.LG]  3 Mar 2025\n\n\nCausality Is Key to Understand and Balance Multiple Goals in Trustworthy ML and Foundation Models\nGebru, 2018). Given the speed and scale of ML-enabled\ndecisions, ensuring fairness is essential to prevent perpetu-\nating and exacerbating societal inequalities at an unprece-\ndented scale.\nPrivacy. Privacy in ML emphasizes the protection of in-\ndividuals’ sensitive and personal data. It has been shown\nthat even after removing identifiers such as names, infor-\nmation can still leak, and individuals can be reidentified\nthrough indirect attributes and data triangulation(Sweeney,\n2000; Narayanan & Shmatikov, 2008; Ohm, 2010; Dwork,\n2006). Additionally, sensitive information can be recon-\nstructed from gradients during model training if data is not\nhandled privately (Zhu et al., 2019; Geiping et al., 2020;\nAono et al., 2017; Fredrikson et al., 2015). Privacy is cru-\ncial for ensuring compliance with data protection laws and\nsafeguarding human rights. It also fosters trust for individ-\nuals to be more willing to contribute their data for model\ntraining if their safety and privacy were ensured.\nRobustness. Robustness refers to the system’s ability to\nperform reliably under varying conditions, including ad-\nversarial attacks, noisy inputs, or distributional shifts. For\nexample, models often underperform when faced with dis-\ntribution shifts, such as changes in data characteristics be-\ntween training and deployment environments (Hendrycks\n& Dietterich, 2019; Recht et al., 2019; Ovadia et al., 2019).\nAdditionally, human-undetectable noise added to images\ncan cause models to make incorrect predictions, highlight-\ning their vulnerability (Szegedy et al., 2014; Goodfellow\net al., 2015). Robustness is critical to ensuring the safety\nand reliability of AI systems, particularly in high-stakes ap-\nplications such as healthcare and autonomous driving.\nExplainability. Explainability refers to the ability of AI\nsystems to provide clear and understandable reasoning be-\nhind their decisions or predictions. Deep neural networks\n(DNNs), often referred to as “black boxes,” are inherently\ncomplex and difficult to interpret, making them hard to au-\ndit and assess for fairness or correctness (Lipton, 2018;\nDoshi-Velez & Kim, 2017; Rudin, 2019). Explainability\nis closely tied to accountability, as it enables stakehold-\ners to evaluate and challenge AI outputs when necessary.\nFurthermore, regulations such as the GDPR emphasize the\n“right to explanation,” which requires that individuals be\ninformed about and understand how automated decisions\naffecting them are made (EC, 2016).\nTrade-offs and Intersections. The trustworthy ML land-\nscape involves complex trade-offs and interdependencies\nbetween key objectives such as fairness, privacy, accuracy,\nrobustness, and explainability. Improving one aspect of-\nten comes at the expense of another, such as the trade-\noff between privacy and accuracy in differential privacy,\nwhere noise added to protect data reduces model accu-\nracy (Xu et al., 2017; Carvalho et al., 2023). Similarly,\nachieving fairness frequently requires sacrificing predic-\ntive performance or resolving conflicts between competing\nfairness notions, such as demographic parity and equalized\nodds (Friedler et al., 2021; Kim et al., 2020). Trade-offs\nalso arise in explainability and accuracy, as complex mod-\nels like DNNs excel in performance but lack interpretabil-\nity. Meanwhile, the relationship between fairness and pri-\nvacy is nuanced, with evidence showing they can either\nconflict, as noise may lead to disparate outcomes, or com-\nplement each other by reducing bias (Pujol et al., 2020;\nDwork et al., 2011).\nCausality. One of the most influential causal frameworks\nis Pearl’s structural causal models (SCMs), which provide a\nsystematic approach to reasoning about causality and inte-\ngrating it into machine learning (Pearl, 2009b). This frame-\nwork defines causality as the relationship between the vari-\nables where a change in one variable (the cause) directly\nleads to a change in another variable (the effect). It estab-\nlishes a directional and often mechanistic link, distinguish-\ning relationships arising from mere correlations.\nA key component of Pearl’s framework is the use of di-\nrected acyclic graphs (DAGs) and do-calculus, which offer\na structured representation of causal dependencies and a\nformal method for performing causal inference. A causal\nDAG, denoted as G = (V, E), consists of a set of nodes V\nrepresenting random variables and directed edges E encod-\ning causal relationships among the variables.\nUnlike correlation-based approaches, causality provides a\nframework for disentangling the underlying mechanisms\nthat drive observed phenomena, offering a deeper interpre-\ntation of data. Causal frameworks have been successfully\napplied to audit and mitigate fairness (Kim et al., 2021; Kil-\nbertus et al., 2017; Loftus et al., 2018) and to improve ro-\nbustness (Sch¨olkopf et al., 2022). The research about the\nconnection between causality and privacy is still very lim-\nited, but some emerging studies show potential for applica-\ntions (Tschantz et al., 2020). Finally, explainability is one\nof the core features of causality and comes pre-packaged\nwith the causal framework. Despite the promising applica-\ntions of causality for individual requirements of trustwor-\nthy AI, the potential to use causality to reconcile individ-\nual requirements of trustworthy ML remains largely under-\nexplored.\nPosition. Despite significant advancements in research on\nindividual dimensions of trustworthy ML such as fairness,\nprivacy, and explainability—there is a notable lack of ef-\nforts to integrate these dimensions into a cohesive and uni-\nfied framework. Each ethical principle addresses distinct\nchallenges, yet their interplay often involves intricate trade-\noffs, particularly concerning model performance metrics\nsuch as accuracy. For example, mitigating fairness-related\nbiases may require adjustments that compromise predic-\n2\n\n\nCausality Is Key to Understand and Balance Multiple Goals in Trustworthy ML and Foundation Models\ntive precision, while enhancing explainability can impose\nconstraints on model complexity. We argue that systemat-\nically addressing these trade-offs is a critical step toward\ndeveloping AI systems that are both ethically sound and\noperationally efficient. While causality has been applied\nto address individual challenges such as fairness or inter-\npretability, its potential to address the intersection of these\nchallenges has largely been overlooked (see Appendix A\nfor a detailed review). In this paper, we argue that inte-\ngrating causality into ML and foundation models offers\na way to balance multiple competing objectives of trust-\nworthy AI.\nThe structure of our paper is as follows. Section 2 analyzes\nhow causality can reconcile multiple dimensions of trust-\nworthy ML and explores how it can be integrated. Sec-\ntion 3 discusses how foundation models amplify existing\nML trade-offs and introduce new challenges, for which we\nargue that causality provides a principled approach to over-\ncoming these issues, and propose strategies for integrating\ncausal reasoning into foundation models at different devel-\nopment stages. Section 4 covers limitations in applying\ncausality to ML and foundation models and proposes fu-\nture research directions, and Section 5 includes alternative\nviews. Finally, Section 6 suggests key steps for advancing\ncausality in ML and foundation models.\n2. Causality for Trustworthy ML\nTrustworthy ML involves inherent trade-offs between core\nobjectives such as accuracy, fairness, robustness, privacy,\nand explainability. Inevitable trade-offs can exist between\naccuracy and other objectives, fairness and privacy, and\nconflicting fairness notions. However, some other goals\nmay reinforce each other, such as explainability aiding fair-\nness assessment, and privacy enhancing robustness (Dwork\n& Lei, 2009; Hopkins et al., 2022).\nCausality provides a principled approach to navigating\nthese trade-offs by explicitly modeling data-generating pro-\ncesses and clarifying assumptions. This section first ex-\nplores causal formulations for these trade-offs, and then in-\ntroduce how causality can mitigate these tensions and sup-\nport a more balanced approach to trustworthy ML.\n2.1. Causality for Trade-offs in Trustworthy ML\nIn this section, we examine key trade-offs in trustworthy\nML and illustrate how causal approaches can help reconcile\nthese competing objectives.\nPrivacy vs. Accuracy. The differential privacy approach\nrelies on adding noise to the data which is controlled by\nthe parameter ϵ (the smaller value of ϵ corresponds to more\nnoise, while the larger value indicates less noise and less\nprivacy). Naturally, it hurts the accuracy of an algorithm\nlearned on the privatized data. It is yet unknown how to\navoid this fundamental trade-off between data protection\nand the utility of the data (Xu et al., 2017; Carvalho et al.,\n2023). One of ways how causality can inform privacy is\nprovided by (Tschantz et al., 2020). The authors define pri-\nvacy violations as causal effects, emphasizing that private\ninformation is leaked when an adversary can infer sensi-\ntive attributes from observable data due to causal pathways.\nTherefore, causal models can help identify, quantify, and\nmitigate such risks, offering a more systematic alternative\nto heuristic-based privacy measures.\nBy aligning privacy interventions with causal relationships,\nmodels can obscure sensitive attributes (e.g., sex, race)\nwhile preserving meaningful data dependencies, reducing\nthe negative impact on accuracy.\nFor example, causal\ngraphs ensure that interdependent variables (e.g., age and\neducation) are randomized together to avoid unrealistic\ncombinations (e.g., “Age: 5; Education: Bachelor”). Pre-\nventing such inconsistencies not only improves accuracy\nbut also reduces the likelihood of adversaries exploiting ob-\nfuscation patterns, enhancing overall privacy protection.\nFairness vs. Accuracy. Most of the statistical fairness lit-\nerature focuses on improving fairness metrics while pre-\nserving accuracy as much as possible (Feldman et al.,\n2015; Wei & Niethammer, 2022; Wang et al., 2021a).\nHowever, fairness often comes at the cost of reduced\naccuracy, as mitigating bias may require either obscur-\ning predictive features that also contribute to discrimi-\nnation or constraining model predictions within fairness-\nimposed boundaries (Pinz´on et al., 2022; Cooper et al.,\n2021; Zliobaite, 2015; Zhao & Gordon, 2022).\nA key issue is that many fairness-accuracy trade-offs arise\nfrom addressing correlations rather than causal relation-\nships. Causal models can resolve these tensions by dis-\ntinguishing legitimate predictive factors from spurious dis-\ncriminatory pathways. By disentangling the direct and in-\ndirect effects of sensitive attributes on outcomes, causal in-\nterventions can mitigate unfair biases without sacrificing\naccuracy. For instance, counterfactual fairness ensures that\nindividuals receive the same prediction regardless of their\nsensitive attributes in a counterfactual world where those\nattributes are altered (Kusner et al., 2017).\nA compelling example comes from the COMPAS dataset,\nwhere Black defendants were more likely to be classified\nas high-risk for recidivism.\nTraditional statistical debi-\nasing approaches treat race as a direct cause of the risk\nscore, but a causal analysis reveals that increased recidi-\nvism risk is confounded by heightened policing in predom-\ninantly Black neighborhoods. By explicitly modeling this\ncausal structure, fairness-enhancing interventions can ad-\njust for the effect of over-policing, ensuring that predictions\nreflect true recidivism risk rather than biased enforcement\n3\n\n\nCausality Is Key to Understand and Balance Multiple Goals in Trustworthy ML and Foundation Models\nFairness\nPrivacy\nExplainability\nRobustness\nAccuracy\nFairness\nPrivacy\nExplainability\nRobustness\nAccuracy\nSynergy\nTension\nCausality\nFigure 2: While trustworthy AI involves inherent trade-offs between its key components, causality can help mitigate these\ntensions and enhance synergies.\npatterns. This results in a more accurate and fairer risk as-\nsessment (Chiappa, 2019; Zafar et al., 2017; Zhang et al.,\n2018).\nConflicting Notions of Fairness. Fairness in ML is often\nconstrained by conflicting definitions and measurement ap-\nproaches. Friedler et al. (2021) highlight the fundamental\ntension between the “what you see is what you get” and\n“we are all equal” worldviews—where the former accepts\ndisparities based on observed merit, while the latter seeks\nto correct historical inequalities. Causal graphs can crisply\nformulate different notions of fairness (Nabi & Shpitser,\n2018; Chen et al., 2024b), thus enabling feasible mitigation\nvia path-specific causal effects (Avin et al., 2005).\nKim et al. (2020) formalize fairness conflicts using the\nfairness-confusion tensor, showing that notions like de-\nmographic parity and equalized odds impose incompatible\nconstraints. The causal approach mitigates these conflicts\nby focusing on fairness as a property of causal pathways\nrather than statistical dependencies (Rahmattalabi & Xi-\nang, 2022). This allows for greater flexibility in aligning\nfairness interventions with real-world causal mechanisms,\nallowing better-informed choice of fairness metric.\nRobustness vs. Accuracy. The trade-off between general-\nizability and accuracy is rooted in the observation that mod-\nels trained to achieve high accuracy on a specific dataset\noften overfit to the peculiarities of that distribution. This\noverfitting compromises their ability to generalize to new,\nunseen distributions (Sch¨olkopf et al., 2022). On the con-\ntrary, causal models focus on invariant relationships that\nhold across different environments, making them robust to\ndistribution shifts. This robustness enhances the model’s\nability to generalize to unseen data, improving accuracy\nin diverse settings.\nFor example, causal representation\nlearning disentangles stable causal factors, allowing the\nmodel to maintain performance when data distributions\nchange. Moreover, Richens & Everitt (2024) prove that ro-\nbust agents implicitly learn causal world models, further\nemphasizing the intrinsic interdependency between robust-\nness and causality.\nExplainability vs. Accuracy. Many complex algorithms,\nsuch as deep neural networks (DNN) or random forest\n(RF), have impressive predictive power but provide “black-\nbox” solutions that are hard to question or evaluate (Lon-\ndon, 2019; van der Veer et al., 2021). Causal models offer\ninherently interpretable structures by quantifying the con-\ntribution of each input feature to the output, providing clear,\nhuman-understandable explanations. Causal recourse fur-\nther enhances explainabilit by offering actionable recom-\nmendations for individuals affected by model decisions,\nhelping them achieve a more favorable outcome (Karimi\net al., 2021).\nFairness vs. Explainability. A particularly powerful ap-\nproach within causal explainability is counterfactual expla-\nnations, which help users understand model decisions by\nasking “what if” questions. Counterfactual methods gener-\nate alternative scenarios where certain features are changed\nwhile keeping others constant, allowing for a direct assess-\nment of how specific inputs influence predictions (Wachter\net al., 2017; Karimi et al., 2020). Counterfactual explana-\ntions are particularly useful for fairness auditing as they can\nhelp identify why certain groups are adversely affected and\nguide corrective measures.\nPrivacy vs. Robustness. Adding noise without consider-\ning the data structure or causal relationships can obscure\nmeaningful patterns and introduce spurious correlations.\nThis indiscriminate noise can make models less robust to\nunseen data, particularly under distribution shifts.\nIn contrast, causal models inherently emphasize invariant\nrelationships—patterns that are stable across various data\ndistributions. Noise that disrupts non-causal relationships\nor spurious correlations can further enhance the robustness\nof these models to shifts in data. Finally, some results show,\nthat causal models provide stronger guarantees for adver-\nsarial robustness with lower epsilon in differential privacy,\nthus allowing for lesser negative impact on accuracy (Tople\net al., 2020).\n4\n\n\nCausality Is Key to Understand and Balance Multiple Goals in Trustworthy ML and Foundation Models\nPrivacy vs.\nFairness.\nPrivacy mechanisms, such as\nnoise addition, can disproportionately impact minority\ngroups, leading to fairness concerns.\nDifferentially Pri-\nvate Stochastic Gradient Descent (DP-SGD), for example,\nhas been shown to degrade model accuracy more severely\nfor underrepresented groups, exacerbating fairness dispar-\nities (Bagdasaryan et al., 2019). However, Causal models\ncan guide privacy interventions by ensuring that noise is\napplied in ways that do not disrupt fairness-critical rela-\ntionships. For instance, a causal graph can reveal which\nfeatures or pathways should be preserved to maintain fair-\nness while protecting privacy.\nPrediction Accuracy vs. Intervention Accuracy. One of\nthe key advantages of the causal framework is its ability\nto support not just prediction but also intervention (Hern´an\n& Robins, 2020; Sch¨olkopf et al., 2022). While predictive\nmodels are sufficient in some domains, many high-stakes\napplications—such as healthcare, policy-making, and per-\nsonalized treatment—require actionable interventions. In\nthese settings, understanding causal relationships is essen-\ntial, as the objective is not only to predict outcomes but also\nto influence them.\n2.2. Integrating Causality into ML\nIntegrating causality into ML enables models to move be-\nyond pattern recognition and learn underlying mechanisms\ngoverning data. This section explores different approaches\nto causal ML, ranging from explicitly constrained models\nthat follow predefined causal structures to methods that in-\nfer causal relationships from data.\nCausally Constrained ML (CCML). CCML refers to\napproaches that explicitly incorporate causal relationships\ninto model training or inference as constraints or guiding\nprinciples. Given a causal graph G = (V, E), where V\nrepresents variables and E denotes directed edges encoding\ncausal relationships, the goal is to ensure that the learned\nmodel f : X →Y adheres to the causal structure encoded\nin G (Berrevoets et al., 2024; Zinati et al., 2024; Afonja\net al., 2024; Sch¨olkopf et al., 2016).\nInvariant feature learning (IFL). IFL relies on discovered\nimplicit or latent causal features and structures. The task\nof Invariant Feature Learning (IFL) is to identify features\nof the data X that are predictive of the target Y across a\nrange of environments E. From a causal perspective, the\ncausal parents Pa(Y ) are always predictive of Y under any\ninterventional distribution (Kaddour et al., 2022). IFL can\nbe achieved by regularizing the model or providing causal\ntraining data that is free of confounding.\nDisentangled VAEs.\nVAEs aim to decompose the data\nX into disentangled latent factors Z that correspond to dis-\ntinct underlying generative causes (Burgess et al., 2018). It\ncan be combined with interventional experiments in mech-\nanistic interpretability that involve “switching off” specific\nneurons or circuits to gain knowledge about causal work-\nings of the complex model (Leeb et al., 2022). Causality is\nalso used to audit models for fairness (Cornacchia et al.,\n2023; Byun et al., 2024) or robustness (Drenkow et al.,\n2024), providing insights into how decisions are influenced\nby sensitive variables and under distribution shifts.\nDouble Machine Learning (DML). DML provides an-\nother causal approach by leveraging modern ML tech-\nniques for estimating high-dimensional nuisance parame-\nters while preserving statistical guarantees in causal infer-\nence (Chernozhukov et al., 2018). DML decomposes the\nestimation problem into two stages: (1) predicting con-\nfounders using ML models and (2) estimating the causal\neffect using residualized outcomes.\nCausal Discovery. Finally, ML can be leveraged for causal\ninference or to discover causal knowledge from observa-\ntional data.\nFor instance, methods for causal discovery\nuse statistical patterns to infer causal relationships, with\nnotable examples including (Spirtes et al., 2000; Shimizu\net al., 2006; Janzing & Sch¨olkopf, 2010; Peters et al., 2011;\nHauser & B¨uhlmann, 2012; Le et al., 2016).\nAll of the above forms a causal ML cycle (Figure 1) in\nwhich ML is enhanced by causal knowledge, controlled by\ncausal tools, and finally contributes to enriching scientific\nknowledge. We include a supplementary introduction to\ncausality and causal ML in Appendices C and D.\n3. Causality for Trustworthy Foundation\nModels\nFoundation models, including state-of-the-art multimodal\nsystems like Large Language Models (LLMs) and vision-\nlanguage models, have demonstrated exceptional capabili-\nties across diverse tasks (Achiam et al., 2023; Team et al.,\n2023; Radford et al., 2023; Brooks et al., 2024). However,\ntheir reliability remains a concern due to issues like spu-\nrious correlations, hallucinations, and unequal representa-\ntion. Trade-offs and causality in trustworthy foundation\nmodels remain underexplored as compared to traditional\nML. In this section, we explore the potential for causality\nto improve fairness, explainability, privacy, and robustness\nin foundation models following slightly different taxonomy\nthan in the previous section due to their unique challenges.\n3.1. Dimensions of Trustworthy Foundation Models\nIn this section, we examine foundation model-specific\ntrade-offs between key dimensions of trustworthy AI and\nillustrate how causal approaches can soften those tensions.\nFairness vs.\nAccuracy.\nCausal frameworks have be-\n5\n\n\nCausality Is Key to Understand and Balance Multiple Goals in Trustworthy ML and Foundation Models\ncome integral to fairness interventions in LLMs by iden-\ntifying and mitigating pathways that lead to unfair predic-\ntions (Madhavan et al., 2023a; Cotta & Maddison, 2024).\nCounterfactual fairness ensures that sensitive attributes\n(e.g., gender, race) do not causally influence outcomes. For\nexample, in job recommendation systems, counterfactual\nfairness guarantees identical recommendations for equally\nqualified candidates regardless of their gender (Madhavan\net al., 2023a). Methods like causal disentanglement iso-\nlate sensitive features from output-relevant causal factors,\nensuring that spurious correlations, such as gender biases\nin job roles, do not propagate through the model (Zhou\net al., 2023a; Chen et al., 2024a). SCMs further enable\nfairness-aware fine-tuning by disentangling causal effects.\nHowever, striving for diversity has been shown to introduce\nnon-factual output in text-to-image models. In early 2024,\nGoogle’s AI tool, Gemini, faced criticism for generating\nhistorically inaccurate images, such as depicting America’s\nFounding Fathers as Black individuals and Nazis as racially\ndiverse (Vincent, 2024). Here, causality could help distin-\nguish historically impossible scenarios from desirable di-\nversity, ensuring both fairness and factual integrity in AI-\ngenerated content. Mode collapse is another foundation\nmodel-specific fairness issue where models generate overly\ngeneric outputs, reducing diversity and disproportionately\nomitting minority group representations. Causal modeling\ncan potentially help preserve minority information by ex-\nplicitly capturing causal relationships, preventing spurious\ncorrelations from erasing underrepresented patterns.\nRobustness vs. Accuracy. Causal frameworks address\nrobustness by training models to rely on invariant causal\nrelationships while penalizing reliance on dataset-specific\nspurious features (Wu et al., 2024). For instance, instead\nof associating ”doctor” with ”male,” causal invariance en-\nforces reliance on task-relevant features like medical termi-\nnology (Zhou et al., 2023a). Causal regularization further\ndiscourages attention to non-causal patterns during infer-\nence achieving better accuracy and robustness.\nPrivacy vs. Attribution. Causal approaches to privacy fo-\ncus on detecting and severing pathways involving person-\nally identifiable information (PII) in LLMs. Causal obfus-\ncation uses SCMs to identify and block sensitive pathways\n(e.g., names, locations) during training or inference (Chu\net al., 2024). Unlike traditional privacy-preserving mech-\nanisms that indiscriminately apply noise or randomization,\nit ensures that only privacy-sensitive dependencies are re-\nmoved, preserving essential predictive relationships.\nBeyond conventional privacy concerns, attribution and\nmemorization pose significant challenges in foundation\nmodels. Attribution is crucial in determining whether spe-\ncific data—such as an artist’s work—has contributed to the\ntraining of a model, enabling rightful recognition and com-\npensation. Memorization, on the other hand, prevents ef-\nfective data removal, meaning that once a copyrighted work\nis embedded into a model, it becomes difficult to erase\nupon request. Causal auditing (Sharkey et al., 2024) poten-\ntially offers a principled way to address these challenges\nby providing a structured framework to verify whether a\ngiven dataset—such as an artist’s work—has influenced the\nmodel’s outputs. Unlike statistical correlation-based meth-\nods, which may falsely associate stylistic elements with\nbroader art movements, causal auditing can disentangle di-\nrect influences from broader historical trends, ensuring that\nattribution is based on actual data contributions rather than\nincidental similarities.\nExplainability vs. Capability. Although foundation mod-\nels demonstrate remarkable capabilities in various tasks,\ntheir outputs often lack interpretability, making it difficult\nto understand or explain their reasoning. Causal models\ncan help quantify how much each input feature contributes\nto a specific output, providing a clear and interpretable\nexplanation. By modeling causal chains, we can explain\nhow different stages of the LLM (e.g., embedding, atten-\ntion layers, output logits) interact to produce a final de-\ncision (Bagheri et al., 2024). This creates a step-by-step\nexplanation of the model’s reasoning process. Another do-\nmain that is related to causality is mechanistic interpretabil-\nity. Mechanistic interpretability seeks to decode the inner\nworkings of LLMs by analyzing their architecture, weights,\nand activation patterns (Conmy et al., 2023). Causality en-\nhances this understanding by identifying cause-effect rela-\ntionships within these mechanisms. Causality can identify\nspecific pathways in neural circuits that contribute to cer-\ntain outputs (Palit et al., 2023; Parekh et al., 2024). For\nexample, specific neurons or attention heads affect token\npredictions, revealing the factors driving outputs.\n3.2. Integrating Causality in Foundation Models\nThis section delves into practical applications of causality\nin FMs across three key stages: pre-training, post-training,\nand auditing. We conclude with a discussion of the practi-\ncal advantages and limitations of the proposed approaches.\nPre-training:\nCausal data augmentation.\nSynthetic\ndatasets with explicit causal structures, such as counter-\nfactual examples or causal-transformable text data, can be\nused to augment training data. Counterfactual data aug-\nmentation introduces scenarios where causal relationships\ndiffer from spurious correlations, helping models learn true\ncausal dependencies instead of misleading patterns (Web-\nster et al., 2020; Chen et al., 2022).\nPre-training: Causal Representation Learning. By dis-\nentangling causal factors from non-causal ones, models\ncan learn representations that separate meaningful causal\nfeatures from irrelevant associations. Techniques such as\n6\n\n\nCausality Is Key to Understand and Balance Multiple Goals in Trustworthy ML and Foundation Models\ncausal embedding methods (Rajendran et al., 2024; Jiang\net al., 2024), which can use training data annotated with\ncausal labels, can guide models in identifying and prior-\nitizing true causal relationships. This has been shown to\nreduce reliance on spurious correlations, such as gender-\nbiased occupational associations (Zhou et al., 2023b).\nPre-training: Entity interventions. SCMs can be used to\nintervene on specific entities (e.g., replacing “Joe Biden”\nwith “ENTITY-A”) during pre-training (Wang et al., 2023),\nthus reducing entity-based spurious associations while pre-\nserving causal relationships in the data.\nPre-training: Loss function. Modifying the pre-training\nloss function to penalize reliance on confounders can help\nalign models with causal principles.\nFor instance, fine-\ntuning models on embeddings pre-trained with debiased\ntoken representations has shown promise for causal learn-\ning (Kaneko & Bollegala, 2021; Guo et al., 2022; He et al.,\n2022; Wang et al., 2023).\nPost-training:\nFine-tuning.\nFine-tuning on datasets\nspecifically designed to highlight causal reasoning (e.g.,\ndatasets emphasizing cause-effect linguistic patterns) en-\nsures that models learn causal-invariant patterns. Further,\ncounterfactual data samples can also improve the fine-\ntuning.\nSynthetic counterfactual examples improve the\nmodel’s robustness to spurious correlations, similar to pre-\ntraining, but with better sample size efficiency.\nFrame-\nworks like DISCO (Chen et al., 2022) generate diverse\ncounterfactuals during fine-tuning to enhance OOD gen-\neralization for downstream tasks. Causally Fair Language\nModels (CFL) (Madhavan et al., 2023b) use SCM-based\nregularization to detoxify outputs or enforce demograph-\nically neutral generation during post-training.\nWang &\nCulotta (2020) use causal reasoning to separate genuine\nfrom spurious correlations by computing controlled direct\neffects, ensuring robust performance.\nPost-training: Alignment. RLHF can be adapted to in-\nclude causal interventions, allowing feedback to act as in-\nstrumental variables that correct biased model behavior.\nCausality-Aware Alignment (CAA) (Xia et al., 2024) incor-\nporates causal interventions to reduce demographic stereo-\ntypes during fine-tuning with alignment objectives.\nEx-\ntending RLHF with causal alignment to support dynamic,\ncontext-sensitive interventions could help address biases\nthat evolve. Integrating causal reasoning into the reward\nmodel’s decision-making process, by critiquing the output\nof LLM using a reward model or a mixture of reward mod-\nels that control for specific confounders or spurious corre-\nlations can potentially improve the downstream reasoning\nabilities potentially mitigating hallucinations.\nAuditing and Evaluation.\nCausality provides a struc-\ntured framework for auditing privacy risks by identifying\nwhether sensitive user data contributes to model outputs.\nThis is particularly important for privacy regulations like\nGDPR’s “right to be forgotten” (EC, 2016), where users\ncan request their data to be removed from an AI system.\nHowever, verifying whether an LLM has truly forgotten a\nuser’s data is a complex challenge, as models can memo-\nrize training information in ways that are difficult to detect\nthrough standard evaluation metrics. One key approach in\nprivacy auditing is using causal attribution, which assesses\nwhether a specific data point influenced a given output. By\nusing do-calculus, privacy auditors can evaluate how an\noutput changes when a particular data source is removed.\nThis enables a principled test of whether an LLM has truly\nforgotten a user’s data.\nPractical Considerations.\nIn supervised fine-tuning and\nalignment, the downstream task and its causal relationships\nare often known, allowing for more targeted interventions\non confounding variables and even the collection of task-\nspecific data to refine causal structures. Additionally, since\npost-training typically requires less data than pre-training,\nintegrating causal insights becomes more feasible.\nPre-\ntraining offers the advantage of learning broad represen-\ntations from diverse data, but it is difficult to enforce causal\nconstraints due to the lack of explicit task definitions and\ncausal structures. Auditing is particularly useful for detect-\ning biases, ensuring fairness, and validating robustness in\nreal-world scenarios. Unlike pre-training and fine-tuning,\nauditing does not require modifying the training pipeline,\nmaking it a cost-effective way to introduce causal reason-\ning retrospectively.\n4. Challenges and Opportunities\nDespite its advantages, there are many challenges when ap-\nplying causality to trustworthy ML, including reliance on\nstrong causal assumptions and limited availability of a pri-\nori causal knowledge, particularly in the form of DAGs.\nFoundation models bring further complications due to their\nscale, high-dimensional data, and the difficulty of validat-\ning causal structures. We outline key obstacles in integrat-\ning causality into ML and foundation models and suggest\nstrategies to overcome them.\nAvailability of Causal Knowledge.\nA major challenge\nin causal ML is the limited availability of causal knowl-\nedge, particularly in the form of DAGs. Expert-constructed\nDAGs may suffer from subjectivity and scalability issues,\nwhile ML-based causal discovery is constrained by iden-\ntifiability assumptions and noise sensitivity. However, re-\ncent hybrid approaches combining classical causal discov-\nery with LLM-based reasoning offer promising solutions.\nCausal Transportability. Scientific knowledge often lacks\ndirect applicability across different populations, making\n7\n\n\nCausality Is Key to Understand and Balance Multiple Goals in Trustworthy ML and Foundation Models\ncausal transportability essential. Pearl and Bareinboim’s\nDAG-based framework adjusts causal knowledge for new\nsettings using targeted data collection (Pearl & Barein-\nboim, 2011b; Bareinboim & Pearl, 2014; Pearl & Barein-\nboim, 2011a). Building on this, Binkyte et al. (2024) pro-\npose an expectation-maximization (EM) approach to adapt\ncausal knowledge for target demographic applications.\nPotentially Unresolvable Tensions. Not all tensions in\ntrustworthy AI can always be fully resolved.\nFor in-\nstance, stronger privacy protections often reduce model\nutility (Dwork et al., 2014; Bassily et al., 2014).\nSim-\nilarly, explainability may sometimes come at the cost of\naccuracy, and robustness can conflict with fairness in cer-\ntain scenarios. However, causality provides a structured\napproach to evaluating these trade-offs, making it possible\nto quantify their impact and identify cases where full rec-\nonciliation is not feasible. Importantly, it is crucial to be\ntransparent about these limitations, as this fosters societal\ntrust, promotes accountability, and enables more informed\ndecision-making in AI development.\nChallenges in Causal Foundation Models. One founda-\ntion model-specific challenge is concept superposition, par-\nticularly in LLMs, where multiple meanings are entangled\nwithin a single representation, complicating causal reason-\ning (Elhage et al., 2022). Vision models exhibit this issue\nto a lesser extent due to their structured data formats. Being\naware of superposition is imporant for effectively integrat-\ning causality.\nAnother challenge is the lack of high-quality causal data.\nTraining foundation models with causal reasoning requires\ndatasets annotated with explicit causal structures or inter-\nventional data, which are scarce and expensive to produce.\nScalable methods for generating synthetic causal datasets\nshow a promising direction.\nAlternatively, focusing on\npost-training methods allows causal interventions in a more\ndata-efficient way.\nAdditionally, the computational complexity of integrating\ncausal reasoning into foundation models poses a significant\nchallenge. For fine-tuning, low-rank adaptation methods\nsuch as LoRA can be employed to reduce the number of\nlearnable parameters, making causal integration more effi-\ncient without compromising performance (Hu et al., 2021).\n5. Alternative View\nSome may argue that different domains prioritize differ-\nent requirements for trustworthy ML, and there is no need\nto reconcile them. However, this perspective is unlikely\nto hold universally, as most real-world applications inter-\nsect with multiple ethical and trustworthy ML principles,\nsuch as fairness, privacy, and robustness, which must be\nbalanced to ensure reliable outcomes.\nAnother perspective suggests that causal properties can\nemerge spontaneously by training larger models on vast\namounts of data. While this is possible, it is not guaranteed,\nand more importantly, it provides no control over whether\nor how these properties arise. In contrast, much of the sci-\nentific causal knowledge already exists, and finding ways\nto integrate this knowledge with machine learning models\noffers a more resource-efficient, reliable, and explainable\npathway to achieving trustworthy ML.\n6. Conclusion and Call for Action\nCausal models offer a principled approach to trustworthy\nAI by prioritizing relationships that are causally justified\nand invariant across contexts. This approach reduces ten-\nsions between competing objectives and can enhance mul-\ntiple dimensions—privacy, accuracy, fairness, explainabil-\nity, and robustness—simultaneously, creating models that\nare not only ethically sound but also practically effective.\nTo further advance trustworthy ML foundation models, we\nemphasize the need for the following actions:\nIncorporate Trade-off Awareness in Model Design: Ensure\nthat foundation models are developed with explicit consid-\neration of trade-offs between key trustworthy AI dimen-\nsions—fairness, privacy, robustness, explainability, and ac-\ncuracy.\nLeverage Causality to Resolve or Soften Trade-offs: Where\npossible, integrate causal reasoning to disentangle compet-\ning objectives and mitigate conflicts.\nDevelop Scalable Methods for Causal Data Integration:\nEncourage the development of algorithms and pipelines to\nintegrate causal knowledge into foundation models at scale.\nCreate and Share High-Quality Causal Datasets: Foster\ninitiatives to curate, annotate, and share datasets with ex-\nplicit causal annotations or interventional information.\nAdvance Causal Discovery Techniques: Invest in research\nto improve causal discovery algorithms.\nHybrid ap-\nproaches combining classical methods with LLM-based\ncontextual reasoning show a promising direction.\nBenchmark and Evaluate Causal Models: Establish evalu-\nation frameworks that assess the ability of causal models to\nbalance trade-offs effectively and provide transparent justi-\nfications for their decisions in high-stakes domains.\nAll these advancements are crucial for expanding the appli-\ncation of causality in ML and foundation models, paving\nthe way for more balanced and trustworthy AI solutions.\n8\n\n\nCausality Is Key to Understand and Balance Multiple Goals in Trustworthy ML and Foundation Models\n7. Impact Statement\nThis paper advocates for integrating causality into founda-\ntion models to enhance fairness, privacy, robustness, and\nexplainability. By reducing reliance on spurious correla-\ntions and improving decision-making, causal methods can\nmake AI systems more reliable, transparent, and aligned\nwith human values—especially in high-stakes domains like\nhealthcare, law, and finance. Adopting causality-driven AI\nhas the potential to improve trust, regulatory compliance,\nand ethical governance, ultimately contributing to a more\nfair, transparent, and socially beneficial technological land-\nscape.\nAcknowledgements\nThis work is funded in part by PriSyn:\nRepresenta-\ntive, synthetic health data with strong privacy guarantees\n(BMBF), grant No.\n16KISAO29K. It is also supported\nby Integrated Early Warning System for Local Recog-\nnition, Prevention, and Control for Epidemic Outbreaks\n(LOKI / Helmholtz) grant.\nThe work is also partially\nfunded by Medizininformatik-Plattform ”Privatsph¨aren-\nschutzende Analytik in der Medizin” (PrivateAIM), grant\nNo. 01ZZ2316G, and ELSA – European Lighthouse on\nSecure and Safe AI funded by the European Union under\ngrant agreement No. 101070617. Views and opinions ex-\npressed are, however, those of the authors only and do not\nnecessarily reflect those of the European Union or Euro-\npean Commission. Neither the European Union nor the\nEuropean Commission can be held responsible for them.\nReferences\nAbdulaal, A., Montana-Brown, N., He, T., Ijishakin,\nA., Drobnjak, I., Castro, D. C., Alexander, D. C.,\net al. Causal modelling agents: Causal graph discov-\nery through synergising metadata-and data-driven rea-\nsoning.\nIn The Twelfth International Conference on\nLearning Representations, 2023.\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,\nAleman, F. L., Almeida, D., Altenschmidt, J., Altman,\nS., Anadkat, S., et al. Gpt-4 technical report. arXiv,\n2023.\nAfonja, T., Sheth, I., Binkyte, R., Hanif, W., Ulas, T.,\nBecker, M., and Fritz, M.\nLlm4grn:\nDiscovering\ncausal gene regulatory networks with llms–evaluation\nthrough synthetic data generation.\narXiv preprint\narXiv:2410.15828, 2024.\nAI4Science, M. R. and Quantum, M. A. The impact of\nlarge language models on scientific discovery: a prelim-\ninary study using gpt-4. arXiv, 2023.\nAoki, R. and Ester, M. Causal inference from small high-\ndimensional datasets. arXiv preprint arXiv:2205.09281,\n2022.\nAono, Y., Hayashi, T., Wang, L., and Moriai, S. Privacy-\npreserving deep learning: Revisited and enhanced. In\nProceedings of the International Conference on Appli-\ncations and Techniques in Information Security (ATIS),\npp. 100–110. Springer, 2017.\nAvin, C., Shpitser, I., and Pearl, J.\nIdentifiability of\npath-specific effects.\nIn Kaelbling, L. P. and Saf-\nfiotti, A. (eds.), IJCAI-05, Proceedings of the Nine-\nteenth International Joint Conference on Artificial In-\ntelligence, Edinburgh, Scotland, UK, July 30 - Au-\ngust 5, 2005, pp. 357–363. Professional Book Center,\n2005. URL http://ijcai.org/Proceedings/\n05/Papers/0886.pdf.\nBagdasaryan, E., Poursaeed, O., and Shmatikov, V. Differ-\nential privacy has disparate impact on model accuracy.\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2019.\nBagheri, A., Alinejad, M., Bello, K., and Akhondi-Asl, A.\nC2p: Featuring large language models with causal rea-\nsoning. arXiv preprint arXiv:2407.18069, 2024.\nBareinboim, E. and Pearl, J. Transportability from multi-\nple environments with limited experiments: Complete-\nness results. Advances in neural information processing\nsystems, 27, 2014.\nBashiri, M. and Kowsari, K.\nTransformative influence\nof llm and ai tools in student social media engage-\nment:\nAnalyzing personalization, communication ef-\nficiency, and collaborative learning.\narXiv preprint\narXiv:2407.15012, 2024.\nBassily, R., Smith, A., and Thakurta, A. Private empirical\nrisk minimization: Efficient algorithms and tight error\nbounds. Annual Conference on Neural Information Pro-\ncessing Systems (NeurIPS), 2014.\nBerrevoets, J., Kacprzyk, K., Qian, Z., and van der Schaar,\nM.\nNavigating causal deep learning.\narXiv preprint\narXiv:2212.00911, 2022.\nBerrevoets, J., Kacprzyk, K., Qian, Z., and van der\nSchaar, M.\nCausal deep learning.\narXiv preprint\narXiv:2303.02186, 2023.\nBerrevoets, J., Kacprzyk, K., Qian, Z., van der Schaar,\nM., et al. Causal deep learning: encouraging impact on\nreal-world problems through causality. Foundations and\nTrends® in Signal Processing, 18(3):200–309, 2024.\n9\n\n\nCausality Is Key to Understand and Balance Multiple Goals in Trustworthy ML and Foundation Models\nBerzuini, C., Dawid, P., and Bernardinell, L. Causality:\nStatistical perspectives and applications. John Wiley &\nSons, 2012.\nBinkyt˙e, R., Grozdanovski, L., and Zhioua, S. On the need\nand applicability of causality for fair machine learning.\narXiv preprint arXiv:2207.04053, 2022.\nBinkyte, R., Gorla, D., and Palamidessi, C. Babe: Enhanc-\ning fairness via estimation of explaining variables. In\nThe 2024 ACM Conference on Fairness, Accountability,\nand Transparency, pp. 1917–1925, 2024.\nBrooks, T., Peebles, B., Holmes, C., DePue, W., Guo, Y.,\nJing, L., Schnurr, D., Taylor, J., Luhman, T., Luhman,\nE., et al. Video generation models as world simulators.\n[LINK], 2024.\nBuolamwini, J. and Gebru, T. Gender shades: Intersec-\ntional accuracy disparities in commercial gender classi-\nfication. In Conference on fairness, accountability and\ntransparency, pp. 77–91. PMLR, 2018.\nBurgess, C. P., Higgins, I., Pal, A., Matthey, L., Watters, N.,\nDesjardins, G., and Lerchner, A. Understanding disen-\ntangling in vae. arXiv preprint arXiv:1804.03599, 2018.\nByun, Y., Sam, D., Oberst, M., Lipton, Z., and Wilder,\nB. Auditing fairness under unobserved confounding. In\nInternational Conference on Artificial Intelligence and\nStatistics, pp. 4339–4347. PMLR, 2024.\nCarvalho, T., Moniz, N., Faria, P., and Antunes, L. Towards\na data privacy-predictive performance trade-off. Expert\nSystems with Applications, pp. 119785, 2023.\nChen, H., Zhang, L., Liu, Y., and Yu, Y. Rethinking the\ndevelopment of large language models from the causal\nperspective: A legal text prediction case study, 2024a.\nChen, Y., Raghuram, V. C., Mattern, J., Mihalcea, R., and\nJin, Z. Causally testing gender bias in llms: A case study\non occupational bias. In Findings of the Association for\nComputational Linguistics: NAACL 2025. Association\nfor Computational Linguistics, 2024b. URL https:\n//doi.org/10.48550/arXiv.2212.10678.\nChen,\nZ.,\nGao,\nQ.,\nBosselut,\nA.,\nSabharwal,\nA.,\nand Richardson, K.\nDisco:\nDistilling counterfac-\ntuals with large language models.\narXiv preprint\narXiv:2212.10534, 2022.\nChernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E.,\nHansen, C., Newey, W., and Robins, J. Double/debiased\nmachine learning for treatment and structural parame-\nters, 2018.\nChiappa, S. Path-specific counterfactual fairness. In Pro-\nceedings of the AAAI Conference on Artificial Intelli-\ngence, volume 33, pp. 7801–7808, 2019.\nChu, Z., Wang, Y., Li, L., Wang, Z., Qin, Z., and Ren, K. A\ncausal explainable guardrails for large language models.\nIn Proceedings of the 2024 on ACM SIGSAC Conference\non Computer and Communications Security, pp. 1136–\n1150, 2024.\nCOMPAS.\nCompas,\n2020.\nhttps://www.equivant.com/\nnorthpointe-risk-need-assessments/.\nConmy, A., Mavor-Parker, A., Lynch, A., Heimersheim,\nS., and Garriga-Alonso, A.\nTowards automated cir-\ncuit discovery for mechanistic interpretability. Advances\nin Neural Information Processing Systems, 36:16318–\n16352, 2023.\nCooper, A. F., Abrams, E., and Na, N. Emergent unfairness\nin algorithmic fairness-accuracy trade-off research. In\nProceedings of the 2021 AAAI/ACM Conference on AI,\nEthics, and Society, pp. 46–54, 2021.\nCornacchia, G., Anelli, V. W., Biancofiore, G. M., Nar-\nducci, F., Pomo, C., Ragone, A., and Di Sciascio, E.\nAuditing fairness under unawareness through counter-\nfactual reasoning. Information Processing & Manage-\nment, 60(2):103224, 2023.\nCotta, L. and Maddison, C. J. Test-time fairness and ro-\nbustness in large language models, 2024.\nDawid, A. P. Seeing and doing: The pearlian synthesis.\nHeuristics, probability and causality: A tribute to Judea\nPearl, 309, 2010.\nDawid, A. P. Statistical causality from a decision-theoretic\nperspective. Annual Review of Statistics and Its Applica-\ntion, 2:273–303, 2015.\nDoshi-Velez, F. and Kim, B.\nTowards a rigorous sci-\nence of interpretable machine learning. arXiv preprint\narXiv:1702.08608, 2017.\nDrenkow, N., Ribaudo, C., and Unberath, M. Causality-\ndriven audits of model robustness.\narXiv preprint\narXiv:2410.23494, 2024.\nDwork, C. Differential privacy. In Proceedings of the 33rd\nInternational Colloquium on Automata, Languages and\nProgramming (ICALP), pp. 1–12. Springer, 2006.\nDwork, C. and Lei, J. Differential privacy and robust statis-\ntics. In Proceedings of the forty-first annual ACM sym-\nposium on Theory of computing, pp. 371–380, 2009.\n10\n\n\nCausality Is Key to Understand and Balance Multiple Goals in Trustworthy ML and Foundation Models\nDwork, C., Hardt, M., Pitassi, T., Reingold, O., and Zemel,\nR. Fairness through awareness, 2011.\nDwork, C., Roth, A., et al. The algorithmic foundations of\ndifferential privacy. Foundations and Trends® in Theo-\nretical Computer Science, 9(3–4):211–407, 2014.\nEC.\nGeneral data protection regulation (GDPR), 2016.\nAvailable online:\nhttps://eur-lex.europa.\neu/legal-content/EN/TXT/?uri=celex%\n3A32016R0679 (accessed on 27 October 2023).\nEhyaei, A.-R., Farnadi, G., and Samadi, S. Causal fair met-\nric: Bridging causality, individual fairness, and adversar-\nial robustness. arXiv preprint arXiv:2310.19391, 2023.\nElhage, N., Olsson, C., Henighan, T., Hernandez, D.,\nJoseph, N., Mann, B., Askell, A., DasSarma, N.,\nTran-Johnson, E., Amodei, D., Brown, T., Clark,\nJ., McCandlish, S., and Olah, C.\nToy models of\nsuperposition.\nTransformer Circuits Thread, 2022.\nURL https://transformer-circuits.pub/\n2022/toy_model/.\nEuropean\nCommission.\nEuropean\nUnion\nAI\nact,\n2021.\nURL https://eur-lex.europa.eu/\nlegal-content/EN/TXT/?uri=CELEX:\n52021PC0206.\nProposal for a regulation laying\ndown harmonized rules on artificial intelligence.\nFeldman, M., Friedler, S. A., Moeller, J., Scheidegger,\nC., and Venkatasubramanian, S. Certifying and remov-\ning disparate impact. In proceedings of the 21th ACM\nSIGKDD international conference on knowledge discov-\nery and data mining, pp. 259–268, 2015.\nFredrikson, M., Jha, S., and Ristenpart, T. Model inver-\nsion attacks that exploit confidence information and ba-\nsic countermeasures. In Proceedings of the ACM Confer-\nence on Computer and Communications Security (CCS),\npp. 1322–1333, 2015.\nFriedler, S. A., Scheidegger, C., and Venkatasubramanian,\nS. The (im) possibility of fairness: Different value sys-\ntems require different mechanisms for fair decision mak-\ning. Communications of the ACM, 64(4):136–143, 2021.\nGanguly, N., Fazlija, D., Badar, M., Fisichella, M., Sik-\ndar, S., Schrader, J., Wallat, J., Rudra, K., Koubarakis,\nM., Patro, G. K., Amri, W. Z. E., and Nejdl, W. A re-\nview of the role of causality in developing trustworthy ai\nsystems, 2023.\nURL https://arxiv.org/abs/\n2302.06975.\nGeiping, J., Bauermeister, H., Dr¨oge, H. P., and Moeller,\nM. Inverting gradients–how easy is it to break privacy in\nfederated learning? In Advances in Neural Information\nProcessing Systems (NeurIPS), volume 33, pp. 16937–\n16947, 2020.\nGoodfellow, I. J., Shlens, J., and Szegedy, C. Explaining\nand harnessing adversarial examples. In International\nConference on Learning Representations (ICLR), 2015.\nGroup of Twenty (G20). G20 AI principles, 2019. URL\nhttps://www.g20.org/. Adopted from the OECD\nAI Principles to guide AI policymaking globally.\nGuo, Y., Yang, Y., and Abbasi, A.\nAuto-debias: Debi-\nasing masked language models with automated biased\nprompts. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Volume\n1: Long Papers), pp. 1012–1023, 2022.\nHauser, A. and B¨uhlmann, P. Characterization and greedy\nlearning of interventional Markov equivalence classes\nof directed acyclic graphs. Journal of Machine Learn-\ning Research, 13:2409–2464, 2012. URL https://\njmlr.org/papers/v13/hauser12a.html.\nHe, J., Xia, M., Fellbaum, C., and Chen, D. Mabel: Atten-\nuating gender bias using textual entailment data. arXiv\npreprint arXiv:2210.14975, 2022.\nHendrycks, D. and Dietterich, T. Benchmarking neural net-\nwork robustness to common corruptions and perturba-\ntions. In International Conference on Learning Repre-\nsentations (ICLR), 2019.\nHern´an, M. A. and Robins, J. M. Causal Inference: What\nIf. Chapman & Hall/CRC, 2020.\nHopkins, S. B., Kamath, G., Majid, M., and Narayanan,\nS. Robustness implies privacy in statistical estimation.\narXiv preprint arXiv:2212.05015, 2022.\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,\nS., and Chen, W. Lora: Low-rank adaptation of large lan-\nguage models. arXiv preprint arXiv:2106.09685, 2021.\nInfocomm Media Development Authority.\nSingapore\nmodel ai governance framework, 2020. URL https:\n//www.imda.gov.sg/. Guidelines to promote re-\nsponsible use of AI in Singapore.\nJanzing, D. and Sch¨olkopf, B. Causal inference using the\nalgorithmic markov condition. IEEE Transactions on In-\nformation Theory, 56(10):5168–5194, 2010.\nJia, J., Yuan, Z., Pan, J., McNamara, P. E., and Chen,\nD.\nDecision-making behavior evaluation framework\nfor llms under uncertain context.\narXiv preprint\narXiv:2406.05972, 2024.\nJiang, Y., Rajendran, G., Ravikumar, P., Aragam, B.,\nand Veitch, V.\nOn the origins of linear represen-\ntations in large language models.\narXiv preprint\narXiv:2403.03867, 2024.\n11\n\n\nCausality Is Key to Understand and Balance Multiple Goals in Trustworthy ML and Foundation Models\nKaddour, J., Lynch, A., Liu, Q., Kusner, M. J., and Silva, R.\nCausal machine learning: A survey and open problems.\narXiv preprint arXiv:2206.15475, 2022.\nKaneko,\nM.\nand\nBollegala,\nD.\nDebiasing\npre-\ntrained contextualised embeddings.\narXiv preprint\narXiv:2101.09523, 2021.\nKarimi, A.-H., Barthe, G., Sch¨olkopf, B., and Valera, I.\nModel-agnostic counterfactual explanations for conse-\nquential decisions. Proceedings of the 37th International\nConference on Machine Learning (ICML), 2020.\nKarimi, A.-H., Sch¨olkopf, B., and Valera, I. Algorithmic\nrecourse: from counterfactual explanations to interven-\ntions. In 4th Conference on Fairness, Accountability, and\nTransparency (ACM FAccT), pp. 353–362. ACM, 2021.\ndoi: 10.1145/3442188.3445899.\nKasetty, T., Mahajan, D., Dziugaite, G. K., Drouin, A.,\nand Sridhar, D.\nEvaluating interventional reasoning\ncapabilities of large language models.\narXiv preprint\narXiv:2404.05545, 2024.\nKatirai, A. and Nagato, Y.\nAddressing trade-offs in co-\ndesigning principles for ethical ai: perspectives from an\nindustry-academia collaboration. AI and Ethics, pp. 1–9,\n2024.\nKemmerzell, N. and Schreiner, A. Quantifying the trade-\noffs between dimensions of trustworthy ai-an empirical\nstudy on fairness, explainability, privacy, and robust-\nness. In German Conference on Artificial Intelligence\n(K¨unstliche Intelligenz), pp. 128–146. Springer, 2024.\nKhatibi, E., Abbasian, M., Yang, Z., Azimi, I., and Rah-\nmani, A. M. Alcm: Autonomous llm-augmented causal\ndiscovery framework. arXiv preprint arXiv:2405.01744,\n2024.\nKıcıman, E., Ness, R., Sharma, A., and Tan, C. Causal\nreasoning and large language models: Opening a new\nfrontier for causality. arXiv preprint arXiv:2305.00050,\n2023.\nKilbertus, N., Carulla, M. R., Parascandolo, G., Hardt, M.,\nJanzing, D., and Sch¨olkopf, B. Avoiding discrimination\nthrough causal reasoning. In Advances in Neural Infor-\nmation Processing Systems, pp. 656–666, 2017.\nKim, H., Shin, S., Jang, J., Song, K., Joo, W., Kang, W.,\nand Moon, I.-C. Counterfactual fairness with disentan-\ngled causal effect variational autoencoder. In Proceed-\nings of the AAAI Conference on Artificial Intelligence,\nvolume 35, pp. 8128–8136, 2021.\nKim, J. S., Chen, J., and Talwalkar, A. Fact: A diagnos-\ntic for group fairness trade-offs. In International Con-\nference on Machine Learning, pp. 5264–5274. PMLR,\n2020.\nKreif, N. and DiazOrdaz, K.\nMachine learning in pol-\nicy evaluation: new tools for causal inference.\narXiv\npreprint arXiv:1903.00402, 2019.\nKusner, M. J., Loftus, J., Russell, C., and Silva, R. Coun-\nterfactual fairness. Advances in neural information pro-\ncessing systems, 30, 2017.\nLe, T. D., Hoang, T., Li, J., Liu, L., Liu, H., and Hu, S. A\nfast pc algorithm for high dimensional causal discovery\nwith multi-core pcs. IEEE/ACM transactions on compu-\ntational biology and bioinformatics, 16(5):1483–1495,\n2016.\nLee, B. K., Lessler, J., and Stuart, E. A. Improving propen-\nsity score weighting using machine learning. Statistics\nin medicine, 29(3):337–346, 2010.\nLee, J., Stevens, N., Han, S. C., and Song, M.\nA sur-\nvey of large language models in finance (finllms). arXiv\npreprint arXiv:2402.02315, 2024.\nLeeb,\nF.,\nBauer,\nS.,\nBesserve,\nM.,\nand Sch¨olkopf,\nB.\nExploring the latent space of autoencoders\nwith interventional assays.\nIn Advances in Neu-\nral Information Processing Systems 35, volume 35,\npp. 21562–21574. Curran Associates,\nInc.,\n2022.\nURL\nhttps://proceedings.neurips.\ncc/paper_files/paper/2022/hash/\n87213955efbe48b46586e37bf2f1fe5b-Abstract-Conferen\nhtml.\nLewis, D., Hogan, L., Filip, D., and Wall, P. Global chal-\nlenges in the standardization of ethics for trustworthy ai.\nJournal of ICT Standardization, 8(2):123–150, 2020.\nLipton, Z. C. The mythos of model interpretability. ACM\nQueue, 16(3):31–57, 2018.\nLiu, H., Chaudhary, M., and Wang, H. Towards trustwor-\nthy and aligned machine learning: A data-centric sur-\nvey with causality perspectives, 2023. URL https:\n//arxiv.org/abs/2307.16851.\nLoftus, J. R., Russell, C., Kusner, M. J., and Silva, R.\nCausal reasoning for algorithmic fairness. arXiv preprint\narXiv:1805.05859, 2018.\nLondon, A. J. Artificial intelligence and black-box medi-\ncal decisions: accuracy versus explainability. Hastings\nCenter Report, 49(1):15–21, 2019.\nMackie, J. L. Causes and conditions. American philosoph-\nical quarterly, 2(4):245–264, 1965.\n12\n\n\nCausality Is Key to Understand and Balance Multiple Goals in Trustworthy ML and Foundation Models\nMadhavan, R., Garg, R., Wadhawan, K., and Mehta, S.\nCfl: Causally fair language models through token-level\nattribute controlled generation, 2023a.\nMadhavan, R., Garg, R., Wadhawan, K., and Mehta,\nS. Cfl: Causally fair language models through token-\nlevel attribute controlled generation.\narXiv preprint\narXiv:2306.00374, 2023b.\nMakhlouf, K., Zhioua, S., and Palamidessi, C.\nWhen\ncausality meets fairness: A survey. Journal of Logical\nand Algebraic Methods in Programming, pp. 101000,\n2024.\nMbiazi, D., Bhange, M., Babaei, M., Sheth, I., and Ken-\nfack, P. J. Survey on ai ethics: A socio-technical per-\nspective. arXiv preprint arXiv:2311.17228, 2023.\nMitrovic, J., McWilliams, B., Walker, J., Buesing, L., and\nBlundell, C. Representation learning via invariant causal\nmechanisms. arXiv preprint arXiv:2010.07922, 2020.\nMooij, J. M., Peters, J., Janzing, D., Zscheischler, J., and\nSch¨olkopf, B. Distinguishing cause from effect using ob-\nservational data: methods and benchmarks. The Journal\nof Machine Learning Research, 17(1):1103–1204, 2016.\nNabi, R. and Shpitser, I. Fair inference on outcomes. In\nProceedings of the... AAAI Conference on Artificial In-\ntelligence. AAAI Conference on Artificial Intelligence,\nvolume 2018, pp. 1931. NIH Public Access, 2018.\nNarayanan,\nA.\nand\nShmatikov,\nV.\nRobust\nde-\nanonymization of large sparse datasets. In Proceedings\nof the IEEE Symposium on Security and Privacy (SP),\npp. 111–125. IEEE, 2008.\nOECD.\nOECD\nprinciples\non\nartificial\nintelli-\ngence, 2019.\nURL https://www.oecd.org/\ngoing-digital/ai/principles/. Recommen-\ndations of the Council on Artificial Intelligence.\nOhm, P. Broken promises of privacy: Responding to the\nsurprising failure of anonymization. UCLA Law Review,\n57:1701, 2010.\nOvadia, Y., Fertig, E., Ren, J., Nado, Z., Sculley, D.,\nNowozin, S., Dillon, J., and Lakshminarayanan, B. Can\nyou trust your model’s uncertainty? evaluating predic-\ntive uncertainty under dataset shift. In Advances in Neu-\nral Information Processing Systems (NeurIPS), 2019.\nPalit, V., Pandey, R., Arora, A., and Liang, P. P. Towards\nvision-language mechanistic interpretability: A causal\ntracing tool for blip. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pp. 2856–\n2861, 2023.\nParekh, J., Khayatan, P., Shukor, M., Newson, A.,\nand Cord, M.\nA concept-based explainability frame-\nwork for large multimodal models.\narXiv preprint\narXiv:2406.08074, 2024.\nPearl, J. Probabilistic reasoning in intelligent systems: net-\nworks of plausible inference. Morgan kaufmann, 1988.\nPearl, J.\nDirect and indirect effects.\nIn Proceedings of\nthe Seventeenth conference on Uncertainty in artificial\nintelligence, pp. 411–420, 2001.\nPearl, J. Causality. Cambridge university press, 2009a.\nPearl, J.\nCausality.\nCambridge University Press,\nCambridge, 2009b.\nISBN 978-0-521-89560-6.\ndoi:\n10.1017/CBO9780511803161. URL https://www.\ncambridge.org/core/books/causality/\nB0046844FAE10CBF274D4ACBDAEB5F5B.\nPearl, J. and Bareinboim, E. Transportability of causal and\nstatistical relations: A formal approach. In Twenty-fifth\nAAAI conference on artificial intelligence, 2011a.\nPearl, J. and Bareinboim, E. Transportability of causal and\nstatistical relations: A formal approach. In Proceedings\nof the 2011 IEEE 11th International Conference on Data\nMining Workshops, ICDMW ’11, pp. 540–547, USA,\n2011b. IEEE Computer Society. ISBN 9780769544090.\ndoi:\n10.1109/ICDMW.2011.169.\nURL https://\ndoi.org/10.1109/ICDMW.2011.169.\nPeters, J., Mooij, J., Janzing, D., and Sch¨olkopf, B. Iden-\ntifiability of causal graphs using functional models. In\nCozman, F. G. and Pfeffer, A. (eds.), 27th Conference\non Uncertainty in Artificial Intelligence, pp. 589–598,\nCorvallis, OR, 2011. AUAI Press.\nPinz´on, C., Palamidessi, C., Piantanida, P., and Valencia, F.\nOn the impossibility of non-trivial accuracy in presence\nof fairness constraints. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, volume 36, pp. 7993–\n8000, 2022.\nPlecko, D. and Bareinboim, E. Fairness-accuracy trade-\noffs: A causal perspective, 2024.\nURL https://\narxiv.org/abs/2405.15443.\nPujol, D., McKenna, R., Kuppam, S., Hay, M., Machanava-\njjhala, A., and Miklau, G. Fair decision making using\nprivacy-protected data. In Proceedings of the 2020 Con-\nference on Fairness, Accountability, and Transparency,\npp. 189–199, 2020.\nRadford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey,\nC., and Sutskever, I.\nRobust speech recognition via\nlarge-scale weak supervision. In ICML, 2023.\n13\n\n\nCausality Is Key to Understand and Balance Multiple Goals in Trustworthy ML and Foundation Models\nRahmattalabi, A. and Xiang, A. Promises and challenges\nof causality for ethical machine learning. arXiv preprint\narXiv:2201.10683, 2022.\nRajendran, G., Buchholz, S., Aragam, B., Sch¨olkopf, B.,\nand Ravikumar, P.\nLearning interpretable concepts:\nUnifying causal representation learning and foundation\nmodels. arXiv preprint arXiv:2402.09236, 2024.\nRawal, A., Raglin, A., Rawat, D. B., Sadler, B. M.,\nand McCoy, J.\nCausality for trustworthy artificial in-\ntelligence: Status, challenges and perspectives.\nACM\nComput. Surv., May 2024.\nISSN 0360-0300.\ndoi:\n10.1145/3665494.\nURL https://doi.org/10.\n1145/3665494. Just Accepted.\nRecht, B., Roelofs, R., Schmidt, L., and Shankar, V. Do\nimagenet classifiers generalize to imagenet?\nIn Inter-\nnational Conference on Machine Learning (ICML), pp.\n5389–5400, 2019.\nRichens, J. and Everitt, T. Robust agents learn causal world\nmodels, 2024.\nURL https://arxiv.org/abs/\n2402.10877.\nRubin, D. B. Causal inference using potential outcomes:\nDesign, modeling, decisions. Journal of the American\nStatistical Association, 100(469):322–331, 2005.\nRudin, C.\nStop explaining black box machine learning\nmodels for high stakes decisions and use interpretable\nmodels instead. Nature Machine Intelligence, 1(5):206–\n215, 2019.\nSanderson, C., Schleiger, E., Douglas, D., Kuhnert, P., and\nLu, Q. Resolving ethics trade-offs in implementing re-\nsponsible ai. In 2024 IEEE Conference on Artificial In-\ntelligence (CAI), pp. 1208–1213. IEEE, June 2024. doi:\n10.1109/cai59869.2024.00215.\nURL http://dx.\ndoi.org/10.1109/CAI59869.2024.00215.\nSch¨olkopf, B., Hogg, D., Wang, D., Foreman-Mackey, D.,\nJanzing, D., Simon-Gabriel, C.-J., and Peters, J. Mod-\neling confounding by half-sibling regression. Proceed-\nings of the National Academy of Science, 113(27):7391–\n7398, 2016.\nSch¨olkopf, B., Locatello, F., Bauer, S., Ke, N. R., Kalch-\nbrenner, N., Goyal, A., and Bengio, Y. Toward causal\nrepresentation learning. Proceedings of the IEEE, 109\n(5):612–634, 2021.\nSch¨olkopf, B., Locatello, F., Bauer, S., Ke, N. R.,\nKaltenpoth, M., Goyal, A., and Bengio, Y. Causality\nfor machine learning. arXiv preprint arXiv:2205.07607,\n2022.\nSharkey, L., Ghuidhir, C. N., Braun, D., Scheurer, J.,\nBalesni, M., Bushnaq, L., Stix, C., and Hobbhahn, M. A\ncausal framework for ai regulation and auditing. 2024.\nSheth, I., Abdelnabi, S., and Fritz, M.\nHypothesiz-\ning missing causal variables with llms. arXiv preprint\narXiv:2409.02604, 2024.\nShimizu, S., Hoyer, P. O., Hyv¨arinen, A., Kerminen, A.,\nand Jordan, M.\nA linear non-gaussian acyclic model\nfor causal discovery. Journal of Machine Learning Re-\nsearch, 7(10), 2006.\nShpitser, I. Structural equations, graphs and interventions.\nWiley Online Library, 2012.\nSinghal, K., Tu, T., Gottweis, J., Sayres, R., Wulczyn, E.,\nAmin, M., Hou, L., Clark, K., Pfohl, S. R., Cole-Lewis,\nH., et al. Toward expert-level medical question answer-\ning with large language models. Nature Medicine, pp.\n1–8, 2025.\nSj¨olander, A. The language of potential outcomes. Wiley\nOnline Library, 2012.\nSpirtes, P., Glymour, C. N., and Scheines, R. Causation,\nprediction, and search. MIT press, 2000.\nSweeney, L. Simple demographics often identify people\nuniquely. Health (San Francisco), 671:1–34, 2000.\nSzegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan,\nD., Goodfellow, I., and Fergus, R. Intriguing proper-\nties of neural networks. In International Conference on\nLearning Representations (ICLR), 2014.\nTeam, G., Anil, R., Borgeaud, S., Alayrac, J.-B., Yu, J.,\nSoricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Milli-\ncan, K., et al. Gemini: a family of highly capable multi-\nmodal models. arXiv, 2023.\nTeam, L., Modi, A., Veerubhotla, A. S., Rysbek, A., Huber,\nA., Wiltshire, B., Veprek, B., Gillick, D., Kasenberg, D.,\nAhmed, D., et al. Learnlm: Improving gemini for learn-\ning. arXiv preprint arXiv:2412.16429, 2024.\nTople, S., Sharma, A., and Nori, A. Alleviating privacy\nattacks via causal learning. In International Conference\non Machine Learning, pp. 9537–9547. PMLR, 2020.\nTschantz, M. C., Sen, S., and Datta, A. Sok: Differential\nprivacy as a causal property. In 2020 IEEE Symposium\non Security and Privacy (SP), pp. 354–371. IEEE, 2020.\nTu, C. Comparison of various machine learning algorithms\nfor estimating generalized propensity score. Journal of\nStatistical Computation and Simulation, 89(4):708–719,\n2019.\n14\n\n\nCausality Is Key to Understand and Balance Multiple Goals in Trustworthy ML and Foundation Models\nVallverd´u, J. Defining and debating algorithmic causality.\nIn Causality for Artificial Intelligence: From a Philo-\nsophical Perspective, pp. 77–82. Springer, 2024.\nvan der Veer, S. N., Riste, L., Cheraghi-Sohi, S., Phipps,\nD. L., Tully, M. P., Bozentko, K., Atwood, S., Hubbard,\nA., Wiper, C., Oswald, M., et al. Trading off accuracy\nand explainability in ai decision-making: findings from\n2 citizens’ juries. Journal of the American Medical In-\nformatics Association, 28(10):2128–2138, 2021.\nVanderWeele, T. J. The sufficient cause framework in statis-\ntics, philosophy and the biomedical and social sciences.\nWiley Online Library, 2012.\nVashishtha, A., Reddy, A. G., Kumar, A., Bachu, S.,\nBalasubramanian, V. N., and Sharma, A.\nCausal in-\nference using llm-guided discovery.\narXiv preprint\narXiv:2310.15117, 2023.\nVincent,\nJ.\nGoogle pauses gemini ai image gen-\neration\nafter\nhistorical\ninaccuracies\nspark\nback-\nlash.\nThe Verge, 2024.\nURL https://www.\ntheverge.com/2024/2/21/24079371/\ngoogle-ai-gemini-generative-inaccurate-historical.\nWachter, S., Mittelstadt, B., and Russell, C. Counterfactual\nexplanations without opening the black box: Automated\ndecisions and the gdpr.\nHarvard Journal of Law and\nTechnology, 31(2):841–887, 2017.\nWang, F., Mo, W., Wang, Y., Zhou, W., and Chen, M. A\ncausal view of entity bias in (large) language models.\narXiv preprint arXiv:2305.14695, 2023.\nWang, W., Lin, X., Feng, F., He, X., Lin, M., and Chua, T.-\nS. Causal representation learning for out-of-distribution\nrecommendation. In Proceedings of the ACM Web Con-\nference 2022, pp. 3562–3571, 2022.\nWang, Y., Wang, X., Beutel, A., Prost, F., Chen, J., and Chi,\nE. H. Understanding and improving fairness-accuracy\ntrade-offs in multi-task learning. In Proceedings of the\n27th ACM SIGKDD Conference on Knowledge Discov-\nery & Data Mining, pp. 1748–1757, 2021a.\nWang, Z. and Culotta, A.\nIdentifying spurious corre-\nlations for robust text classification.\narXiv preprint\narXiv:2010.02458, 2020.\nWang, Z., Shu, K., and Culotta, A. Enhancing model ro-\nbustness and fairness with causality: A regularization ap-\nproach, 2021b. URL https://arxiv.org/abs/\n2110.00911.\nWebster, K., Wang, X., Tenney, I., Beutel, A., Pitler, E.,\nPavlick, E., Chen, J., Chi, E., and Petrov, S. Measuring\nand reducing gendered correlations in pre-trained mod-\nels. arXiv preprint arXiv:2010.06032, 2020.\nWei, S. and Niethammer, M. The fairness-accuracy pareto\nfront. Statistical Analysis and Data Mining: The ASA\nData Science Journal, 15(3):287–302, 2022.\nWhittlestone, J., Nyrup, R., Alexandrova, A., and Cave,\nS.\nThe role and limits of principles in ai ethics: To-\nwards a focus on tensions. In Proceedings of the 2019\nAAAI/ACM Conference on AI, Ethics, and Society, AIES\n’19, pp. 195–200, New York, NY, USA, 2019. Associa-\ntion for Computing Machinery. ISBN 9781450363242.\ndoi: 10.1145/3306618.3314289. URL https://doi.\norg/10.1145/3306618.3314289.\nWright, S. Correlation and causation. Journal of Agricul-\ntural Research, 20:557–585, 1921.\nWu, A., Kuang, K., Zhu, M., Wang, Y., Zheng, Y., Han, K.,\nLi, B., Chen, G.-H., Wu, F., and Zhang, K. Causality for\nlarge language models, 2024.\nWu, Y., Zhang, L., Wu, X., and Tong, H.\nPc-fairness:\nA unified framework for measuring causality-based fair-\nness. In Advances in Neural Information Processing Sys-\ntems, pp. 3404–3414, 2019.\nXia, Y., Yu, T., He, Z., Zhao, H., McAuley, J., and\nLi, S.\nAligning as debiasing: Causality-aware align-\nment via reinforcement learning with interventional\nfeedback.\nIn Duh, K., Gomez, H., and Bethard,\nS. (eds.), Proceedings of the 2024 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies\n(Volume 1:\nLong Papers), pp. 4684–4695, Mexico\nCity, Mexico, June 2024. Association for Computa-\ntional Linguistics.\ndoi: 10.18653/v1/2024.naacl-long.\n262. URL https://aclanthology.org/2024.\nnaacl-long.262/.\nXu, L., Jiang, C., Qian, Y., Li, J., Zhao, Y., and Ren,\nY.\nPrivacy-accuracy trade-off in differentially-private\ndistributed classification: A game theoretical approach.\nIEEE Transactions on Big Data, 7(4):770–783, 2017.\nZafar, M. B., Valera, I., Rodriguez, M. G., and Gummadi,\nK. P. Fairness constraints: Mechanisms for fair classifi-\ncation. In Artificial intelligence and statistics, pp. 962–\n970. PMLR, 2017.\nZhang, B. H., Lemoine, B., and Mitchell, M. Fairness be-\nyond disparate treatment & disparate impact: Learning\ncausal latent-variable models. In Advances in Neural In-\nformation Processing Systems, pp. 4921–4930, 2018.\nZhang, K. and Hyvarinen, A.\nOn the identifiability\nof the post-nonlinear causal model.\narXiv preprint\narXiv:1205.2599, 2012.\n15\n\n\nCausality Is Key to Understand and Balance Multiple Goals in Trustworthy ML and Foundation Models\nZhao, H. and Gordon, G. J. Inherent tradeoffs in learning\nfair representations. The Journal of Machine Learning\nResearch, 23(1):2527–2552, 2022.\nZhou, F., Mao, Y., Yu, L., Yang, Y., and Zhong, T. Causal-\ndebias: Unifying debiasing in pretrained language mod-\nels and fine-tuning via causal invariant learning, 2023a.\nZhou, F., Mao, Y., Yu, L., Yang, Y., and Zhong, T. Causal-\ndebias: Unifying debiasing in pretrained language mod-\nels and fine-tuning via causal invariant learning. In Pro-\nceedings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers),\npp. 4227–4241, 2023b.\nZhu, L., Han, Z., and Li, S. Deep leakage from gradients.\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume 32, pp. 14747–14756, 2019.\nZinati, Y., Takiddeen, A., and Emad, A.\nGroundgan:\nGrn-guided simulation of single-cell rna-seq data using\ncausal generative adversarial networks. Nature Commu-\nnications, 15(1):4055, 2024.\nZliobaite, I.\nOn the relation between accuracy and\nfairness in binary classification.\narXiv preprint\narXiv:1505.05723, 2015.\n16\n\n\nCausality Is Key to Understand and Balance Multiple Goals in Trustworthy ML and Foundation Models\nA. Related work\nThe literature on trade-offs in ethical AI emphasizes the\ninherent tensions and competing objectives involved in de-\nsigning and deploying AI systems that align with ethical\nprinciples (Mbiazi et al., 2023).\nThe works of Sander-\nson et al. (2024); Whittlestone et al. (2019); Katirai &\nNagato (2024) explore frameworks for balancing fairness,\naccuracy, and other conflicting priorities. Kemmerzell &\nSchreiner (2024) explore trade-offs between robustness,\naccuracy, fairness, and privacy, and suggest data augmen-\ntation techniques that minimize the trade-offs.\nThe surveys on the use of causality for trustworthy\nAI (Ganguly et al., 2023; Rawal et al., 2024; Liu et al.,\n2023) provide a comprehensive overview of the existing\nuse cases. However, they do not explicitly discuss the need\nfor causality and do not focus on the role of causality in al-\nleviating tensions in trustworthy AI. Notably, only Ganguly\net al. (2023) overview the use of causality in privacy and\nexclusively focuses on the adversarial robustness through\ngeneralization. Discussions on causality for ethical AI fo-\ncusing on challenges and applications are provided in (Rah-\nmattalabi & Xiang, 2022; Vallverd´u, 2024).\nSeveral works discuss the benefits of use of causality for\none or two of the aspects of trustworthy AI. Discussion\non the need for causality for fairness can be found in the\nwork of (Binkyt˙e et al., 2022; Plecko & Bareinboim, 2024;\nMakhlouf et al., 2024). The study by (Wang et al., 2021b;\nEhyaei et al., 2023) explored causality to enhance fairness\nand robustness.\nB. Causality. Frameworks and Definitions\nThe field of statistical causality encompasses a diverse\nrange of theories and approaches that often complement\nor compete with each other, rather than forming a uni-\nfied framework. Researchers have likened the current state\nof statistical causality to ”probability theory before Kol-\nmogorov” (Dawid, 2015). In practice, the application of\nstatistical causality typically involves combining tools and\nmethods from multiple frameworks. This section provides\nan overview of the existing landscape, highlighting key the-\nories and definitions. Most approaches conceptualize cau-\nsation either as a relationship revealed through linear re-\ngression, grounded in the notion of real or hypothetical\ninterventions, or requiring a mechanistic understanding of\nthe underlying processes (Berzuini et al., 2012). In this\nwork, we primarily rely on the structural probabilistic mod-\nels framework (Pearl, 2009a) and the potential outcomes\nframework (Rubin, 2005). Below, we provide an overview\nof these frameworks and briefly touch on other approaches\nto causality. For technical definitions of relevant causal\nconcepts, refer to the Technical Preliminaries C.\nB.1. Potential Outcome Framework\nThe potential outcomes framework is one of the earliest\nformal theories of causal inference (Sj¨olander, 2012). It\ndefines causal effects as the difference in potential out-\ncomes under different levels of exposure or treatment (Ru-\nbin, 2005). This framework uses the language of potential\noutcomes to express causal effects in terms of joint distri-\nbutions of potential outcomes represented as random vari-\nables. Causal assumptions in this framework are encoded\nas constraints on these distributions (Shpitser, 2012).\nPotential outcomes can be categorized as factual (repre-\nsenting what actually occurred) or counterfactual (repre-\nsenting what would have occurred under different condi-\ntions). For example, if an individual took a medication\nand recovered, the factual outcome is ”recovery,” while the\ncounterfactual outcome represents what would have hap-\npened if the medication had not been taken. Since coun-\nterfactual outcomes are inherently unobservable for an in-\ndividual, estimating subject-specific causal effects is often\nimpractical (Sj¨olander, 2012).\nAt the population level, however, counterfactual outcomes\nand causal effects can be estimated.\nPopulation-level\ncausal effects contrast outcomes when everyone receives a\ntreatment versus when no one does. Although only factual\noutcomes are observed, randomization allows for causal ef-\nfect estimation under the Stable Unit Treatment Value As-\nsumption (SUTVA) (Sj¨olander, 2012). Randomization en-\nsures that potential outcomes are statistically independent\nof exposure, enabling identification of causal effects (Ru-\nbin, 2005). These principles are formally established in the\nliterature (Rubin, 2005; Sj¨olander, 2012).\nWhile the potential outcomes framework is widely used,\nit has limitations. Pearl has critiqued the framework for\nnot providing systematic guidelines on which covariates to\ninclude for adjustment (Pearl, 1988). He warns that in-\ncluding all available covariates may inadvertently increase\nbias, highlighting the need for caution when selecting ad-\njustment variables.\nB.2. Non-Parametric Structural Models (NPSEM)\nThe framework proposed by Pearl (Pearl, 2009a) is of-\nten celebrated for its coherence and robust formal foun-\ndations (Dawid, 2010).\nPearl integrates principles from\nagency causality (focused on interventions), probabilis-\ntic graphical models (Dawid, 2010), and counterfactual\nreasoning (Sj¨olander, 2012).\nHis approach balances\nthe probabilistic view of causality from Bayesian mod-\nels and the deterministic view from structural equation\nmodels (SEMs) common in econometrics and social sci-\nences (Pearl, 2009a).\nThe NPSEM framework represents causal relationships us-\n17\n\n\nCausality Is Key to Understand and Balance Multiple Goals in Trustworthy ML and Foundation Models\ning directed acyclic graphs (DAGs). A DAG G = (V, E)\nconsists of a set of variables V and directed edges E that\nencode causal dependencies. The structure ensures no cy-\ncles are formed. DAGs connect causal structure with joint\nprobability distributions via the Markov condition, which\nstates that each variable is conditionally independent of its\nnon-descendants given its parents.\nDAGs not only capture conditional independence but also\ndistinguish causal from non-causal data-generating pro-\ncesses. If a variable Y has an incoming edge from X, X\nis a direct cause of Y . Indirect causation is mediated by\nintermediate variables; for instance, if X influences Y via\nZ, then Z is a mediator. The NPSEM framework also pro-\nvides criteria for determining the identifiability of causal\nquantities from observational data (Pearl, 2009a), making\nit a powerful tool for causal inference (Shpitser, 2012).\nB.3. Alternative Approaches\nThe sufficient cause framework views causation as a set\nof sufficient conditions leading to an event (VanderWeele,\n2012; Mackie, 1965).\nUnlike the potential outcomes\napproach, which emphasizes causes, this framework fo-\ncuses on effects (VanderWeele, 2012). Pearl extends this\nby proposing probabilistic notions of necessity and suffi-\nciency (Pearl, 2009a).\nThe decision-theoretic approach incorporates stochastic\ncounterfactuals to facilitate inference transportability be-\ntween observational and experimental settings (Berzuini\net al., 2012). This approach relaxes strong assumptions of-\nten required by potential outcomes (Dawid, 2015).\nFinally, structural equation models (SEMs), rooted in de-\nterministic relationships expressed through structural linear\nequations, remain widely used but are limited by their para-\nmetric assumptions and inability to model complex, non-\nlinear causal relationships (Wright, 1921).\nC. Causality: Technical Preliminaries\nC.1. Causal Structures\nVariables are represented by capital letters (e.g., X, Y ),\nwhile specific values of variables are indicated using low-\nercase letters (e.g., A = a, W = w). Sets of variables and\ntheir values are denoted by bold capital letters (e.g., V) and\nbold lowercase letters (e.g., v), respectively.\nA causal graph, denoted as G = (V, E), is a Directed\nAcyclic Graph (DAG) consisting of a set of variables or\nnodes V and edges E. Each edge X →Y signifies a causal\nrelationship, meaning changes in X directly influence Y .\nImportantly, altering X impacts Y , but modifying Y does\nnot affect X.\nCausal graphs include three foundational structures: medi-\nators, confounders, and colliders (Pearl, 2009b), as illus-\ntrated in Figure 3:\n• Mediator: A variable W (Figure 3a) mediates the ef-\nfect of X on Y . For instance, X →W →Y shows\nX’s influence on Y through W. Mediators are also\ncalled chain structures.\n• Confounder: A variable C (Figure 3b) is a common\ncause of X and Y , resulting in a non-causal correla-\ntion between them. While X and Y are correlated in\nthis structure, X does not directly cause Y .\n• Collider: A variable Z (Figure 3c) is influenced by\nX and Y . Unlike the other structures, X and Y are\nuncorrelated unless conditioned on Z. Colliders are\nalso known as v-structures.\nX\nY\nW\n(a) Mediator\nX\nY\nC\n(b) Confounder\nX\nY\nZ\n(c) Collider\nFigure 3: Basic structures of causal graphs.\nMediation Analysis\nCausal relationships often involve multiple pathways, re-\nquiring mediation analysis to distinguish between them.\nFor example, the causal effect between X and Y can be\ndecomposed into:\n• Direct effect: The path X →Y .\n• Indirect effects: Paths such as X →R →Y and\nX →E →Y .\n• Path-specific effects: Effects through a specific path,\nsuch as X →E →Y .\nThis decomposition is critical for fairness. A direct effect\nof X on Y is typically considered unfair when X is a sen-\nsitive attribute (e.g., gender or race). In contrast, indirect\neffects may be fair or unfair, depending on the mediator.\nFor example:\n• An indirect effect through a discriminatory variable\n(R) is unfair.\n18\n\n\nCausality Is Key to Understand and Balance Multiple Goals in Trustworthy ML and Foundation Models\n• An indirect effect through an acceptable explanatory\nvariable (E) is considered fair.\nA variable is deemed a proxy (e.g., R) if it serves as a sub-\nstitute for X and leads to the same discriminatory outcome.\nDetermining whether a variable is a proxy or an acceptable\nmediator often requires domain expertise.\nC.2. Causal Fairness Notions\nCausal fairness aims to ensure that sensitive attributes, such\nas race or gender, do not unfairly influence outcomes. Be-\nlow, we describe key causal fairness notions and their for-\nmal definitions.\nC.2.1. TOTAL EFFECT (TE)\nTotal Effect (TE) (Pearl, 2009b) is a causal fairness notion\nthat quantifies the overall effect of a sensitive attribute X\non an outcome Y . Formally, TE is defined as:\nTEx1,x0(y) = P(Y = y | do(X = x1))−P(Y = y | do(X = x0)),\n(1)\nwhere do(X = x) denotes an intervention that sets X to x.\nTE measures the causal impact of changing X from x0 to\nx1 on Y across all causal paths connecting X to Y .\nC.2.2. MEDIATION ANALYSIS: NDE, NIE, AND PSE\nMediation analysis decomposes the causal effect of X on\nY into direct and indirect effects. This is essential for iden-\ntifying the pathways through which X influences Y .\nNatural Direct Effect (NDE) (Pearl, 2001): The NDE\nquantifies the direct effect of X on Y , bypassing any me-\ndiators. For a binary variable X with values x0 and x1, the\nNDE is:\nNDEx1,x0(y) = P(yx1,Zx0) −P(yx0),\n(2)\nwhere Z represents the set of mediator variables, and\nP(yx1,Zx0) is the probability of Y = y if X is set to x1\nwhile the mediators are set to values they would take under\nX = x0.\nNatural Indirect Effect (NIE) (Pearl, 2001): The NIE\ncaptures the influence of X on Y through mediators. It\nis given by:\nNIEx1,x0(y) = P(yx0,Zx1) −P(yx0),\n(3)\nwhere P(yx0,Zx1) represents the probability of Y\n= y\nwhen X = x0 but mediators take values they would un-\nder X = x1.\nPath-Specific Effect (PSE) (Pearl, 2009b; Chiappa, 2019;\nWu et al., 2019): The PSE isolates the causal effect of X\non Y transmitted through a specific path or set of paths π.\nFormally, it is defined as:\nPSEπ\nx1,x0(y) = P(yx1|π,x0|π) −P(yx0),\n(4)\nwhere P(yx1|π,x0|π) is the probability of Y = y if X = x1\nalong path π, while other paths (π) remain unaffected by\nthe intervention.\nC.2.3. NO UNRESOLVED DISCRIMINATION\nNo unresolved discrimination (Kilbertus et al., 2017) re-\nquires that any causal effect of a sensitive attribute X on\nan outcome Y occurs only through resolving (explanatory)\nvariables. A resolving variable, such as education level,\nreflects a non-discriminatory influence of X on Y . The cri-\nterion prohibits direct and proxy effects of X on Y .\nC.2.4. NO PROXY DISCRIMINATION\nNo proxy discrimination (Kilbertus et al., 2017) ensures\nthat decisions are not influenced by variables R that act as\nproxies for sensitive attributes X. Proxy discrimination is\nabsent if:\nP(Y | do(R = r)) = P(Y | do(R = r′)),\n∀r, r′ ∈dom(R).\n(5)\nThis guarantees that changes in R do not affect the outcome\nY if R is a proxy for X.\nC.2.5. COUNTERFACTUAL FAIRNESS\nCounterfactual fairness (Kusner et al., 2017) requires that\nthe outcome Y for an individual remains the same in both\nfactual and counterfactual scenarios. Formally, counterfac-\ntual fairness holds if:\nP(yx1 | V = v, X = x0) = P(yx0 | V = v, X = x0),\n(6)\nwhere V represents all other variables in the causal graph.\nThis definition ensures fairness at the individual level by\nrequiring that the sensitive attribute X does not influence\nY in any hypothetical scenario.\nD. Causality and ML\nThe use of causality in AI falls mainly into one of two cat-\negories. The first approach is to employ artificial intelli-\ngence to enhance the qualitative discovery and/or quantifi-\ncation of causal connections from the data. The second one\nis to use causal tools to improve Machine Learning (ML)\npredictions. Next, we elaborate on both of these methods\nto combine causality and ML.\nD.1. ML for causality\nCausal Discovery Most of the techniques for obtaining\ncausal quantities rely on knowing the causal structure of\n19\n\n\nCausality Is Key to Understand and Balance Multiple Goals in Trustworthy ML and Foundation Models\nthe data. It was previously assumed to be provided by ex-\nperts. Recent advances in causal discovery offer algorith-\nmic tools for recovering causal graphs from observational\ndata. The basis for causal discovery is the probabilistic\nand graphical concepts of causality (Dawid, 2010). Two\nmain groups of causal discovery algorithms can be distin-\nguished based on their attempt to identify conditional or\nunconditional (including pairwise) independencies in the\ndistribution from which the observational data is gener-\nated.\nThe first category includes constraints and score-\nbased algorithms such as PC (Le et al., 2016), FCI (Spirtes\net al., 2000), and GES (Hauser & B¨uhlmann, 2012). They\nusually produce a partially oriented causal graph.\nThe\nsecond category consists of algorithms based on causal\nasymmetries such as LiNGAM (Shimizu et al., 2006), and\nPNL (Zhang & Hyvarinen, 2012). The algorithms based\non Kolmogorov’s (algorithmic) complexity assume that if\nknowing the shortest compression of one variable does not\nreveal the shorter compression of the other, two variables\nare considered independent (Janzing & Sch¨olkopf, 2010;\nSch¨olkopf et al., 2022). The summary of the principles and\nperformance for pairwise causal discovery is provided by\nMooij et al. (Mooij et al., 2016). If the assumptions of the\nalgorithms are satisfied, they are capable of identifying a\nunique causal graph or a causal direction between the two\nvariables.\nML Tools for Causal Inference Supervised or semi-\nsupervised machine learning methods can be used to es-\ntimate causal quantities from the data or for variable selec-\ntion in situations with a high number of covariates (Kreif\n& DiazOrdaz, 2019; Aoki & Ester, 2022). ML algorithms\nsuch as, for example, logistic regression, bagging, random\nforest, and others, can be beneficial in estimating propen-\nsity scores used to estimate causal effects in the potential\noutcome framework (Lee et al., 2010; Tu, 2019).\nLLMs for Causal Discovery The recent advancements in\nlarge language models (LLMs) have inspired their use in\ncausal discovery (Kıcıman et al., 2023; Kasetty et al., 2024;\nVashishtha et al., 2023; AI4Science & Quantum, 2023; Ab-\ndulaal et al., 2023; Khatibi et al., 2024). Most of the above\nmethods involve the refinement of the statistically inferred\ncausal graph by LLM. However, emerging research shows\nthat, LLMs excel at synthesizing vast amounts of heteroge-\nneous knowledge, making them well-suited for tasks that\nrequire the integration of diverse datasets, such as con-\nstructing full causal graphs based on scientific literature in\ndiverse domains (Sheth et al., 2024; Afonja et al., 2024).\nD.2. Causality for ML\nOne of the main arguments that motivated the use of causal-\nity for machine learning is that causal modeling can lead to\nmore invariant or robust models (Sch¨olkopf et al., 2022).\nThe problem of overfitting and vulnerability to a domain\nshift is a known problem in ML. It is intuitive that learn-\ning the correlation between two phenomena, for example,\nrain and umbrellas, will not help to predict rain in sit-\nuations where people prefer raincoats instead of umbrel-\nlas. A causal understanding of phenomena is more gen-\neral to multiple circumstances.\nFollowing Pearl, ”...we\nmay as well view our unsatiated quest for understand-\ning how data is generated or how things work as a quest\nto acquire the ability to make predictions under a wider\nrange of circumstances, including circumstances in which\nthings are taken apart, reconfigured, or undergo sponta-\nneous change” (Pearl, 2009a). One of the methods to com-\nbine the ML model with the causal approach is to incorpo-\nrate causal knowledge (usually in the form of a complete\nor partial causal graph) in the learning process (Berrevoets\net al., 2023; 2022). Causal representation learning is an\nattempt to combine latent variables derived from unstruc-\ntured data and causal structure to arrive at a more invariant\nor fair model (Sch¨olkopf et al., 2021; Mitrovic et al., 2020;\nSch¨olkopf et al., 2022; Wang et al., 2022).\nThe causal\nstructure can also be used for feature selection, assuming\nthat it is known. Models based on direct causes to pre-\ndict the outcome are considered more robust (Tople et al.,\n2020).\n20\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21123v2.pdf",
    "total_pages": 20,
    "title": "Causality Is Key to Understand and Balance Multiple Goals in Trustworthy ML and Foundation Models",
    "authors": [
      "Ruta Binkyte",
      "Ivaxi Sheth",
      "Zhijing Jin",
      "Mohammad Havaei",
      "Bernhard Schölkopf",
      "Mario Fritz"
    ],
    "abstract": "Ensuring trustworthiness in machine learning (ML) systems is crucial as they\nbecome increasingly embedded in high-stakes domains. This paper advocates for\nintegrating causal methods into machine learning to navigate the trade-offs\namong key principles of trustworthy ML, including fairness, privacy,\nrobustness, accuracy, and explainability. While these objectives should ideally\nbe satisfied simultaneously, they are often addressed in isolation, leading to\nconflicts and suboptimal solutions. Drawing on existing applications of\ncausality in ML that successfully align goals such as fairness and accuracy or\nprivacy and robustness, this paper argues that a causal approach is essential\nfor balancing multiple competing objectives in both trustworthy ML and\nfoundation models. Beyond highlighting these trade-offs, we examine how\ncausality can be practically integrated into ML and foundation models, offering\nsolutions to enhance their reliability and interpretability. Finally, we\ndiscuss the challenges, limitations, and opportunities in adopting causal\nframeworks, paving the way for more accountable and ethically sound AI systems.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}