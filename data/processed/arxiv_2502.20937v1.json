{
  "id": "arxiv_2502.20937v1",
  "text": "Variations in Relevance Judgments\nand the Shelf Life of Test Collections\nAndrew Parry\nUniversity of Glasgow\nMaik FrÃ¶be\nFriedrich-Schiller-\nUniversitÃ¤t Jena\nHarrisen Scells\nUniversity of Kassel\nFerdinand Schlatt\nFriedrich-Schiller-\nUniversitÃ¤t Jena\nGuglielmo Faggioli\nUniversity of Padua\nSaber Zerhoudi\nUniversitÃ¤t Passau\nSean MacAvaney\nUniversity of Glasgow\nEugene Yang\nJohns Hopkins University\nAbstract\nThe fundamental property of Cranfield-style evaluations, that sys-\ntem rankings are stable even when assessors disagree on individual\nrelevance decisions, was validated on traditional test collections.\nHowever, the paradigm shift towards neural retrieval models af-\nfected the characteristics of modern test collections, e.g., documents\nare short, judged with four grades of relevance, and information\nneeds have no descriptions or narratives. Under these changes, it is\nunclear whether assessor disagreement remains negligible for sys-\ntem comparisons. We investigate this aspect under the additional\ncondition that the few modern test collections are heavily re-used.\nGiven more possible query interpretations due to less formalized\ninformation needs, an â€œexpiration dateâ€ for test collections might be\nneeded if top-effectiveness requires overfitting to a single interpre-\ntation of relevance. We run a reproducibility study and re-annotate\nthe relevance judgments of the 2019 TREC Deep Learning track.\nWe can reproduce prior work in the neural retrieval setting, show-\ning that assessor disagreement does not affect system rankings.\nHowever, we observe that some models substantially degrade with\nour new relevance judgments, and some have already reached the\neffectiveness of humans as rankers, providing evidence that test\ncollections can expire.\nCCS Concepts\nâ€¢ Information systems â†’Evaluation of retrieval results.\nKeywords\nEvaluation; Test collections; Collection Reliability\nACM Reference Format:\nAndrew Parry, Maik FrÃ¶be, Harrisen Scells, Ferdinand Schlatt, Guglielmo\nFaggioli, Saber Zerhoudi, Sean MacAvaney, and Eugene Yang. 2025. Varia-\ntions in Relevance Judgments and the Shelf Life of Test Collections. In Pro-\nceedings of the 48th International ACM SIGIR Conference on Research and De-\nvelopment in Information Retrieval (SIGIR â€™25), July 13â€“18, 2025, Padua, Italy.\nACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/XXXXXXX.\nXXXXXXX\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nSIGIR â€™25, Padua, Italy\nÂ© 2025 ACM.\nACM ISBN 978-1-4503-XXXX-X/XX/XX\nhttps://doi.org/10.1145/XXXXXXX.XXXXXXX\nTable 1: Overview of annotator agreement studies that we\nreproduce in the TREC Deep Learning scenario. We contrast\ninformation needs (length of title, description, narrative),\ndocuments (domain and length), evaluations (measure and\nrelevance grades) and observed results of each study (we show\nthe agreement as overlap and the system correlations as ğœ).\nStudy\nInformation Need\nDocs.\nEvaluation\nResults\nTitle Desc.\nNarr.\nDom.\nLen.\nMeas.\nRel.\nAgr. Corr.\nLesk, Salton [23]\nÃ—\n100.0\nÃ—\nMedical 103.7\nPrec.\n0â€“1\n0.30\nÃ—\nBurgin [4]\nÃ—\n30.4\nÃ—\nIR\n15.7\nMAP\n0â€“1\n0.49\nÃ—\nVoorhees [46]\n2.6\n20.4\n64.7\nNews\n550.5\nMAP\n0â€“1\n0.36 0.890\nOur Reproduction 8.3\nÃ—\nÃ—\nQA\n30.4\nnDCG\n0â€“3\n0.15 0.897\n1\nIntroduction\nTREC-style test collections enable the evaluation and, thereby, the\ndevelopment of retrieval systems. Making these collections robust\nand reusable is not trivial, partly because of the inherent subjec-\ntivity of relevance [45]. Assessing the reliability of relevance judg-\nments involves assessing inter-annotator agreement and is com-\nparably expensive [4, 23, 46]. Consequently, only a few studies\nhave investigated how the variability of relevance judgments af-\nfects the evaluation of retrieval systems. However, those findings\nwere highly important for the design of retrieval test collections as\nthey showed that even when relevance is subjective and therefore\nannotators disagree [38, 42], subsequent aggregated effectiveness\nevaluations [4, 23] and system rankings [46] are stable, ensuring\nthe reusability of test collections.\nContrasting older collections, topics in modern test collections\noften do not have descriptions or narratives, which is known to\naffect agreement [33], documents are often shorter, and relevance\nis assessed on a graded scale, which has not been investigated in\nre-annotation studies. Table 1 contrasts previous studies and the\nproperties of the corresponding test collections with the TREC Deep\nLearning 2019 [8] (DLâ€™19) test collection we use in our reproducibil-\nity work. The changes in modern collections potentially increase\nassessment variability because an assessor can freely interpret the\nquery intent and context that may be missing from the document to\nassess its relevance on a fine-grained scale. In this reproducibility\nstudy, we investigate if the robustness of classical test collections\nstill holds for modern test collections under these new conditions.\nThese shifts in test collection design coincide with advances\nin retrieval methodologies; retrieval pipelines increasingly com-\nprise components where neural language models estimate rele-\nvance. These systems have continuously improved over multiple\narXiv:2502.20937v1  [cs.IR]  28 Feb 2025\n\n\nSIGIR â€™25, July 13â€“18, 2024, Padua, Italy\nA. Parry et al.\niterations through improved training processes [31], through dis-\ntillation from large language models [32, 40], or by directly using\nlarge language models for ranking [43]. We hypothesize that iter-\native model improvements on static benchmarks may overfit to\nthe specific opinions expressed in static relevance judgements, in-\ntroducing systemic bias, which can be characterized as â€œbias by\nresearch designâ€ in which the influence of previous works can bias\nnew findings implicitly towards a common benchmark [18]. The\nTREC DLâ€™19 track remains a commonly used standard for bench-\nmarking systems, being cited and most likely evaluated hundreds\nof times (Cited 32 times at ECIRâ€™24 and SIGIRâ€™24 alone). Craswell\net al. [9] alluded to this point, warning against the iterative design\nprocesses employed in ad-hoc neural ranking.\nThus, we look to replicate the classic setting for evaluating the\nrobustness of collections proposed by Voorhees [46] for modern\ncollections, with the additional hypothesis that bias in the system\norderings may be introduced through model iteration instead of\nhuman disagreement. Since a single expert human assessorâ€™s rele-\nvance grades on a given topic are subjective and â€œequally plausibleâ€\nas another assessorâ€™s, we can compare the effectiveness of systems\nusing multiple â€œequally correctâ€ sets of relevance judgments. If\na systemâ€™s relative effectiveness degrades when evaluated using\nalternative judgments, it may indicate that models overfit the pref-\nerences expressed by the collectionâ€™s official judgments.\nIn this paper, we conduct a re-annotation of TREC DLâ€™19 using\ntwo â€œsecondaryâ€ annotators for every topic. Based on our new rel-\nevance judgments, we first provide a qualitative analysis of this\nprocess and validate the findings of previous work in a modern\nsetting. That is, we observe low agreement among assessors but sys-\ntem rankings remain stable. Secondly, treating human annotators\nas manual TREC runs, we find that one can no longer meaning-\nfully discriminate between the state-of-the-art and human judges\non this collection. A reasonable â€œupper boundâ€ effectiveness is\naround nDCG@10 of 0.81 (due to the assessment ambiguity). We\nsuggest that reported results that are higher than this bound on\nDLâ€™19 might already overfit to the relevance labels. Note that we\nare not aware of any existing system reporting results exceeding\nthis bound.1 However, the leading systems today are very close,\nwith RankZephyr [32] and RankGPT [43] reaching between 0.78â€“\n0.80 nDCG@10 with a sufficiently strong first stage. We additionally\nfind that closed-sourced large language models and systems dis-\ntilled from them can fluctuate greatly in relative effectiveness, often\ndegrading system rank on our new relevance judgments.\nOur reproducibility effort spawned from the ECIR 2024 Collab-\na-thon on Neural IR [24]. A subset of the IR community discussed\nthat the current generation of retrieval models may have reached\na practical upper bound on existing test collections, but we were\nunsure when to consider a test collection to be saturated or expired.\nOur study reproduced prior findings on the inherent subjectivity\nand variability of relevance judgments (especially in the absence\nof a well-defined narrative), and it established an empirical upper\nbound on TREC DLâ€™19. Therefore, we encourage caution when\nattempting to maximize a given metric as we approach and exceed\nhuman bounds on relevance estimation. Exceeding human effec-\ntiveness, given the subjectivity of relevance, may not lead to an\n1We do not consider large ensemble systems, given their risks [2].\noptimal system but instead stagnation of progress in the field. We\npublish our code and relevance judgments for reproducibility and\nto encourage future studies.2\n2\nRelated Work\nThis section outlines evaluation frameworks in retrieval, previous\nefforts to assess collection reliability, and judgement stability.\nOffline Evaluation. The Cranfield experiments introduced the\nmodern offline test collection in ad-hoc ranking; experts curated\na set of queries, and texts were retrieved from a static collection\nand exhaustively annotated, allowing for offline relevance judg-\nments [6]. Completeness of relevance judgments, i.e., the exhaustive\nannotation of corpora, is a key factor in the reliability of an offline\ntest collection [48]; however, the scale of modern corpora makes\nsuch a task infeasible. As such, TREC-style collections are collated\nby pooling the runs of multiple diverse systems to create a set of\nqueryâ€“document pairs to be annotated under the assumption that\nunpooled documents are not relevant [47]. Assessors then label\nthese pairs and assign a grade of relevance. A single annotator is\noften assigned to each topic; this decision generally reduces costs\nand has been validated by several studies measuring agreement\nbetween annotators as an effect on the reliability of a collection.\nRelevance judgments are subjective and, therefore, can vary\namong annotators [25], with low inter-annotator agreement [37],\nwhich can indicate low collection reliability. As such, narratives\nand training are usually provided to annotators describing the\ninformation need underlying a query [15]. We consider that these\nhomogenized judgments may be a weak point for ongoing collection\nreliability as we learn from them in a semi-supervised fashion\nthrough effective systems acting as teachers.\nNotions of Relevance. The reliability of offline test collections is\ncontested due to the intrinsic subjectivity of relevance under differ-\nent human annotators [34, 45], annotation settings [33], relevance\ngrading [10] and, more recently, the process of collecting candidate\ndocuments via pooling [54]. As these concerns have been raised, one\nmethod to validate offline test collections is to re-annotate judge-\nments. Lesk and Salton [23] and Burgin [4] empirically validated\nthe collection reliability under annotator disagreement, observing\nthat system order is maintained under different annotators by mea-\nsuring variance in precision and recall. When web-scale corpora\nrendered exhaustive annotation impossible, depth pooling became\npopularized to provide a minimal set of documents that should\nbe annotated to discriminate system effectiveness. Concerns were\ntwo-fold: that systems that contributed to the pool would have an\nadvantage over new systems and that the reduced annotations may\ndisrupt the stability of system orderings. Voorhees [46] found that\nsystem order stability could be maintained under the re-annotation\nof test collection pools from TREC-4 [14] and -6 [50].\nCollection Retirement. Previous work has discussed the retire-\nment of a test collection in recent years, though not retired before\ndissemination the use of TREC 2021 Deep Learning test collec-\ntion [9] is discouraged by NIST due to the large increase in corpora\nsize leading to pooling bias; both NIST [49] and participants in the\n2https://github.com/Parry-Parry/sigir25-annotation\n\n\nVariations in Relevance Judgments and the Shelf Life of Test Collections\nSIGIR â€™25, July 13â€“18, 2024, Padua, Italy\ntrack [20] had concerns regarding the ability to measure recall due\nto incomplete labeling. However, we are not aware of any work\nthat considers the effect of collection popularity and subsequent\nbias induced by fitting to relevance judgments over time; our focus\non precision is in line with the philosophy of neural systems and\nthe Deep Learning track.\nCraswell et al. [9] warns against using TREC Deep Learning test\ncollections for experimental iteration to reduce the chance of overfit-\nting to judgments. Though this guidance is generally followed with\nthe MSMARCO-dev set acting as a suitable alternative, the broad\nuse of existing strong systems validated on TREC Deep Learning\n2019 [17, 32, 52] may lead to overfitting via semi-supervision. We\ndraw a parallel with the work of Armstrong et al. [3], which warned\nthat progress might stagnate due to poor practice in evaluation.\nSimilarly, we note that the continued comparison to a dataset that\nguides our architecture and training pipeline may lead to stagnation\nin system development.\nWithin a modern ranker training pipeline, several components\ndepend on existing models for effectiveness. Hard negative min-\ning is a process in which, instead of random negative examples\nin a contrastive loss, samples are taken from the top most rele-\nvant documents to a query with the assumption that the majority\nwill be non-relevant [21]. It is common to employ one or several\nmodelsâ€™ pooled rankings to sample negatives [41, 53], with these\nmodels often chosen based on their in-domain effectiveness. More\nexplicitly, pipelines are frequently composed of multiple stages of\ndistillation [39, 51, 53], which is a semi-supervised process in which\na student model is optimized to approximate the output of a teacher\nmodel [16]. Again, these models will be chosen based on in-domain\neffectiveness and coupled with hard negatives.\n3\nEvaluating Reliability through Re-Annotation\nThe reliability of a test collection, given the subjectivity of relevance,\nis often validated by re-annotation [4, 23, 46]. Modern IR models\nare often optimized to mimic previously effective systems through\ndata-driven processes. We examine how these models, designed\nafter a collectionâ€™s release, perform on new judgments following\nthe process of Voorhees [46].\n3.1\nReproduction Source\nWe describe the original annotation process applied by Voorhees\n[46]. This work was carried out as a component of the curation of\nthe TREC-4 [14] and TREC-6 [50] corpora. As is standard in the cura-\ntion of pooled TREC corpora, one â€œprimary\" annotator was assigned\nto each topic, subsequently judging each document in the pool. Two\nadditional annotators were assigned to each topic to validate the\neffect of re-annotation on depth-pooled corpora. A sub-sample of\njudgements was taken from the pool and re-annotated. In TREC-4,\na maximum of 200 relevant documents per topic were taken as a\nsecondary pool for re-annotation. Additionally, 200 non-relevant\ndocuments were sampled per topic. A binary judgement was as-\nsigned to each queryâ€“document pair, relevant or non-relevant. The\ntexts constituting the primary annotation pool, left unjudged in the\nsecondary pool, are included in all system evaluations, maintaining\nidentical pooling biases. The comparison of â€œsecondary\" annota-\ntions was carried out to assess hypotheses that resurface frequently\nin IR literature. This work focuses on the subjectivity of relevance\nand how pooling affects this subjectivity. Thus, experiments were\nconducted over a depth-pooled corpus under construction at the\ntime. This process produces a subset of judgements where each\nqueryâ€“document pair was re-annotated twice.\nWhere possible, we follow the evaluation process of Voorhees\n[46], including analysis of the probability of two systems swapping\nin effectiveness under different annotators. We define an annota-\ntion ğ‘as a tuple {ğ‘,ğ‘‘,ğ‘Ÿ} composed of query ğ‘, document ğ‘‘, and\njudgement ğ‘Ÿâˆˆ[0, 1, 2, 3]. For a given annotator pair and a primary\nannotator, assigned ğ‘›query-document pairs, we have three anno-\ntation sets ğ´1,ğ´2,ğ´3 where ğ´ğ‘–= {ğ‘ğ‘—}ğ‘›\nğ‘—=0. For ğ‘šqueries, as noted\nby Voorhees [46], there are 3ğ‘špossible permutations. Thus, these\npermutations allow for the greater exploration of judgements that\ncould have been produced from a given annotation process. From\nthe 3 annotation sets, a permutation ğ´âˆ—is sampled ğ‘ times, and\nsystems are evaluated against each permutation. The probability\nof a swap between systems ğ‘–and ğ‘—can be evaluated under a given\nmeasure ğ‘“given judgements ğ´âˆ—. For systems ğ‘†, we produce a ma-\ntrix ğµâˆˆZ|ğ‘†|Ã—|ğ‘†| where ğµ[ğ‘–, ğ‘—] is the number of annotation samples\nwhich caused systems ğ‘†ğ‘–to have higher effectiveness than ğ‘†ğ‘—. The\nswap probability between these systems can then be computed as\nmin(ğµ[ğ‘–,ğ‘—],ğµ[ğ‘—,ğ‘–])\nğ‘ \n. Due to the symmetric treatment of system com-\nparisons, this computation is bounded between 0 and 0.5.\n3.2\nRe-Annotation of a Modern Collection\nWhile the original study focused on binary judgments, our work\nadapts this methodology to graded relevance and neural systems\nunder a modern evaluation setting. Our reproduction is conducted\non the TREC Deep Learning 2019 track (DLâ€™19) [8]. Specifically,\nwe use the passage collection, which is generally more popular.\nThe proposal of more granular measures that explicitly consider\nthe ordering of a ranking, such as discounted cumulative gain, as\nopposed to set-based measures, such as Precision@ğ‘˜, generally re-\nquires graded relevance as opposed to the binary relevance grades of\nolder collections. DLâ€™19 was annotated under this setting with four\ngrades of relevance: â€˜perfectly relevant,â€™ â€˜highly relevant,â€™ â€˜relatedâ€™,\nand â€˜non-relevant.â€™ This distinction leads to some key differences\nin our process and analysis.\nWe treat the original judgements of DLâ€™19 as those of the â€œpri-\nmaryâ€ annotator. In our study, two annotators from a pool of 8\nretrieval academics are assigned to each topic and act as â€œsecondaryâ€\nannotators aligning with the original study in that each topic is an-\nnotated by 2 new annotators; however, the original study does not\nspecify the number of total annotators. The size of the annotator\npool used in this work is similar to that of TREC collections [42],\nalbeit with a reduced total annotation budget due to cost and follow-\ning in the sampling setting of Voorhees [46]. We create a secondary\npool composed of all documents in the pool with a grade of 1\nor more and sample 100 documents per annotator from the non-\nrelevant documents of the primary pool. We reduce the number of\nnon-relevant judgments from the pool motivated by the hypoth-\nesis that agreement is far greater over what is non-relevant; for\nset-based measures, this choice leads to a balance between relevant\nand non-relevant texts and has minimal effect on nDCG.\nOur process for pool creation uses all judged documents at least\nrelated to a topic and assigns topics to annotators in a round-robin\n\n\nSIGIR â€™25, July 13â€“18, 2024, Padua, Italy\nA. Parry et al.\nfashion to balance annotation load while maintaining two annota-\ntors per topic. A priority queue of annotator pairs ordered by their\ncurrent annotation load is produced and assigned the next topic\nwith the greatest number of judgements. The load on each annota-\ntor is then balanced by swapping large topics to annotators with\nminimal load in a second round-robin. We then assign non-relevant\ntexts to each annotator pair with a total mean annotation load of\n1125 pairs per annotator.\nAnnotator Instruction. Each annotator was provided with annota-\ntion guidelines, including the original relevance grade descriptions\nfrom DLâ€™19. Annotators were made aware that instances had been\npreviously annotated for DLâ€™19; however, they were not provided\nwith relevance grades or a grade distribution. Due to the specialist\nknowledge required to assess relevance in, for example, medical\ntopics, annotators were allowed to familiarise themselves with\ntopics at their discretion. To facilitate reproducibility and prevent\nexposure to discussion and instances of DLâ€™19 relevance judgments,\nwhich may be indexed on common search engines, we instead en-\nforce the use of the ChatNoir search endpoint [30], which indexes\nClueWeb22 [28] and retrieves using BM25.\n3.3\nFurther Investigation\nBeyond our reproduction setting, we utilise re-annotated query-\ndocument pairs to investigate other aspects of collection reliability.\nJudgement Aggregation. Voorhees [46] apply several aggregation\noperations including permutation as described in Section 3.1 and\naggregate operators. We perform maximum, minimum, and mean\npooling over judgments. In the case of mean pooling, we do not\nperform further quantization; for example, three texts of grades 1,\n1.5, and 2 where the second text was annotated 1 and 2 by different\nannotators would be compared in that order.\nAbsent Narratives. The absence of narratives or larger descrip-\ntions for the TREC Deep Learning collections allows an additional\nfocus for our investigation: Annotators were prompted to note cases\nof ambiguity and, more generally, their difficulties in annotation.\nFurthermore, annotators were asked to create their narratives for\neach query so that agreement among annotators could be investi-\ngated with more granularity in the presence of a natural language\ndescription of their thought process during annotation. In doing so,\nwe measure query intent as a factor in agreement.\nTo ablate this factor, we fix narratives after the curation of judge-\nments to assess how narrative interpretation can affect agreement\nand downstream measures of effectiveness. We take three topics of\nlow, median, and high agreement measured via Fleiss ğœ…between the\nprimary and secondary annotators; half of the annotators reference\none narrative, and half reference the other. We can then further iso-\nlate sources of disagreement to query intent and intrinsic ambiguity\nof a query title both qualitatively and quantitatively.\nAnnotators as Oracles. User models underly many common IR\nmeasures such as discounted cumulative gain [19] and mean re-\nciprocal rank (@ğ‘˜) [5]. In performing offline evaluations of search\nsystems using these measures, we aim to model effectiveness via\nuser behavior approximated by relevance judgments. In principle,\nan optimal system would approach a score of 1 for a normalized\nTable 2: Modern systems included in our analysis. Rightmost\ncolumns are the source of labels for semi-supervised learn-\ning; neural is any encoder-type model, and LLM is a Large\nLanguage Model. Absence of both indicates binary labels.\nDistillation\nModel\nType\nNeuralLLM\nColBERT [22]\nLate interaction bi-encoder\nÃ—\nÃ—\nSPLADE ED++ [11, 12] Learned sparse encoder\nâœ“\nÃ—\nMonoT5 [27]\nSeq2Seq cross-encoder\nÃ—\nÃ—\nMonoELECTRA [31, 40]BERT-based cross-encoder\nâœ“\nâœ“\nSetEncoder [39]\nBERT-based list-wise cross-encoder\nâœ“\nâœ“\nRankZephyr [32]\nDecoder-only list-Wise cross-encoder\nÃ—\nâœ“\nRankGPT [43]\nDecoder-only list-wise cross-encoder\n?\n?\nmetric (assuming ğ‘˜relevant documents exist). However, given the\nsubjective and variable nature of relevance under different query\nintents, such effectiveness, though possible, may not be indicative\nof a truly effective system. Human annotators outside the original\nannotation setting may represent strong search systems as they\nhave exhaustively annotated available documents to some cutoff ğ‘˜.\nAs such, we measure the performance of each annotator under\noriginal relevance judgments as an indicator of how non-pooled\nmodern systems are performing relative to a more realistic ora-\ncle. Similarly to idealized discounted cumulative gain, we sort by\nrelevance grade post-aggregation. Though graded relevance lacks\ngranularity in contrast to scalar relevance scores, through in-grade\npermutations (shuffling), we validate that variations in downstream\nmeasures of effectiveness are minimal (SD < 0.0001).\n4\nEvaluation\nIn this section, we outline our investigation and analyze findings\nwhen evaluating using secondary annotations.\n4.1\nExperiment Design\nFollowing the re-annotation process of Voorhees [46], we observe\nthe impact of re-annotation on the DLâ€™19 test collection, using\npermutations to assess modern system bias.\nDataset. The MSMARCO passage corpus is a collection of around\n8.8 million segmented documents covering a broad range of ad-hoc\nsearch topics [26]; this collection and accompanying training topics\nwere mined and annotated from Bing query logs. We re-annotate the\nTREC Deep Learning 2019 test collection [8], composed of 43 topics.\nUnlike the development split released originally with MSMARCO,\nthe DLâ€™19 test set uses densely annotated depth-pooled topics taken\nfrom the associated TREC track that year.\nPool Description. The DLâ€™19 track primarily focused on the com-\nparison of neural systems in ad-hoc ranking. For equivalence to\nthe re-ranking task, the top 100 texts of all submitted systems from\nboth re-ranking and retrieval tasks were included in our annotation\npool. The top 10 texts from each of the 75 systems (44% Neural LM,\n27% Neural, 29% lexical/traditional) were added to our pool, and\nwe use HiCal [1] to collect an additional 100 texts per topic.\n\n\nVariations in Relevance Judgments and the Shelf Life of Test Collections\nSIGIR â€™25, July 13â€“18, 2024, Padua, Italy\nModels. We compare several neural architectures and training\nsettings. In addition to models that contributed to the original anno-\ntation pool, we include systems that use other models for either data\naugmentation or semi-supervised training via distillation. Table 2\nillustrates the degree of teacher distillation used in each architec-\nture; ColBERT and SPLADE act as end-to-end retrieval or first-stage\nretrievers, whereas all others re-rank BM25 [35] or ColBERT as\nnoted. For monoT5, monoELECTRA, and SetEncoder, we include\nmultiple model sizes, base and large for BERT/ELECTRA variants\nand base and 3B for T5. For monoELECTRA, we do not use the\noriginal checkpoints released by Pradeep et al. [31]; we instead use\ndistilled checkpoints released by Schlatt et al. [40] as we are inter-\nested in systems that use modern processes. When using RankGPT,\nwe employ GPT-4, GPT-4 Turbo, and GPT-4o.\nMeasures. Voorhees [46] measure inter-annotator agreement in\na pair-wise setting using the overlap of judgements defined as\nthe number of common judgements divided by the total number\nof judgements. We additionally use Cohenâ€™s ğœ…over both 4-grade\nand binarized relevance and Fleissâ€™ ğœ…, allowing for comparisons\nbetween 3 annotators (1 primary, 2 secondary) simultaneously. In\nthe TREC-4 and 6 test collections, macro-averaged precision was\nthe primary measure of effectiveness, and therefore, the previous\nstudy measured systems by MAP. The TREC DL tracks with four\ngrades of relevance instead primarily measure nDCG@10, and thus,\nour study is conducted over this metric. Aligned with Voorhees\n[46], we measure downstream system order correlation between\njudgement sets using measure Kendallâ€™s ğœ; additionally, we measure\nSpearmanâ€™s ğœŒand RBO.\nSystem Order Robustness. As we have multiple annotators per\nquery, we can observe pooled judgments across several hypothetical\nsets; we take all possible combinations of annotators and evaluate\nboth pooled and new models to observe the stability of system order\nunder new judgments. For a given judgment set, we produce a rank-\ning of systems by a target metric (i.e., nDCG@10) and compare this\nsystem order to the system ordering under the original judgments.\nWe aggregate rank Î” for all possible natural combinations over each\nsystem to observe how frequently a system degrades over other\nassessed systems. We measure significant differences in retrieval\neffectiveness via a paired t-test with Bonferroni correction for dif-\nferent annotator combinations. We measure significant changes\nin system rank by a Wilcoxon signed test comparing annotator\ncombinations with the system order by official TREC judgements.\n4.2\nPilot Study Findings\nWe conducted a pilot study over the top 10 pooled texts for 10 ran-\ndomly sampled topics. Table 3 summarizes the results. We observe\nthat the prevalence of grades 1 and 3 is inflated in these annotations\ncompared to the original annotations. In discussions with anno-\ntators, there was ambiguity about what separates a relevant text\nfrom a highly relevant text. Additionally, cultural context leads to\nambiguity in interpreting a query; for example, a query â€œdog day af-\nternoon meaningâ€ was difficult to interpret without broader context\nas a film exists with that name. The query could be interpreted as\nthe meaning of the phrase or the film. From these ambiguities, we\nallowed annotators to use a controlled search engine to familiarise\nthemselves with the topic. Annotators not from the USA generally\nnoted the difficulty in USA-centric topics.\nTable 3: Comparing the ration of annotation grades between\nthe primary annotators and secondary annotators in a pilot\nstudy of 10 topics.\nGrade Ratio\nAnnotators\n0\n1\n2\n3\n|ğ´|\nPrimary\n0.487\n0.190\n0.308\n0.015\n1143\nSecondary\n0.560\n0.213\n0.108\n0.118\n600\nTable 4: Inter-Annotator agreement over combinations of\nannotators. Agreement is shown under binarised and 4-grade\nrelevance.\nFleissâ€™ ğœ…\nOverlap\nCohenâ€™s ğœ…\nGroup\n4\n2\n4\n2\n4\n2\n|Q|\nPrimary+1\n0.22\n0.44\n0.12\n0.46\n0.12\n0.31\n14\n1\nâ€”\nâ€”\n0.42\n0.72\n0.19\n0.37\nPrimary+2\n0.28\n0.40\n0.17\n0.48\n0.17\n0.31\n12\n2\nâ€”\nâ€”\n0.47\n0.74\n0.22\n0.38\nPrimary+3\n0.17\n0.31\n0.11\n0.45\n0.11\n0.28\n8\n3\nâ€”\nâ€”\n0.63\n0.89\n0.27\n0.47\nPrimary+4\n0.28\n0.37\n0.19\n0.48\n0.19\n0.30\n9\n4\nâ€”\nâ€”\n0.43\n0.71\n0.19\n0.34\n4.3\nCore Assessment of Agreement\nQuantification of relevance is confounded by many factors with sev-\neral works reporting what is generally considered low agreement\namong annotators [4, 23, 46]. Voorhees [46] report overlap among\nannotators as a measure of agreement observing values around 0.4.\nIn Table 4, we present agreement measurements over both 4 grade\nand binarized relevance. We observe that under 4-grade relevance,\nall agreement values degrade relative to numbers stated in previous\nstudies. Fleissâ€™ ğœ…measured over primary and secondary annota-\ntors indicates near-random agreement but generally improves by\naround 10 points in all cases over binary judgments. Overlap values\nover binary judgments generally correspond to those observed by\nVoorhees [46], and over solely secondary annotators, we observe\nhigh overlap values, suggesting that while disagreement stems from\nthe subjectivity of relevance and the annotation setting, the process\nof annotating over 4 grades is generally more variable regardless\nof topic granularity. In the third group, we find the lowest values\nin both 4 grade and binary relevance, which is again reflected in\nlow overlap and Cohenâ€™s ğœ…values, potentially suggesting diverging\nquery intents or that this allocation of queries was particularly\nambiguous. However, we see increases in overlap values when mea-\nsured solely over secondary annotators, again suggesting that the\nannotation setting under which judgments are collected contributes\nto the improved agreement, as noted by Cuadra and Katter [10] and\nRegazzi [34] in previous annotation studies. Values of ğœ…are low in\nall cases, suggesting that generally quantifying relevance over 4\n\n\nSIGIR â€™25, July 13â€“18, 2024, Padua, Italy\nA. Parry et al.\nFigure 1: Transitions in relevance grades from primary to\nsecondary annotations.\ngrades is more challenging for annotators than the re-annotation\nsetting of Voorhees [46]. The lack of pre-defined narratives clarify-\ning intent means annotators could interpret a query as they please,\npotentially resulting in three diverging topic interpretations.\nIn Figure 1, observe a Sankey flow diagram between the origi-\nnal relevance grade assigned by primary annotators and the new\ngrades by secondary annotators. Like previous work, a secondary\nannotation setting often leads to the reduction of relevance grades,\ne.g., 1 to 0 or 2 to 1. Validating our decision to sample a subset of\nnon-relevant judgements, 13% of 800 non-relevant texts change\ngrade, with 77% being a change to a grade of 1; we analyse outlier\ncases (providing a grade of 2 or more) in Section 4.4. Measuring\nfrequencies agnostic of previous judgments, grades 1 and 2 are\nannotated similarly across annotators, while grades 0 and 3 were\nannotated with greater variance. This variance in the case of grade\n0 is generally consistent across annotators, and we provide a deeper\nanalysis of grade 3 due to varying notions of relevance below.\n4.4\nNarratives, Intent, and Relevance\nThe observations of grade transitions in our work and generally low\nagreement noted by prior work, even in a binary setting, may be\nattributed to several factors noted in Section 2. Though we cannot\nfully isolate factors in the subjectivity of relevance, we choose to\ninvestigate the effect of narratives on inter-annotator agreement,\ngiven that recent corpora often do not have fixed narratives [7, 8],\nunlike those used in prior re-annotation studies [14]. We select\ntopics by median, minimum and max Fleissâ€™ ğœ…values from the\nannotation pool and consider where disagreement occurs and how\nnarratives differ. Chosen examples illustrate different aspects of\nagreement, differing query intents, and a narrativeâ€™s specificity.\nThe first, taken from Group 1, is a technical query in the med-\nical domain with a Fleiss ğœ…of 0.06, indicating disagreement near\nrandom guessing. The query â€œtypes of dysarthria from cerebral\npalsyâ€ was described by one annotator as a query from a sibling\nof someone with cerebral palsy looking to understand how their\nsibling would be affected by their condition in the future. The other\nannotator described a medical context in which the definition of all\nconditions is known, and therefore, solely the connection between\ndysarthria and cerebral palsy was required. The second annotator\nstated a requirement that relevance was determined by the types\nand specificity of provided types of dysarthria connected to cere-\nbral palsy, with all other documents being non-relevant. Though\nboth are valid information needs, they represent vastly different\nquery intents, with one annotatorâ€™s grade frequency matching the\noverall annotation pool and the other stricter narrative leading to\nnear-total non-relevance.\nThe second, taken from Group 4, is a more general query, â€œis cdg\nairport in main parisâ€ with a Fleiss ğœ…of 0.08, again indicating low\nagreement. However, both annotators described a need for exact\ndistances from the airport to either the center of Paris or, more\ngenerally, the city of Paris. Both annotators required a succinct\ndescription of the distance or travel times to the airport from Paris\nto be considered perfectly relevant. To be highly relevant, both con-\nsidered any mention of either travel time or distance measurements.\nIntents then diverged for partially relevant documents, with one an-\nnotator allowing for the mention of costs and other aspects of travel\nto and from the airport. In contrast, the other annotator required\nthe airportâ€™s position, at minimum, mentioning that it is situated\nwith respect to Paris. Non-relevance was denoted by any other\nmention of the airport or mentions of distances to destinations near\nParis. We find larger differences in lower relevance grades, with\none annotator considering the majority of pooled documents to be\nnon-relevant when there is no mention of travel details. In contrast,\nthe other considers them to be relevant because they mention that\nthe airport is situated near Paris in a generic manner. The specificity\nof a query generally leads to lower agreement in our observations.\nWhile queries in domains such as law or medicine have a lower\noverall agreement, we still find cases where generic queries have\nlow agreement due to the interpretations of information needs.\nWe analyze outlier cases of texts initially labeled non-relevant\nthat, when evaluated under an alternative query intent, were\ndeemed perfectly relevant. In the 9 cases of this, there are cases\nwhere a reference is made to the entity of focus, for example â€œgold-\nfishâ€ in the query, â€œdo goldfish growâ€ but only implicitly, for exam-\nple â€œitâ€ or â€œtheyâ€. The query was satisfied by a given document if\nthe context is inferred by the surrounding spans of text or from\nknowing that the document has been retrieved by some system\n(this is inevitable under pooling), but the information is not self-\ncontained. In some cases, annotators explicitly stated that they\nconsidered a document relevant, but when the information is not\nself-contained, it cannot be annotated as relevant; others took this\napproach without note.\nOverall, many annotation disagreements can occur not only in\nthe high-level query intent of the narrative but also in descriptions\nof relevance. A diagnostic tool and query-specific grade specifica-\ntions could be helpful in future work.\n4.5\nNarrative and Intent Ambiguity\nGiven the lack of public narratives provided in the release of TREC\nDeep Learning 2019, it is infeasible to isolate query intent as a factor\nin reliability without first collecting narratives. In fixing narratives,\nwe produce an additional set of relevance judgements for queries of\nlow, median, and high agreement by Fleiss ğœ…, including the primary\nannotator. In Figure 2, We classify these queries as difficult, median\nand easy with respect to query intent determination and measure\n\n\nVariations in Relevance Judgments and the Shelf Life of Test Collections\nSIGIR â€™25, July 13â€“18, 2024, Padua, Italy\nAnnotator 2 (1)\nAnnotator 3 (1)\nAnnotator 6 (1)\nAnnotator 8 (1)\nAnnotator 1 (2)\nAnnotator 4 (2)\nAnnotator 5 (2)\nAnnotator 7 (2)\nAnnotator 2 (1)\nAnnotator 3 (1)\nAnnotator 6 (1)\nAnnotator 8 (1)\nAnnotator 1 (2)\nAnnotator 4 (2)\nAnnotator 5 (2)\nAnnotator 7 (2)\n0.2\n0.3 0.0\n0.2 0.5 0.0\n0.1 0.0 0.1 0.1\n0.2 0.1 0.2 0.1 0.3\n0.0 0.1 -0.1 0.1 0.3 0.2\n0.2 0.3 0.1 0.4 0.1 0.1 0.1\nDifficult Query\nAnnotator 2 (1)\nAnnotator 3 (1)\nAnnotator 6 (1)\nAnnotator 8 (1)\nAnnotator 1 (2)\nAnnotator 4 (2)\nAnnotator 5 (2)\nAnnotator 7 (2)\nAnnotator 2 (1)\nAnnotator 3 (1)\nAnnotator 6 (1)\nAnnotator 8 (1)\nAnnotator 1 (2)\nAnnotator 4 (2)\nAnnotator 5 (2)\nAnnotator 7 (2)\n0.3\n0.3 0.2\n0.4 0.6 0.1\n0.4 0.5 0.3 0.4\n0.2 0.4 0.2 0.4 0.3\n0.4 0.5 0.4 0.4 0.5 0.3\n0.3 0.4 0.3 0.3 0.5 0.2 0.6\nMedian Query\nAnnotator 2 (1)\nAnnotator 3 (1)\nAnnotator 6 (1)\nAnnotator 8 (1)\nAnnotator 1 (2)\nAnnotator 4 (2)\nAnnotator 5 (2)\nAnnotator 7 (2)\nAnnotator 2 (1)\nAnnotator 3 (1)\nAnnotator 6 (1)\nAnnotator 8 (1)\nAnnotator 1 (2)\nAnnotator 4 (2)\nAnnotator 5 (2)\nAnnotator 7 (2)\n0.5\n0.3 0.1\n0.2 0.3 0.3\n0.1 0.2 0.5 0.3\n0.3 0.2 0.6 0.4 0.4\n0.3 0.4 0.5 0.4 0.6 0.7\n0.1 0.2 0.4 0.4 0.7 0.3 0.6\nEasy Query\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nCohen's \nFigure 2: Comparing 3 queries of varying difficulty by inter-annotator agreement under fixed narratives (2 narratives (1) and (2)).\nAnnotators were divided into two groups and were assigned identical document pools. Color indicates strength of agreement;\nredder indicates stronger agreement, bluer indicates weaker agreement.\nCohenâ€™s ğœ…between the judgements of both narrative groupings. It\nis clear that a fixed narrative reduces ambiguity with Cohenâ€™s ğœ…,\nimproving by 24% and 46% for each annotation group between the\ndifficult and median query; however, between the median and easy\nquery, a decrease of 8% and an increase of 19% is observed.\nIn several cases, high agreement across different narrative groups\noccurs; in each case, narratives are of similar detail, defining what\nintent the user had in searching as well as what should be labelled\nrelevant; we consider that one factor in relevance annotation be-\nyond random noise may therefore be interpretation of a narrative\nwhich in the case of a primary annotator is of minimal concern but\nin human-in-the-loop annotation with generative systems may lead\nto implications for control as ultimately we can only express our\nneed or a demonstration to the system in natural language. When\ncomputing global improvements in agreement, we measure a 79%\nincrease in agreement between the difficult and median query and\nan additional 1% increase comparing the median and easy query.\nFrom these observations, we propose that the greatest factor in\ndisagreement is simply the ambiguity of the natural language used\nto express a query regardless of the narrative applied. Whether or\nnot this is only inherent in pooled corpora would require further\ninvestigation as in a setting such as that of TREC DLâ€™19, the ad-hoc\nrealisation of a query intent will be biased by the top-ğ‘˜texts chosen\nby pooled systems.\n4.6\nCore Assessment of System Order\nThe core objective in re-annotation studies is to validate that down-\nstream evaluation is not compromised by human bias in subjective\nrelevance judgements. By measuring the stability of system order\nand the correlation of system order by target metrics, we can assess\ncollection reliability. Following Voorhees [46] and as described in\nSection 3.1, we sample permutations of relevance judgements and\nobserve the stability of pair-wise system comparisons in Figure 3.\nComparisons closest to the top right of the figure represent those\nwhere nDCG differences are large; however, they frequently swap\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n nDCG@10\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nProbability of Swap\nNeural\nLLM-Neural\nLLM\nFigure 3: Comparing the stability of judgements through\nnDCG@10 and system swap probabilities over 10000 judge-\nment permutations. We filter lexical comparisons to improve\nvisibility in the central mass. â€˜LLM-Neuralâ€™ denotes that a\npair contains one LLM-based and one Neural-based ranker.\nin system order. A clear pareto-frontier is composed of compar-\nisons between LLM and neural systems under judgement permuta-\ntions. Lexical-neural comparisons generally lie in the main body\nof neural-neural and llm-llm comparisons, suggesting similar sta-\nbility as was noted by Voorhees [46] over TREC-6. However, more\ngenerally, we replicate the findings of previous works that despite\nlow annotator agreement, under re-annotation, system ordering is\nhighly correlated with the original DLâ€™19 annotations even under\n4-grade relevance. In Table 5, observe aggregate correlation values\nusing both natural combinations of annotators and the in-sample\napproach described by Voorhees [46].\nAs the In-Sample approach includes primary judgements, a\nmarginally higher correlation compared to combinations of solely\n\n\nSIGIR â€™25, July 13â€“18, 2024, Padua, Italy\nA. Parry et al.\nModels\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nnDCG@10\nModel Type\nLexical\nNeural\nLLM\nFigure 4: Variance in nDCG@10 over annotator combina-\ntions. Effectiveness values on new annotations are translu-\ncent. Color of points indicates model type.\nTable 5: System ranking correlation over different annotator\ngroups measuring Kendallâ€™sğœ, Spearmansâ€™s ğœŒand rank-biased\noverlap. â€˜Combinationâ€™ is the mean of natural permutations\nof the 8 annotators, â€˜In-Sampleâ€™ is the mean of judgment per-\nmutations as described by Voorhees [46]. All ğœand ğœŒvalues\nrepresent significant correlation.\nType\nğœ\nğœŒ\nRBO\nCombination\n0.879\n0.972\n0.888\nIn-Sample\n0.897\n0.977\n0.902\nsecondary annotators is found. To investigate where system order\ndiverges, we analyse ranking changes at a system level. In Figure 4\nsee that generally, new annotations led to a reduction in absolute\nnDCG@10 values relative to the original annotations, effectiveness\nmeasures on original annotations order systems, and that system\norder is not fully stable but LLM-based or distilled systems ap-\npear to be outliers, as such we explicitly measure the changes in\nsystem order concentrated around these systems. In Figure 5, we\ncombine all possible permutations of annotators across annotation\npairs. We measure changes in system order under these combina-\ntions and aggregate by system. Many lexical systems remain stable\nin ordering (Î” â‰ˆ0). Older neural approaches in the original sys-\ntem pool display higher variance but can improve or degrade in\nrank without significant difference. When considering only modern\nsystems, we observe that smaller models trained with supervised\nand semi-supervised approaches consistently improve system rank.\nWithin this group, however, larger semi-supervised models are\nmore robust than their smaller counterparts under new judgments.\nList-wise encoders generally degrade in rank. These models use an\nTable 6: Comparing aggregate annotations as rankings with\nseveral ranking architectures. Judged Only indicates that\noriginal runs have been filtered to contain solely annotated\ndocuments, allowing for a similar setting to the annotator\nrankings. Significance is with respect to Minimum (A), Mean\n(B) or Maximum (C) aggregation via a paired t-test with Bon-\nferroni correction (ğ‘< 0.05).\nModel\nnDCG@10\nP@10\nMRR@10\nR@100\nJudged & Unjudged\nBM25\n0.51Â±0.08ABC 0.41Â±0.09ABC 0.70Â±0.12BC 0.49Â±0.10ABC\nSplade ED++\n0.73Â±0.07BC 0.62Â±0.10BC 0.91Â±0.07\n0.60Â±0.09ABC\nColBERTÂ»RankZephyr 0.75 Â± 0.07B\n0.67Â±0.10\n0.84Â±0.09\n0.67Â±0.09ABC\nColBERTÂ»RankGPT-4o 0.78Â±0.06\n0.71Â±0.09\n0.87Â±0.08\n0.67Â±0.09ABC\nJudged Only\nBM25\n0.51Â±0.08ABC 0.41Â±0.09ABC 0.70Â±0.12BC 0.65Â±0.09ABC\nSplade ED++\n0.73 Â± 0.07B\n0.63Â±0.10BC 0.91Â±0.07\n0.69Â±0.09BC\nColBERTÂ»RankZephyr 0.77Â±0.07\n0.70Â±0.09\n0.85Â±0.08\n0.67Â±0.09ABC\nColBERTÂ»RankGPT-4o 0.80Â±0.05\n0.73Â±0.09\n0.89Â±0.08\n0.67Â±0.09ABC\nMinimum (A)\n0.76Â±0.07\n0.67Â±0.10\n0.84Â±0.09\n0.75Â±0.06\nMean (B)\n0.81Â±0.05\n0.71Â±0.10\n0.90Â±0.08\n0.86Â±0.06\nMaximum (C)\n0.79Â±0.05\n0.70Â±0.09\n0.86Â±0.08\n0.86Â±0.06\ninteraction token to facilitate inter-document attention, assisting\nin out-of-domain effectiveness [39]. Changes in effectiveness are\nmore pronounced and are significant in smaller distilled language\nmodels. This is notable as though we see continuing improvement\nin smaller models out-of-domain through distillation. One should\nbe cautious that in-domain effectiveness may be less robust than\nbenchmarks indicate, which for many applications may be more\nimportant than robust effectiveness across heterogeneous corpora.\nRuns that are either zero-shot or distilled from LLM re-rankers\nshow the highest variance in relative effectiveness even within a\nsub-group such as those based on GPT 4 endpoints. GPT-4o-based\nruns significantly degrade in system rank when re-ranking BM25\nbut are more stable under a neural first-stage ColBERT. This is most\nlikely caused by positional bias found in list-wise ranking towards\nalready precise first-stage rankings [29, 44]. This hypothesis is\nvalidated by the frequent rank improvement of RankZephyr (the\nleft-most model in Figure 4), which was trained explicitly to reduce\nthis bias [32]. Overall, we consider that though high-quality data\n(i.e., granular training data from strong models) is important in\nimproving the robustness of neural models, these findings would\nsuggest that smaller models in our current training strategies may\nnot have the capacity to robustly generalize even in-domain.\n4.7\nSystems versus Humans\nRecall our argument that a â€œperfectâ€ score on a normalized metric\nmay not be a feasible or desirable goal in ranking evaluation; as\nsuch, we measure the effectiveness of human annotators aggre-\ngated in multiple forms outlined in Section 3.3. In Table 6, see that\nsystems can exceed or reach parity with secondary annotators, par-\nticularly with respect to nDCG@10 and MRR@10, with multiple\nsystems, both LLM and encoder-based exceeding annotators. When\nexcluding unjudged documents (which is known to overestimate\n\n\nVariations in Relevance Judgments and the Shelf Life of Test Collections\nSIGIR â€™25, July 13â€“18, 2024, Padua, Italy\nBM25 >> RankZephyr\nMono T5 3B\nColBERT >> RankZephyr\nBM25 >> MonoELECTRA Large\nSrchvrs PS Run 2\nColBERT >> RankGPT4O Full\nMono T5 Base\nColBERT >> RankGPT4\nBM25 >> MonoELECTRA Base\nBM25 >> RankGPT4\nColBERT >> RankGPT4 Turbo\nBM25 >> SetEncoder Large\nTUW19 P3 RE\nBM25 Base AX P\nColBERT\nICT-CKNRM B50\nColBERT >> RankGPT4O\nBM25 Base RM3 P\nBM25 Base PRF P\nSrchvrs PS Run 1\nBM25 >> Sparse Cross-Encoder\nBM25 Tuned AX P\nTUW19 P2 RE\nRun ID 2\nRun ID 5\nSPLADE ED++\nTUW19 P3 F\nUNH BM25\nUNH ExDL BM25\nBM25 Tuned P\nColBERT >> MonoELECTRA Large\nRun ID 4\nBM25 >> SetEncoder Base\nBM25 Base P\nRun ID 3\nBM25 Tuned RM3 P\nColBERT >> MonoELECTRA Base\nICT-CKNRM B\nIDST BERT P3\nIDST BERT P2\nIDST BERT P1\nMS DUET Passage\nICT-BERT2\nBM25 Tuned PRF P\nTUW19 P2 F\nTUW19 P1 RE\nBM25 >> RankGPT4 Turbo\nIDST BERT PR1\nSrchvrs PS Run 3\nIDST BERT PR2\nP Exp BERT\nP Exp RM3 BERT\nTUW19 P1 F\nColBERT >> SetEncoder Large\nBM25 >> RankGPT4O\nTUA1-1\nColBERT >> SetEncoder Base\nTest 1\nP BERT\nBM25 >> RankGPT4O Full\nSystem\n15\n10\n5\n0\n5\n10\n15\n System Rank\nType\nLexical\nNeural\nLLM\nFigure 5: Aggregated system rank changes over all natural combinations of secondary versus primary annotations measured\nusing nDCG@10. Negative Î” indicates a system frequently improves in rank.\nthe retrieval effectiveness [13, 36]), annotator performance mea-\nsured by precision can be exceeded by neural systems across nDCG,\nP, and MRR; however, recall remains lower in all settings. Given\nour findings in Figure 4, an observation that systems are better\nthan humans should be considered cautiously. However, the robust-\nness of the LLM-based RankZephyr observed both by system order\nrobustness over new judgments and its parity with human anno-\ntators suggests that with sufficient capacity, a smaller system can\nbe distilled from larger systems while remaining robust. Observing\nsimilar effectiveness to humans and confidence intervals providing\nno way to discriminate systems, we consider that achieving signifi-\ncant improvements would suggest far exceeding humans, which\nmay not be possible in a way conducive to improving information\naccess and may solely serve to improve precision on specific topics\nand intents. The ability of distilled systems to reach parity with\nhuman effectiveness across multiple annotator combinations while\nobtaining largely reduced recall in a judged setting, even when\nre-ranking a neural first-stage retriever, is undesirable for several\ndownstream tasks. Although precision is approaching or overtaking\nthe effectiveness of an oracle, recall remains a challenge in densely\nannotated collections.\n5\nConclusion\nWe considered important studies on the impact of the annotator\nagreement on retrieval evaluations in light of modern neural evalu-\nation scenarios. In reproducing re-annotation processes and evalu-\nations by Voorhees [46] on a modern test collection, we validate\nthe stability system ordering under re-annotation under 4-grade\nrelevance and ambiguous query intent. We raised concerns that\nthe broad and frequent use of test collections to â€œtuneâ€ learned\nsystems implicitly through influences from published works may\nreduce the reliability of a collection over time. We identified that\nthe subjectivity of relevance through query intent may be a factor\nin determining â€œoverfittingâ€ in ranking tasks. As a query may have\nseveral interpretations, we re-annotate the popular TREC DLâ€™19\ncollection. We found that system order varies when evaluating large\nlanguage models and systems distilled from them. Furthermore,\nwe observed that the current state-of-the-art can outperform com-\nbinations of human annotators on original relevance judgments,\nsuggesting we may have reached a realistic bound on precision for\nthis collection. We posit that further improvement on this collection\nmay not indicate that a system is better than another in a meaning-\nful way. Our process has limitationsâ€”most notably, re-annotation is\nincredibly costly. Future work will investigate how to draw similar\nconclusions about a collectionâ€™s expiration without the need for\nextensive human effort.\nAcknowledgments\nWe thank Sean MacAvaney and others involved in making the\nsessions of the Collab-a-thon at ECIRâ€˜24 [24] an engaging space for\ncollaboration.\nReferences\n[1] Mustafa Abualsaud, Nimesh Ghelani, Haotian Zhang, Mark D Smucker,\nGordon V Cormack, and Maura R Grossman. 2018. A System for Efficient\nHigh-Recall Retrieval. In The 41st International ACM SIGIR Conference on\nResearch & Development in Information Retrieval. ACM, 1317â€“1320.\n[2] Xavier Amatriain and Justin Basilico. 2017. Netflix recommendations: Beyond\nthe 5 stars (part 1). https://netflixtechblog.com/netflix-recommendations-\nbeyond-the-5-stars-part-1-55838468f429\n\n\nSIGIR â€™25, July 13â€“18, 2024, Padua, Italy\nA. Parry et al.\n[3] Timothy G. Armstrong, Alistair Moffat, William Webber, and Justin Zobel. 2009.\nImprovements that donâ€™t add up: ad-hoc retrieval results since 1998. In\nProceedings of the 18th ACM Conference on Information and Knowledge\nManagement (Hong Kong, China) (CIKM â€™09). Association for Computing\nMachinery, New York, NY, USA, 601â€“610.\nhttps://doi.org/10.1145/1645953.1646031\n[4] Robert Burgin. 1992. Variations in relevance judgments and the evaluation of\nretrieval performance. Inf. Process. Manage. 28, 5 (July 1992), 619â€“627.\nhttps://doi.org/10.1016/0306-4573(92)90031-T\n[5] Olivier Chapelle, Donald Metlzer, Ya Zhang, and Pierre Grinspan. 2009.\nExpected reciprocal rank for graded relevance. In CIKM â€™09: Proceeding of the\n18th ACM conference on Information and knowledge management. New York, NY,\nUSA, 621â€“630. http://doi.acm.org/10.1145/1645953.1646033\n[6] C. Cleverdon, J. Mills, and M. Keen. 1966. Factors Determining the Performance of\nIndexing Systems. Volume I. Design. Part 2. Appendices. Technical Report\nPB169574. Association of Special Libraries and Information Bureau, Cranfield\n(England). https:\n//ntrl.ntis.gov/NTRL/dashboard/searchResults/titleDetail/PB169574.xhtml Num\nPages: 261.\n[7] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2020.\nOverview of the TREC 2020 Deep Learning Track. In Proceedings of the 29th Text\nREtrieval Conference, TREC 2020, Virtual Event, Gaithersburg, MD, USA,\nNovember 16-20, 2020 (NIST Special Publication, Vol. 1266), Ellen M. Voorhees and\nAngela Ellis (Eds.). National Institute of Standards and Technology (NIST).\n[8] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M.\nVoorhees. 2019. Overview of the TREC 2019 Deep Learning Track. In 28th\nInternational Text Retrieval Conference, TREC 2019, Gaithersburg, Maryland, USA\n(NIST Special Publication), Ellen M. Voorhees and Angela Ellis (Eds.). National\nInstitute of Standards and Technology (NIST).\n[9] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Fernando Campos, Ellen M.\nVoorhees, and Ian Soboroff. 2021. TREC Deep Learning Track: Reusable Test\nCollections in the Large Data Regime. Proceedings of the 44th International ACM\nSIGIR Conference on Research and Development in Information Retrieval (2021).\nhttps://api.semanticscholar.org/CorpusID:233296851\n[10] Carlos A. Cuadra and Robert V. Katter. 1967. Opening the Black Box of\nRelevance. Journal of Documentation 23, 4 (April 1967), 291â€“303.\nhttps://doi.org/10.1108/eb026436\n[11] Thibault Formal, Carlos Lassance, Benjamin Piwowarski, and StÃ©phane\nClinchant. 2022. From Distillation to Hard Negative Sampling: Making Sparse\nNeural IR Models More Effective. In Proceedings of the 45th International ACM\nSIGIR Conference on Research and Development in Information Retrieval (Madrid,\nSpain) (SIGIR â€™22). Association for Computing Machinery, New York, NY, USA,\n2353â€“2359. https://doi.org/10.1145/3477495.3531857\n[12] Thibault Formal, Benjamin Piwowarski, and StÃ©phane Clinchant. 2021. SPLADE:\nSparse Lexical and Expansion Model for First Stage Ranking. Association for\nComputing Machinery, New York, NY, USA, 2288â€“2292.\nhttps://doi.org/10.1145/3404835.3463098\n[13] Maik FrÃ¶be, Lukas Gienapp, Martin Potthast, and Matthias Hagen. 2023.\nBootstrapped nDCG Estimation in the Presence of Unjudged Documents. In\nAdvances in Information Retrieval - 45th European Conference on Information\nRetrieval, ECIR 2023, Dublin, Ireland, April 2-6, 2023, Proceedings, Part I (Lecture\nNotes in Computer Science, Vol. 13980), Jaap Kamps, Lorraine Goeuriot, Fabio\nCrestani, Maria Maistro, Hideo Joho, Brian Davis, Cathal Gurrin, Udo\nKruschwitz, and Annalina Caputo (Eds.). Springer, 313â€“329.\nhttps://doi.org/10.1007/978-3-031-28244-7_20\n[14] Donna Harman. 1995. Overview of the Fourth Text REtrieval Conference\n(TREC-4). In Proceedings of The Fourth Text REtrieval Conference, TREC 1995,\nGaithersburg, Maryland, USA, November 1-3, 1995 (NIST Special Publication,\nVol. 500-236), Donna K. Harman (Ed.). National Institute of Standards and\nTechnology (NIST). http://trec.nist.gov/pubs/trec4/overview.ps.gz\n[15] Donna Harman. 2012. TREC-Style Evaluations. In Information Retrieval Meets\nInformation Visualization - PROMISE Winter School 2012, Zinal, Switzerland,\nJanuary 23-27, 2012, Revised Tutorial Lectures (Lecture Notes in Computer Science,\nVol. 7757), Maristella Agosti, Nicola Ferro, Pamela Forner, Henning MÃ¼ller, and\nGiuseppe Santucci (Eds.). Springer, 97â€“115.\nhttps://doi.org/10.1007/978-3-642-36415-0_7\n[16] Geoffrey Hinton. 2015. Distilling the Knowledge in a Neural Network. arXiv\npreprint arXiv:1503.02531 (2015).\n[17] Sebastian HofstÃ¤tter, Sophia Althammer, Michael SchrÃ¶der, Mete Sertkan, and\nAllan Hanbury. 2020. Improving Efficient Neural Ranking Models with\nCross-Architecture Knowledge Distillation. CoRR abs/2010.02666 (2020).\narXiv:2010.02666 https://arxiv.org/abs/2010.02666\n[18] Dirk Hovy and Shrimai Prabhumoye. 2021. Five sources of bias in natural\nlanguage processing. Linguistics and Language Compass 15, 8 (aug 2021).\nhttps://doi.org/10.1111/lnc3.12432\n[19] Kalervo JÃ¤rvelin and Jaana KekÃ¤lÃ¤inen. 2002. Cumulated gain-based evaluation\nof IR techniques. ACM Trans. Inf. Syst. 20, 4 (2002), 422â€“446.\nhttps://doi.org/10.1145/582415.582418\n[20] Jaap Kamps and David Rau. [n. d.]. University of Amsterdam at TREC 2021:\nDeep Learning Track. https://api.semanticscholar.org/CorpusID:266088115\n[21] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu,\nSergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for\nOpen-Domain Question Answering. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing, EMNLP 2020, Online,\nNovember 16-20, 2020, Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu\n(Eds.). Association for Computational Linguistics, 6769â€“6781.\nhttps://doi.org/10.18653/V1/2020.EMNLP-MAIN.550\n[22] Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective\nPassage Search via Contextualized Late Interaction over BERT. In Proceedings of\nthe 43rd International ACM SIGIR conference on research and development in\nInformation Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020, Jimmy\nHuang, Yi Chang, Xueqi Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen,\nand Yiqun Liu (Eds.). ACM, 39â€“48. https://doi.org/10.1145/3397271.3401075\n[23] M.E. Lesk and G. Salton. 1968. Relevance assessments and retrieval system\nevaluation. Information Storage and Retrieval 4, 4 (Dec. 1968), 343â€“359.\nhttps://doi.org/10.1016/0020-0271(68)90029-6\n[24] Sean MacAvaney, Adam Roegiest, Aldo Lipani, Andrew Parry, BjÃ¶rn Engelmann,\nChristin Katharina Kreutz, Chuan Meng, Erlend Frayling, Eugene Yang,\nFerdinand Schlatt, Guglielmo Faggioli, Harrisen Scells, Iana Atanassova, Jana\nFriese, Janek Bevendorff, Javier Sanz-Cruzado, Johanne Trippas, Kanaad Pathak,\nKaustubh D. Dhole, Leif Azzopardi, Maik FrÃ¶be, Marc Bertin, Nishchal Prasad,\nSaber Zerhoudi, Shuai Wang, Shubham Chatterjee, Thomas JÃ¤nich, Udo\nKruschwitz, Xi Wang, and Zijun Long. 2024. Report on the Collab-a-Thon at\nECIR 2024. SIGIR Forum 58, 1 (2024), 1â€“11.\nhttps://doi.org/10.1145/3687273.3687287\n[25] Stefano Mizzaro. 1997. Relevance: The Whole History. J. Am. Soc. Inf. Sci. 48, 9\n(1997), 810â€“832.\n[26] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan\nMajumder, and Li Deng. 2016. MS MARCO: A Human Generated MAchine\nReading COmprehension Dataset. In Proceedings of the Workshop on Cognitive\nComputation: Integrating neural and symbolic approaches 2016 co-located with the\n30th Annual Conference on Neural Information Processing Systems (NIPS 2016),\nBarcelona, Spain, December 9, 2016 (CEUR Workshop Proceedings, Vol. 1773),\nTarek Richard Besold, Antoine Bordes, Artur S. dâ€™Avila Garcez, and Greg Wayne\n(Eds.). CEUR-WS.org.\n[27] Rodrigo Nogueira, Zhiying Jiang, Ronak Pradeep, and Jimmy Lin. 2020.\nDocument Ranking with a Pretrained Sequence-to-Sequence Model. In Findings\nof the Association for Computational Linguistics: EMNLP 2020, Trevor Cohn,\nYulan He, and Yang Liu (Eds.). Association for Computational Linguistics,\nOnline, 708â€“718. https://doi.org/10.18653/v1/2020.findings-emnlp.63\n[28] Arnold Overwijk, Chenyan Xiong, and Jamie Callan. 2022. ClueWeb22: 10\nBillion Web Documents with Rich Information. In SIGIR â€™22: The 45th\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval, Madrid, Spain, July 11 - 15, 2022, Enrique AmigÃ³, Pablo Castells, Julio\nGonzalo, Ben Carterette, J. Shane Culpepper, and Gabriella Kazai (Eds.). ACM,\n3360â€“3362. https://doi.org/10.1145/3477495.3536321\n[29] Andrew Parry, Sean MacAvaney, and Debasis Ganguly. 2024. Top-Down\nPartitioning for Efficient List-Wise Ranking. arXiv:2405.14589 [cs.IR]\nhttps://arxiv.org/abs/2405.14589\n[30] Martin Potthast, Matthias Hagen, Benno Stein, Jan GraÃŸegger, Maximilian\nMichel, Martin Tippmann, and Clement Welsch. 2012. ChatNoir: a search engine\nfor the ClueWeb09 corpus. In The 35th International ACM SIGIR conference on\nresearch and development in Information Retrieval, SIGIR â€™12, Portland, OR, USA,\nAugust 12-16, 2012, William R. Hersh, Jamie Callan, Yoelle Maarek, and Mark\nSanderson (Eds.). ACM, 1004. https://doi.org/10.1145/2348283.2348429\n[31] Ronak Pradeep, Yuqi Liu, Xinyu Zhang, Yilin Li, Andrew Yates, and Jimmy Lin.\n2022. Squeezing Water from a Stone: A Bag of Tricks for Further Improving\nCross-Encoder Effectiveness for Reranking. In Advances in Information Retrieval:\n44th European Conference on IR Research, ECIR 2022, Stavanger, Norway, April\n10â€“14, 2022, Proceedings, Part I (Stavanger, Norway). Springer-Verlag, Berlin,\nHeidelberg, 655â€“670. https://doi.org/10.1007/978-3-030-99736-6_44\n[32] Ronak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin. 2023. RankZephyr:\nEffective and Robust Zero-Shot Listwise Reranking is a Breeze! CoRR\nabs/2312.02724 (2023). https://doi.org/10.48550/ARXIV.2312.02724\narXiv:2312.02724\n[33] Alan M. Rees and Douglas G. Schultz. 1967. A Field Experimental Approach to the\nStudy of Relevance Assessments in Relation to Document Searching. Final Report to\nthe National Science Foundation. Volume I. Technical Report. Clearinghouse for\nFederal Scientific and Technical Information, Springfield, Va.\n[34] John J. Regazzi. 1988. Performance measures for information retrieval\nsystemsâ€”an experimental approach. Journal of the American Society for\nInformation Science 39, 4 (1988), 235â€“251. https://doi.org/10.1002/(SICI)1097-\n4571(198807)39:4<235::AID-ASI3>3.0.CO;2-H\n[35] Stephen E. Robertson, Steve Walker, Micheline Hancock-Beaulieu, Mike Gatford,\nand A. Payne. 1995. Okapi at TREC-4. In Proceedings of The Fourth Text REtrieval\nConference, TREC 1995, Gaithersburg, Maryland, USA, November 1-3, 1995 (NIST\n\n\nVariations in Relevance Judgments and the Shelf Life of Test Collections\nSIGIR â€™25, July 13â€“18, 2024, Padua, Italy\nSpecial Publication, Vol. 500-236), Donna K. Harman (Ed.). National Institute of\nStandards and Technology (NIST).\nhttp://trec.nist.gov/pubs/trec4/papers/city.ps.gz\n[36] Tetsuya Sakai. 2008. Comparing metrics across TREC and NTCIR: the robustness\nto system bias. In Proceedings of the 17th ACM Conference on Information and\nKnowledge Management, CIKM 2008, Napa Valley, California, USA, October 26-30,\n2008, James G. Shanahan, Sihem Amer-Yahia, Ioana Manolescu, Yi Zhang,\nDavid A. Evans, Aleksander Kolcz, Key-Sun Choi, and Abdur Chowdhury (Eds.).\nACM, 581â€“590. https://doi.org/10.1145/1458082.1458159\n[37] Tetsuya Sakai. 2019. How to Run an Evaluation Task - With a Primary Focus on\nAd Hoc Information Retrieval. In Information Retrieval Evaluation in a Changing\nWorld - Lessons Learned from 20 Years of CLEF, Nicola Ferro and Carol Peters\n(Eds.). The Information Retrieval Series, Vol. 41. Springer, 71â€“102.\nhttps://doi.org/10.1007/978-3-030-22948-1_3\n[38] Linda Schamber, Michael B. Eisenberg, and Michael S. Nilan. 1990. A\nRe-Examination of Relevance: Toward a Dynamic, Situational Definitionâˆ—.\nInformation Processing & Management 26, 6 (Jan. 1990), 755â€“776.\nhttps://doi.org/10.1016/0306-4573(90)90050-C\n[39] Ferdinand Schlatt, Maik FrÃ¶be, Harrisen Scells, Shengyao Zhuang, Bevan\nKoopman, Guido Zuccon, Benno Stein, Martin Potthast, and Matthias Hagen.\n2024. Set-Encoder: Permutation-Invariant Inter-Passage Attention for Listwise\nPassage Re-Ranking with Cross-Encoders. arXiv preprint arXiv:2404.06912 (2024).\n[40] Ferdinand Schlatt, Maik FrÃ¶be, Harrisen Scells, Shengyao Zhuang, Bevan\nKoopman, Guido Zuccon, Benno Stein, Martin Potthast, and Matthias Hagen.\n2024. A Systematic Investigation of Distilling Large Language Models into\nCross-Encoders for Passage Re-ranking. arXiv:2405.07920 [cs.IR]\nhttps://arxiv.org/abs/2405.07920\n[41] Tao Shen, Xiubo Geng, Chongyang Tao, Can Xu, Xiaolong Huang, Binxing Jiao,\nLinjun Yang, and Daxin Jiang. 2022. LexMAE: Lexicon-Bottlenecked Pretraining\nfor Large-Scale Retrieval. ArXiv abs/2208.14754 (2022).\nhttps://api.semanticscholar.org/CorpusID:251953412\n[42] Ian Soboroff. 2024. Donâ€™t Use LLMs to Make Relevance Judgments.\narXiv:2409.15133 [cs.IR] https://arxiv.org/abs/2409.15133\n[43] Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin\nChen, Dawei Yin, and Zhaochun Ren. 2023. Is ChatGPT Good at Search?\nInvestigating Large Language Models as Re-Ranking Agents. In Proceedings of\nthe 2023 Conference on Empirical Methods in Natural Language Processing, Houda\nBouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational\nLinguistics, Singapore, 14918â€“14937.\nhttps://doi.org/10.18653/v1/2023.emnlp-main.923\n[44] Yubao Tang, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Wei Chen, and Xueqi\nCheng. 2024. Listwise Generative Retrieval Models via a Sequential Learning\nProcess. ACM Trans. Inf. Syst. 42, 5 (2024), 133:1â€“133:31.\nhttps://doi.org/10.1145/3653712\n[45] Mortimer Taube. 1965. A Note on the Pseudo-Mathematics of Relevance.\nAmerican Documentation 16, 2 (1965), 69â€“72.\nhttps://doi.org/10.1002/asi.5090160204\n[46] Ellen Voorhees. 2000. Variations in Relevance Judgments and the Measurement\nof Retrieval Effectiveness. 36 No. 5 (2000-01-01 2000).\n[47] Ellen M. Voorhees. 1998. Variations in relevance judgments and the\nmeasurement of retrieval effectiveness. In Proceedings of the 21st annual\ninternational ACM SIGIR conference on Research and development in information\nretrieval (SIGIR â€™98). Association for Computing Machinery, New York, NY, USA,\n315â€“323. https://doi.org/10.1145/290941.291017\n[48] Ellen M. Voorhees. 2019. The Evolution of Cranfield. In Information Retrieval\nEvaluation in a Changing World - Lessons Learned from 20 Years of CLEF, Nicola\nFerro and Carol Peters (Eds.). The Information Retrieval Series, Vol. 41. Springer,\n45â€“69. https://doi.org/10.1007/978-3-030-22948-1_2\n[49] Ellen M. Voorhees, Nick Craswell, and Jimmy Lin. 2022. Too Many Relevants:\nWhither Cranfield Test Collections?. In Proceedings of the 45th International ACM\nSIGIR Conference on Research and Development in Information Retrieval (Madrid,\nSpain) (SIGIR â€™22). Association for Computing Machinery, New York, NY, USA,\n2970â€“2980. https://doi.org/10.1145/3477495.3531728\n[50] Ellen M. Voorhees and Donna K. Harman. 1997. Overview of the Sixth Text\nREtrieval Conference (TREC-6). Inf. Process. Manag. 36 (1997), 3â€“35.\nhttps://api.semanticscholar.org/CorpusID:9854240\n[51] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang,\nRangan Majumder, and Furu Wei. 2022. SimLM: Pre-training with\nRepresentation Bottleneck for Dense Passage Retrieval. In Annual Meeting of the\nAssociation for Computational Linguistics.\nhttps://api.semanticscholar.org/CorpusID:250311114\n[52] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang,\nRangan Majumder, and Furu Wei. 2023. SimLM: Pre-training with\nRepresentation Bottleneck for Dense Passage Retrieval. In Proceedings of the 61st\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers). Association for Computational Linguistics, Toronto, Canada, 2244â€“2258.\nhttps://doi.org/10.18653/v1/2023.acl-long.125\n[53] Xing Wu, Guangyuan Ma, Meng Lin, Zijia Lin, Zhongyuan Wang, and Songlin\nHu. 2023. Contextual masked auto-encoder for dense passage retrieval. In\nProceedings of the AAAI Conference on Artificial Intelligence, Vol. 37. 4738â€“4746.\n[54] Justin Zobel. 1998. How reliable are the results of large-scale information\nretrieval experiments?. In Proceedings of the 21st Annual International ACM SIGIR\nConference on Research and Development in Information Retrieval (Melbourne,\nAustralia) (SIGIR â€™98). Association for Computing Machinery, New York, NY,\nUSA, 307â€“314. https://doi.org/10.1145/290941.291014\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20937v1.pdf",
    "total_pages": 11,
    "title": "Variations in Relevance Judgments and the Shelf Life of Test Collections",
    "authors": [
      "Andrew Parry",
      "Maik FrÃ¶be",
      "Harrisen Scells",
      "Ferdinand Schlatt",
      "Guglielmo Faggioli",
      "Saber Zerhoudi",
      "Sean MacAvaney",
      "Eugene Yang"
    ],
    "abstract": "The fundamental property of Cranfield-style evaluations, that system rankings\nare stable even when assessors disagree on individual relevance decisions, was\nvalidated on traditional test collections. However, the paradigm shift towards\nneural retrieval models affected the characteristics of modern test\ncollections, e.g., documents are short, judged with four grades of relevance,\nand information needs have no descriptions or narratives. Under these changes,\nit is unclear whether assessor disagreement remains negligible for system\ncomparisons. We investigate this aspect under the additional condition that the\nfew modern test collections are heavily re-used. Given more possible query\ninterpretations due to less formalized information needs, an ''expiration\ndate'' for test collections might be needed if top-effectiveness requires\noverfitting to a single interpretation of relevance. We run a reproducibility\nstudy and re-annotate the relevance judgments of the 2019 TREC Deep Learning\ntrack. We can reproduce prior work in the neural retrieval setting, showing\nthat assessor disagreement does not affect system rankings. However, we observe\nthat some models substantially degrade with our new relevance judgments, and\nsome have already reached the effectiveness of humans as rankers, providing\nevidence that test collections can expire.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}