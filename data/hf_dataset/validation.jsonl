{"text": "IEEE TRANSACTIONS ON XXX XXXX, XXX, XXX, SEPTEMBER 2023\n1\nL-Lipschitz Gershgorin ResNet Network\nMarius F. R. Juston1, William R. Norris2, Dustin Nottage3, Ahmet Soylemezoglu3\nAbstract—Deep residual networks (ResNets) have demon-\nstrated outstanding success in computer vision tasks, attributed to\ntheir ability to maintain gradient flow through deep architectures.\nSimultaneously, controlling the Lipschitz bound in neural net-\nworks has emerged as an essential area of research for enhancing\nadversarial robustness and network certifiability. This paper uses\na rigorous approach to design L-Lipschitz deep residual networks\nusing a Linear Matrix Inequality (LMI) framework. The ResNet\narchitecture was reformulated as a pseudo-tri-diagonal LMI with\noff-diagonal elements and derived closed-form constraints on\nnetwork parameters to ensure L-Lipschitz continuity. To address\nthe lack of explicit eigenvalue computations for such matrix\nstructures, the Gershgorin circle theorem was employed to ap-\nproximate eigenvalue locations, guaranteeing the LMI’s negative\nsemi-definiteness. Our contributions include a provable param-\neterization methodology for constructing Lipschitz-constrained\nnetworks and a compositional framework for managing recursive\nsystems within hierarchical architectures. These findings enable\nrobust network designs applicable to adversarial robustness,\ncertified training, and control systems. However, a limitation\nwas identified in the Gershgorin-based approximations, which\nover-constrain the system, suppressing non-linear dynamics and\ndiminishing the network’s expressive capacity.\nIndex Terms—Linear Matrix Inequalities, Lipschitz continuity,\nDeep residual networks, Adversarial robustness, Gershgorin\ncircle theorem, semi-definite programming\nI. INTRODUCTION\nT\nHE robustness of deep neural networks (DNNs) is a crit-\nical challenge, mainly when applied in safety-sensitive\ndomains where small adversarial perturbations can lead to\ndangerous situations such as the misclassification of important\nobjects. One approach to address this issue is by enforcing\nLipschitz constraints on the network architectures. These con-\nstraints guarantee that small changes in the input will not cause\nsignificant changes in the output. This property is vital for\ncertifying robustness against adversarial attacks, which involve\nintroducing slight noise to modify the expected classification\noutput result [1], [2]. The Lipschitz constant is a key measure\nto bound the network’s sensitivity to input perturbations.\nSpecifically, a L-Lipschitz network can be theoretically guar-\nanteed that the output remains stable within a defined ”stability\nMarius F. R. Juston1 is with The Grainger College of Engineering,\nIndustrial and Enterprise Systems Engineering Department, University of\nIllinois Urbana-Champaign, Urbana, IL 61801-3080 USA (email: mjus-\nton2@illinois.edu).\nWilliam R Norris2 is with The Grainger College of Engineering, In-\ndustrial and Enterprise Systems Engineering Department, University of\nIllinois Urbana-Champaign, Urbana, IL 61801-3080 USA (email: wrnor-\nris@illinois.edu).\nConstruction Engineering Research Laboratory3, U.S. Army Corps of\nEngineers Engineering Research and Development Center, IL, 61822, USA\nThis research was supported by the U.S. Army Corps of Engineers\nEngineering Research and Development Center, Construction Engineering\nResearch Laboratory.\nsphere” around each input, making it resistant to adversarial\nattacks up to a certain magnitude [3].\nTo achieve this, several methods have been proposed to\nenforce Lipschitz constraints on neural networks, including\nspectral normalization [4], [5], orthogonal parameterization\n[6], and more recent approaches such as Convex Potential\nLayers (CPL) and Almost-Orthogonal Layers (AOL) [6],\n[7]. The previous works have been shown to be formulated\nunder a unifying semi-definitive programming architecture,\nwhich possesses the constraints on the networks as LMIs [8].\nHowever, ensuring Lipschitz constraints in deep architectures,\nparticularly residual networks (ResNets), presents unique chal-\nlenges due to their recursive structure. While prior work has\nmade strides in constraining individual layers [8], [9] and\ngenerating a unifying semi-definite programming approach,\nthe generalized deep residual network formulation presents\nissues in the pseudo-tri-diagonal structure of its imposed LMI.\nFurthermore, multi-layered general Feedforward Neural\nNetworks (FNN) have been shown to generate block tri-\ndiagonal matrix LMi formulations [10] due to their inherent\nnetwork structure, which in contrast to the residual formula-\ntion, yield explicit solutions [11], [12]. However, due to the\noff-diagonal structure of the network, the direct application of\nthe exact eigenvalue computation is not feasible, making the\nsolution process significantly more complex.\nPrevious work has also demonstrated an iterative approach\nby utilizing projected gradient descent optimization or a reg-\nularization term on the estimated Lipschitz constant to ensure\na constraint on the Lipschitz constraint [13]–[15]. While this\nguarantees an iterative enforcement of the Lipschitz constraint,\nit does not ensure a theoretical Lipschitz guarantee across the\nentire network until this convergence. However, the advantage\nof this technique is its generalizability, which allows for the\nutilization of more general network structures.\nA. Contributions\nThis paper introduces the formulation of deep residual net-\nworks as Linear Matrix Inequalities (LMI). It derives closed-\nform constraints on network parameters to ensure theoretical\nL-Lipschitz constraints. The LMI was structured as a tri-\ndiagonal matrix with off-diagonal components, which inher-\nently complicates the derivation of closed-form eigenvalue\ncomputations. To address this limitation, the Gershgorin circle\ntheorem was employed to approximate the eigenvalue loca-\ntions. The Gershgorin circles enabled the derivation of closed-\nform constraints that guaranteed the negative semi-definiteness\nof the LMI.\nAdditionally, this paper demonstrates a significant limita-\ntion of the Gershgorin circle theorem in this context: the\narXiv:2502.21279v1  [cs.LG]  28 Feb 2025\n\n\nIEEE TRANSACTIONS ON XXX XXXX, XXX, XXX, SEPTEMBER 2023\n2\nderived approximations lead to over-constraining the system,\neffectively suppressing the network’s non-linear components.\nThis, in turn, makes the network act as a simple linear\ntransformation instead.\nMoreover, while [8]’s work generates a closed-form solution\nfor a residual network, it is limited to considering a single\ninner layer. In contrast, this paper presents a more general\nformulation that accommodates a more expressive inner layers\nsystem within the residual network system, offering greater\nflexibility and broader applicability.\nII. LMI FORMULATION\nFollowing the works for [8], who defined a Lipschitz\nneural network as a constrained LMI problem to define a\nresidual network, limitations in their approach were identified.\nSpecifically, their formulation resulted in a single-layered\nresidual network, which is inherently less expressive compared\nto the generalized deep-layered residual network popularized\nby architectures such as ResNet and its variants [16]–[20].\nThese deeper networks perform better due to the multiple\ninner layers that compose the modules, which allows for more\ncomplex latent space transformations and thus increases the\nnetwork’s expressiveness. This research focuses on establish-\ning constraints for the inner layers to maintain the L-Lipschitz\ncondition while maximizing the expressiveness of the residual\nnetwork for larger inner layers. As such, the inner layers of\nthe residual network were represented as a recursive system\nof linear equations:\nxk+1 = Akxk + Bkwk,n\nwk,n = σn(Cnwk,n−1 + bn)\n⋮\nwk,1 = σ1(C1xk + b1).\n(1)\nWhere each of the layer parameters were defined as Cl ∈\nRdl×dl−1,bl ∈Rdl for l ∈{1,⋯,n}. When n = 1, the formula-\ntion reduced to the one presented in [8], rendering it redundant\nin its derivation. The goal of the LMI was to maintain the Lip-\nschitz constraint formulated as ∥x′\nk+1 −xk+1∥≤L∥x′\nk −xk∥.\nGiven that this system could be represented as a large\nrecursive system, it was possible to split all the constraints\nof the inner layers as a set of LMI conditions similar to [8],\n[10], [21]. For the most general LMI constraint definition, the\nactivation functions were assumed to not necessarily be the\nReLU function but a general element-wise activation function,\nwhich were L-smooth and m-strongly convex, where Li ≥mi.\nThus, the general activation function quadratic constraint was\nused [8], [10]:\n[ vk −v′\nk\nw′\nk,i −wk,i]\n⊺\n[ −2LimiΛi\n(mi + Li)Λi\n(mi + Li)Λi\n−2Λi\n] [ vk −v′\nk\nw′\nk,i −wk,i] ≤0\n(2)\nwhere Λn must be a positive definitive diagonal matrix.\nGiven that vk −v′\nk = Cn (wk,n−1 −w′\nk,n−1) the inequality thus\nbecomes the following quadratic constraints, where ∆wk,i was\ndefined as ∆wk,i = w′\nk,i −wk,i,\n⎡⎢⎢⎢⎢⎣\nC1 (x′\nk −xk)\n∆wk,1\n⎤⎥⎥⎥⎥⎦\n⊺\n[ −2L1m1Λ1\n(m1 + L1)Λ1\n(m1 + L1)Λ1\n−2Λ1\n]\n⎡⎢⎢⎢⎢⎣\nC1 (x′\nk −xk)\n∆wk,1\n⎤⎥⎥⎥⎥⎦\n≤0,\n⎡⎢⎢⎢⎢⎣\nC2 (∆wk,1)\n∆wk,2\n⎤⎥⎥⎥⎥⎦\n⊺\n[ −2L2m2Λ2\n(m2 + L2)Λ2\n(m2 + L2)Λ2\n−2Λ2\n]\n⎡⎢⎢⎢⎢⎣\nC2 (∆wk,1)\n∆wk,2\n⎤⎥⎥⎥⎥⎦\n≤0,\n⋮\n⎡⎢⎢⎢⎢⎣\nCn (∆wk,n−1)\n∆wk,n\n⎤⎥⎥⎥⎥⎦\n⊺\n[ −2LnmnΛn\n(mn + Ln)Λn\n(mn + Ln)Λn\n−2Λn\n]\n⎡⎢⎢⎢⎢⎣\nCn (∆wk,n−1)\n∆wk,n\n⎤⎥⎥⎥⎥⎦\n≤0.\n(3)\nTo combine the LMIs, a concatenated vector of all the wk,n\nand xk was created to sum all the conditions and solve them\nall together. The following LMI could thus be formulated as\nthe summation in Equation (4). Where,\nDl =\nl\n∑\ni=1\ndi,\n(5)\nEl ∶{0,1}(dl+dl−1)×Dn,\n(6)\n[El]ij =\n⎧⎪⎪⎨⎪⎪⎩\n1\nif j −Dl = i\n0\nelse\n,\nmoreover, i and j represented the row and column, re-\nspectively. The El matrix represented a ”selection” vector\nto ensure that the proper variables were used for the pa-\nrameterization. Which gave the following resultant LMI in\nEquation (7). The question then became what parameterization\nof {Λ1,⋯,Λn},{C1,⋯,Cn}, and B would be needed to ensure\nthat the LMI was indeed negative semi-definitive to satisfy\nthe Lipschitz constraint where ideally {C1,⋯,Cn} would be\nas unconstrained as possible to ensure expressive inner layers.\nFrom the LMI, it could be noticed that it was exceedingly\ncomplex to derive the constraint of the network explicitly\nbased on the eigenvalues of the network. As such, although it\nonly provided loose bounds on the eigenvalues, the Gershgorin\ncircle theorem could be used to derive bounds on the network.\nTheorem II.1. Let A be a complex matrix n × n matrix, with\nentries aij. For i ∈{1,⋯,n} let Ri be the sum of the absolute\nvalues of the non-diagonal entries of the i-th row:\nRi = ∑\nj≠i\n∣aij∣.\n(8)\nLet D(aii,Ri) ⊆C be a closed disc centered at aii with\nradius Ri, every eigenvalue of A lies within at least one of\nthe Gershgorin discs D(aii,Ri).\nThe following corollary was thus derived to generate con-\nditions to ensure the LMI would be negative semi-definitive.\nCorollary 1. If all the Gershgorin discs of a matrix A are\ndefined in the negative real plane, R−, for i ∈{1,⋯,n}\nR(aii + Ri) ≤0, then the matrix A must be negative semi-\ndefinitive.\nThe conditions necessary to ensure that the overall LMI\nmatrix M was negative semi-definite were derived by analyz-\ning its Gershgorin discs. The analysis required demonstrating\nthat all Gershgorin discs were entirely contained within the\nleft half-plane, ensuring that the eigenvalues of M were non-\npositive. Given the structure of the LMI, the matrix could\nbe decomposed into three distinct sections: the first block,\nthe middle blocks, and the last block. For each block, a\ncorresponding set of constraints on the desired parameters was\ndetermined to ensure the feasibility of the problem.\nAs the LMI matrix was symmetric, the Gershgorin discs\nderived from the rows were shown to coincide with those\n\n\nIEEE TRANSACTIONS ON XXX XXXX, XXX, XXX, SEPTEMBER 2023\n3\n⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣\nx′\nk −xk\nw′\nk,1 −wk,1\nw′\nk,2 −wk,2\n⋮\nw′\nk,n−1 −wk,n−1\nw′\nk,n −wk,n\n⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦\n⊺⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣\n0dx\nIdx\n⋮\n0d1\n⋮\n⋮\n⋮\n⋮\n0dn−1\n⋮\nIdx\n0dx\n⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦\n[A⊺\nkAk −L2I\nA⊺\nkBk\nB⊺\nkAk\nB⊺\nkBk]\n⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣\n0dx\nIdx\n⋮\n0d1\n⋮\n⋮\n⋮\n⋮\n0dn−1\n⋮\nIdx\n0dx\n⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦\n⊺⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣\nx′\nk −xk\nw′\nk,1 −wk,1\nw′\nk,2 −wk,2\n⋮\nw′\nk,n−1 −wk,n−1\nw′\nk,n −wk,n\n⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦\n+\nn\n∑\nl=1\n⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣\nx′\nk −xk\nw′\nk,1 −wk,1\nw′\nk,2 −wk,2\n⋮\nw′\nk,n−1 −wk,n−1\nw′\nk,n −wk,n\n⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦\n⊺\nE⊺\ni [Cl\n0\n0\nI]\n⊺\n[ −2LlmlΛl\n(ml + Ll)Λi\n(ml + Ll)Λl\n−2Λl\n] [Cl\n0\n0\nI] Ei\n⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣\nx′\nk −xk\nw′\nk,1 −wk,1\nw′\nk,2 −wk,2\n⋮\nw′\nk,n−1 −wk,n−1\nw′\nk,n −wk,n\n⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦\n≤0,\n(4)\n⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣\nA⊺A −I −2L1m1C⊺\n1 Λ1C1\n(L1 + m1)C⊺\n1 Λ1\n0\n0\n0\nA⊺B\n(L1 + m1)Λ1C1\n−2L2m2C⊺\n2 Λ2C2 −2Λ1\n⋱\n0\n0\n0\n0\n⋱\n⋱\n⋱\n0\n0\n0\n0\n⋱\n⋱\n⋱\n0\n0\n0\n0\n⋱\n−2LnmnC⊺\nnΛnCn −2Λn−1\n(Ln + mn)C⊺\nnΛn\nB⊺A\n0\n0\n0\n(Ln + mn)ΛnCn\nB⊺B −2Λn\n⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦\n⪯0,\n(7)\nderived from the columns. This symmetry allowed the analysis\nto be carried out equivalently from either perspective without\nlosing generality.\nIII. GENERAL LMI SOLUTION\nFor notation, the parameters Sa and Pa were defined as\nSa = La + ma and Pa = Lama to help reduce the notation\nsize.\nA. Last block\nBelow is the derivation of the constraints for the parameters\ndefined in the last block portion of the LMI.\nTheorem III.1. For the parameter Cn, the norm of the rows\nmust be upper bounded by,\n∥Cn,i∥1 <\n2\n∣Ln + mn∣,\n(9)\nwhile λn,i must be lower bounded by,\nλn,i ≥\nb2\ni + ∣ai∣∣bi∣\n2 −∣Ln + mn∣∥Cn,i∥1\n.\n(10)\nProof. The final matrix row block was represented through\nthe parameters where l = n:\n{B⊺A,(Ln + mn)ΛnCn,B⊺B −2Λn}.\nWhich gave the following Gershgorin discs for ∀i{1,⋯,mn}\n(where mx = mn):\nD ⎛\n⎝b2\ni −2λn,i,∣ai∣∣bi∣+\ndn−1\n∑\nj=1\n∣(Ln + mn)cn,i,jλn,i∣⎞\n⎠,\n(11)\nFor which the upper bound constraint was thus:\nϵ3,n,i,max = b2\ni −2λn,i + ∣ai∣∣bi∣+ ∣Sn∣\ndn−1\n∑\nj=1\n∣cn,ijλn,i∣,\n(12)\nApplying the negative-semi definitive constraint, the following\nconstraint was derived:\n0 ≥b2\ni −2λn,i + ∣ai∣∣bi∣+ ∣Ln + mn∣\ndn−1\n∑\nj=1\n∣cn,i,jλn,i∣,\n≥b2\ni + ∣ai∣∣bi∣+ λn,i\n⎛\n⎝∣Ln + mn∣\ndn−1\n∑\nj=1\n∣cn,ij∣−2⎞\n⎠,\nλn,i ≥\nb2\ni + ∣ai∣∣bi∣\n2 −∣Ln + mn∣∑dn−1\nj=1 ∣cn,ij∣\n.\n(13)\nGiven that all λn,i must be positive definitive, the only way\nto ensure this was to ensure that:\ndn−1\n∑\nj=1\n∣cn,ij∣= ∥Cn,i∥1 <\n2\n∣Ln + mn∣.\n(14)\nThus enforcing that all the rows of Cn must be strictly upper\nbound by\n2\n∣Ln+mn∣.\nQED\nB. Middle blocks\nBelow is the derivation of the constraints for the parameters\ndefined in the middle blocks of the LMI.\nTheorem III.2. For all l = 1,⋯,n −1 the parameter Cl must\nhave its row norm be upper bounded by,\n∥Cn,i∥1 <\n2\n∣Ln + mn∣,\n(15)\nand element-wise upper bounded by\n∣cl+1,ji∣≤\n∣Sl+1∣2 + 4∣Pl+1∣\n2(∣Pl+1∣+ Pl+1)∣Sl+1∣.\n(16)\nwhile λl,i must be lower bounded by,\nλl,i ≥∑dl+1\nj=1 λl+1,j (∣Sl+1∣∣cl+1,ji∣−2Pl+1c2\nl+1,ji)\n2 −∣Sl∣∑dl−1\nj=1 ∣cl,ij∣\n+\n2∣Pl+1∣∑dl\nz=1,z≠i ∣∑dl+1\nj=1 λl+1,jcl+1,jicl+1,jz∣\n2 −∣Sl∣∑dl−1\nj=1 ∣cl,ij∣\n.\n(17)\n\n\nIEEE TRANSACTIONS ON XXX XXXX, XXX, XXX, SEPTEMBER 2023\n4\nProof. The set of block matrices that represent the middle\nblock represented the parameters where ∀l{1,⋯,n −1} were:\n{SlΛlCl,−2Pl+1C⊺\nl+1Λl+1Cl+1 −2Λl,Sl+1C⊺\nl+1Λn}.\nWhich gave the following Gershgorin discs, D (aii,Ri), for\n∀i{1⋯ml}∀l{1⋯n −1}:\naii = −2(Ll+1ml+1)\ndl+1\n∑\nj=1\nλl+1,jc2\nl+1,ji −2λl,i,\n(18)\nRi =λl,i∣Ll + ml∣\ndl−1\n∑\nj=1\n∣cl,ij∣\n+ ∣Ll+1 + ml+1∣\ndl+1\n∑\nj=1\nλl+1,j∣cl+1,ji∣\n+ 2∣Ll+1ml+1∣\ndl\n∑\nz=1,z≠i\nRRRRRRRRRRR\ndl+1\n∑\nj=1\nλl+1,jcl+1,jicl+1,jz\nRRRRRRRRRRR\n.\n(19)\nFor which the upper bound constraint was thus:\nϵ2,l,i,max =aii + Ri,\n(20)\n=λl,i\n⎛\n⎝∣Sl∣\ndl−1\n∑\nj=1\n∣cl,ij∣−2⎞\n⎠\n+\ndl+1\n∑\nj=1\nλl+1,j (∣Sl+1∣∣cl+1,ji∣−2Pl+1c2\nl+1,ji)\n+ 2∣Pl+1∣\ndl\n∑\nz=1,z≠i\nRRRRRRRRRRR\ndl+1\n∑\nj=1\nλl+1,jcl+1,jicl+1,jz\nRRRRRRRRRRR\n.\n(21)\nApplying the negative-semi definitive constraint, the following\nconstraint was derived:\nλl,i ≥∑dl+1\nj=1 λl+1,j (∣Sl+1∣∣cl+1,ji∣−2Pl+1c2\nl+1,ji)\n2 −∣Sl∣∑dl−1\nj=1 ∣cl,ij∣\n+\n2∣Pl+1∣∑dl\nz=1,z≠i ∣∑dl+1\nj=1 λl+1,jcl+1,jicl+1,jz∣\n2 −∣Sl∣∑dl−1\nj=1 ∣cl,ij∣\n,\n(22)\nGiven that all λ must be positive definitive and that the\nnumerator and denominator are independent, the following\nconstraints could thus be derived:\ndn−1\n∑\nj=1\n∣cl,ij∣= ∥Cl,i∥1 <\n2\n∣Ll + ml∣.\n(23)\nThus enforcing that all the rows of Cl had to be strictly upper\nbound by\n2\n∣Ll+ml∣.\nIn addition, the numerator was analyzed to ensure that the\nsystem remained positive and definite. A simplified approach\nwas adopted by imposing the following conditions:\n∣Sl+1∣∣cl+1,ji∣≥2Pl+1c2\nl+1,ji,\n∣Sl+1∣≥2Pl+1∣cl+1,ji∣,\n∣cl+1,ji∣≤∣Sl+1∣\n2Pl+1\n.\n(24)\nThe off-diagonal terms were examined to derive a less re-\nstrictive upper bound for the variable Cl. The radius Ri was\nincreased to produce a more conservative estimate, which,\nalthough broader, facilitated the inclusion of the off-diagonal\nterms within the inequality framework.\n=\ndl+1\n∑\nj=1\nλl+1,j (∣Sl+1∣∣cl+1,ji∣−2Pl+1c2\nl+1,ji)\n+ 2∣Pl+1∣\ndl\n∑\nz=1,z≠i\nRRRRRRRRRRR\ndl+1\n∑\nj=1\nλl+1,jcl+1,jicl+1,jz\nRRRRRRRRRRR\n,\n≤\ndl+1\n∑\nj=1\nλl+1,j (∣Sl+1∣∣cl+1,ji∣−2Pl+1c2\nl+1,ji)\n+ 2∣Pl+1∣\ndl\n∑\nz=1,z≠i\ndl+1\n∑\nj=1\nλl+1,j∣cl+1,jicl+1,jz∣,\n=\ndl+1\n∑\nj=1\nλl+1,j (∣Sl+1∣∣cl+1,ji∣−2Pl+1c2\nl+1,ji)\n+ 2∣Pl+1∣\ndl+1\n∑\nj=1\nλl+1,j∣cl+1,ji∣\ndl\n∑\nz=1,z≠i\n∣cl+1,jz∣,\n=\ndl+1\n∑\nj=1\nλl+1,j (∣Sl+1∣∣cl+1,ji∣−2Pl+1c2\nl+1,ji)\n+ 2∣Pl+1∣\ndl+1\n∑\nj=1\nλl+1,j∣cl+1,ji∣(\ndl\n∑\nz=1\n∣cl+1,jz∣−∣cl+1,ji∣),\n≤\ndl+1\n∑\nj=1\nλl+1,j (∣Sl+1∣∣cl+1,ji∣−2Pl+1c2\nl+1,ji)\n+ 2∣Pl+1∣\ndl+1\n∑\nj=1\nλl+1,j∣cl+1,ji∣(\n2\n∣Sl+1∣−∣cl+1,ji∣),\n=\ndl+1\n∑\nj=1\nλl+1,j\n⎡⎢⎢⎢⎢⎣\n∣Sl+1∣∣cl+1,ji∣−2Pl+1c2\nl+1,ji\n+ 2∣Pl+1∣∣cl+1,ji∣(\n2\n∣Sl+1∣−∣cl+1,ji∣)\n⎤⎥⎥⎥⎥⎦\n.\n(25)\nWhere the inner term needed to be constrained:\n0 ≤∣Sl+1∣∣cl+1,ji∣−2Pl+1c2\nl+1,ji\n+ 2∣Pl+1∣∣cl+1,ji∣(\n2\n∣Sl+1∣−∣cl+1,ji∣),\n=∣Sl+1∣∣cl+1,ji∣−2Pl+1c2\nl+1,ji\n+ 2∣Pl+1∣(2∣cl+1,ji∣\n∣Sl+1∣\n−c2\nl+1,ji),\n=∣Sl+1∣∣cl+1,ji∣+ 4∣Pl+1∣\n∣Sl+1∣∣cl+1,ji∣\n−2(Pl+1 + ∣Pl+1∣)c2\nl+1,ji,\n0 ≤∣Sl+1∣+ 4∣Pl+1∣\n∣Sl+1∣−2(Pl+1 + ∣Pl+1∣)∣cl+1,ji∣\n∣cl+1,ji∣≤\n∣Sl+1∣2 + 4∣Pl+1∣\n2(∣Pl+1∣+ Pl+1)∣Sl+1∣.\n(26)\nGiven the additional element-wise constraint, it was observed\nthat there were two situations in which the constraint became\nirrelevant. The first scenario occurred when Ll+1ml+1 ≤0,\nand the second arose when Ll+1 + ml+1 = 0. This condition\nwas satisfied, for instance, in the case of a ReLU activation\nfunction, where L = 1 and m = 0.\nQED\n\n\nIEEE TRANSACTIONS ON XXX XXXX, XXX, XXX, SEPTEMBER 2023\n5\nC. First layer\nFinally, below is the derivation of the constraints for the\nparameters defined in the first-row block of the LMI.\nTheorem III.3. The parameter C1 must be element-wise\nupper bounded by\n∣c1,ji∣≤\n(L2 −a2\ni −∣ai∣∣bi∣)∣S1∣\nd1λ1,j (∣S1∣2 + 4∣P1∣)\n,\n(27)\nProof. This block represented the parameters where l = 1:\n{A⊺A −L2I −2L1m1C⊺\n1 Λ1C1,(L1 + m1)C⊺\n1 Λ1,A⊺B}.\nWhich gave the following Gershgorin discs for ∀i{1⋯mx}\n(where mx = mn):\naii =a2\ni −L2 −2(L1m1)\nd1\n∑\nj=1\nλ1,jc2\n1,ji,\n(28)\nRi =∣ai∣∣bi∣+ ∣L1 + m1∣\nd1\n∑\nj=1\nλ1,j∣c1,ji∣\n+ 2∣L1m1∣\ndx\n∑\nz=1,z≠i\nRRRRRRRRRRR\nd1\n∑\nj=1\nλ1,jc1,jic1,jz\nRRRRRRRRRRR\n.\n(29)\nFor which the upper bound constraint was thus:\nϵ1,1,i,max =a2\ni −L2 −2P1\nd1\n∑\nj=1\nλ1,jc2\n1,ji + ∣ai∣∣bi∣\n+ ∣S1∣\nd1\n∑\nj=1\nλ1,j∣c1,ji∣\n+ 2∣P1∣\ndx\n∑\nz=1,z≠i\nRRRRRRRRRRR\nd1\n∑\nj=1\nλ1,jc1,jic1,jz\nRRRRRRRRRRR\n.\n(30)\nApplying the negative-semi definitive constraint, the following\nconstraint was derived:\n0 ≥a2\ni −L2 −2P1\nd1\n∑\nj=1\nλ1,jc2\n1,ji + ∣ai∣∣bi∣+ ∣S1∣\nd1\n∑\nj=1\nλ1,j∣c1,ji∣\n+ 2∣P1∣\ndx\n∑\nz=1,z≠i\nRRRRRRRRRRR\nd1\n∑\nj=1\nλ1,jc1,jic1,jz\nRRRRRRRRRRR\n,\n≤a2\ni −L2 −2P1\nd1\n∑\nj=1\nλ1,jc2\n1,ji + ∣ai∣∣bi∣+ ∣S1∣\nd1\n∑\nj=1\nλ1,j∣c1,ji∣\n+ 2∣P1∣\ndx\n∑\nz=1,z≠i\nd1\n∑\nj=1\nλ1,j∣c1,jic1,jz∣,\n=a2\ni −L2 −2P1\nd1\n∑\nj=1\nλ1,jc2\n1,ji + ∣ai∣∣bi∣+ ∣S1∣\nd1\n∑\nj=1\nλ1,j∣c1,ji∣\n+ 2∣P1∣\nd1\n∑\nj=1\nλ1,j∣c1,ji∣\ndx\n∑\nz=1,z≠i\n∣c1,jz∣,\n=a2\ni −L2 + ∣ai∣∣bi∣+\nd1\n∑\nj=1\nλ1,j\n⎛\n⎝∣S1∣∣c1,ji∣−2P1c2\n1,ji\n+ 2∣P1∣∣c1,ji∣\ndx\n∑\nz=1,z≠i\n∣c1,jz∣⎞\n⎠,\n≤a2\ni −L2 + ∣ai∣∣bi∣+\nd1\n∑\nj=1\nλ1,j\n⎡⎢⎢⎢⎢⎣\n∣S1∣∣c1,ji∣−2P1c2\n1,ji\n+ 2∣P1∣∣c1,ji∣( 2\n∣S1∣−∣cl+1,ji∣)\n⎤⎥⎥⎥⎥⎦\n,\n=\nd1\n∑\nj=1\n⎡⎢⎢⎢⎢⎣\na2\ni −L2 + ∣ai∣∣bi∣\nd1\n+ λ1,j\n⎛\n⎝∣S1∣∣c1,ji∣−2P1c2\n1,ji\n+ 2∣P1∣∣c1,ji∣( 2\n∣S1∣−∣c1,ji∣)⎞\n⎠\n⎤⎥⎥⎥⎥⎦\n. (31)\nThe constraint L1m1 ≤0 was enforced to ensure the solvability\nof the system. Consequently, the system of equations was\nformulated as follows:\n0 ≥a2\ni −L2 + ∣ai∣∣bi∣\nd1\n+ λ1,j\n⎛\n⎝∣S1∣∣c1,ji∣−2P1c2\n1,ji\n+ 2∣P1∣∣c1,ji∣( 2\n∣S1∣−∣c1,ji∣)⎞\n⎠,\n=a2\ni −L2 + ∣ai∣∣bi∣\nd1\n+ λ1,j (∣S1∣∣c1,ji∣+ 4∣P1∣\n∣S1∣∣c1,ji∣),\n∣c1,ji∣≤L2 −a2\ni −∣ai∣∣bi∣\nd1λ1,j\n1\n∣S1∣+ 4∣P1∣\n∣S1∣\n,\n=\n(L2 −a2\ni −∣ai∣∣bi∣)∣S1∣\nd1λ1,j (∣S1∣2 + 4∣P1∣)\n.\n(32)\nWhich then induced the inequality that L2 −a2\ni −∣ai∣∣bi∣≥0,\nwhich in turn gave the consrtaints that ai ∈(−L,L) with ∣bi∣<\nL2−a2\ni\n∣ai∣. This thus completed the LMI constraints.\nQED\nGiven all the derived constraints, the complete set of con-\nstraints of the neural network was listed in Table II.\nThe generalized versions of the equations could additionally\nbe presented in matrix forms in Table II, where the absolute\nvalue function is applied element-wise, i.e., ∣A∣= {∣aij∣}, for\nsimplified computation and practicality, the diagonal matrices\nΛl,A,B are represented as column vectors where the elements\nare the diagonal values.\nD. Weighted norm constraint\nFor a weighted ℓ1-norm, it was desired to derive an unpa-\nrameterized optimization formulation scheme for xi while en-\nsuring the system remained upper bounded. Where vi > 0,∀i.\na ≤∥x∥1,v =\nn\n∑\ni=1\nvi∣xi∣\n(33)\nThe reparameterization ∣xi∣≤± a\nn\n1\nvi was introduced, such that:\na ≤\nn\n∑\ni=1\nvi∣xi∣≤\nn\n∑\ni=1\nvi∣a\nn\n1\nvi\n∣= a\nn\nn\n∑\ni=1\n1 = a\n(34)\nWhere the constraint,\nxi = a\nn\n1\nvi\npi,\n(35)\nwhere pi ∈(−1,1). As such, to parameterize xi, the op-\ntimization parameter for the network becomes optimizing\n\n\nIEEE TRANSACTIONS ON XXX XXXX, XXX, XXX, SEPTEMBER 2023\n6\nTable I: General LMI Condensed Constraints\nParameter\nInequality\nConstraint\nIndexing\nSl\n=\nLl + ml\n∀l{1, ⋯, n}\nPl\n=\nLlml\n∀l{1, ⋯, n}\nλn,i\n≥\nb2\ni +aibi\n2−∣Sn∣∑\ndn−1\nj=1\n∣cn,ij∣\n∀i{1, ⋯, dn}\n∥Cn,i∥1\n<\n2\n∣Sl∣\n∀i{1, ⋯, dl}∀l{1, ⋯, n}\nλl,i\n≥\n∑\ndl+1\nj=1\nλl+1,j(∣Sl+1∣∣cl+1,ji∣−2Pl+1c2\nl+1,ji)+2∣Pl+1∣∑\ndl\nz=1,z≠i ∣∑\ndl+1\nj=1\nλl+1,jcl+1,jicl+1,jz∣\n2−∣Sl∣∑\ndl−1\nj=1\n∣cl,ij∣\n∀i{1, ⋯, dl}∀l{1, ⋯, n −1}\n∣cl,ij∣\n≤\n∣Sl∣2+4∣Pl∣\n2(∣Pl∣+Pl)∣Sl∣\n∀i{1, ⋯, dl}∀j{1, ⋯, dl−1}∀l{2, ⋯, n}\n∣c1,ij∣\n≤\n(L2−a2\ni −aibi)∣S1∣\nd1λ1,i(∣S1∣2+4∣P1∣)\n∀i{1, ⋯, dx}\n∣ai∣\n∈\n(0, L)\n∀i{1, ⋯, dx}\n∣bi∣\n∈\n[0, L2−a2\ni\n∣ai∣\n)\n∀i{1, ⋯, dx}\nTable II: General LMI Matrix Condensed Constraints\nParameter\nInequality\nConstraint\nIndexing\nSl\n=\nLl + ml\nPl\n=\nLlml\nDl\n=\n[∥Cl,1∥1\n⋯\n∥Cl,ml∥1]\n⊺\n∀l{1, ⋯, n}\nΛn\n≥\nB2+∣A∣∣B∣\n2−∣Sn∣Dn\nDl\n<\n2\n∣Sl∣\n∀l{1, ⋯, n}\nQl\n=\nC⊺\nl diag(Λl)Cl\nΛl\n≥\nΛ⊺\nl+1(∣Sl+1∣∣Cl+1∣−2Pl+1C○2\nl+1)+2∣Pl+1∣1⊺∣Ql+1−diag(diag(Ql+1))∣\n2−∣Sl∣Dl\n∀l{1, ⋯, n −1}\n∣Cl∣\n≤\n∣Sl∣2+4∣Pl∣\n2(∣Pl∣+Pl)∣Sl∣\n∀l{2, ⋯, n}\n∣C1∣\n≤\n∣S1∣(L2−A2−∣A∣∣B∣)\nd1(∣S1∣2+4∣P1∣)\nΛ−1\n1\n∣A∣\n∈\n(0, L)\n∣B∣\n∈\n[0, L2\n∣A∣−∣A∣)\npi, where pi = tanh(wi), where wi was an unconstrained\noptimization parameter. As demonstrated by the normalization\nfactor ∂xi\n∂pi ∝O( 1\nnvi ), which implied that the gradients of xi\nbecame proportionally smaller as the dimension of the vector\nbecame smaller. Small or vanishing gradients could cause\nproblems for large and deeper networks.\nE. Elementwise vs. row constraint bound switching\n1) Cl constraints: For the matrices Cl with l ∈{2,⋯,n},\ntwo simultaneous constraints were imposed on the system: the\nrow-wise and element-wise constraints. In this context, the\nobjective was to derive the upper bounds for the values of Cl\nthat the optimization would be based on.\nThe following constraint was derived from the norm con-\nstraints established in the unparameterized optimization for-\nmulation given by Equation (35), where the upper bound was\nset as a =\n2\n∣Sl∣and vi = 1. The goal, therefore, was to identify\nthe conditions under which the row-wise element constraint\nwould dominate over the overall element-wise constraint.\n2\n∣Sl∣dl−1\n≤\n∣Sl∣2 + 4∣Pl∣\n2(∣Pl∣+ Pl)∣Sl∣,\ndl−1 ≥4(∣Pl∣+ Pl)\n∣Sl∣2 + 4∣Pl∣\n.\n(36)\nThis thus informed us that when Pl ≤0 (∀l,dl ≥0), the\nelement-wise constraint would always be greater than the\nelement-wise, and if Pl > 0 and,\ndl−1 ≥\n8Pl\n∣Sl∣2 + 4Pl\n.\n(37)\nBy examining the maximum value of the bound, it was found\nthat, due to the equation’s symmetry concerning Ll and ml,\nsolving for either the optimal value of ml or Ll led to the\noptimal solution. This symmetry implied that both parameters\ncontributed equivalently to the system, and thus, optimizing\none in isolation was sufficient to determine the overall optimal\nconfiguration.\n∂\n∂ml\n8Llml\n(Ll + ml)2 + 4Llml\n= 8Ll(Ll −ml)(Ll + ml)\n(L2\nl + 6Llml + m2\nl )\n2 ,\n(38)\nsolving for 0 the optimal value was obtained when ml =\n{−Ll,Ll}, where only the ml = Ll solution was kept due\nto the Pl > 0 constraint. Which gave the solution that (when\nml = Ll, Sl = 2L,Pl = L2\nl ):\n2\n∣Sl∣dl−1\n≥\n∣Sl∣2 + 4∣Pl∣\n2(∣Pl∣+ Pl)∣Sl∣,\n1\n∣Ll∣dl−1\n≥4L2\nl + 4L2\nl\n8L2\nl ∣Ll∣,\n\n\nIEEE TRANSACTIONS ON XXX XXXX, XXX, XXX, SEPTEMBER 2023\n7\n1\n∣Ll∣dl−1\n≥\n1\n∣Ll∣.\n(39)\nThis demonstrated that even in the specific condition when\nml = Ll and dl ≤1, the element and row-wise constraints\nwould be equivalent to each other. This implied that the row-\nwise constraint would always be smaller than the element-wise\nconstraint and should thus be the only one considered when\nconstraining Cl for ∀{2,⋯,n}.\n2) C1 constraints: Upon analyzing the constraints derived\nfor C1, it was observed that a mutual dependence existed\nbetween C1 and Λ1. Specifically, the definition of C1 ne-\ncessitated the prior specification of Λ1, and conversely, the\ndetermination of Λ1 was contingent upon the specification of\nC1. This interdependence introduced significant complexity in\nderiving an appropriate parameterization for C1. As a result,\nan additional constraint was imposed on C1 to address this\nissue, such that:\n∣Sl∣\ndx\n∑\nj=1\n∣c1,ij∣≤1,\n(40)\nWhich thus enforced the constraint that,\nGl =Λ⊺\nl (∣Sl∣∣Cl∣−2PlC○2\nl )\n+ 2∣Pl∣1⊺∣Ql −diag(diag(Ql))∣,\n(41)\nλ1,i ≥\nG2\n2 −∣Sl∣∑dx\nj=1 ∣c1,ij∣\n≥G2,\n(42)\nwhere, Gl represented the numerator of the Λl parameteri-\nzation. Enforcing this additional constraint on the row norm\nof C1 thus imposed an upper bound of Λ1, which no longer\ncontained a dependence on C1, breaking the cyclical parame-\nterization.\nF. LMI parameterization\nThe eigenvalue distribution of the LMI was displayed below\nin Figure 1 (The eigenvalue range was truncated to 10 times\nthe quartile range; however, some of the eigenvalues have\nreached a magnitude of −1011). To generate this distribution,\nall the parameters, the weights Cl ( parameterized by pl), and\nthe biases bl were initialized with a uniform distribution. The\nbiases and weights were initialized using the standard Kaiming\ninitialization scheme, where the weights used tanh gains (i.e.,\nscaling of 1 for tanh [22]) given that the variables pl were\nconstrained by tanh.\nThe constraints above, when implemented, thus generated\nthe following example of Gershgorin circles for the LMI\nillustrated in Figure 2. The Figure demonstrates that the\nGershgorin circles were all constrained on the negative real\nplane.\nIt was also interesting to observe that due to the recursive\nnature of the Λl parameterization, the Gershgorin circles ended\nup encapsulating each other most of the time (this is not a\ngeneral statement).\nFor the sake of completeness, the L and m constants of\nthe activation functions defined in PyTorch (assuming default\nvalues if not specified) were derived and defined in Table III.\nIt should be noted that the Hardshrink and RReLU could not\nFigure 1: Eigenvalue distribution\nFigure 2: LMI Gershgorin Circles\nbe used due to their infinite L,m constants; Hardshink has\ninfinite L,m due to its noncontinuous piece-wise definition,\nand PReLU due to its stochastic definition, which no longer\nmade it’s L,m computable. Where,\nerfc(z) = 1 −erf(z),\n(43)\nerf(z) =\n2\n√π ∫\nz\n0 e−t2dt.\n(44)\nIV. EXPERIMENT\nBased on the designed network constraints, a network\nwas thus generated. To run such a network due to the co-\ndependence of the Λl,Ll and ml from the next layer and the\nfirst layer, the evaluation of the network needed to be run in\ntwo passes, a backward pass which computed the Λl and Cl\nparameters, as illustrated in Figure 3, and then, the forward\npass performed the inferences using the computed parameters\nas a standard residual network as illustrated in 4. It should\nbe noted that it was not possible to make use of techniques\n\n\nIEEE TRANSACTIONS ON XXX XXXX, XXX, XXX, SEPTEMBER 2023\n8\nTable III: Convexity constants of the element-wise activation functions in PyTorch\nActivation Function\nL\nm\nS\nP\nELU (α = 1) [23]\nmax(1, α)\n0\nmax(1, α)\n0\nHardshrink [24]\n∞\n0\n∞\n∞\nHardsigmoid [25]\n1\n6\n0\n1\n6\n0\nHardtanh [26]\n1\n0\n1\n0\nHardswish [27]\n1.5\n-0.5\n1\n-0.75\nLeakyReLU (α = 1e−2) [28]\n1\nα\n1 + α\nα\nLogSigmoid\n1\n0\n1\n0\nPReLU (α = 1\n4 ) [29]\n1\nα\n1 + α\nα\nReLU [30]\n1\n0\n1\n0\nReLU6 [31]\n1\n0\n1\n0\nRReLU [32]\n∞\n−∞\n∞\n∞\nSELU [33]\nα × scale ≈1.758099341\n0\nα × scale ≈1.758099341\n0.0\nCELU [34]\n1\n0\n1\n0\nGELU [35]\nerfc(1)\n2\n−\n1\ne√π\n1\n2 (erf(1) + 1) +\n1\ne√π\n1\n(e√π(erf(1)+1)+2)(e√πerfc(1)−2)\n4e2π\n≈1.128904145\n≈−0.1289041452\n≈−0.145520424\nSigmoid [36]\n1\n0\n1\n0\nSiLU [37]\n1.099839320\n-0.09983932013\n1\n-0.1098072100\nSoftplus [38]\n1\n0\n1\n0\nMish (α ≥1\n2 ) [39]\n1.199678640\n-0.2157287822\n0.8060623125\n-0.2204297485\nSoftshrink [24]\n1\n0\n1\n0\nSoftsign [40]\n1\n0\n1\n0\nTanh [36]\n1\n0\n1\n0\nTanhshrink\n1\n0\n1\n0\nThreshold\n1\n0\n1\n0\nFigure 3: Backwards pass\nFigure 4: Forward pass\nsuch as a batch normalization [17], [41], [42], which is a\ncommon practice in more modern ResNet architectures. This\nwas because normalization was not a constrained L-Lipschitz\nformulation as the normalized features were computed as [13],\n[43]:\nˆx(k) = x(k) −E[x(k)]\n√\nVar[x(k)]\n,\n(45)\nWhich could be represented as a linear layer where,\nCb = diag(\n√\nVar[x(1)],⋯,\n√\nVar[x(d)])\n−1\n,\n(46)\nbb = −[\nE[x(1)]\n√\nVar[x(1)]\n⋯\nE[x(d)]\n√\nVar[x(d)]]\n⊺\n.\n(47)\nWhere it would only be in particular conditions that the batch\nnormalization would follow the constraints posed by Table II;\nthis is due to the variance scaling term being very hard to\ncontrol and is defined by the dataset that is inputted into the\nsystem.\nTo test the network’s capabilities, it was initially tested\non a straightforward dataset to fit y =\n1\n2 sin(x) on x ∈\n(−2π,2π), which is a\n1\n2\nLipchitz bounded function as\narg maxx ∂\n∂x\n1\n2 sin(x) = 1\n2arg maxx cos(x) = 1\n2, which should\nthus make it possible to train the network on. However, it\nwas noticed that no matter what optimizer, activation function,\nsize or number of hidden layers, learning rate, or other hyper-\nparameters used, the system would be unable to fit the function\nto any degree of accuracy using the MSE loss function. This\n\n\nIEEE TRANSACTIONS ON XXX XXXX, XXX, XXX, SEPTEMBER 2023\n9\nis illustrated from the output results in Figures 5 and 6.\nFigure 5: Trained L-Lipschitz network output over multiple\noptimizers\nFigure 6: MSE training loss over multiple optimizers\nThe network’s output looked like a single regression line, and\nno expressiveness of the network could be observed. For a\nminimum error to fit with this line, it would have to be a line\ndefined as:\nLl = 1\n4π ∫\n2π\n−2π (ax + b −sin(x))2dx,\n= 1\n6 (4a(2π2a + 3) + 6b2 + 3).\n(48)\nWhose minimum would be defined when a = −\n3\n4π2 and b = 0,\nwith a total error of Ll = 1\n2 −\n3\n4π2 . After further inspection\nof the network, the main culprit in the decay issue was C1’s\nmagnitude as the Λ1 magnitude in the network became very\nlarge, which caused an over-constraining of the C1 matrix\nparameter. Given that the Gershgorin circle theorem is only\nan approximation of the eigenvalue locations, this caused\nthe overall network’s compounding approximations to over-\nconstrain the network and thus disable the non-linear portion\nof the system as such, the network comes the simple y ≈Ax\nformula, where A is the parameterized diagonal matrix. It is\nthus sadly noted that this type of network with the current\ntype of parameterization for the weights and biases of the\nsystem does not function as a universal function approximator.\nAs such this paper is only able to elaborate on the current\nmethodology for solving the LMI using the Gershgorin circle\nfor more complicated general LMI structures; however, it\nshould be noted that if the LMI follows a more standard\nmatrix structure such as a tri-diagonal form [10] common in\na standard Feedforward Neural Network (FNN) or the likes it\nis possible to derive more exact eigenvalue constraints on the\nsystem.\nV. CONCLUSION\nThis study rigorously derived constraints for the pseudo-tri-\ndiagonal matrix LMI representing a residual network. Given\nthe absence of explicit eigenvalue computations for the tri-\ndiagonal matrix with off-diagonal elements, the Gershgorin\ncircle theorem was employed to approximate the eigenvalue\nlocations of this complex recursive system. The system was\ndecomposed into three distinct blocks, and weight parameter-\nizations were systematically derived and summarized in Table\nII.\nA two-step process was detailed once the constraints were\nestablished and the network was constructed. Due to the resid-\nual network’s recursive nature, the normalization parameters\nneeded to be computed and propagated in advance to enable\nthe creation of layer weight parameterizations. This stage was\ndefined as the backward pass. The forward pass involved\nperforming inference on the network.\nUpon evaluating the implemented network, it was observed\nthat the Gershgorin circle approximations caused the normal-\nization factors of the inner layers to deactivate the network’s\nnon-linear components. Consequently, based on the Gersh-\ngorin formulation, the final implementation proved ineffective\nand unsuitable as a functional approximation. This study\nestablishes a foundation for future research into alternative\neigenvalue approximations and refined parameterization strate-\ngies, advancing robust deep learning architectures’ theoretical\nand practical development.\nREFERENCES\n[1] M. Inkawhich, Y. Chen, and H. Li, “Snooping attacks on deep\nreinforcement\nlearning,”\nProceedings\nof\nthe\nInternational\nJoint\nConference on Autonomous Agents and Multiagent Systems, AAMAS,\nvol. 2020-May, pp. 557–565, 5 2019. [Online]. Available: https:\n//arxiv.org/abs/1905.11832v2\n[2] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing\nadversarial examples,” 3rd International Conference on Learning\nRepresentations, ICLR 2015 - Conference Track Proceedings, 12 2014.\n[Online]. Available: https://arxiv.org/abs/1412.6572v3\n[3] Y. Tsuzuku, I. Sato, and M. Sugiyama, “Lipschitz-margin training:\nScalable\ncertification\nof\nperturbation\ninvariance\nfor\ndeep\nneural\nnetworks,” CoRR, vol. abs/1802.04034, 2018. [Online]. Available:\nhttp://arxiv.org/abs/1802.04034\n[4] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida, “Spectral\nnormalization for generative adversarial networks,” 6th International\nConference on Learning Representations, ICLR 2018 - Conference\nTrack Proceedings, 2 2018. [Online]. Available: https://arxiv.org/abs/\n1802.05957v1\n\n\nIEEE TRANSACTIONS ON XXX XXXX, XXX, XXX, SEPTEMBER 2023\n10\n[5] P. L. Bartlett, D. J. Foster, and M. Telgarsky, “Spectrally-normalized\nmargin bounds for neural networks,” Advances in Neural Information\nProcessing Systems, vol. 2017-December, pp. 6241–6250, 6 2017.\n[Online]. Available: https://arxiv.org/abs/1706.08498v2\n[6] B. Prach and C. H. Lampert, “Almost-orthogonal layers for efficient\ngeneral-purpose lipschitz networks,” 8 2022. [Online]. Available:\nhttps://arxiv.org/abs/2208.03160v2\n[7] L.\nMeunier,\nB.\nJ.\nDelattre,\nA.\nAraujo,\nand\nA.\nAllauzen,\n“A\ndynamical system perspective for lipschitz neural networks,” pp.\n15 484–15 500, 6 2022. [Online]. Available: https://proceedings.mlr.\npress/v162/meunier22a.html\n[8] A. Araujo, A. Havens, B. Delattre, A. Allauzen, and B. Hu, “A unified\nalgebraic perspective on lipschitz neural networks,” 3 2023. [Online].\nAvailable: http://arxiv.org/abs/2303.03169\n[9] L. Meunier, B. Delattre, A. Araujo, and A. Allauzen, “A dynamical\nsystem perspective for lipschitz neural networks,” Proceedings of\nMachine Learning Research, vol. 162, pp. 15 484–15 500, 10 2021.\n[Online]. Available: https://arxiv.org/abs/2110.12690v2\n[10] Y. Xu and S. Sivaranjani, “Eclipse: Efficient compositional lipschitz\nconstant estimation for deep neural networks,” 4 2024. [Online].\nAvailable: https://arxiv.org/abs/2404.04375v2\n[11] A.\nSandryhaila\nand\nJ.\nM.\nF.\nMoura,\n“Eigendecomposition\nof\nblock\ntridiagonal\nmatrices,”\n6\n2013.\n[Online].\nAvailable:\nhttps:\n//arxiv.org/abs/1306.0217v1\n[12] E. Agarwal, S. Sivaranjani, V. Gupta, and P. Antsaklis, “Sequential\nsynthesis of distributed controllers for cascade interconnected systems,”\nProceedings of the American Control Conference, vol. 2019-July, pp.\n5816–5821, 7 2019.\n[13] H. Gouk, E. Frank, B. Pfahringer, and M. J. Cree, “Regularisation\nof\nneural\nnetworks\nby\nenforcing\nlipschitz\ncontinuity,”\nMachine\nLearning,\nvol.\n110,\npp.\n393–416,\n2\n2021.\n[Online].\nAvailable:\nhttp://link.springer.com/10.1007/s10994-020-05929-w\n[14] S. Aziznejad, H. Gupta, J. Campos, and M. Unser, “Deep neural\nnetworks with trainable activations and controlled lipschitz constant,”\nIEEE\nTransactions\non\nSignal\nProcessing,\nvol.\n68,\npp.\n4688–\n4699, 1 2020. [Online]. Available: http://arxiv.org/abs/2001.06263http:\n//dx.doi.org/10.1109/TSP.2020.3014611\n[15] J. Bear, A. Pr¨ugel-Bennett, and J. Hare, “Rethinking deep thinking:\nStable learning of algorithms using lipschitz constraints,” 10 2024.\n[Online]. Available: https://arxiv.org/abs/2410.23451v1\n[16] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning\nfor image recognition,” Proceedings of the IEEE Computer Society\nConference\non\nComputer\nVision\nand\nPattern\nRecognition,\nvol.\n2016-December, pp. 770–778, 12 2015. [Online]. Available: https:\n//arxiv.org/abs/1512.03385v1\n[17] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi, “Inception-v4,\ninception-resnet and the impact of residual connections on learning,”\n31st AAAI Conference on Artificial Intelligence, AAAI 2017, pp. 4278–\n4284, 2 2016. [Online]. Available: https://arxiv.org/abs/1602.07261v2\n[18] S. Zagoruyko and N. Komodakis, “Wide residual networks,” British\nMachine Vision Conference 2016, BMVC 2016, vol. 2016-September,\npp. 87.1–87.12, 5 2016. [Online]. Available: https://arxiv.org/abs/1605.\n07146v4\n[19] J. Hu, L. Shen, S. Albanie, G. Sun, and E. Wu, “Squeeze-and-excitation\nnetworks,” IEEE Transactions on Pattern Analysis and Machine\nIntelligence, vol. 42, pp. 2011–2023, 9 2017. [Online]. Available:\nhttps://arxiv.org/abs/1709.01507v4\n[20] S. Xie, R. Girshick, P. Doll´ar, Z. Tu, and K. He, “Aggregated\nresidual transformations for deep neural networks,” Proceedings - 30th\nIEEE Conference on Computer Vision and Pattern Recognition, CVPR\n2017, vol. 2017-January, pp. 5987–5995, 11 2016. [Online]. Available:\nhttps://arxiv.org/abs/1611.05431v2\n[21] M. Fazlyab, A. Robey, H. Hassani, M. Morari, and G. J. Pappas, Efficient\nand accurate estimation of lipschitz constants for deep neural networks.\nRed Hook, NY, USA: Curran Associates Inc., 2019.\n[22] S. K. Kumar, “On weight initialization in deep neural networks,” 4\n2017. [Online]. Available: https://arxiv.org/abs/1704.08863v2\n[23] D.-A. Clevert, T. Unterthiner, and S. Hochreiter, “Fast and accurate\ndeep\nnetwork\nlearning\nby\nexponential\nlinear\nunits\n(elus),”\n4th\nInternational Conference on Learning Representations, ICLR 2016\n-\nConference\nTrack\nProceedings,\n11\n2015.\n[Online].\nAvailable:\nhttp://arxiv.org/abs/1511.07289\n[24] H. F. Cancino-De-Greiff, R. Ramos-Garcia, and J. V. Lorenzo-Ginori,\n“Signal de-noising in magnetic resonance spectroscopy using wavelet\ntransforms,” Concepts in Magnetic Resonance, vol. 14, pp. 388–401, 1\n2002. [Online]. Available: https://onlinelibrary.wiley.com/doi/10.1002/\ncmr.10043\n[25] M. Courbariaux, Y. Bengio, and J. P. David, “Binaryconnect: Training\ndeep neural networks with binary weights during propagations,”\nAdvances in Neural Information Processing Systems, vol. 2015-January,\npp. 3123–3131, 11 2015. [Online]. Available: https://arxiv.org/abs/1511.\n00363v3\n[26] R. Collobert, “Large scale machine learning,” Ph.D. dissertation, Uni-\nversit´e de Paris VI, 2004.\n[27] A. Howard, M. Sandler, B. Chen, W. Wang, L. C. Chen, M. Tan,\nG. Chu, V. Vasudevan, Y. Zhu, R. Pang, Q. Le, and H. Adam,\n“Searching for mobilenetv3,” Proceedings of the IEEE International\nConference on Computer Vision, vol. 2019-October, pp. 1314–1324, 5\n2019. [Online]. Available: https://arxiv.org/abs/1905.02244v5\n[28] A. L. Maas, Y. H. Awni, and A. Y. Ng, “Rectifier nonlinearities improve\nneural network acoustic models,” Proceedings of the 30th International\nConference on Machine Learning, vol. 28, 6 2013.\n[29] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectifiers:\nSurpassing\nhuman-level\nperformance\non\nimagenet\nclassification,”\nCoRR,\nvol.\nabs/1502.01852,\n2\n2015.\n[Online].\nAvailable:\nhttp:\n//arxiv.org/abs/1502.01852\n[30] W.\nS.\nMcCulloch\nand\nW.\nPitts,\n“A\nlogical\ncalculus\nof\nthe\nideas immanent in nervous activity,” The Bulletin of Mathematical\nBiophysics,\nvol.\n5,\npp.\n115–133,\n12\n1943.\n[Online].\nAvailable:\nhttps://link.springer.com/article/10.1007/BF02478259\n[31] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,\nT. Weyand, M. Andreetto, and H. Adam, “Mobilenets: Efficient\nconvolutional neural networks for mobile vision applications,” 4 2017.\n[Online]. Available: http://arxiv.org/abs/1704.04861\n[32] B. Xu, N. Wang, T. Chen, and M. Li, “Empirical evaluation of rectified\nactivations in convolutional network,” 5 2015. [Online]. Available:\nhttp://arxiv.org/abs/1505.00853\n[33] G. Klambauer, T. Unterthiner, A. Mayr, and S. Hochreiter, “Self-\nnormalizing neural networks,” Advances in Neural Information Process-\ning Systems, vol. 30, 2017.\n[34] J. T. Barron, “Continuously differentiable exponential linear units,” 4\n2017. [Online]. Available: http://arxiv.org/abs/1704.07483\n[35] D. Hendrycks and K. Gimpel, “Gaussian error linear units (gelus),” 6\n2016. [Online]. Available: https://arxiv.org/abs/1606.08415v5\n[36] H. Sak, A. Senior, and F. Beaufays, “Long short-term memory based\nrecurrent neural network architectures for large vocabulary speech\nrecognition,” 2 2014. [Online]. Available: https://arxiv.org/abs/1402.\n1128v1\n[37] S. Elfwing, E. Uchibe, and K. Doya, “Sigmoid-weighted linear units\nfor neural network function approximation in reinforcement learning,”\nNeural Networks, vol. 107, pp. 3–11, 2 2017. [Online]. Available:\nhttps://arxiv.org/abs/1702.03118v3\n[38] M. Zhou, “Softplus regressions and convex polytopes,” 8 2016.\n[Online]. Available: http://arxiv.org/abs/1608.06383\n[39] D. Misra, “Mish: A self regularized non-monotonic activation function,”\n31st British Machine Vision Conference, BMVC 2020, 8 2019. [Online].\nAvailable: http://arxiv.org/abs/1908.08681\n[40] W. Ping, K. Peng, A. Gibiansky, S. O. Arik, A. Kannan, S. Narang,\nJ. Raiman, and J. Miller, “Deep voice 3: Scaling text-to-speech with\nconvolutional sequence learning,” 6th International Conference on\nLearning Representations, ICLR 2018 - Conference Track Proceedings,\n10 2017. [Online]. Available: http://arxiv.org/abs/1710.07654\n[41] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, “Rethinking\natrous convolution for semantic image segmentation,” 6 2017. [Online].\nAvailable: https://arxiv.org/abs/1706.05587\n[42] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning\nfor image recognition,” Proceedings of the IEEE Computer Society\nConference\non\nComputer\nVision\nand\nPattern\nRecognition,\nvol.\n2016-December, pp. 770–778, 12 2015. [Online]. Available: https:\n//arxiv.org/abs/1512.03385v1\n[43] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift,” 32nd International\nConference on Machine Learning, ICML 2015, vol. 1, pp. 448–456, 2\n2015. [Online]. Available: https://arxiv.org/abs/1502.03167v3\n\n\n"}
{"text": "PREDICTING CLINICAL OUTCOMES\nFROM PATIENT CARE PATHWAYS\nREPRESENTED WITH TEMPORAL KNOWLEDGE GRAPHS\nJONG HO JHEE1, ALBERTO MEGINA1, PACÔME CONSTANT DIT BEAUFILS2,3,\nMATILDE KARAKACHOFF3,4, RICHARD REDON2, ALBAN GAIGNARD2, AND ADRIEN COULET1\nAbstract.\nBackground: With the increasing availability of healthcare data, predictive modeling finds\nmany applications in the biomedical domain, such as the evaluation of the level of risk for\nvarious conditions, which in turn can guide clinical decision making. However, it is unclear\nhow knowledge graph data representations and their embedding, which are competitive in\nsome settings, could be of interest in biomedical predictive modeling. Method: We simu-\nlated synthetic but realistic data of patients with intracranial aneurysm and experimented\non the task of predicting their clinical outcome. We compared the performance of various\nclassification approaches on tabular data versus a graph-based representation of the same\ndata. Next, we investigated how the adopted schema for representing first individual data\nand second temporal data impacts predictive performances.\nResults: Our study illus-\ntrates that in our case, a graph representation and Graph Convolutional Network (GCN)\nembeddings reach the best performance for a predictive task from observational data. We\nemphasize the importance of the adopted schema and of the consideration of literal values\nin the representation of individual data. Our study also moderates the relative impact of\nvarious time encoding on GCN performance.\nKeywords: Temporal knowledge graph, Knowledge graph embedding, Graph convolu-\ntional networks, Clinical data, Outcome prediction.\n1. Introduction\nIntracranial aneurysms are abnormal bulges in the blood vessels of the brain that pose sig-\nnificant health risks particularly when they rupture, leading to severe neurological damage\nor death [24]. The ability to predict the evolution, and the best management of intracranial\naneurysms, especially in the context of their various treatments, is crucial for improving pa-\ntient outcomes. This predictive capability is particularly important in a clinical setting where\nunderstanding the relationship between patient features, the treatments they receive, and their\nsubsequent outcomes can serve as a basis for early or preventive interventions. In this context,\nwe aim at establishing a method for identifying patients who are at a higher risk of adverse\noutcomes following intracranial aneurysm rupture. By combining patients’ personal features\nand the treatments observed during their hospital stays, we hope to uncover patterns that lead\nto more accurate predictions of patient outcomes. This would not only enhance the under-\nstanding of the effectiveness of different disease management, but also provides a foundation\nfor the development of targeted intervention strategies that could mitigate adverse outcomes.\nGraph embeddings are mapping of the nodes and edges that compose a graph into a con-\ntinuous vector space [16]. This transformation allows for such complex relational data to be\nrepresented in a form that is more amenable to computational analysis. In particular, it allows\n1\narXiv:2502.21138v1  [cs.LG]  28 Feb 2025\n\n\nto apply machine learning algorithms to perform tasks such as link prediction, node classifica-\ntion or clustering with high performance [34]. In this work, we especially consider embeddings\non Knowledge Graphs (KGs), as defined in the context of the Semantic Web [19]. The atomic\nelements composing those are triples of the form of ⟨subject, predicate, object⟩, where the subject\nand object are nodes of the graph, representing entities; and the predicate is a labelled and\noriented edge, representing that a particular relationship stands between the subject and object\n[18]. KG can be encoded in Resource Description Framework (RDF) a standard where entities\nand predicates are uniquely identified with a Uniform Resource Identifier (URI), facilitating\ninteroperability across different datasets and ontologies.\nHowever, it is unclear how KG representations and KG embeddings may be of interest in\nnode classification task to predict clinical outcomes [8], and in particular to predict outcomes of\nintracranial aneurysm management. We propose in this work to advance some initial answers,\nby exploring three key scientific questions that guided our investigation. First, we wonder if\nthe use of various graph embedding approaches is competitive with regard to classical pre-\ndictive approaches on tabular data. Second, we wonder how these approaches are impacted\nby modeling choices made for representing individual data in the form of a graph. Third, we\nwonder about the impact of various possible representation of time, i.e. timestamps, sequential\nrelations or both on predictive performances.\nThe contributions of this article are: (i) A publicly shared synthetic but realistic dataset\nof pathways of patients treated for a ruptured intracranial aneurysm; along with scripts that\ntransform data from a tabular form to various graph representations; (ii) Elements of answer\nto our three questions, illustrating that, in our context, graph embeddings learned with Graph\nConvolutional Network (GCN) outperform other approaches, that the more compact represen-\ntation of patient features is associated with better performance and that the representation of\ntime does not impact prediction performances.\nThe remainder of this article introduces related works in section 2, material and methods in\nsection 3, empirical results and their interpretation in section 4 and finally presents a discussion\nin section 5.\n2. Related work\n2.1. Standard schema for clinical data. Several data schema have been proposed to model,\nharmonize and facilitate the exchange of clinical data. For example, FHIR [28] and the OMOP\nCDM [33] are standards proposed to tackle clinical data interoperability.\nFHIR is loosely\nspecified, which makes it hard to associate this schema with precisely defined semantics. Even\nif RDF transformations of FHIR [29] and OMOP CDM have been proposed, none has been\nwidely adopted yet [6, 35]. Similarly, Phenopackets [20] is a uniform data structure to ease\nthe combination and exchange of genomic data and clinical observations. If some works have\nbeen proposed to align Phenopackets with semantic web technologies [21], it is insufficient to\nrepresent the full spectrum of clinical data.\nBeside, two ontologies have recently been proposed to represent individual clinical data in\nthe form of knowledge graphs. The first is the SPHN (Swiss Personalized Health Network)\nontology [32], which has been adopted by the five Swiss academic hospitals for better data\nsharing and integration. As illustrated in Figure 1, in this semantic model, patient is a central\nentity, that can be associated with diagnoses, drug administrations and procedures, each of\nwhich can potentially be associated with starting and ending times with literals.\nThe second is the CARE-SM (Care and Registry Semantic Model) ontology [22]. It was\ninitially designed to represent clinical data in the context of rare diseases and largely relies on\n2\n\n\nPatient#1\ndrugAdmin#1\nrdf:type\nsphn:DrugAdministrationEvent\nrdfs:label\nNimodipine\nsphn:hasStartDateTime\nsphn:hasSubjectPseudoIdentifier\nsphn:hasDrug\ndrug#1\ndiagnosis#1\nrdf:type\nsphn:Diagnosis\nsphn:hasSubjectPseudoIdentifier\nrdfs:label\nHTA\nsphn:hasCode\nNCIT:C3117\ndate#1\nFigure 1. Example of individual clinical data represented in a SPHN knowl-\nedge graph. Temporal information is specified with RDF literals associated to\nevents through sphn:hasStartDateTime properties.\nthe reuse of the Semantic Science Integrated Ontology (SIO) [11]. As depicted in Figure 2, it\nprovides fine-grained representations for the multiple roles with which a person can participate\nin the context of a care procedure or a research study. The originality of CARE-SM resides in its\nuse of RDF quads. They are used to associate a semantic context to each data element through\nRDF named graph. This particularly enables the representation of provenance metadata for\nclinical observations or diagnosis.\nNotably, these named graphs can be used to represent\ntimelines of clinical events.\nTimeline#1\nOutput#1\nPatient#1\nsio:has-role\nRole#1\ndate#1\nrdf:type\nOBI_0000093\n(Patient role)\nsio:Person\nrdf:type\nsio:is-part-of\ndate#2\nsio:has-start-time\nsio:has-end-time\nDiagnosis#1\nContext#1\nsio:is-realized-in\nrdf:type\nsio:Process\nsio:has-output\nsio:InformationContentEntity\nrdf:type\nobo:NCIT_C2985\nrdf:type\nsio:is-about\ndate#1\ndate#2\nsio:has-value\nsio:has-value\nFigure 2. Example of individual clinical data represented in a CARE-SM\nKG. Temporal information is directly linked to the Context#1 named graph\nwith start and end dates. Multiple events can be associated to a given timeline\nthrough the sio:is-part-of property.\n3\n\n\nIn this work, we focus on SPHN and CARE-SM first because both of them provide sufficiently\nprecise specification to let one represent an arbitrary clinical dataset in the terms of their\nontologies, which is not the case of FHIR-RDF; second because they propose very different\nmodeling choices to represent individual clinical data; and third because they are adopted in\nlarge scale projects.\n2.2. Time in knowledge graphs. The OWL-Time is a standard ontology that provides\nclasses, predicates and patterns to represent time, duration and intervals [7]. In particular, it\nenables one to instantiate relations between events with the Allen’s interval algebra and can\nbe associated with time reasoning mechanisms [1, 37].\nBoth SPHN and CARE-SM enable\nvarious way to represent time, including the use of OWL-Time and Allen’s interval algebra. In\npractice, one may want to restrict themselves to absolute time by only associating timestamps\nto events, whereas another might want relative time by instantiating relationships such as “is\nbefore” between events, or even to use both absolute and relative relationships. In addition, if\none choose relative time, they also have to decide on a level of saturation between events going\nfrom a simple sequence, where only directly subsequent events are related, to a fully saturated\nlevel where each event is timely related to every other event. Even if it is clear that a fully\nsaturated graph is associated with several drawbacks (e.g., the high connectivity of temporal\nnodes makes the exploration of the graph complex), the most adapted modeling of time to\nadopt for a particular task is not always clear [37].\n2.3. KG embeddings and node classification. Many approaches for representing entities\nof a KG within a latent space exist, and categorisations of them have been proposed in [4]. As\nan illustration, TransE [2] is an embedding approach that works at the triple level as it aims\nat minimizing the following scoring function:\nfr(h, t) = ∥h + r −t∥1,\nwhere h, r, and t are the embedding vectors of the head entity, relation, and tail entity, respec-\ntively. Here the relationship between the head and the tail (i.e., the subject and object) can\nbe seen as a translation r in the embedding space. RDF2Vec follows a very different approach\nthat works at the sequence level [30]. It uses random walks drawn from the knowledge graph.\nThese sequences of either edges, nodes, or subtrees are used to feed a word2vec model that\noutputs embeddings for each node in a sequence, for example, by maximizing the probability\nof a node given the previous nodes of the sequence. For instance, the Continuous Bag-of-Words\nmodel (CBOW), one of the algorithms associated with word2vec, maximizes the average log\nprobability of the target node given a sequence of nodes:\n1\nT\nT\nX\nt=1\nlog p(et|et−c, ..., et+c),\nwhere et is a target node and c denotes the context window. In contrast with TransE and\nRDF2Vec, which work at the triple and sequence levels, GCNs [26] work at the level of the\nneighborhood of nodes. They have been introduced for classification over graphs and extended\nfor node classification and link prediction in knowledge graphs [31]. GCNs compute the em-\nbeddings of a node by considering its neighborhood in the graph. GCNs can be seen as a\nmessage-passing framework of multiple layers, in which the embedding h(l+1)\ni\nof a node i at\n4\n\n\nlayer (l + 1) depends on the embeddings of its neighbors at layer (l), as follows:\nh(l+1)\ni\n= σ\n X\nj∈Ni\n1\nci\nW (l)h(l)\nj\n|\n{z\n}\nNeighborhood\n+\nW (l)h(l)\ni\n|\n{z\n}\nSelf-connection\n!\n.\nThe convolution over the neighboring nodes j of i is computed with a learnable weight matrix\nW (l) and each layer (l); and is normalized by a constant ci. The last term enables self-connection\ni.e., the fact that the embeddings of the node i at a particular layer (l + 1) also depends on\nits embeddings at the layer (l).\nRelational Graph Convolutional Networks (RGCN) [31] is\na particular type of GCNs that takes into consideration multi-relational data, i.e., considers\ndifferently a same neighbor node when related through different labelled edges to i.\nThis\nis particularly adapted to the semantic web KGs since they encompass various predicates to\nrepresent relations associated with different semantics. To this aim, RGCNs incorporate entity\nembeddings with multi-relations in the neighborhood aggregation scheme, as follows:\nh(l+1)\ni\n= σ\n X\nr∈R\nX\nj∈N r\ni\n1\nci,r\nW (l)\nr h(l)\nj\n+ W (l)\n0 h(l)\ni\n!\n.\nwhere the convolution over the neighbor nodes is computed using a specific weight matrix\nW (l)\nr\nfor each predicate (relation type) r ∈R; and ci,r is a normalizing constant such as the\nnumber of neighboring nodes |N r\ni |. The parameter sharing and sparsity constraints using basis-\ndecomposition allow RGCN to deal with the large number of relations. In the following, we\nconsider TransE, RDF2Vec as two baseline approaches for KG embedding (KGE) models and\nRGCN as it seems a well adapted candidate to take into consideration the variety of relationship\ntypes of our clinical KG.\nValues associated with entities through the use of literals are generally disregarded by KGE\napproaches. However various works investigates the interest of incorporating them to node\nrepresentations. To this aim LiteralE [27], proposes an approach where two vectors represent\na single node, the first representing the node embedding itself, and the second containing each\nof the numerical literal it is associated with. Both vectors are then combined before to be\npassed to a scoring function. Knowledge Embedding with Numbers (KEN) [9] uses an encoder,\na single neural network, to inject the literal values into the same vector space with entities.\nOther approaches have been proposed to consider textual literals or combinations of various\ntype of literals [15].\nOne of the main interest of graph embedding is to facilitate the application of machine learn-\ning algorithms on complex relational data to perform a variety of tasks such as link prediction,\nnode classification, graph classification or node clustering [19]. In this work we consider three\nembedding approaches, but only one subsequent task that is node classification. This learn-\ning task can be defined as the estimation of the likelihood that a given entity, not explicitly\nasserted in a KG, is assigned to a specific type [34]. Using KG embeddings, this task relies\non the assumption that the embedding space effectively captures the inherent characteristics\nand structural properties of the original graph. In practice, the spatial configuration of vectors\nshould reflect the relational information observed in the KG, thus allowing predictions about\nnode labels. This task is formally described in the Materials and Methods section of this article.\n2.4. Temporal knowledge graph embeddings. A temporal knowledge graph (TKG) ex-\ntends existing KG by incorporating temporal information in a specific time or interval. This\naddition poses significant challenges because it requires integrating the temporal validity of\n5\n\n\nfacts into the models in order to accurately capture the dynamics of entities and relations over\ntime [10]. Effectively modeling the temporal aspect is crucial for applications where the evolu-\ntion of relations is significant, such as in clinical data where the time of treatments, procedures\nand the patient state can critically influence outcomes. Temporal knowledge graph embedding\nmodels use triples tagged with each timestamp, which are quadruples, and this is considered\nas learning the representation of each time-snapshot graph. However, the time interval could\nbe critical information, or the graph might not be compatible with quadruple forms, as not\nall triples necessarily possess temporal information. Encoding or transforming temporal in-\nformation in the form of vectors of literals or relations can be applied in this type of graph\n[3].\n3. Materials and Methods\n3.1. Synthetic data generation. We built a synthetic data set of 10,000 patients with 30\nclinical features and 1 outcome variable. Out of the 30 clinical features, 8 are associated with\na timestamp and for this reason are hereafter named events, to distinguish from demographic\nor historical features not associated with a particular time. In our dataset, events have the\nparticularity of being associated with only one timestamp, which is the time between hospital\nadmission and the first occurrence of the event during the patient’s stay. Table 4 in Appendix\nlists the 22 clinical features (non-temporal) and 8 events of our synthetic dataset.\nTo make our synthetic dataset as realistic as possible, patient features and care events\nwere generated according to observations made on a real-world clinical dataset of 552 patients\ndiagnosed with a ruptured intracranial aneurysm, provided by the Nantes University Hospital.\nAccess to this dataset has been granted under an IRB agreement.\nFirst we estimated the closest distribution for each feature of the real-world dataset using\na Kolmogorov–Smirnov test. For instance, we observed that the duration of patients’ hospi-\ntal stay was following a generalized extreme value distribution. Second, after performing a\nfactorization for categorical variables, we computed correlations across each possible pair of\nfeatures, and computed the transition probabilities across types of events. We observed that\nthe duration of hospital stay was highly correlated with the number of medical procedures\nreceived; and a high probability for Paracetamol to be administered subsequently to Nimodip-\nine. Figure 3 shows a Sankey diagram built from the observed transition probabilities, which\nvisually summarizes the possible care pathways. To simplify the representation of time in our\ndataset, the 8 events were binarized by a transformation into pairs of events set to one if the\npatient observed a transition between the first and second event, and to zero otherwise. After\nvalidation by clinical experts of the consistency of the distributions, correlations and transition\nprobabilities, we used them to constrain the generation of our synthetic dataset.\nFinally, we generated the patient outcome feature with one of the three distinct values\n{BackHome, Rehabilitation, Death} associated with the following proportions in the synthetic\ndataset: 44.14%, 43.33% and 12.53%, respectively, mirroring the real-word distribution.\n3.2. Graph representation of clinical data. We developed transformations of our syn-\nthetic tabular dataset using various modeling choices. By using RDF templates and rules, we\ngenerated a set of graphs that instantiate the SPHN ontology:\n• SPHN-nl (no literals) where all the literals were removed;\n• SPHN-nt (no time) where temporal information were removed;\n• SPHN-ts (timestamps) with timestamps only;\n6\n\n\nFigure 3. A visual representation of care pathways where the larger connec-\ntions between care events correspond to the higher transition probabilities.\nmorphine: morphine use, paracetamol: paracetamol use, corotrop: milri-\nnone use, ATL: percutaneous transluminal angioplasty, NAD: norepinephrin use,\nnimodipine: nimodipine use, IOT: orotracheal intubation, and DVE: external\nventricular drainage.\n• SPHN-tr (time relations) with a single time:before predicate between directly subse-\nquent events;\n• SPHN-tsr (timestamps and relations) with both timestamps and relations between\nsubsequent events;\n• SPHN-sat1 (saturation level 1) where SPHN-tr is enriched with additional time:before\npredicates obtained by applying once the following transitivity rule time:before ◦\ntime:before ⊑time:before;\n• SPHN-sat2 (saturation level 2) by applying the same transitivity twice.\nFigure 4 illustrates timestamps and time relation between events, at three level of saturation\ncorresponding to SPHN-tr, SPHN-sat1 and SPHN-sat2.\nBy using RDF templates, we also generated a graph that instantiate CARE-SM ontology.\nIn this case, we performed an additional step where quads were transformed into triples, in\nparticular by using a nvasc:hasTimePoint link to associate directly instances such as diagnoses\nor drug administrations to their absolute time. This step was motivated by the fact that KGE\napproaches do not support quads, but only triples.\nThe resulting graph is named CARE-\nSM∗. We generated the same variants of graphs as we did for SPHN, but report only here\nabout CARE-SM∗-ts that is the version with timestamps only. Table 5 in Appendix shows the\nstatistics of SPHN and CARE-SM∗graphs.\nWe make two last transformations to each variant of our graphs: first, to ensure that the\norientation of predicates do not influence our experiments, inverse relationships were systemati-\ncally asserted; we encoded timestamp literals into continuous numbers in the range of [0, 1] using\na quantile transformation. This spreads out the most frequent values thus reduces the impact of\noutliers [12]. Scripts for these various transformation are provided at https://anonymous.4open.\nscience/r/Predicting-clinical-outcomes-with-TKG-B2B9/.\n3.3. KG embedding and patient outcome prediction. The objective of our prediction\ntask is to forecast patient outcomes based on their clinical features and experienced events. In\n7\n\n\nFigure 4. Examples of temporal information: (a) two event associated with\na timestamp; (b) sequence of events related with time:before relations. In\nplain lines are relations between directly subsequent events. Applying a tran-\nsitivity rule once, add 2 relations in dashed doted line (saturation 1) and twice,\nadd the last relation depicted with the dash-dotted line (saturation 2). iot:\norotracheal intubation, dve: external ventricular drainage, nad: nicotinamide\nadenine dinucleotide.\nthe case of the graph dataset, we model this task as a node classification problem aiming at\nassociating patient nodes with the class that corresponds to their correct outcome.\n3.3.1. RGCN for patient outcome prediction. In this section, we describe in detail the graph\nembedding model, RGCN+Literals for the outcome prediction.\nThe overall architecture is\nillustrated in Figure 5. Given a graph G = (V, E, R, X), where V denotes the set of nodes\n(entities), E the set of edges, R the set of relations (predicates), and X ∈Rn×d0 denotes\nthe initial input embeddings of dimension d0. The representation of a target node (patient)\nh(1)\ni\n∈Rd1 for i ∈|V| after passing a first RGCN layer is defined as:\nh(1)\ni\n= σ\n X\nr∈R\nX\nj∈N r\ni\n1\nci,r\nW (0)\nr\nxj + W (0)\n0\nxi\n!\n,\nwhere xi ∈X is an initial patient embedding, Wr ∈Rdo×d1 denotes a weight matrix for each\nrelation r and σ is a non-linear activation function. Additionally, the number of parameters\nincreases with the number of relations, which can lead to overfitting on some of the rare\nrelations. Thus we apply basis-decomposition method for the regularization of the model [31].\nThe final patient node embedding is extracted after passing L stacks of RGCN-layers and\nthe softmax function is applied to output the probability of outcomes. The model is trained\n8\n\n\nFigure 5. An illustration of the model denoted RGCN+lit for patient out-\ncome prediction.\nby minimizing the cross-entropy loss on the patient nodes:\nL = −\nX\np∈P\nK\nX\nk=1\nypk log zpk,\nwhere P is the set of patient nodes in the training set, K the number of outcomes and zpk the\nprobability of the outcome.\n3.3.2. RGCN with Literals. In our clinical KG, some clinical features are in the form of literals\n(e.g., the age of a patient). However, RGCN model only consider entity nodes and relations,\nand thus do not take literals into account. To mitigate this problem, we propose a model\ndenoted RGCN+Literals (or RGCN+lit for short) that employs an additional function for\nthe literals. Before the input of initial embeddings to RGCN, the function of a multi-layer\nperceptron (MLP) is used to transform the value of the literal into embeddings:\nxliteral = σ(Wv + b),\nwhere v denotes the attribute value, σ the non-linear activation function, W the weight matrix\nand b the bias. Note that other encoding functions could be applied instead of the MLP.\n4. Experimental setting and results\nIn the experiments, we compare various outcome prediction approaches along three distinct\nexperiments on our synthetic dataset. Each experiment is repeated ten times for each model\nto assess the variability of the performance. Each time the 10,000 patients are randomly split\ninto train (80%), validation (10%) and test (10%) set. Datasets and codes are available at\nhttps://anonymous.4open.science/r/Predicting-clinical-outcomes-with-TKG-B2B9. The three\nexperiments compare:\n• Tabular vs.\nGraph data.\nWe compare the performance of standard predictive\napproaches applied on tabular data with those of various KGE approaches to examine\nwhether the multi-relational information within the graph would benefit the predictive\ntask.\n• SPHN vs. CARE-SM ontology. We compare the impact on the prediction of using\neither one schema or another to evaluate whether the structure of the graph affects the\nperformance.\n9\n\n\n• Various time modeling. We compare the impact on the prediction of using various\ntime modeling, such as no time data, absolute time only, relative time only, both abso-\nlute and relative time and different levels of saturation of the relative time relationships\nthat connect temporal events.\n4.1. Tabular vs. Graph data. We considered the following methods to establish baseline\nperformance from tabular data, Logistic Regression (LR), Random Forest (RF), Feed-forward\nNeural Network (NN) to compare to KGE approaches. RF were set up with 100 trees, and NN\nwith three layers with hidden dimension sizes of [100, 50, 10] and hyperbolic tangent was used\nas an activation function.\nFor this first comparison with various KGE approaches, we arbitrarily chose to only con-\nsider the SPHN ontology. In addition, to enable a fair comparison with tabular data (which\nencode only the sequence of event), SPHN-tr is considered as the comparative SPHN graph.\nTo represent three main families of KGE we considered TransE, RDF2Vec and RGCN+lit.\nAll models used an initial embedding dimension of 100. For TransE, 1-norm is applied for\nthe regularization of the scoring function. For RDF2Vec, ten walks of a maximum depth of\nthree for each node was applied using the random walk strategy. The patient representations\nobtained from the first two KGE approaches are input to a NN model as a classifier. For\nRGCN+lit, three RGCN layers are applied to aggregate the information within the three-hop\nneighborhood of patient nodes and Parametric Rectified Linear Unit (PReLU) [17] is used as a\nnon-linear activation function. And all models were optimized using Adam optimizer [25] with\nthe learning rate of 1e-3 and the weight decay of 5e-4.\nThe obtained performance is shown in Table 1. All three baseline approaches from tabular\ndata reach poor performances (F1-score = [0.44,0.49], AUC=[0.63, 0.71]). RF showed the best\nperformance (AUC=0.71), closely followed by LR. Especially, the two models showed decent\nF1-score for BackHome outcome, compare to other models. For graph data, neither TransE nor\nRDF2Vec seem to succeed in predicting outcomes (AUC=[0.49,0.5]). However, the RGCN+lit\nmodel showed the overall best result (F1=0.78, AUC=0.91).\nTable 1. The patient outcome prediction comparison on Tabular and Graph\n(SPHN-tr). RGCN3+lit refers to RGCN+lit with 3 layers.\nType\nModel\nF1-score\nAccuracy\nAUC\nBackHome\nRehab\nDeath\nMacro\nWeighted\nTabular\nLR\n0.63±0.02\n0.55±0.02\n0.25±0.05\n0.47±0.03\n0.55±0.02\n0.56±0.02\n0.70±0.02\nRF\n0.63±0.01\n0.55±0.02\n0.28±0.04\n0.49±0.02\n0.55±0.01\n0.56±0.01\n0.71±0.01\nNN\n0.58±0.03\n0.48±0.03\n0.26±0.04\n0.44±0.02\n0.50±0.02\n0.50±0.02\n0.63±0.02\nGraph\n(SPHN-tr)\nTransE\n0.49±0.04\n0.40±0.10\n0.02±0.04\n0.30±0.03\n0.40±0.03\n0.43±0.02\n0.50±0.01\nRDF2Vec\n0.50±0.05\n0.39±0.14\n0.01±0.02\n0.30±0.03\n0.39±0.04\n0.44±0.02\n0.49±0.01\nRGCN3+lit\n0.84±0.01\n0.76±0.02\n0.64±0.08\n0.75±0.03\n0.75±0.02\n0.78±0.01\n0.91±0.01\n4.2. SPHN vs. CARE-SM ontologies. Similarly to the first experiment, TransE, Rdf2Vec\nand RGCN+lit are considered, but here applied to a KG instantiating either the SPHN or\nCARE-SM ontology. The model configuration and hyperparameter settings are the same as\nthe first experiment. Because CARE-SM uses more predicates thus longer paths to connect\npatients with their features, we conducted an additional experiment on CARE-SM KG that\n10\n\n\nconsists in using RGCN with five layers. This aims to check if five-hop neighbors enable to\ncapture enough information to predict the outcome. The obtained performance is reported in\nTable 2. TransE and RDF2Vec showed relatively weak performance on both KG. RGCN on\nSPHN showed the best performance (AUC=0.91). For CARE-SM, all models have difficulty\non predicting the outcomes. Using this ontology, we note that RGCN with three layers is not\nperforming better than TransE or RDF2Vec (AUC=0.50). Increasing the number of layers to\n5 did not improve the performance (AUC=0.50).We note that all the models barely predict the\nDeath outcome.\nTable 2. The performance comparison of SPHN (SPHN-ts) and CARE-SM\n(CARESM∗-ts). RGCN3+lit and RGCN5+lit refers to RGCN+lit with 3 and 5\nlayers, respectively. CARESM∗is the variant of CARE-SM. See Section 3.2 for\nmore details.\nKG\nModel\nF1-score\nAccuracy\nAUC\nBackHome\nRehab\nDeath\nMacro\nWeighted\nSPHN-ts\nTransE\n0.51±0.07\n0.33±0.16\n0.02±0.04\n0.29±0.04\n0.37±0.05\n0.43±0.02\n0.50±0.01\nRDF2Vec\n0.49±0.04\n0.42±0.09\n0.01±0.03\n0.30±0.02\n0.40±0.02\n0.44±0.01\n0.50±0.02\nRGCN3+lit\n0.83±0.02\n0.76±0.02\n0.66±0.08\n0.75±0.03\n0.78±0.02\n0.78±0.02\n0.91±0.01\nCARESM∗-ts\nTransE\n0.47±0.04\n0.44±0.04\n0.02±0.03\n0.31±0.01\n0.40±0.01\n0.43±0.01\n0.49±0.01\nRDF2Vec\n0.51±0.07\n0.38±0.11\n0.00±0.00\n0.29±0.02\n0.39±0.03\n0.44±0.02\n0.50±0.01\nRGCN3+lit\n0.53±0.08\n0.30±0.17\n0.00±0.00\n0.28±0.04\n0.37±0.05\n0.44±0.01\n0.50±0.02\nRGCN5+lit\n0.48±0.08\n0.30±0.19\n0.00±0.00\n0.26±0.04\n0.34±0.05\n0.44±0.05\n0.50±0.01\n4.3. Various time modeling. In this experiment, we compare predictive performance of\ngraphs associated with various modeling of time as listed in Section 3.2. All experiments are\nconducted using the RGCN+lit model, except with no literals where it uses RGCN. Without\nliterals, the model performs poorly, and under the standard approach from tabular data. The\nAUC increases about 33% on average when the literals are added. This increase mainly come\nfrom a better prediction of the Death outcome. When the timestamps are added to SPHN-nt,\nthe AUC increases of 7% on average. Adding the time relations also increases AUC of a 7%.\nAdding both temporal information showed similar results as well. With saturation, we observe\na slight incrase of performance for the Death outcome, though the overall performance is similar\nto SPHN-tsr. Eventually, adding temporal information give the model better prediction, but\nthe type of time modelling and the level of saturation did not make a significant difference.\n5. Discussion\nThe comparative analysis reveals first that the RGCN+lit model outperforms baseline meth-\nods on tabular data, or other KGE approaches in terms of accuracy and F1-score. We hypoth-\nesis that the multi-relational modeling associated with the multiple RGCN-layers makes the\nmodel capable of aggregating information from multiple hop of neighbors to the patient node,\nwhat cannot be achieved from single relational modeling with tabular data, or with TransE or\nRDF2Vec which only partially consider the multi-hop neighbors. Second, we observed that the\nchoice of the schema for individual KG impacts the performance. In particular in our setting,\nthe more compact and patient-oriented schema of SPHN is more favorable than the one of\nCARE-SM KGE approaches. This could be explained by the longer distance between patient\n11\n\n\nTable 3. The Performance of RGCN+lit on SPHN with various temporal\ninformation and modeling. RGCN without literals is applied to SPHN-nl.\nKG\nF1-score\nAccuracy\nAUC\nBackHome\nRehab\nDeath\nMacro\nWeighted\nSPHN-nl\n0.64±0.03\n0.46±0.11\n0.05±0.07\n0.38±0.06\n0.49±0.06\n0.53±0.04\n0.64±0.06\nSPHN-nt\n0.75±0.02\n0.65±0.02\n0.55±0.06\n0.65±0.02\n0.68±0.01\n0.68±0.01\n0.85±0.01\nSPHN-ts\n0.83±0.02\n0.76±0.02\n0.66±0.08\n0.75±0.03\n0.78±0.02\n0.78±0.02\n0.91±0.01\nSPHN-tr\n0.84±0.01\n0.76±0.02\n0.64±0.08\n0.75±0.03\n0.75±0.02\n0.78±0.01\n0.91±0.01\nSPHN-tsr\n0.83±0.02\n0.76±0.02\n0.66±0.04\n0.75±0.02\n0.78±0.01\n0.78±0.01\n0.91±0.01\nSPHN-sat1\n0.83±0.01\n0.76±0.02\n0.64±0.06\n0.75±0.02\n0.78±0.01\n0.78±0.01\n0.91±0.01\nSPHN-sat2\n0.83±0.01\n0.76±0.02\n0.68±0.05\n0.76±0.02\n0.78±0.02\n0.78±0.02\n0.91±0.01\nand feature nodes in CARE-SM, however, increasing the size of the neighbor to five-hops did\nnot solve the issue. However, we note that we did not expend further the number of hops\nfor computational cost. A limitation to the conclusion relies on the fact that we considered\nonly SPHN and CARE-SM as two prototypical ontologies. A more complete study would have\nconsidered concurrent models such as RDF-FHIR, PhenoPackets and others. Third, adding\ntime information helps RGCN+lit in classifying properly patient nodes, but we were enable to\nobserve any difference in performance associated with various time modeling choices. Here we\nacknowledge that we focus on rather simple time modeling and that more complex scenarios\nexist such as those associated with dynamic graphs.\nOverall, our study illustrates that KGE such as RGCN+lit represents a promising approach\nfor predictive modeling in healthcare. However we note a strong class disbalance in our dataset\n(44.14%, 43.33% and 12.53% for BackHome, Rehabilitation, Death, respectively), reflecting the\nreal world. This might explain to some extent the difficulty for most of the approaches to predict\nthe Death class. It is further possibly mitigated by over-sampling techniques, such as Synthetic\nMinority Over-sampling [5]. Also our study considers solely the task of node classification,\nwhereas prediction could have been modeled as a link prediction or triple classification problem.\nMore investigation would be necessary to assess if our conclusions stand in the context of\nother learning tasks. This could necessitate the consideration of problem classically associated\nwith KGE such as negative sampling [23].\nAdditionally, we observed that including literal\nembeddings in the model improved the performance. In this study, our model focused on the\nsimple embedding of numerical literals, including timestamps. However, we plan to develop a\nmodel capable of handling multi-modal literals, such as a combination of text and numerical\nliterals [15].\nAnother important position we took is between transductive and inductive learning ap-\nproaches [36]. We follow a transductive learning approach, which mean that node classification\nis made on the basis of the set of entities that has potentially been seen during training. Most\nof the KGE models are based on a transductive learning approach, because the model learns\nthe complex relational information within a whole large graph. In contrast, inductive learning\nattempts to generalize the model to new entities and relations not presented in the training\ngraph. For instance, it is representing the entities or relations based on the combination of\nobserved entities or relations in the training graph [13]. Inductive approach can be adapted\n12\n\n\nto the scenario where new patient information and medical events continuously emerges. This\nremains a future work of our study.\nThe effective evaluation of KG embedding models is crucial for advancing the field and\nensuring that the models developed are robust, accurate, and useful in practical applications\n[14]. But it suffers from a lack of standardized benchmarks and reference representations of\nthe temporal dimension. In this study, we particularly aimed at advancing this agenda, by\nproposing a real-word task, a shared dataset and well documented baseline experiment, which\nwill serve as a baseline for prediction modeling with graph data.\nAn important area for future research is evaluating the impact of individual patient features\non prediction outcomes. By analyzing the relative importance of different features, such as\nspecific medical conditions or demographic factors, researchers can refine their models to focus\non the most predictive attributes. This targeted approach can enhance the model’s efficiency\nand ensure that the most relevant clinical information is prioritized in decision-making. While\nat the moment, efforts are underway to obtain a small subset of a real dataset to test these\nmodels and generate predictions based on real patient data thanks to the collaboration with\nthe Nantes Hospital, in the longer term, the ultimate goal is to achieve clinical validation of the\npredictive models. This step involves applying the models to actual patient data and rigorously\nassessing their performance in a clinical setting. Clinical validation is critical for ensuring that\nthe models are theoretically sound and practically useful in improving patient outcomes. This\nresearch phase will require close collaboration with healthcare providers and institutions to test\nthe models in real-world environments and to gather feedback for further refinement.\nIn conclusion, while this study has made significant strides in demonstrating the potential\nof knowledge graph embeddings for predicting patient outcomes, a substantial amount of work\nremains to be done. By continuing to refine the models, enhance the realism of synthetic data,\nand pursue clinical validation, future research can build on these foundations to develop precise\nand clinically relevant predictive tools that ultimately enhance patient care and treatments.\nCredits\nAcknowledgments. This work is supported by CombO (Health Data Hub) project and the\nAgence Nationale de la Recherche under the France 2030 program, ANR-22-PESN-0007 Share-\nFAIR, and ANR-22-PESN-0008 NEUROVASC.\nCompeting interests. The authors have no competing interests to declare that are relevant to\nthe content of this article.\nReferences\n[1] Batsakis, S., Petrakis, E.G., Tachmazidis, I., Antoniou, G.: Temporal representation and reasoning in owl\n2. Semantic Web 8(6), 981–1000 (2017)\n[2] Bordes, A., Usunier, N., Garcia-Duran, A., Weston, J., Yakhnenko, O.: Translating embeddings for mod-\neling multi-relational data. In: Advances in Neural Information Processing Systems. vol. 26 (2013)\n[3] Cai, B., Xiang, Y., Gao, L., Zhang, H., Li, Y., Li, J.: Temporal knowledge graph completion: a survey. In:\nProceedings of the Thirty-Second International Joint Conference on Artificial Intelligence. pp. 6545–6553\n(2023)\n[4] Cai, H., Zheng, V.W., Chang, K.C.C.: A comprehensive survey of graph embedding: Problems, techniques,\nand applications. IEEE transactions on knowledge and data engineering 30(9), 1616–1637 (2018)\n[5] Chawla, N.V., Bowyer, K.W., Hall, L.O., Kegelmeyer, W.P.: Smote: synthetic minority over-sampling\ntechnique. Journal of artificial intelligence research 16, 321–357 (2002)\n[6] Chytas, A., Bassileiades, N., Natsiavas, P.: Mapping omop-cdm to rdf: Bringing real-world-data to the\nsemantic web realm. In: Digital Health and Informatics Innovations for Sustainable Health Care Systems,\npp. 1406–1410. IOS Press (2024)\n13\n\n\n[7] Cox, S.J.D., et al.: Time ontology in owl. W3C Candidate Recommendation (November 2022), https:\n//www.w3.org/TR/owl-time/, accessed: 2024-12-18\n[8] Cui, H., Lu, J., Wang, S., Xu, R., Ma, W., Yu, S., Yu, Y., Kan, X., Fu, T., Ling, C., Ho, J., Wang, F.,\nYang, C.: A survey on knowledge graphs for healthcare: Resources, application progress, and promise. In:\nICML 3rd Workshop on Interpretable Machine Learning in Healthcare (IMLH) (2023)\n[9] Cvetkov-Iliev, A., Allauzen, A., Varoquaux, G.: Relational data embeddings for feature enrichment with\nbackground information. Machine Learning 112(2), 687–720 (2023)\n[10] Dall’Amico, L., Barrat, A., Cattuto, C.: An embedding-based distance for temporal graphs. Nature Com-\nmunications 15(1), 9954 (2024)\n[11] Dumontier, M., Baker, C.J.O., Baran, J., Callahan, A., Chepelev, L.L., Cruz-Toledo, J., Rio, N.R.D.,\nDuck, G., Furlong, L.I., Keath, N., Klassen, D., McCusker, J., Queralt-Rosinach, N., Samwald, M.,\nVillanueva-Rosales, N., Wilkinson, M.D., Hoehndorf, R.: The semanticscience integrated ontology (sio)\nfor biomedical research and knowledge discovery. Journal of Biomedical Semantics 5 (2014), https:\n//api.semanticscholar.org/CorpusID:17652831\n[12] Ehm, W., Gneiting, T., Jordan, A., Krüger, F.: Of quantiles and expectiles: consistent scoring functions,\nchoquet representations and forecast rankings. Journal of the Royal Statistical Society Series B: Statistical\nMethodology 78(3), 505–562 (2016)\n[13] Galkin, M., Denis, E., Wu, J., Hamilton, W.L.: Nodepiece: Compositional and parameter-efficient rep-\nresentations of large knowledge graphs. In: International Conference on Learning Representations (2022),\nhttps://openreview.net/forum?id=xMJWUKJnFSw\n[14] Gastinger, J., Sztyler, T., Sharma, L., Schuelke, A.: On the evaluation of methods for temporal knowledge\ngraph forecasting. In: NeurIPS 2022 Temporal Graph Learning Workshop. New Orleans, United States\n(2022)\n[15] Gesese, G.A., Biswas, R., Alam, M., Sack, H.: A survey on knowledge graph embeddings with literals:\nWhich model links better literal-ly? Semantic Web 12(4), 617–647 (2021)\n[16] Hamilton, W.L., Ying, R., Leskovec, J.: Representation learning on graphs: Methods and applications.\narXiv preprint arXiv:1709.05584 (2017)\n[17] He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into rectifiers: Surpassing human-level performance\non imagenet classification. In: Proceedings of the IEEE international conference on computer vision. pp.\n1026–1034 (2015)\n[18] Hitzler, P., Krötzsch, M., Rudolph, S.: Foundations of Semantic Web Technologies. Chapman & Hall/CRC\n(2009)\n[19] Hogan, A., Blomqvist, E., Cochez, M., D’amato, C., Melo, G.D., Gutierrez, C., Kirrane, S., Gayo, J.E.L.,\nNavigli, R., Neumaier, S., Ngomo, A.C.N., Polleres, A., Rashid, S.M., Rula, A., Schmelzeisen, L., Sequeda,\nJ., Staab, S., Zimmermann, A.: Knowledge graphs. ACM Computing Surveys 54(4) (July 2021)\n[20] Jacobsen, J.O., Baudis, M., Baynam, G.S., Beckmann, J.S., Beltran, S., Buske, O.J., Callahan, T.J., Chute,\nC.G., Courtot, M., Danis, D., et al.: The ga4gh phenopacket schema defines a computable representation\nof clinical data. Nature biotechnology 40(6), 817–820 (2022)\n[21] Kaliyaperumal, R., Singh, G., Queralt-Rosinach, N., Bayjanov, J.R., ’t Hoen, P.B., Roos, M.: Phenopackets\nfor the semantic web. In: SWAT4HCLS (2022), https://api.semanticscholar.org/CorpusID:248397161\n[22] Kaliyaperumal, R., Wilkinson, M.D., Moreno, P.A., Benis, N., Cornet, R., dos Santos Vieira, B., Dumon-\ntier, M., Bernabé, C.H., Jacobsen, A., Cornec, C.M.A.L., Godoy, M.P., Queralt-Rosinach, N., Kool, L.J.S.,\nSwertz, M.A., van Damme, P., van der Velde, K.J., Lalout, N., Zhang, S., Roos, M.: Semantic modelling\nof common data elements for rare disease registries, and a prototype workflow for their deployment over\nregistry data. Journal of Biomedical Semantics (March 15 2022)\n[23] Kamigaito, H., Hayashi, K.: Comprehensive analysis of negative sampling in knowledge graph representa-\ntion learning. In: International Conference on Machine Learning. pp. 10661–10675. PMLR (2022)\n[24] Keedy, A.: An overview of intracranial aneurysms. McGill Journal of Medicine: MJM 9(2), 141 (2006)\n[25] Kingma, D.P.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014)\n[26] Kipf, T.N., Welling, M.: Semi-supervised classification with graph convolutional networks. In: 5th Interna-\ntional Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference\nTrack Proceedings. OpenReview.net (2017), https://openreview.net/forum?id=SJU4ayYgl\n[27] Kristiadi, A., Khan, M.A., Lukovnikov, D., Lehmann, J., Fischer, A.: Incorporating literals into knowledge\ngraph embeddings. In: The Semantic Web–ISWC 2019: 18th International Semantic Web Conference,\nAuckland, New Zealand, October 26–30, 2019, Proceedings, Part I 18. pp. 347–363. Springer (2019)\n14\n\n\n[28] Lehne, M., Luijten, S., Vom Felde Genannt Imbusch, P., Thun, S.: The use of fhir in digital health–a\nreview of the scientific literature. German Medical Data Sciences: Shaping Change–Creative Solutions for\nInnovative Medicine pp. 52–58 (2019)\n[29] Prud’hommeaux, E., Collins, J., Booth, D., Peterson, K.J., Solbrig, H.R., Jiang, G.:\nDevelopment\nof a fhir rdf data transformation and validation framework and its evaluation. Journal of Biomed-\nical Informatics 117, 103755 (2021). https://doi.org/https://doi.org/10.1016/j.jbi.2021.103755, https:\n//www.sciencedirect.com/science/article/pii/S1532046421000848\n[30] Ristoski, P., Paulheim, H.: Rdf2vec: Rdf graph embeddings for data mining. In: International semantic\nweb conference. pp. 498–514. Springer (2016)\n[31] Schlichtkrull, M., Kipf, T.N., Bloem, P., Van Den Berg, R., Titov, I., Welling, M.: Modeling relational\ndata with graph convolutional networks. In: The semantic web: 15th international conference, ESWC 2018,\nHeraklion, Crete, Greece, June 3–7, 2018, proceedings 15. pp. 593–607. Springer (2018)\n[32] Touré, V., Krauss, P., Gnodtke, K., Buchhorn, J., Unni, D., Horki, P., Raisaro, J.L., Kalt, K., Teixeira, D.,\nCrameri, K., Österle, S.: Fairification of health-related data using semantic web technologies in the swiss\npersonalized health network. Scientific Data 10 (2023), https://api.semanticscholar.org/CorpusID:\n257433763\n[33] Voss, E.A., Makadia, R., Matcho, A., Ma, Q., Knoll, C., Schuemie, M., DeFalco, F.J., Londhe, A., Zhu,\nV., Ryan, P.B.: Feasibility and utility of applications of the common data model to multiple, disparate\nobservational health databases. Journal of the American Medical Informatics Association 22(3), 553–564\n(2015)\n[34] Wang, Q., Mao, Z., Wang, B., Guo, L.: Knowledge graph embedding: A survey of approaches and appli-\ncations. IEEE transactions on knowledge and data engineering 29(12), 2724–2743 (2017)\n[35] Xiao,\nG.,\nPfaff,\nE.,\nPrud’hommeaux,\nE.,\nBooth,\nD.,\nSharma,\nD.K.,\nHuo,\nN.,\nYu,\nY.,\nZong,\nN.,\nRuddy,\nK.J.,\nChute,\nC.G.,\nJiang,\nG.:\nFhir-ontop-omop:\nBuilding clinical knowledge\ngraphs in fhir rdf with the omop common data model. Journal of Biomedical Informatics 134,\n104201 (2022). https://doi.org/https://doi.org/10.1016/j.jbi.2022.104201, https://www.sciencedirect.\ncom/science/article/pii/S1532046422002064\n[36] Xu, D., Ruan, C., Korpeoglu, E., Kumar, S., Achan, K.: Inductive representation learning on temporal\ngraphs. In: International Conference on Learning Representations (2020), https://openreview.net/forum?\nid=rJeW1yHYwH\n[37] Zhang, F., Li, Z., Peng, D., Cheng, J.: Rdf for temporal data management–a survey. Earth science infor-\nmatics 14, 563–599 (2021)\n15\n\n\nAppendix\nAppendix A. List of features of our synthetic dataset\nThe list of clinical features in our synthetic data is shown in Table 4. It includes 22 non-\ntemporal features (4 numerical, 6 categorical, 12 binary) and 8 temporal features (named events\nin this manuscript). Note that events correspond to the time between hospital admission and\nthe first occurrence of the event (e.g., the first administration of Nimodipine).\nTable 4. List of clinical features.\nType\nName\nDescription\nNumerical\nhospital_stay_length\nNumber of days in the hospital\ngcs\nGlasgow coma score\nact_nb\nNumber of medical acts\nage\nPatient’s age\nCategorical\ngender\nGender\nentry\nMode of entry (e.g., emergency room)\nentry_code\nCode of entry\nica.y\nIntracranial aneurysm (ICA) location and size\nica_treatment\nICA treatment (e.g., endovascular or surgical)\nica_therapy\nCalcium channel blockers therapy\nBinary\nfever\nFever\no2_clinic\nDecrease of oxygen, indirect measure\no2\nDeacrease of oxygen, blood measure\nhta\nHypertension\nhct\nHypercholesterolemia\nsmoking\nSmoking\netOH\nAlcohol consumption\ndiabetes\nDiabetes\nheadache\nHeadache\nunstable_ica\nHemodynamic instability\nvasospasm\nVasospasm\nivh\nIntraventricular hemorrhage\nEvent\nnimodipine\nNimodipine\nparacetamol\nParacetamol use\nnad\nNorepinephrin use\ncorotrop\nMilrinone use\nmorphine\nMorphine use\ndve\nExternal ventricular drainage\natl\nPercutaneous transluminal angioplasty\niot\nOrotracheal intubation\nAppendix B. Statistics of Datasets\nThe statistics of KGs, SPHN and CARE-SM.\n16\n\n\nTable 5. Statistics of Datasets\nDataset\n# Entities\n# Relations\n# Literals\n# Triples\nSPHN\n295,307\n15\n36,415\n1,127,467\nCARE-SM∗\n576,733\n13\n24,766\n1,754,505\nAffiliations:\n1.\nInria, Inserm, Université Paris Cité, HeKA, UMR 1346, Paris, France, –\n2. CNRS, Inserm, Université de Nantes, Institut du Thorax, UMR 1087, Nantes, France, – 3.\nUniversité de Nantes, CHU Nantes, Nantes, France, – 4.\nInserm, Clinique des données, CIC\n1413, Nantes, France,\nEmail address:\n1.\n{jong-ho.jhee, alberto.megina, adrien.coulet}@inria.fr, – 2.\n{richard.redon,\nalban.gaignard}@univ-nantes.fr, – 3.\n{pacome.constantditbeaufils, matilde.karakachoff}@chu-nantes.fr\n17\n\n\n"}
{"text": "Managing Federated Learning on Decentralized Infrastructures as a\nReputation-based Collaborative Workflow\nYuandou Wanga, Zhiming Zhaoa,b\naMultiscale Networked System, University of Amsterdam, Science Park 900, Amsterdam, 1098 XH, The Netherlands\nbLifeWatch ERIC Virtual Lab and Innovation Center (VLIC), Science Park 904, Amsterdam, 1098 XH, The Netherlands\nA R T I C L E I N F O\nKeywords:\nFederated Learning\nCollaborative Workflows\nIncentives\nReputation\nOptimal Contract\nBlockchain\nA B S T R A C T\nFederated Learning (FL) has recently emerged as a collaborative learning paradigm that can train a\nglobal model among distributed participants without raw data exchange to satisfy varying require-\nments. However, there remain several challenges in managing FL in a decentralized environment,\nwhere potential candidates exhibit varying motivation levels and reliability in the FL process\nmanagement: 1) reconfiguring and automating diverse FL workflows are challenging, 2) difficulty\nin incentivizing potential candidates with high-quality data and high-performance computing to join\nthe FL, and 3) difficulty in ensuring reliable system operations, which may be vulnerable to various\nmalicious attacks from FL participants. To address these challenges, we focus on the workflow-based\nmethods to automate diverse FL pipelines and propose a novel approach to facilitate reliable FL system\noperations with robust mechanism design and blockchain technology by considering a contribution\nmodel, fair committee selection, dynamic reputation updates, reward and penalty methods, and\ncontract theory. Moreover, we study the optimality of contracts to guide the design and implementation\nof smart contracts that can be deployed in blockchain networks. We perform theoretical analysis and\nconduct extensive simulation experiments to validate the proposed approach. The results show that\nour incentive mechanisms are feasible and can achieve fairness in reward allocation in unreliable\nenvironment settings.\n1. Introduction\nFederated learning (FL) is a promising distributed ma-\nchine learning (ML) paradigm that enables collaborative\nlearning over decentralized data to mitigate many systematic\nprivacy risks and costs resulting from traditional, centralized\nlearning. In recent years, FL has been studied in many\nresearch and application domains, particularly in the fields\nof digital medicine and health [1], open banking [2], and\nthe Internet of Things [3], in which data cannot be shared\nor exchanged due to privacy and security concerns.\nFL often involves a number of participants, which fea-\nture highly heterogeneous training infrastructures and non-\nindependently and identically distributed (non-IID) data [4].\nIn a traditional setting of FL, several participants, each\nkeeping its data and training process on-premise, aim to\ncollaboratively train a joint model under a central aggregator\nthat initializes, collects, aggregates, and redistributes the\nmodels to and from the training participants, as shown in\nFigure 1 (a). Over time, the federated model aggregation has\nemerged in varying modes [5] and performed as different\ntopology types in the distributed computing ecosystem [6],\nwhile the data and its training infrastructure are still decen-\ntralized. It is known that such diversity can be attributed\nto different FL design choices and customizations to satisfy\ndifferent user requirements. For instance, some researchers\nintroduced hierarchical FL, as shown in Figure 1 (b), to\ntackle the bottleneck of communication overhead in the core\ny.wang8@uva.nl (Y. Wang); z.zhao@uva.nl (Z. Zhao)\nhttps://www.linkedin.com/in/yuandou-w-aa717b135/ (Y. Wang);\nhttps://staff.fnwi.uva.nl/z.zhao/ (Z. Zhao)\nORCID(s): 0000-0003-4694-9572 (Y. Wang)\nnetwork [7, 8]. Some proposed decentralized FL, as depicted\nin Figure 1 (c), with a Peer-to-Peer (P2P) communication\narchitecture to cope with the drawbacks of a single point of\nfailure and scaling issues for the increasing network size [9,\n10]. Moreover, various aggregation strategies associated\nwith FL topologies for updating the FL model parameters\nhave been studied, including but not limited to sequen-\ntial updates [11], (a)synchronous parallel updates [12], and\npeer-to-peer approaches [13], as illustrated in Figure 1 (d),\n(e), and (f), respectively. The final well-trained model can\nbe obtained through the iteratively collaborative training\nrounds. However, due to the heterogeneity of decentralized\ninfrastructure or platforms, it poses execution challenges.\nFL development involves three typical activities: 1) dis-\ncover potential participants with high-quality data or re-\nsource providers and stimulate them to join the FL, 2) cus-\ntomize the FL application workflows for specific purposes,\nand 3) define a reliable training process with local updates\nevaluation.\nIn real-world scenarios, collaborative FL typically fol-\nlows two main approaches. The first involves pre-established\ncollaborations, where participants are obligated to join FL\ndue to prior agreements within research projects. These\nparticipants are considered trustworthy and contribute to\nthe entire FL training process with complete information.\nThe second approach relies on incentive-driven participa-\ntion, where FL proposers solicit collaboration through in-\ncentives [14, 15] from a decentralized community. In this\ncase, participation is voluntary, and the trustworthiness of\npotential candidates is initially unknown.\nDue to diverse availability of local resources and in-\nterests, FL participants may have different motivation in\nYuandou Wang et al.: Preprint submitted to Elsevier\nPage 1 of 16\narXiv:2502.20882v1  [cs.DC]  28 Feb 2025\n\n\nManaging Federated Learning on Decentralized Infrastructures as a Reputation-based Collaborative Workflow\nData\nTraining Node\nAggregator\nModel Aggregation\nInitial Model\nFinal Model\nWeight Exchange\n(a) Centralized FL\n(d) Sequential\n(e) Parallel\n(f) Peer to Peer\nLegend\nTopologies in FL\nModel updates' patterns in FL\nFederated Node\n(c) Decentralized FL\n(b) Hierarchical FL\nFigure 1: An overview of diverse FL design choices. FL topologies. (a) Centralized FL: a central aggregation server manages the\ntraining process by coordinating iterative training rounds. (b) Hierarchical FL: typically, the FL network has a tree structure with\nat least three tiers. (c) Decentralized FL: each training node is connected to one or more peers and aggregation happens on the\nselected node. FL model updates’ paths. (d) Sequential. (e) Parallel. (f) Peer to Peer.\njoining an FL application, and deliver diverse quality in con-\ntributions. This heterogeneity introduces several challenges,\nincluding unequal participation [16], free-riding [17], and\ndifficulties in accurately assessing and rewarding individual\ncontributions [18]. Encouraging active engagement from\ndiverse stakeholders, such as data owners, infrastructure\nproviders, and model developers, remains a critical chal-\nlenge. This requires incentivizing participation in customized\nFL workflows, facilitating local model update sharing, and\nensuring fair contributions to global model development\nwithin a decentralized community.\nFurthermore, these complexities create efficiency chal-\nlenges, as substantial time and effort are needed to set up\nFL workflows, identify suitable participants, and maintain\nlearning quality. In addition, FL operations are vulnerable\nto both intentional and unintentional threats, including data\npoisoning, malicious servers, inference attacks, system dis-\nruptions, and service unavailability [19]. Addressing these\nrisks is particularly difficult when some participants are\nunreliable during the training process.\nIn this work, we address the efficiency challenges in FL\ndevelopment and management within decentralized com-\nmunity settings. We begin by automating FL operations\nby modeling them as workflows and leveraging workflow\nengines to orchestrate their execution on remote infrastruc-\ntures at scale. Our approach emphasizes workflow-based FL\nmanagement, focusing on FL workflow composition, data\nowner incentivization, and reliable training processes via\ncombining with decentralized technologies.\nThe reminder of this article is structured as follows:\nSection 2 reviews the state-of-the-art reliable FL practices,\ncompares with existing work, and analyzes their research\ngap. Section 3 describes how we can describe FL as a\ncomputational workflow and scale it out to decentralized\ninfrastructure via the CWL open standards. Section 4 intro-\nduces a decentralized collaboration framework for FL and\nillustrates the mechanism design approach and studies the\ncontract optimality to answer research questions. Section 5\ndetails the experiments and the analysis of results; finally,\nSection 6 concludes this work.\n2. Related Work\nThis section reviews the state-of-the-art reliable fed-\nerated learning operation practices and explores existing\nframeworks for managing FL in decentralized infrastruc-\ntures.\nIn recent years, several FL frameworks have made no-\ntable advancements in managing the FL research and de-\nvelopment lifecycle. Yang et al. [20] introduced FLScalize,\nwhich extends the machine learning operations (MLOps)\nconcept to generate a baseline model, integrating FL clients,\nthe FL server, and model performance to oversee the entire\nFL lifecycle. Colonnelli et al. [21] employed the open stan-\ndard workflow language, CWL [22], to abstract and auto-\nmate FL applications in a hybrid Cloud-HPC environment.\nDespite these advances, their approach remains constrained\nby the traditional client-server architecture of centralized FL,\nalthough the open standard workflow language holds signif-\nicant promise for describing diverse FL workflows [23].\nYuandou Wang et al.: Preprint submitted to Elsevier\nPage 2 of 16\n\n\nManaging Federated Learning on Decentralized Infrastructures as a Reputation-based Collaborative Workflow\nDaga et al. [24] introduced the Flame framework, of-\nfering flexibility in the topology configuration of FL ap-\nplications tailored to the specific deployment context using\nnew high-level abstraction topology graphs (TAGs) that\nincorporate five types of FL topologies. While both Colon-\nnelli et al. [21] and Daga et al. [24] align closely with our\nstudy by utilizing high-level abstractions to enable flexibility\nin topology-aware FL, a key assumption in many existing\nworks is that potential participants will join the FL process\nunconditionally and are sufficiently trustworthy to contribute\nthroughout the entire FL training process. Our study diverges\nfrom previous works in several critical ways. We explore\nclassic workflow patterns [25] and FL architectural pat-\nterns [26], making fundamental workflow concepts — such\nas automation, scalability, abstraction, portability, flexibility,\nand reusability — applicable to the context of federated\nlearning operations (FLOps) via the open standard CWL.\nPreliminary results of this approach have been presented in\nCWL-FLOps [23].\nFurthermore, Cheng and Long [27] introduced a novel\nmethodology called FLOps for managing the FL lifecycle\ncontinuously and efficiently. FLOps integrates a range of\nprocesses, technologies, and tools to enhance the efficiency\nand quality of developing and deploying cross-silo FL sys-\ntems. They highlight that workflow-related approaches, such\nas metadata engineering, dual deployment, and checkpoints,\ncan help establish and automate FLOps practices, enabling\nsmoother and more efficient operations from an engineering\nperspective. However, the automation of FLOps remains an\nopen challenge, particularly when addressing security and\nprivacy concerns, which differ from those encountered in\ntraditional DevOps [28] and MLOps [29] practices. More-\nover, they do not address the issue of incentivizing partic-\nipants to make high-quality contributions if the assumption\nof their willingness and trustworthiness does not hold — this\nis a crucial factor for maintaining a reliable and secure FL\nprocess.\nOur work connects to the broader discussion of pro-\nmoting collaborative fairness among federated participants\nwithin decentralized communities, where incentives and\nfairness have been extensively explored [30, 18, 31]. Wang\net al. [32] examined incentive mechanism design in the con-\ntext of resource allocation for FL clients in Blockchain-based\nFederated Learning (BCFL). They modeled the problem\nas a two-stage Stackelberg game under both complete and\nincomplete information. Using the Shapley value approach,\nthey quantified clients’ contributions to the training process.\nBy transforming the game model into two optimization\nproblems and solving them sequentially, they derived the\noptimal strategies for both players. Gao et al. [33] introduced\nFGFL, an attack-resistant incentive mechanism for FL that\ndetects and repels abnormal updates, thereby protecting the\nsystem in unreliable scenarios. The task publisher rewards\nefficient workers and punishes or eliminates malicious ones\nbased on reputation and contribution indicators.\nWhile both incentive mechanisms are deemed feasible\nthrough experimental evaluation, neither explores the op-\ntimal contract design required for creating smart contracts\nfor FL operations. The question of whether the incentives\nembedded in smart contracts are either over-rewarding or\ninsufficient to effectively motivate participants remains un-\nclear. Kang et al. [34] proposed a joint optimization approach\ncombining a reputation-based worker selection scheme with\ncontract theory to determine the optimal computation re-\nsources (e.g., contributed CPU cycles) and corresponding\nrewards for all participants. However, this approach does\nnot fully capture the entire contribution to the FL training\nprocess. Furthermore, although existing works touch on fair\nrewards, there is a clear need for precise fairness metrics that\nalign with specific incentives, particularly in the context of\nreliable FL systems.\n3. Federated Learning as a Workflow\nThis section explores the advantages of managing feder-\nated learning as a workflow within decentralized infrastruc-\nture, addressing the following research question: \"How can\nwe abstract FL workflows in an open standard manner and\norganize their execution on remote infrastructure?\"\n3.1. Federated Learning\nWe consider an FL application scenario consisting of\n𝑁= {1, 2, ⋯, 𝑛} clients, each client 𝑖holds a local dataset\n𝐷𝑖= {(𝑥, 𝑦)} ∼𝑖consisting of 𝑠𝑖data samples, where 𝑥\nand 𝑦denote the data sample and its corresponding labels,\nrespectively [35]. The total sample size 𝑆sample from 𝑁\nclients is given by 𝑆sample = ∑𝑛\n𝑖=1 𝑠𝑖, ∀𝑖∈𝑁. Additionally,\nthe union of all client datasets denote by 𝐷= ∪𝑛\n𝑖=1𝐷𝑖∼.\nFL aims to minimize a global loss function (⋅) from the\n𝑁clients’ local loss function 𝑖∈𝑁(⋅) through the model\naggregation strategy techniques. The overall FL problem can\nbe formulated as\nmin\n𝐰∈ℝ(𝐰)\nwhere\n𝑁\n∑\n𝑖=1\n𝑠𝑖\n𝑆sample\n𝑖(𝐰)\n(1)\nHere, 𝑖(𝐰) = ∑𝑠𝑖\n𝑘=1 𝓁𝑖(𝐰; (𝑥𝑘, 𝑦𝑘) ∈𝐷𝑖) is the expected\nlocal loss of the 𝑖𝑡ℎclient on its local dataset 𝐷𝑖, and\n𝓁𝑖(𝐰; (𝑥𝑘, 𝑦𝑘)) is the local loss function of the shared model\nweights 𝐰on sample data (𝑥𝑘, 𝑦𝑘) ∈𝐷𝑖.\nTraditionally, clients train their local models 𝐰(𝑡)\n𝑖\ninde-\npendently at round 𝑡by optimizing the loss function 𝑖(⋅) on\ntheir local datasets. Clients validate the trained model using\na validation dataset and upload their updated models to an\naggregator for federated model aggregation. The aggrega-\ntor then performs the aggregation strategy and updates the\nshared global model of all the local model updates. Take the\nmodel averaging method, i.e., FedAvg [36], as an example.\nIt is a technique developed to reduce the variance of a global\nmodel update by periodically averaging models trained over\nmultiple communication rounds in FL. The model averaging\nYuandou Wang et al.: Preprint submitted to Elsevier\nPage 3 of 16\n\n\nManaging Federated Learning on Decentralized Infrastructures as a Reputation-based Collaborative Workflow\naggregation is often formulated as,\n𝐰(𝑡+1) =\n𝑛\n∑\n𝑖=1\n𝑠𝑖\n𝑆sample\n𝐰(𝑡)\n𝑖\n(2)\nwhere 𝐰(𝑡+1) denotes the updated global model weight for\nthe new round 𝑡+ 1. Then, it will be sent back to all active\nclients to initialize the next round of local training, itera-\ntively. This process repeats until the global loss converges.\nIt is known that high-quality data, efficient computation,\nand reliable communication at each round may contribute to\nthe local model training with high model performance (e.g.,\nhigh accuracy) and can lead to faster convergence of the local\nloss function 𝑖(𝐰). The faster speed of the convergence\nof the local training and high-quality model updates will\nmake the convergence of the global loss function quicker,\nand thus, the training time and cost will decrease for a\ntargeted model performance [34]. Therefore, participants\nwith high-quality data, high-performance computation, and\ncommunication efficiency can significantly improve the FL\nquality and efficiency.\n3.2. Federated Learning as a Workflow\nWe define an FL workflow 𝐹\n= (𝐰, 𝑁, 𝐴, 𝑡𝑇) as a\ncombination of four random sets of variables: the global\nmodel 𝐰, the participant set 𝑁, the aggregation strategy 𝐴,\nand the topology type 𝑡𝑇. These variables consist of the\nfundamental building blocks of FL workflows, and users\ncan reconfigure such blocks and customize 𝐹with their\npreferences under an agreement of all participants.\nWe build on existing solutions to develop a flexible,\nadaptable approach tailored to end users’ specific needs.\nAs a first step, we explored the component containerizer\nin NaaVRE [37] to create and store FL building blocks —\nsuch as models and aggregation strategies — as research\nassets. For instance, a model developer can build ML models\nin a local NaaVRE environment using small-scale datasets,\nperforming testing and validation. The model provider can\nthen package the ML model or code files with the FL\nframework into a container and store them on Docker Hub. It\nfacilitates the global distribution of reusable, containerized\napplications [38]. This approach enhances reproducibility,\nportability, and scalability, streamlining the integration of\nthese building blocks into reconfigurable FL workflows [39]\nand accelerating FL development.\nIn [38], we primarily used docker-nvidia for local client\ntraining with CUDA GPU resources, automating FL pipelines\nwhile allowing clients to retain control over their data and\ncomputation. For large-scale automated deployment, Docker\nSwarm serves as an alternative solution. The case study\non histological image analysis demonstrated the feasibility\nof the proposed approach, where we utilized the Flower\nframework [40] within the Jupyter environment for FL code\ndevelopment. However, most existing frameworks, such as\nTensorFlow Federated (TFF)1, NVFlare2, IBM FL [41],\n1https://www.tensorflow.org/federated\n2https://github.com/NVIDIA/NVFlare\nOpenFL [42], PySyft [43], and Flower, follow a centralized\nFL paradigm with a client-server architecture, in which the\naggregator cannot be replaced or changed during the training\nprocess. They typically rely on direct bidirectional commu-\nnication (e.g., gRPC [44]) between the aggregator and client\ntraining nodes, which limits their flexibility in supporting\ndiverse FL deployment scenarios. For example, locations\nof decentralized data infrastructure can be heterogeneous,\nexposing different hardware resources and protocols for\nauthentication, communication, resource allocation, and job\nexecution. Plus, they can be independent of each other,\nmeaning that direct communication among them may not\nbe allowed [21].\nTo overcome this limitation, we model the FL pipeline\nas a computational workflow and utilize a workflow engine\nto orchestrate its execution across remote infrastructures.\nSpecifically, we employ CWL open standards to define FL\npipelines, and make them more manageable to automate and\nparallelize the joint model training from the local computer\nto the remote cloud environments. We first employ the\nclassic workflow patterns to describe diverse computational\nFL workflows. Table 1 introduces a taxonomy of 43 control\npatterns and 40 data patterns identified in the Workflow\nPatterns Initiative (WPI)3, mapping them to a diverse range\nof FL topology types.\nA review of the 43 WPI patterns related to the control-\nflow perspective reveals that 11 patterns are supported by\nCWL v1.2 or earlier. Of the 40 data patterns, only 17 are\nsupported by CWL constructs, and just 2 of the 43 resource\npatterns are covered. Additionally, CWL standards offer\nlimited support for exception handling and do not address the\nevent log imperfection patterns defined in the WPI patterns.\nFor workflow presentation patterns, 19 are supported by\nCWL v1.2 or earlier, with one pattern still unsupported.\nCertain environments, such as blockchain networks, mobile\ndevices, and wireless systems, cannot directly interface with\nCWL-supported workflow patterns.\nFL requires loop patterns for describing the iterative\ntraining. While CWL v1.2 lacks loop constructs and re-\ncursion support, there are alternative methods to enable\nloops in describing iterative FL workflows. These include\n1) combining sub-workflows with loop extensions in the\ncurrent stable version (v1.2) and 2) the upcoming v1.3.0-\ndev1 version of CWL, which is expected to introduce loop\nconstructs in a future stable release. Figure 2 shows the\nsnippet of the main steps of a decentralized FL work-\nflow example written in CWL version v1.2. It orches-\ntrates multiple steps, including service discovery, client\nreading, initialization, and decentralized training rounds.\nThis CWL snippet set up a loop within a sub-workflow\nnamed decentralized_training_round. It manages decen-\ntralized training rounds in FL, iterating until the specified\nnumber of rounds is completed. Unlike a fixed aggregator\nin the centralized FL workflow, the aggregator is randomly\nchosen from the distributed clients to perform federated\n3http://www.workflowpatterns.com/\nYuandou Wang et al.: Preprint submitted to Elsevier\nPage 4 of 16\n\n\nManaging Federated Learning on Decentralized Infrastructures as a Reputation-based Collaborative Workflow\nTable 1\nThe mappings of different FL topology types on WPI’s workflow patterns.\nFL Topology Types\nStar\nTree\nDecentralized\nMinor\nSynchronous\nAsynchronous\nHierarchical\nDynamic\nDe-Mesh\nDe-Wireless\nBlockchain\nRing\nClique\nGrid\nFog\nSemi-Ring\nPatterns\nControl (43)\nBasic control (5/5)\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\nAdvanced Branching and Synchronization (1/14)\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\nIteration (3/3) –>loops extension\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\nMultiple Instance (1/7)\n1\n1\n1\n1\n0\n1\n1\n0\n1\n1\n1\n1\nState-based (0/5)\n0\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\nTrigger (0/2)\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\nCancelation and Force Completion (0/5)\n0\n1\n1\n1\n1\n1\n0\n0\n0\n1\n1\n1\nTermination (1/2)\n0\n0\n1\n1\n1\n1\n0\n0\n1\n1\n1\n1\nData (40)\nData Visibility (4/8)\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\nInternal Data Interaction (5/6)\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\nExternal Data Interaction (0/12)\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\nData Transfer (3/7)\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\nData-based Routing (2/7)\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\nFigure 2: Snippet of the CWL workflow.\nmodel aggregation in the decentralized training round de-\nsign. The loop feature implements a dynamic mechanism\nto handle multiple training rounds, ensuring scalability and\nflexibility in FL operations. Additionally, it can be fully\nautomated using GitHub Action workflows. More details can\nbe found in the source code, which is available in the GitHub\nrepository4.\n4https://github.com/CWL-FLOps/DecentralizedFL-CWL/\n3.3. Demonstration of FL Workflow\nWe demonstrate how the workflow-based approach en-\nables scalable automation of FL operations across cloud\ninfrastructures. Additionally, we integrate the implemented\nCWL-supported FL workflows with the Jupyter environment\nto facilitate collaborative learning and seamless workflow\nmanagement. Some related preliminary results have been\npublished in [45, 23].\nVisualized CWL-supported FL workflow. Figure 3 dis-\nplays the screenshot of the visualized CWL workflow graph\nnamed decentralizedFL.cwl via the CWL viewer tool5. It\nhas been verified with cwltool version 3.1.20230201224320\nand consists of five inputs (highlighted in a blue rectangle),\nthree steps (highlighted in a yellow rectangle), and a nested\nworkflow (highlighted in an orange rectangle). The connec-\ntions between the workflow inputs and steps illustrate the\ndependencies within the process. This workflow can serve\nas a research object bundle, enriched with comprehensive\nmetadata and licensing information to ensure its reusability.\nFor example, it is publicly available as an open-source\nworkflow and can be reused under the terms of the Apache\nLicense 2.0.\nFL workflow execution on hybrid Clouds. To demon-\nstrate the FL workflow execution over decentralized data\ninfrastructures, we utilized a well-known dataset named\nMNIST6 and five cloud instances from different providers.\nThese included one t4g.small instance (2 ARM-based vC-\nPUs, 2 GiB memory) and t3.small instance (2 x86-based vC-\nPUs, 2 GiB memory) from Amazon Web Services (AWS),\none t2a-standard-1 instance (1 ARM-based vCPU, 4 GiB\nmemory) and e2-medium instance (2 x86-based vCPUs, 4\nGiB memory) from Google Cloud Platform (GCP), and\none lab.uvalight.net instance (2 x86-based vCPUs, 2 GiB\n5https://view.commonwl.org/\n6https://git-disl.github.io/GTDLBench/datasets/mnist_datasets/\nYuandou Wang et al.: Preprint submitted to Elsevier\nPage 5 of 16\n\n\nManaging Federated Learning on Decentralized Infrastructures as a Reputation-based Collaborative Workflow\nFigure 3: The screenshot of the visualized FL workflow in the\nCWL viewer with detailed license, specification, and metadata.\nmemory) from the OpenLab infrastructure provided by the\nuniversity. Each dataset stored on the cloud instances was\nsplit from the MNIST dataset, with a size of approximately\n60 MB. The FL source code is based on PyTorch, reproduced\nfrom [46], where the author used the FedAvg method for the\nfederated model aggregation. We re-configured the FL work-\nflow with ten local training epochs and 12 communication\nrounds. Each setup was executed ten times to collect results\nfor statistical analysis.\nThe average training time is approximately (9.786 ±\n0.238) minutes, and the average test accuracy achieved by\nthe FL workflow execution is (98.127 ± 0.085)%. These\nempirical results demonstrate that CWL-supported FL work-\nflows are scalable, reusable, and portable, making them\nsuitable for execution in off-chain decentralized infrastruc-\ntures such as hybrid cloud environments. Since the CWL\ncommunity provides standardized alternatives for defining\nportable and reusable workflows that remain engine- and\nvendor-neutral, any CWL-compliant workflow execution en-\ngine should be able to execute this standardized workflow\ndescription and produce consistent results, regardless of the\nunderlying infrastructure [47]. Furthermore, by incorporat-\ning the principles of FAIR computational workflows within\nCWL, we can advance toward realizing FAIR FL workflow\nmanagement in future work.\n3.4. Lessons and Challenges\nThe empirical results demonstrate that CWL effectively\ndescribes FL workflows and automates their execution using\ncloud technologies. However, this approach assumes a pre-\nestablished collaboration, where distributed data sources are\npredefined within the workflow.\nCWL does not inherently address trust and reliable col-\nlaboration among multiple participants in FL workflows.\nThe decentralized nature of data providers challenges the tra-\nditional centralized workflow management paradigm, where\nproviders are predefined for composing application-specific\nFL workflows. To overcome this, an effective collaboration\nframework is needed to enable the dynamic selection of\ndata or resource providers and ensure the quality of their\ncontributions.\nIn the next section, we will explore how to incentivize\ncollaborations and ensure training quality by addressing the\nfollowing research questions:\n• How can we incentivize participants with high-quality\ncontribution to join the FL training process?\n• How can we assure training quality from decentralized\nparticipants in an unreliable environment?\n4. Decentralized Collaboration Framework\nTo tackle the challenges identified in 3.4, this section in-\ntroduces a decentralized collaboration framework that aims\nto manage the dynamic collaboration among participants\nwithin a decentralized community. The basic design ideas\ninclude:\n• Using blockchain environment to manage the decen-\ntralized relation among FL participants and to record\ntheir interaction history.\n• Managing the collaboration agreement by employing\nthe smart contract of the blockchain, where an FL\ndeveloper can explicitly describe the conditions for\ndesired participants in a smart contract.\n• Verifying the contribution through blockchain ledgers\nand manage the update of the FL aggregations via\ndynamic consensus among participants through smart\ncontracts.\n4.1. System Overview\nFigure 4 depicts the basic idea for FL applications, build-\ning on our previous work on the D-VRE framework [48].\nThis framework equips Jupyter users with essential com-\nponents for enabling robust FL collaboration, integrating a\npersonal Jupyter working environment, a blockchain net-\nwork, and off-chain legacy resources, such as data storage,\ncomputation, and networking, within a decentralized ecosys-\ntem. It enables a group of Jupyter users with different roles\nto engage in scientific community activities and create FL\nbuilding blocks as research assets. This is achieved through\nthe Create FL Building Blocks function (Component Con-\ntainerizer within NaaVRE).\nYuandou Wang et al.: Preprint submitted to Elsevier\nPage 6 of 16\n\n\nManaging Federated Learning on Decentralized Infrastructures as a Reputation-based Collaborative Workflow\n \nfrontend \n \n \n \n \n \n \nSystem status \n updates\nBlockchain Network\n \nData owner stakes their native  \ntokens in staking pool\n \n \n \n \nBlock data\n... ...\nBlockchain network\nNode 1\nNode 2\nNode i\nNode j\nNode N\nMake an agreement for the FL workﬂow\n \nLocal updates evaluation\nPost an ad to call for collaboration (CfC)\n \n \n \n \n \n \n \n \n \n \nIndividual user with digital wallet\n \nData owner\n \nData owner \nCompute\nPrivate Cloud\n \n \n \n \nData\n \n \n \nData\nPrivate Cloud\n \n \n \n \n \n \n \n \n \n \nfrontend \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nfrontend \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Create ML Model \n  [ Component Containerizer ]\n Customize FL Workﬂows \n   [ Workﬂow Composer ]\nSign Multidimensional Contracts \n  [ Make Agreements ]\nFL computational workﬂow examples\nDeliver models to data owners (stakeholders), trigger the deployment and execution of FL workﬂow(s) off-chain\nCWL engine \nScale FL workﬂows\n...\nHierarchical FL\nDecentralized FL\nCentralized FL\n......\n \n \nData\n \nfrontend \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nInfra owner \nCompute\nfrontend\nLocal Virtual Research Environment\nTraining Node\nAggregator\nFederated Node\nSmart contract\nBlockchain\n \n \n \n \nTokens\n Access to blockchain\n  [ Auth via Digital Wallet ]\nFigure 4: This is an overview of the system for FL applications. A local virtual research environment enables a user to develop\nML models on-premise and unlocks the potential of establishing robust collaboration in a decentralized environment via ❶❷\nand ❸. It consists of two main interfaces: 1) via blockchain interfaces, users can access to the blockchain network to call for\ncollaboration, make an agreement, and evaluate local updates on-chain; 2) via workflow runtime interfaces, users can create ML\nmodels, customize FL workflows, operate collaborative workflows on decentralized infrastructure.\nThrough the function Access to blockchain (Auth via\nDigital Wallet within D-VRE), the system allows users to\nconfigure their digital wallets like MetaMask, to unlock the\ndecentralized Web and applications to call for collaboration\nand secure asset sharing in a decentralized environment [48].\nDue to the diverse data access restrictions, data can not\nbe moved out of the institutions [38]. The system allows\nmodel providers (also called FL task publishers) to Call\nfor Collaborations (CfCs) to enable secure data sharing,\ncomputing resource sharing, and collaborative learning from\nthe decentralized community, via the Sign multidimensional\ncontracts function (Make Agreements within D-VRE). Data\nor resource owners can respond to the CfCs via the interfaces\nembedded in their personal frontends.\nEach CfC will include specific requirements, e.g., the\ndata and resource specifications, required stakes, targeted\nmodel performance, minimum number of participants, and\nwhat the publisher can offer (such as reward), rules of\npenalty, and reputation updates resulting from different con-\ntribution behaviours. These requirements can be explicitly\ndescribed in a smart contract. The stake is related to crypto\nstaking in the blockchain system, which is the practice of\nlocking participants’ digital tokens to a blockchain network\nto earn rewards. For instance, a participant’s stake can be re-\nturned later with an additional reward if there is no indication\nof malicious behaviours [49]. The stake plays a crucial role\nin reliable FL, as 1) it can increase the cost of attacks and\n2) promote risk-sharing during FL system operations. The\ncandidate has to join the contract to make agreements.\nVia the function of Customize FL workflow (Workflow\nComposer and CWL-based FL workflow composition),\nusers who signed contracts acting as FL participants can\ncustomize a computational FL workflow through verified\nresources from the off-chain decentralized (data) infras-\ntructures. We assume that data are stored in the infras-\ntructures which provide computation capacity and network\nfor communication. Some participants may only contribute\nto infrastructure, e.g., computing, storage, and network\nresources. Each participant can be identified and verified\nfor their resources through trusted external committees,\nsuch as Oracle and DAO [50]. Hence, the composition of\ncomputational FL workflows can combine the on-chain and\noff-chain services to scale different FL scenarios.\nAfter the FL workflow with on-chain engagement has\nbeen confirmed, the system generates a number of smart con-\ntract instances to enforce the contract items and ensures that\nprecise, mutually agreed-upon terms govern collaborations\nin the decentralized network. The system enables users to\nstart the CWL engine for automating FL workflow deploy-\nment and execution over legacy data infrastructures while\nenabling secure and transparent execution in the blockchain\nenvironment. Since the block data has stored ledgers about\nFL actions for all participants in the blockchain network,\nit can correlate the captured data with user behaviours to\nmaintain a transparent record of any changes made to an\nFL operation. Finally, users can evaluate local updates of\ntheir FL workflows related to domain-specific subjects and\nanalyze the consequences of participants’ actions regarding\nrelevant task contributions, reputation updates, and rewards.\n4.2. How to Incentivize Collaboration?\nIn real-life scenarios, the FL task publisher does not\nknow which users in the system would join the FL training\ndue to the lack of prior information. The local data quality\nYuandou Wang et al.: Preprint submitted to Elsevier\nPage 7 of 16\n\n\nManaging Federated Learning on Decentralized Infrastructures as a Reputation-based Collaborative Workflow\nand the computing power of available resources from poten-\ntial candidates are unknown to the publisher owing to data\nprivacy and security concerns. Therefore, it is essential for\nthe task publisher to design an efficient incentive mechanism\nto stimulate active engagement from high-quality candidates\nwhile reducing the over-reward risks caused by the issues\nof asymmetric information [34, 51]. This study employs\nthe contract theory with a multidimensional scheme as an\nefficient approach to address the incentive collaboration\nproblem.\nAssume a set of ℕpotential candidates responds to the\nCfC task in the system. Let 𝑁= 1, 2, ⋯, 𝑛, where 𝑁⊆ℕ,\nrepresent the set of identity-verified candidates who have\nsigned contracts with stakes 𝑆= 𝑆1, 𝑆2, ⋯, 𝑆𝑛and initial\nreputations 𝑟0 = 𝑟0\n1, 𝑟0\n2, ⋯, 𝑟0\n𝑛. These candidates act as FL\nparticipants, continuously contributing to the FL network.\nEach participant’s reputation is dynamically updated based\non their behaviour.\nLet 𝜃denote a standard type space to differentiate the\nparticipant nodes. Suppose participants are classified into 𝑁\ntypes, which can be sorted in a descending order regarding\ncontribution types: 𝜃1 > 𝜃2 > ⋯> 𝜃𝑛, 𝑖∈{1, 2, ⋯, 𝑛}.\nLet ̂𝜃𝑖be the claimed type by participant 𝑖with the true\n𝜃𝑖. We assume that candidates who join the FL must claim\ntheir types ̂𝜃and select the corresponding contract item 𝜙̂𝜃.\nAlthough the task publisher does not know about the true\ntype of a given participant due to the information asym-\nmetry, it has information about the reported type ̂𝜃𝑖by the\nparticipant with the true type 𝜃𝑖(e.g., data and infrastructure\nspecifications), from which the publisher can inform the\nprobability that a participant belongs to a particular type 𝜃𝑖\nwith reported (meta)data and computing power categories,\ndenoted as ∑𝑁\n𝑖=1 𝑝𝑖(̂𝜃𝑖) = 1.\nFor participants with different contributions, the task\npublisher signs different contract items with them. We define\nthe contract = (𝑇max, Φ) is comprised of a maximum\nwaiting time 𝑇max and contract terms Φ = {𝜙𝑖}𝑖∈𝑁. 𝑇max\ndenotes the maximum allowable time for receiving partic-\nipants’ contributions. Each contract item 𝜙𝑖≜(𝐶𝑖, 𝑆𝑖, 𝑅𝑖)\nspecifies the relationship among each type-𝑖participant’s\nstake 𝑆𝑖, contribution 𝐶𝑖(often associated with a completion\ntime 𝜏𝑖≤𝑇max), and its corresponding reward 𝑅𝑖. Any par-\nticipant who completes their contributions to the FL network\nwithin the required time during each communication round\nwill receive a reward 𝑅𝑖. Conversely, participants whose\ncompletion time 𝜏𝑖exceeds 𝑇max will receive a zero reward,\nresulting in a zero contract instance.\nContribution model. We define a contribution model for\neach FL workflow. Let 𝐶(𝑡)\n𝑖\ndenote an estimated contribution\nbased on the 𝑖𝑡ℎparticipant’s local model training 𝑓local and\nits behaviours 𝑔behaviour at round 𝑡, which can be formally\ndefined as:\n𝐶(𝑡)\n𝑖\n=𝑓local(𝐷𝑖, 𝐰(𝑡)\n𝑖) ⋅𝑔behaviour(ℎ(𝑡)\n𝑖),\n∀𝑖∈𝑁\n(3)\nwhere 𝐶(𝑡)\n𝑖\n≥0 and ℎ(𝑡)\n𝑖\nis the historical behaviour records\nregarding reputation and stake which can be used to differ-\nentiate honest participant nodes and malicious ones.\nLet 𝑇max denote the upper limit on the duration in FL\nsynchronous settings, within which participants must sub-\nmit their contributions, such as local model updates 𝐰(𝑡)𝑖,\nbefore the aggregation is triggered within the federation.\nSpecifically, participants with 𝜏𝑖≤𝑇max can complete the\nsubmission of model updates to allow the aggregation of\nfederated models to proceed on time per communication\nround. We define the contribution value function 𝑉(⋅) for\nany participant 𝑖∈𝑁as follows:\n𝑉(𝐶(𝑡)\n𝑖, 𝜏𝑖) = 𝑋𝑐\n𝜏𝑖\n⋅𝑞(𝑡)\n𝑖,\n𝑞(𝑡)\n𝑖\n= 𝜎(\n𝐶(𝑡)\n𝑖\n−𝐶min\n𝐶max −𝐶min\n)\n(4)\nwhere 𝜏𝑖is the time cost for participant 𝑖to make contribu-\ntion 𝐶(𝑡)\n𝑖, and 𝑋𝑐\n𝜏𝑖represents the unit price of the contribution\nfor participant 𝑖, where 𝑋𝑐is a constant factor representing\nthe value associated with the contribution. We define 𝑞(𝑡)\n𝑖\nas a dynamic quality parameter via the sigma function 𝜎(⋅),\nwhere 𝐶max represents the saturation threshold for contri-\nbution, and 𝐶min is the minimum required contribution. A\nhigher value of 𝑉(⋅) indicates better quality in the local\ntraining contribution, leading to fewer training iterations\nneeded to reach a targeted model performance score (e.g.,\naccuracy, precision, recall, or F1 score).\nUtility function of participants. For a signed contract 𝜙𝑖,\nwe define the utility function of the type-𝑖participant at\nround 𝑡as follows,\n𝑈𝑐(𝜙𝑖|𝜃𝑖) = 𝑅𝑖𝕀compl −𝜆𝑠𝑆𝑖(1 −𝕀compl) −𝑐(𝐶(𝑡)\n𝑖) (5)\n𝕀compl = exp(−\n𝐾\n∑\n𝑘=1\n𝜔𝑘⋅VSL𝑘).\n(6)\nThe compliance condition, 𝕀compl ∈[0, 1], is derived from\nthe violation levels. For example, minor violations (e.g.,\noccasional timeouts) will result in a partial retention of\nproceeds, while severe violations (e.g., malicious behaviour)\nwill reduce the proceeds to zero. The normalized violation\nlevel for the 𝐾violation types is denoted by VSL𝑘∈[0, 1],\nwith 𝜔𝑘representing the weight of the 𝑘𝑡ℎviolation type. A\nvalue of 𝕀compl = 1 indicates no violations, while 𝕀compl = 0\nsignifies severe violations.\nThe parameter 𝜆𝑠defines the penalty factor applied to\nthe 𝑖𝑡ℎparticipant’s stake, 𝑆𝑖. If a node operates without\nviolations, a proportion of the stake, 𝜆𝑠𝑆𝑖, is allocated to\nthe system’s risk reserve. However, if violations occur, the\nparticipant risks forfeiting the entire stake, 𝜆𝑠𝑆𝑖. Addition-\nally, 𝑐(⋅) represents the effort cost for contributing 𝐶(𝑡)\n𝑖\nwith\ntime 𝜏𝑖by the type-𝑖participant. This cost can be further\nextended to more complex expressions by factoring in the\nresource utilization required for model training, validation,\nand aggregation.\nProfit function of the task publisher. Task publisher’s\nprofit derived from a type-𝑖participant is influenced by\nYuandou Wang et al.: Preprint submitted to Elsevier\nPage 8 of 16\n\n\nManaging Federated Learning on Decentralized Infrastructures as a Reputation-based Collaborative Workflow\nmultiple factors. Although a high-quality contribution can\nincrease the task publisher’s profit, it also incurs a higher re-\nward cost for the publisher. In addition, any violations (e.g.,\ntimeouts, malicious actions, or mismatched behaviours) by\nparticipants will result in a deduction from their stake (or\ntokens) as penalties, which are then allocated as profits to the\npublisher. Therefore, we define the profit function obtained\nfrom participant 𝑖as follows:\n𝜋(𝜙𝑖) =(𝑉(𝐶(𝑡)\n𝑖) −𝑅𝑖)𝕀compl + 𝜆𝑠𝑆𝑖(1 −𝕀compl),\n(7)\nwhere 𝑉(𝐶(𝑡)\n𝑖) −𝑅𝑖≥0, the profit is positive; otherwise,\nthe publisher risks incurring a negative profit, even if no\nviolations occur from any participant 𝑖.\nTo make the contract feasible, it must satisfy the follow-\ning constraints simultaneously.\nDefinition 1 (Individual Rationality (IR)). Let 𝑈𝑐(𝜙𝑖|𝜃𝑖) be\nthe utility for the type-𝑖participant under the contract 𝜙𝑖.\nEach type-𝑖participant achieves the non-negative utility if it\nchooses the contract item 𝜙𝑖that is designed for its own type\n𝜃𝑖, which is given by\n𝑈𝑐(𝜙𝑖|𝜃𝑖) ≥0.\n(8)\nDefinition 2 (Incentive Compatibility (IC)). Each partici-\npant 𝑖achieves the maximum utility by truthfully reporting\nits type 𝜃𝑖if it chooses the contract item 𝜙𝑖that is designed\nfor its own type 𝜃𝑖rather than other types 𝜃−𝑖in contract\nitems 𝜙𝜃−𝑖. The mechanism is incentive compatible if\n𝑈𝑐(𝜙𝑖|̂𝜃𝑖= 𝜃𝑖, 𝜙𝜃−𝑖) ≥𝑈𝑐(𝜙𝑖|̂𝜃𝑖≠𝜃𝑖, 𝜙𝜃−𝑖),\n(9)\n𝑈𝑐(𝜙𝑖|̂𝜃𝑖= 𝜃𝑖, 𝜙𝜃−𝑖) ≥𝑈𝑐(𝜙𝑗|̂𝜃𝑗= 𝜃𝑗, 𝜙𝜃−𝑖),\n(10)\nwhere 𝜃−𝑖∪𝜃𝑖= 𝜃, 𝜃−𝑖≠𝜃𝑖, and 𝜃𝑖≥𝜃𝑗for all participants\n𝑖, 𝑗∈𝑁.\nAs a rational individual, the task publisher aims to max-\nimize its profit in the system [52]. Therefore, the total\nincentive problem is given by\nmax Π(Φ) =\n𝑁\n∑\n𝑖=1\n𝑝𝑖(̂𝜃𝑖) ⋅𝜋(𝜙𝑖),\n∀𝜙𝑖∈Φ\n(11)\ns.t., IR(𝐸𝑞. 8) and IC(𝐸𝑞𝑠. 9, 10) constraints.\n(12)\nTo mitigate the risks of over-rewarding or insufficient re-\nwards, it is crucial to examine the optimality of the contract\nproblem.\n4.3. How to Assure Training Quality along with\nContract Optimality?\nThis section first employs a mechanism design approach\nthat integrates a reputation system and dynamic incentive\nindicators within contracts to ensure training quality in an\nunreliable environment. Then, leveraging a quantified re-\nward measurement, we analyze the optimality of the con-\ntract problem. Figure 5 illustrates the key steps involved in\nassuring training quality, including reputation-based com-\nmittee selection, malicious detection and penalty, reputation\nupdates, and reward calculation.\nReputation-based committee selection. We introduce a\ncommittee of high-reputable and highly effective nodes\namong participants to do extra work like being responsible\nfor partial model validation and aggregation. Let (𝑡) be the\nfinal selected committee in each round 𝑡, where (0) = ∅\nand is the committee size. We use a stratified sampling\nmethod [53] based on reputation 𝑟(𝑡) = {𝑟(𝑡)\n1 , 𝑟(𝑡)\n2 , ⋯, 𝑟(𝑡)\n𝑛}\nand a cooling-down mechanism, to balance the advantages\nof highly reputable nodes.\nFirst, we sort the nodes with reputation descending\nand divide them into several strata layers. Let 𝑁sorted =\nsort(𝑁, 𝑟𝑖↓) be the sorted set and 𝐿be the number of\nstrata to divide the nodes into. For each stratum layer 𝐿𝑘=\n𝑁sorted[ (𝑘−1)𝑁\n𝐿\n, 𝑘𝑁\n𝐿), 1 ≤𝑘≤𝐿, the initial quota per stratum\n𝑄𝑘is calculated by\n𝑄𝑘= ⌊\n𝐿⌋+\n{\n1,\nif 𝑘≤\nmod 𝐿,\n0,\notherwise.\n(13)\nFor each layer, the set of eligible nodes is 𝑘= {𝑖∈\n𝐿𝑘|𝑐𝑑(𝑡)\n𝑖\n= 0}, where 𝑐𝑑(𝑡)\n𝑖\n= 0 indicates the node 𝑖is\nnot cooling down. Hence, we can determine the number of\nselected nodes 𝑚𝑘= max(1, min(𝑄𝑘, |𝑘|)) per layer, using\nthe selection probability sampling performed as follows:\nP(𝑖) =\n(𝑟𝑖)𝛾\n∑\n𝑗∈𝑘(𝑟𝑗)𝛾,\n𝑗∈𝑘\n(14)\nwhere 𝛾∈(0, 1] is a decay exponent of reputation co-\nefficient. Then, 𝑘is obtained by randomly selecting 𝑚𝑘\nnodes in 𝑘based on selection probabilities P(𝑖) without\nreplacement. Hence, it follows a hypergeometric distribu-\ntion in probability theory and statistics [54], which can be\nformally stated as 𝑘∼Hypergeometric(𝑚𝑘, 𝑘, P(𝑖)).\nIn the case of ∑𝐿\n𝑘=1 |𝑘| < , we calculate a re-\nmaining quota remain = −∑𝐿\n𝑘=1 |𝑘| and gather all\neligible nodes not yet selected as a global candidate pool,\nglobal = (∪𝐿\n𝑘=1𝑘)∖(∪𝐿\n𝑘=1𝑘). Again, using the selection\nprobability sampling on the global eligible pool to calculate\nthe remaining committee remain, which is given by,\nP′(𝑖) =\n(𝑟𝑖)𝛾\n∑\n𝑗∈global (𝑟𝑗)𝛾,\n𝑗∈global\n(15)\nHence, remain ∼Hypergeometric(remain, global, P′(𝑖)),\nand the total committee is obtained by = (∪𝐿\n𝑘=1𝑘) ∪\nremain.\nAfter the final selection, the system updates each node’s\nstatuses, including the selected history and the cooldown\ntime 𝑐𝑑𝑖, which is formulated by\n𝑐𝑑(𝑡+1)\n𝑖\n=\n{\n3,\nif 𝑖∈,\nmax(0, 𝑐𝑑(𝑡)\n𝑖\n−1),\notherwise.\n(16)\nYuandou Wang et al.: Preprint submitted to Elsevier\nPage 9 of 16\n\n\nManaging Federated Learning on Decentralized Infrastructures as a Reputation-based Collaborative Workflow\n   \n   The proposed FL workﬂow run per round\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ncontracts\nstart a new FL round\nSelect committee\nmembers\nPerform federated model\naggregation\nTraverse all nodes\nYes\nNo\nMalicious?\nApply penalty to\nmalicious nodes\nUpdate\nreputation\nCalculate reward\nRecord metrics\nEnd the round\nCollect all contributions\nMake contributions\nFigure 5: Flowchart of FL system operations per round.\nwhere we specify that if 𝑖is selected as a committee, the\nsystem will update the cooldown as three at the current round\nand decrease the cooling time of the unselected nodes by one\ncooldown per round.\nReputation update model. We employ a dynamic reputa-\ntion update model to encourage long-term high-quality and\nstable contributions for all participants through a bonus for\nthe quality and stability of contributions while discouraging\nmalicious behaviours by penalties on reputation. To achieve\nthat, we consider a dynamic decay factor 𝛿∈[0, 1] to\nincentivize long-term participation by obtaining a reputation\ncompensation from the last round, which is given by\n𝛿= 𝛿𝑏+ 𝜆𝑝⋅(1 −\n1\n1 + participation𝑖∕100),\n(17)\nwhere 𝛿𝑏is a base decay factor. 𝜆𝑝denotes a decay com-\npensation parameter and participation𝑖is the 𝑖𝑡ℎpartic-\nipant’s historical participation times. As such, the more\nparticipation𝑖, the higher the reputation compensation 𝛿⋅\n𝑟(𝑡−1)\n𝑖\nfor round 𝑡.\nBesides, a good quality of contribution is good for an\nincreased reputation. Let 𝑋𝑐denote a unit bonus from the\ncontribution quality to reputation updates, the amount of in-\ncreased reputation resulting from good contribution is given\nby 𝑋𝑐⋅𝑞(𝑡)\n𝑖, where 𝑞(𝑡)\n𝑖\nis quantified by Eq. 4. Additionally,\nwe define 𝜆stab = 1 −std([𝐶𝑡−𝜏\n𝑖\n, ⋯, 𝐶𝑡\n𝑖])∕𝜏as a stability\nparameter during the last 𝜏rounds to avoid malicious nodes\nwith random attacks to obtain rewards. Hence, the new\nreputation at round 𝑡can be defined by\n𝑟(𝑡)\n𝑖\n= 𝛿𝑟(𝑡−1)\n𝑖\n+ 𝑞(𝑡)\n𝑖𝑋𝑐+ 𝜆stab𝑋𝑠,\n(18)\nwhere 𝑋𝑠denote unit bonus from the stability to reputation\nupdates and 𝑟(𝑡)\n𝑖\n∈[0, 𝑟max]. We employ a dynamic maxi-\nmum capability 𝑟max per round to limit the upper bound of\nreputation updates.\nMalicious detection and penalty. To safeguard the FL pro-\ncess, we consider malicious detection and penalty through\na set of conditions. It consists of: 1) persistent low contri-\nbutions (condition 1), 2) abnormal fluctuations (condition\n2), and 3) sudden behavioural changes (condition 3), which\nserve as indicators for detecting malicious nodes. For ease\nof analysis, we assume that any participants’ behaviours\nmeet the condition set (condition 1 AND condition 2) OR\ncondition 3, the system will detect them as a set of malicious\nnodes 𝑁malicious and perform a penalty to them. The penalty\nfunction is defined by,\npenalty = min(𝜆𝑟𝑟𝑗+ 𝜆𝑠𝑆𝑗,\n𝑟𝑗\n2 ),\n(19)\nwhere 𝜆𝑟, 𝜆𝑠denote the factors of applying penalty to repu-\ntation updates and stake, respectively. The upper bound\n𝑟𝑗\n2\nis intended to avoid excessive penalties on the reputation.\nHence, the updated reputation for node 𝑗at round 𝑡is given\nby 𝑟(𝑡)\n𝑗= 𝑟(𝑡−1)\n𝑗\n−penalty, ∀𝑗∈𝑁malicious.\nReward calculation. Let 𝐵denote the base reward of the\nreward pool. The reward calculation considers a participant’s\nstake, historical contribution performance, and committee\nbonus. The stake and contribution proportions relative to\nthe overall records influence the rewards participants receive\nfrom the reward pool.\nWe introduce two dynamic weights, 𝛼and 𝛽, based on\nreputation to balance the influence of stake and historical\ncontributions in reward calculation, where 𝛽= 1 −𝛼. The\nweight 𝛼is defined as 𝛼= 𝜎(\n𝑟−𝑟0\n𝑖\n𝑓scale ) ⋅𝜆stake, where 𝜆stake\nis the global stake weight, and reputation 𝑟determines the\nadjustment.\nTo prevent monopolization, we introduce an effective\nstake limit for large nodes, ensuring no single node domi-\nnates. The effective stake is given by 𝑆eff\n𝑖\n= min(𝑆𝑖, 3 ⋅𝑆),\nwhere nodes holding more than three times the average stake\n𝑆are capped.\nThe historical contribution of participant 𝑖over the last 𝜏\nrounds is computed as 𝐶hist\n𝑖\n= ∑𝜏\n𝑡=0 𝐶(𝑡−𝜏)\n𝑖\n⋅𝜁𝑡, where 𝜁𝑡(𝜁∈\n(0, 1)) is an exponential decay factor that prioritizes more\nrecent contributions. The total historical contribution across\nall participants is given by 𝐶total = ∑\n𝑖∈𝑁\n∑𝜏\n𝑡=0 𝐶(𝜏−𝑡)\n𝑖\n⋅𝜁𝑡.\nYuandou Wang et al.: Preprint submitted to Elsevier\nPage 10 of 16\n\n\nManaging Federated Learning on Decentralized Infrastructures as a Reputation-based Collaborative Workflow\nAny participant selected as a committee member will\nreceive a bonus for their contributions to model validation\nand aggregation, which is formulated by,\n𝑅cmm =\n{\n𝐵cmm ⋅𝐽c(𝑟),\nif 𝑖∈(𝑡),\n0,\notherwise.\n(20)\nHere, 𝐽c(𝑟) =\n(∑\n𝑖=1 𝑟𝑖)2\n⋅∑\n𝑖=1 𝑟𝑖2+𝜖⋅𝜎( 𝑟\n10), 𝐵cmm represents a base\nbonus, and 𝐽𝑐(𝑟) is an improved Jain’s fairness index [55]\ncalculated from the reputation scores of the committee. A\nsmall constant 𝜖is introduced to prevent division by zero\nerrors. The sigmoid function 𝜎( 𝑟\n10) ensures that the average\nreputation score is mapped to the range (0, 1). When the\nmean reputation 𝑟is low, the overall score decreases accord-\ningly. If the fairness index 𝐽𝑐(𝑟) is low, the committee reward\n𝑅cmm is also reduced, which promotes fair and balanced\nincentives.\nFurthermore, we employ 𝐽(𝑟) based on reputation, as\na fairness indicator to ensure equitable reward distribution\namong all participants 𝑖∈𝑁when earning rewards from\ntheir stake and historical contributions. Consequently, the to-\ntal reward allocated to each participant at round 𝑡is formally\ndefined as\n𝑅(𝑡)\n𝑖\n= (𝛼⋅𝐵\n𝑆eff\n𝑖\n∑𝑆𝑗\n+ 𝛽⋅𝐵\n𝐶hist\n𝑖\n𝐶total\n) ⋅𝐽(𝑟) + 𝑅cmm (21)\nNote that if a participant has no contribution history (i.e.,\n𝐶hist\n𝑖\n= ∅or 𝐶(𝑡)\n𝑖\n= 0), the current reward is set to zero,\n𝑅(𝑡)\n𝑖\n= 0. Therefore, it enables adjustable weights and zero-\ncontribution handling to calculate the participant’s reward.\nThe entire procedure can be implemented as a reputation-\nbased consensus in a smart contract for reliable FL pro-\ncesses, whose pseudocode is presented in Algorithm 1.\nContract optimality. With the above formulated optimiza-\ntion problem and detailed reward model, we can now study\nthe optimality of the contract problem.\nLet 𝜙∗\n𝑖\n≜(𝐶∗\n𝑖, 𝑆∗\n𝑖, 𝑅∗\n𝑖) ∈Φ∗, ∀𝑖∈𝑁in the new\nordering be the optimal contract that the task publisher\ncan derive from maximizing its total profit Π(⋅) and each\nparticipant performs optimal behaviours to maximize its\nutility. These contract items are found through solving the\noptimization problem as presented in Eq. 11 under the IC\nand IR constraints. To derive the optimal contract for this\nproblem, there may be several solutions.\nFor ease of analysis, we simplify the derivation process.\nBy assuming that: 1) all participants comply with the rules\nviz 𝕀compl = 1 (no violations), 2) consider a single participant\nscenario that can ignore the IC constraint, and 3) the cost of\ncontribution 𝑐(𝐶) = 1\n2𝛾𝑐𝐶2 where 𝛾𝑐> 0 is a cost parameter\nand 𝐶∈[0, 𝐶max], the relaxation version of the optimal\nproblem can be formulated as\nmax\n𝐶,𝑆,𝑅Π = 𝑉(𝐶) −𝑅,\ns.t. 𝑅−1\n2𝛾𝑐𝐶2 ≥0.\n(22)\nIf we ignore the IR constraint and directly maximize profit\nΠ, we need to define the relationship between the reward 𝑅\nAlgorithm 1: Reputation-based reliable FL con-\ntract.\nOutput: Well-trained model 𝐰, updated reputation\n𝑟, reward 𝑅.\nInput: FL workflow 𝐹, consisting of: an initial\nmodel 𝐰, a set of 𝑁participants indexed by\n𝑖, an aggregation strategy 𝐴, and system\nconfiguration.\n1 Initialize the FL workflow 𝐹;\n/* Define the data structures:\n*/\n2 struct Node { id, stake, reputation, totalReward,\nviolations, participation, cooldown, ...,\nidentityVerified }\n3 struct SystemConfig { baseReward, committeeSize,\nstakeWeight, ..., maliciousPercent }\n4 contract FLSystem {\n// Define the main contract\n5\nInitialize the nodes;\n6\nInitialize the SystemConfig;\n7 ⋯\n/* Core logic of the main contract:\n*/\n8\nfunction runRound() {\n9\nfor each round 𝑡do\n10\nCollect 𝐶(𝑡) = {𝐶(𝑡)\n𝑖\n∣∀𝑖∈𝑁} within 𝑇max;\n11\n(𝑡) ←selectCommittee(𝑟(𝑡), , 𝐿);\n12\ncommittee performs aggregation;\n13\n/* Malicious detection and penalty:\n*/\n14\nif ∀𝑖∈𝑁malicious = ∅then\n15\nUpdate reputation: 𝑟(𝑡)\n𝑖\n←Eq. 18;\n16\nelse\n17\nApply penalty: 𝑝𝑒𝑛𝑎𝑙𝑡𝑦𝑖←Eq. 19;\n18\n𝑟(𝑡)\n𝑖\n←𝑟(𝑡−1)\n𝑖\n−𝑝𝑒𝑛𝑎𝑙𝑡𝑦𝑖, ∀𝑖∈𝑁malicious;\nviolation + +;\n19\nCalculate rewards: 𝑅(𝑡)\n𝑖\n←Eq. 21, ∀𝑖∈𝑁;\n20\n𝑡+ +;\n/* Stop when an end condition triggers\n*/\n21\nreturn final model weights 𝐰, 𝑟, 𝑅.\n22\n}\n23 Each FL client executes:\n24\nfor each client 𝑖∈𝑁at round 𝑡in parallel do\n25\nUpdate model and calculate contributions:\n𝐰(𝑡)\n𝑖, 𝐶(𝑡)\n𝑖\n←Eqs. 1, 3;\n26 Committee executes:\n27\nAggregate model weights: 𝐰(𝑡+1) ←Eq. 2;\n(Eq. 21) and contribution value 𝑉(𝐶) (Eq. 4). We further\nassume 1) there are no monopoly stakes 𝑆≤3𝑆, then\n𝑆eff = 𝑆, 2) 𝑟𝑖= 𝑟, then 𝛼= 𝜆stake, and 3) both 𝐶hist and\n𝐶total are constants. At this point, maximizing Π is equivalent\nto differentiating the objective function with respect to the\nfirst term 𝐶and solving the first-order condition, which can\nbe rearranged as\n𝜕Π\n𝜕𝐶= 𝑉′(𝐶)−(1 −𝜆stake)𝐵𝐽(𝑟)\n𝐶total\n⋅𝜕𝐶hist\n𝜕𝐶\n= 0,\n(23)\nYuandou Wang et al.: Preprint submitted to Elsevier\nPage 11 of 16\n\n\nManaging Federated Learning on Decentralized Infrastructures as a Reputation-based Collaborative Workflow\nwhere we assume 𝐶hist = 𝐶⋅∑𝜏\n𝑡=0 𝜁𝑡, which leads to\n𝑉′(𝐶) = (1 −𝜆stake)𝐵𝐽(𝑟)\n𝐶total\n⋅1 −𝜁𝜏+1\n1 −𝜁\n.\n(24)\nCorrespondingly, we define 𝑘=\n1\n𝐶max to extend the 𝑉(𝐶)\nas the full version and calculate its derivative 𝑉′(𝐶), which\nis calculated by,\n𝑉′(𝐶) =\n𝜕( 𝑋𝑐\n𝜏⋅\n1\n1+𝑒−𝑘𝐶)\n𝜕𝐶\n= 𝑋𝑐⋅𝑘𝑒−𝑘𝐶\n𝜏(1 + 𝑒−𝑘𝐶)2 ,\n(25)\nThus, the optimal contribution 𝐶∗made by participant can\nbe derived from 𝑉′(𝐶∗) (Eq. 24) = 𝑉′(𝐶∗) (Eq. 25), which\nis given by\n𝐶∗= 1\n𝑘ln(\n𝑋𝑐⋅𝑘𝜏(1 −𝜁)\n(1 −𝜆stake)𝐵𝐽(𝑟)(1 −𝜁𝜏+1)𝐶total\n−1).\n(26)\nSince 1\n𝑘= 𝐶max, then we have 𝐶∗= 𝐶max ⋅𝑦where\n𝑦= ln(𝑥−1) is a logarithmically increasing function\nfor 𝑥=\n𝑋𝑐⋅𝑘𝜏(1−𝜁)\n(1−𝜆stake)𝐵𝐽(𝑟)(1−𝜁𝜏+1)𝐶total that asymptotically con-\nverges, theoretically, the optimal contribution would ap-\nproach to positive infinity. However, the maximum prac-\ntical constraints 𝐶∈[𝐶min = 0, 𝐶max], as a result, the\nactual optimal solution is 𝐶∗\n= 𝐶max. In other words,\n𝐶∗\n𝑖∝𝑓local(𝐷𝑖, 𝐰) ⋅𝑔behaviour(ℎ𝑖), adhering to optimal local\ntraining while demonstrating good behaviour, becomes the\ndominant strategy to maximize its utility for any participant.\nMeanwhile, ln(\n𝑋𝑐⋅𝑘𝜏(1−𝜁)\n(1−𝜆stake)𝐵𝐽(𝑟)(1−𝜁𝜏+1)𝐶total −1) = 1 can serve\nas a guideline for tuning the parameters (e.g., 𝜆stake, 𝑋𝑐and\n𝐵if given fixed others) to achieve the optimal solution. By\nbinding IR constraint, the optimal reward is 𝑅∗= 𝑐(𝐶∗). As\nall participants stake the same amount 𝑆, which leads to\n𝑆∗=\n𝜆stake𝐵𝐽(𝑟)\n𝑛⋅(𝑅∗−𝑅cmm −(1−𝜆stake)𝐵𝐽(𝑟)𝐶hist\n𝐶total\n)\n,\n(27)\n𝑅∗=𝑐(𝐶∗).\n(28)\nIf we consider the IR constraint, the optimization prob-\nlem can be reorganized as a Lagrangian function with a\nLagrange multiplier and then repeat the derivation process\nfor the three terms 𝐶, 𝑆, 𝑅, respectively. With the help of\nmath tools (e.g., SciPy methods in Python) to solve for the\nproblem, we can obtain the optimal 𝐶∗, 𝑆∗, 𝑅∗by adjusting\nthe parameters as needed by numerical analysis. Suppose\nthe distribution of the participants’ violations (concerning\nByzantine failures) is not predictable and the stakes are\nunbalanced. In those cases, we can consider using advanced\napproaches, such as Bayesian priors [56] or reinforcement\nlearning optimization [57], to meet the requirements.\n5. Experiments and Evaluation\nThis section illustrates simulation setup, demonstrates\nthe feasibility of the proposed incentive mechanisms, and\nevaluates the outcomes for validation and guidance of (opti-\nmal) smart contract design.\n5.1. Simulation Setup\nIn the simulation, we conducted extensive simulation\nexperiments on a local computer equipped with an Apple\nM1 chip with eight (four performance and four efficiency)\ncores, 16GB memory, and a 500 GB Macintosh HD disk.\nFor the evaluation of solutions, we utilize a normal\ndistribution with random fluctuation for the ease of\ntraining process to present the normal behaviour pattern\nfor participants acting normally, 𝐶normal = max(0, (𝜇=\n7, 𝜎2 = 1)⋅), where 𝜇and 𝜎denote the mean and standard\ndeviation of the set of contributions collected from the\nnormal (or honest) FL participants. For malicious behaviour\npatterns, we simulate malicious patterns with three different\nattacks: 1) false high contribution attack, where malicious\nnodes disguise themselves as high-value nodes through ab-\nnormally high contributions to avoid detection based on low\ncontributions; 2) zero contribution attack where malicious\nnodes directly destroy the federated model aggregation and\nreduce model performance, and 3) random attack which is\ngiven by a random choice of 60% false high contribution\nattack and 40% zero contribution attack.\nWe introduce a parameter 𝜂switch that allows switching\nbetween different patterns to simulate various malicious be-\nhaviours during the training process. The parameter defines\nthe longest time window threshold at which the system tran-\nsitions from base to progressive malicious phases — rang-\ning from high-contribution and zero-contribution attacks to\nrandomly hybrid attacks. This enables the implementation of\nmalicious pattern-switch logic for progressive attack testing,\nperiodic behavioural perturbations, and defense mechanism\nverification. By periodically switching attack strategies (e.g.,\nevery 30 rounds), we simulate the adaptability of malicious\nparticipants. Additional details are available in the source\ncode.\nAdditionally, we set different malicious percentages as\nan adjustable parameter to distinguish between malicious\nand honest nodes among the total participants. An attack\ndetection function is implemented to identify such malicious\nbehaviours, which allows us to test and evaluate the effects\nof penalties, reputation updates, and rewards on the system.\nA summary of the other parameters used in the simulation is\npresented in Table 2.\nTo measure the fairness in reward allocation based on\nactual contributions, we consider two metrics for evaluation.\nJain’s fairness index [55] is a quantitative measure of fairness\nand discrimination for resource allocation in shared systems.\nWe have used it to maintain the fairness of committee\nselection on reputation presented in Eq. 20, whereas it can\nalso be employed to measure the fairness of reward allo-\ncation 𝐽(𝑅) by replacing the input as 𝑅. Additionally, we\nconsider an opposite metric — the Gini coefficient [58], as\na measurement of reward inequality, which is formulated by\n𝐽(𝑅) =\n(∑𝑛\n𝑖=1 𝑅𝑖)2\n𝑛∑𝑛\n𝑖=1 𝑅𝑖\n2 + 𝜖\n⋅𝜎( 𝑅\n10),\n(29)\nYuandou Wang et al.: Preprint submitted to Elsevier\nPage 12 of 16\n\n\nManaging Federated Learning on Decentralized Infrastructures as a Reputation-based Collaborative Workflow\nTable 2\nParameter setting in the simulation.\nParameter\nDescription\nSetting\n𝑆𝑖\ninitial stake\n100\n𝑟0\n𝑖\ninitial reputation score\n100\n𝐶min\nminimal contribution score\n0\n𝐶max\nmaximum contribution score\n10\n𝛾\nreputation decay coefficient\n0.5\n𝑟𝑡≤5\nmax, 𝑟𝑡>5\nmax\nmaximum capabilities of reputation updates\n300, 500\n𝜖\na small constant\n10−8\n𝑐𝑑(⋅)\ncooldown period\n3\n𝐿\nnumber of strata\n3\n𝐵cmm\nbase bonus to committee member\n40\n𝛿𝑏\nbase decay factor\n0.88\n𝜆𝑝\ndecay compensation parameter\n0.07\n𝜏\nrecent historical rounds\n5\n𝜏stab\ndefault stability for new participants\n0.8\n𝑋𝑐\nbase bonus of contribution\n50\n𝑋𝑠\nbase bonus of stability\n30\n𝜆𝑟\npenalty factor to reputation\n0.3\n𝜆𝑠\npenalty factor to stake\n0.1\n𝜁\nhistorical decay factor\n0.9\n𝐵\nbase reward value in the pool\n1200\n𝑛\nthe number of participants\n100\n𝜆stake\nstake weight\n0.4\n\ncommittee size\n5\n𝑚\nmalicious percent\n15%\n𝜂switch\nthe first time window of observing a switch of malicious patterns\n5\n𝑅𝑜𝑢𝑛𝑑\nthe total number of the FL rounds\n90\n𝐺(𝑅) = (𝑛+ 1 −2\n∑𝑛\n𝑖=1\n∑𝑖\n𝑗=1 𝑅𝑗\n∑𝑛\n𝑗=1 𝑅𝑗\n)∕𝑛\n(30)\nwhere the input set 𝑅is performed in ascending order. The\nsource code is available online at the GitHub repository7.\n5.2. Simulation Results\nThis section presents an overview of the simulation\nresults under the parameter setting presented in Table 2. Fig-\nure 6 showcases the dynamics of malicious node detection,\nreputation updates, and reward distribution per round.\nMalicious detection. Sub Figure 6a illustrates the malicious\ndetection performance per round. The detection method\nfollows the condition set (condition 1 AND condition 2) OR\ncondition 3, based on a malicious percentage of 15%. The\nmalicious pattern switch parameter 𝜂switch plays a crucial\nrole in shaping the behaviour records of malicious partici-\npants, thereby influencing detection performance.\nNotably, false high-contribution malicious nodes were\nnot detected during the first five rounds, as the current\ndetection conditions do not account for high-contribution\nattacks, allowing such behaviours to appear normal. When\nthe round 𝑡∈[5, 30), the malicious pattern shifts to zero\ncontributions. By the eighth round, the system successfully\nidentified 12 out of 15 malicious nodes. When the round\n𝑡∈[30, 60), the malicious pattern transitions to random\nattacks, with a 60% probability of high-contribution at-\ntacks and a 40% probability of zero-contribution attacks.\nIn the final 30 rounds, malicious behaviours revert to zero-\ncontribution attacks. The detection method proves to be\nmore effective against zero-contribution attacks but remains\nineffective against false high-contribution attacks due to\ncurrent detection limitations.\nReputation updates. Sub Figure 6b illustrates the repu-\ntation update dynamics per round, distinguishing between\n7https://github.com/yuandou168/reliableFLOps\nhigh-contribution attack\nzero-contribution attack\nrandom contribution attack\n60% (high), 40% (zero)\nzero-contribution attack\n(a) Total malicious nodes detected per round.\n(b) Reputation update dynamics.\n(c) Reward distribution.\nFigure 6: Overview of malicious detection, reputation updates,\nand reward distribution per round.\nhonest and malicious nodes among 85 honest participants\nand 15 malicious ones. Initially, all nodes start with a repu-\ntation score of 100, which increases as new contributions are\ncollected during the first few rounds. This growth continues\nuntil it reaches a plateau around a reputation score of 300.\nNotably, there is no difference in reputation growth between\nhonest and malicious nodes until the eighth round.\nAt the eighth round, some malicious nodes experience\na sharp drop in reputation due to detected malicious be-\nhaviours, as shown in Figure 6a. Since most malicious nodes\nYuandou Wang et al.: Preprint submitted to Elsevier\nPage 13 of 16\n\n\nManaging Federated Learning on Decentralized Infrastructures as a Reputation-based Collaborative Workflow\nwere identified during this period, it caused their reputation\nscores to decline significantly, eventually approaching zero.\nDuring the random attack phase, the system fails to detect all\nmalicious activities, allowing some malicious nodes to tem-\nporarily regain their reputation through high-contribution at-\ntacks. This results in fluctuations in their reputation updates,\nas not all malicious nodes receive penalties in this phase.\nHowever, once all 15 malicious nodes are successfully de-\ntected, the fluctuations cease, and their reputation scores\nrapidly decline, dropping by an average of 148.3 points.\nMeanwhile, the reputation of the 85 honest nodes steadily\nincreased as expected in the first five rounds by 300. Then\nthey gradually reach the upper limit of 500. The average\nreputation increase for honest nodes is approximately 354.6,\nsignificantly higher than that of the malicious nodes.\nReward distribution dynamics. Sub Figure 6c illustrates\nthe reward distribution dynamics among 85 honest and 15\nmalicious nodes out of 100 participants. Because malicious\nnodes mimic high-contribution participants, they accumu-\nlate higher rewards than honest nodes. Hence, they exhibit\na higher average reward value and a faster growth rate\nin the early rounds. However, as the system detects zero-\ncontribution attacks, their rewards drop sharply to zero.\nAlthough some malicious nodes temporarily regain their\nreputation by engaging in high-contribution attacks in the\nperiod of random attacks, as more malicious nodes are\nprogressively detected, their corresponding rewards decline.\nAlthough all malicious nodes ultimately reach a very\nlow reward level over the 90 rounds, their total rewards are\nnot zero. Overall, the final reward values range between 4.1\nand 40.5 at round 90, satisfying the IR constraint. However,\nparticipants acting maliciously or continuing such behaviour\nunder this reward incentive mechanism cannot achieve op-\ntimal utility. Specifically, the average reward for malicious\nnodes dropped by 10.7, while the average reward for honest\nnodes increased by 23.0. The total reward accumulated by\nthe 85 honest nodes is 8.38 times greater than that of the 15\nmalicious nodes in the simulation system.\nConsequently, this incentive mechanism encourages par-\nticipants to adopt a more rational strategy — avoiding ma-\nlicious behaviour to maximize their rewards. Therefore, it\npreserves the trend of rewarding aligned with the incentives\nwhile reducing the risk of over-penalty and over-reward.\nFairness. Figure 7 compares the fairness of reward dis-\ntribution across 100 participant nodes under five different\nmalicious percentage settings. Similar to the reward dynam-\nics observed in Figure 6c, the fairness initially improves,\nas indicated by an increase in Jain’s fairness index and a\ndecrease in the Gini coefficient (see Eq. 29), that suggests a\ntrend toward a more equitable reward distribution. However,\ndue to the influence of malicious nodes, the reward allocation\nis not always fair.\nFor instance, given a malicious percentage 15%, we\nobserved that malicious detection significantly influences\nreward allocation’s fairness. When most or all malicious\nnodes are successfully identified, i.e., during rounds between\n8 and 30, as well as 60 to 90, the fairness index decreases\nFigure 7: Comparisons of total fairness metric dynamics across\nthe last 90 rounds.\nwhile the Gini coefficient increases. Conversely, fairness\nremains relatively stable when the system detects only a few\nmalicious nodes. It reflects an imbalanced reward allocation\ninfluenced by the performance of malicious detection.\nAs the percentage of malicious nodes in the system\nincreases, e.g., from 10% to 30%, the reward distribution\nbecomes more inequitable between honest and malicious\nnodes. However, such inequity remains at a healthy threshold\n(<0.3). Such a change is positive because it still preserves\nfairness among honest participants while preventing mali-\ncious participants from obtaining unfair rewards.\nThe optimal parameter space. In the simulation, we em-\nploy the controlled variable method and an optimization\nsolver ‘SLSQP’, which is suitable for constrained optimiza-\ntion, to analyze the applicable approximations of the optimal\ncontract items (𝐶∗\n𝑖, 𝑆∗\n𝑖, 𝑅∗\n𝑖) ∈Φ∗. The numerical analysis\nvalidates the optimal contribution 𝐶∗→𝐶max for all par-\nticipants. It also verifies that the IR-constrained satisfaction\nrate is 100% while minimum utility is more than zero. With\nthis numerical analysis, users can adjust their contract terms\nfor different requirements with optimal parameter settings.\nMore technical details have been presented in the source\ncode.\nThese simulation results suggest that the proposed incen-\ntive mechanisms are feasible in an FL system with honest\nand malicious participants. By coding these mechanisms\ninto smart contracts and bounding blockchain, the system\ncan effectively constrain and guide participant behaviour,\nensuring that honest participants can receive fair compensa-\ntion while discouraging malicious activities. The numerical\nanalysis can help customize (approximately) optimal con-\ntract items written in smart contracts to adopt diverse FL\nscenarios.\n6. Summary\nThis study demonstrated how FL can be structured as a\nworkflow using open workflow standards and executed on\nremote infrastructure to address automation challenges. To\nbridge the gap between traditional workflow-based automa-\ntion and decentralized collaboration, we propose a novel\nYuandou Wang et al.: Preprint submitted to Elsevier\nPage 14 of 16\n\n\nManaging Federated Learning on Decentralized Infrastructures as a Reputation-based Collaborative Workflow\nstrategy that builds upon our prior work to enhance re-\nliability in FL management over decentralized infrastruc-\ntures. We leverage contract theory with a multidimensional\nscheme to tackle the incentivizing collaboration problem\nby constructing a contribution model, implementing fair\ncommittee selection, dynamically updating reputations, cal-\nculating rewards, and defining corresponding profit and util-\nity functions. Additionally, we explore the optimality of\ncontracts to guide the design and implementation of smart\ncontracts that can be deployed on blockchain networks to\nassure training quality. We conduct extensive simulation\nexperiments to validate the proposed approach. The results\ndemonstrate that our incentive mechanisms are effective,\nensuring fair reward allocation despite malicious attacks.\nFuture improvements include encoding the optimal contract\nitems into smart contracts for real-world system performance\nevaluation and prototyping an integrated on-chain and off-\nchain system demonstration.\nAcknowledgments\nWe thank Mr. Anandan Krishnasamy for running FL\nworkflow experiments over the hybrid cloud environment\nand validating the FL training results. This research was\nmade possible through partial funding from several Eu-\nropean Union projects: CLARIFY (860627), ENVRI-Hub\nNext (101131141), EVERSE (101129744), BlueCloud-2026\n(101094227), OSCARS (101129751), LifeWatch ERIC,\nBioDT (101057437, through LifeWatch ERIC), and Dutch\nNWO LTER-LIFE project.\nReferences\n[1] Rodolfo Stoffel Antunes, Cristiano André da Costa, Arne Küderle,\nImrana Abdullahi Yari, and Björn Eskofier.\nFederated learning\nfor healthcare: Systematic review and architecture proposal. ACM\nTransactions on Intelligent Systems and Technology (TIST), 13(4):1–\n23, 2022.\n[2] Guodong Long, Yue Tan, Jing Jiang, and Chengqi Zhang. Federated\nlearning for open banking.\nIn Federated learning: privacy and\nincentive, pages 240–254. Springer, 2020.\n[3] Dinh C Nguyen, Ming Ding, Pubudu N Pathirana, Aruna Seneviratne,\nJun Li, and H Vincent Poor. Federated learning for internet of things:\nA comprehensive survey. IEEE Communications Surveys & Tutorials,\n23(3):1622–1658, 2021.\n[4] Biwei Yan, Hongliang Zhang, Minghui Xu, Dongxiao Yu, and Xi-\nuzhen Cheng. Fedrfq: Prototype-based federated learning with re-\nduced redundancy, minimal failure, and enhanced quality.\nIEEE\nTransactions on Computers, 2024.\n[5] Pian Qi, Diletta Chiaro, Antonella Guzzo, Michele Ianni, Giancarlo\nFortino, and Francesco Piccialli. Model aggregation techniques in\nfederated learning: A comprehensive survey.\nFuture Generation\nComputer Systems, 2023.\n[6] Jiajun Wu, Fan Dong, Henry Leung, Zhuangdi Zhu, Jiayu Zhou, and\nSteve Drew. Topology-aware federated learning in edge computing: A\ncomprehensive survey. ACM Computing Surveys, 56(10):1–41, 2024.\n[7] Bo Xu, Wenchao Xia, Wanli Wen, Pei Liu, Haitao Zhao, and Hongbo\nZhu. Adaptive hierarchical federated learning over wireless networks.\nIEEE Transactions on Vehicular Technology, 71(2):2070–2083, 2021.\n[8] Lumin Liu, Jun Zhang, Shenghui Song, and Khaled B Letaief. Hier-\narchical federated learning with quantization: Convergence analysis\nand system design. IEEE Transactions on Wireless Communications,\n22(1):2–18, 2022.\n[9] Stefano Savazzi, Monica Nicoli, and Vittorio Rampa.\nFederated\nlearning with cooperating devices: A consensus approach for massive\niot networks. IEEE Internet of Things Journal, 7(5):4641–4654, 2020.\n[10] Yuan Liu, Zhengpeng Ai, Shuai Sun, Shuangfeng Zhang, Zelei Liu,\nand Han Yu. Fedcoin: A peer-to-peer payment system for federated\nlearning. In Federated learning: privacy and incentive, pages 125–\n138. Springer, 2020.\n[11] Nicola Rieke, Jonny Hancox, Wenqi Li, Fausto Milletari, Holger R\nRoth, Shadi Albarqouni, Spyridon Bakas, Mathieu N Galtier, Ben-\nnett A Landman, Klaus Maier-Hein, et al. The future of digital health\nwith federated learning. NPJ digital medicine, 3(1):1–7, 2020.\n[12] Jianchun Liu, Hongli Xu, Lun Wang, Yang Xu, Chen Qian, Jinyang\nHuang, and He Huang. Adaptive asynchronous federated learning in\nresource-constrained edge computing. IEEE Transactions on Mobile\nComputing, 22(2):674–690, 2021.\n[13] Minghui Xu, Zongrui Zou, Ye Cheng, Qin Hu, Dongxiao Yu, and\nXiuzhen Cheng.\nSpdl: A blockchain-enabled secure and privacy-\npreserving decentralized learning system.\nIEEE Transactions on\nComputers, 72(2):548–558, 2022.\n[14] Yufeng Zhan, Jie Zhang, Zicong Hong, Leijie Wu, Peng Li, and Song\nGuo. A survey of incentive mechanism design for federated learning.\nIEEE Transactions on Emerging Topics in Computing, 10(2):1035–\n1044, 2021.\n[15] Xuezhen Tu, Kun Zhu, Nguyen Cong Luong, Dusit Niyato, Yang\nZhang, and Juan Li. Incentive mechanisms for federated learning:\nFrom economic and game theoretic perspective. IEEE transactions\non cognitive communications and networking, 8(3):1566–1593, 2022.\n[16] Han Xu, Priyadarsi Nanda, and Jie Liang.\nReciprocal federated\nlearning framework: Balancing incentives for model and data owners.\nFuture Generation Computer Systems, 161:146–161, 2024.\n[17] Yann Fraboni, Richard Vidal, and Marco Lorenzi. Free-rider attacks\non model aggregation in federated learning. In International Con-\nference on Artificial Intelligence and Statistics, pages 1846–1854.\nPMLR, 2021.\n[18] Leon Witt, Mathis Heyer, Kentaroh Toyoda, Wojciech Samek, and\nDan Li. Decentral and incentivized federated learning frameworks: A\nsystematic literature review. IEEE Internet of Things Journal, 10(4):\n3642–3663, 2022.\n[19] Viraaji Mothukuri, Reza M Parizi, Seyedamin Pouriyeh, Yan Huang,\nAli Dehghantanha, and Gautam Srivastava. A survey on security and\nprivacy of federated learning. Future Generation Computer Systems,\n115:619–640, 2021.\n[20] Semo Yang, Jihwan Moon, Jinsoo Kim, Kwangkee Lee, and Kangy-\noon Lee. Flscalize: Federated learning lifecycle management plat-\nform. IEEE Access, 11:47212–47222, 2023.\n[21] Iacopo Colonnelli, Bruno Casella, Gianluca Mittone, Yasir Arfat,\nBarbara Cantalupo, Roberto Esposito, Alberto Riccardo Martinelli,\nDoriana Medić, and Marco Aldinucci. Federated learning meets hpc\nand cloud. In ML4Astro International Conference, pages 193–199.\nSpringer, 2022.\n[22] Peter Amstutz, Michael R. Crusoe, Nebojša Tijanić, Brad Chapman,\nJohn Chilton, Michael Heuer, et al.\nCommon workflow language,\nv1.0, 2016.\n[23] Chronis Kontomaris, Yuandou Wang, and Zhiming Zhao. Cwl-flops:\nA novel method for federated learning operations at scale. In 2023\nIEEE 19th International Conference on e-Science (e-Science), pages\n1–2. IEEE, 2023.\n[24] Harshit Daga, Jaemin Shin, Dhruv Garg, Ada Gavrilovska, Myungjin\nLee, and Ramana Rao Kompella.\nFlame: Simplifying topology\nextension in federated learning.\nIn Proceedings of the 2023 ACM\nSymposium on Cloud Computing, pages 341–357, 2023.\n[25] Wil MP van Der Aalst, Arthur HM Ter Hofstede, Bartek Kie-\npuszewski, and Alistair P Barros. Workflow patterns. Distributed\nand parallel databases, 14:5–51, 2003.\n[26] Sin Kit Lo, Qinghua Lu, Liming Zhu, Hye-Young Paik, Xiwei Xu, and\nChen Wang. Architectural patterns for the design of federated learning\nsystems. Journal of Systems and Software, 191:111357, 2022.\nYuandou Wang et al.: Preprint submitted to Elsevier\nPage 15 of 16\n\n\nManaging Federated Learning on Decentralized Infrastructures as a Reputation-based Collaborative Workflow\n[27] Qi Cheng and Guodong Long. Federated learning operations (flops):\nChallenges, lifecycle and approaches.\nIn 2022 International Con-\nference on Technologies and Applications of Artificial Intelligence\n(TAAI), pages 12–17. IEEE, 2022.\n[28] Christof Ebert, Gorka Gallardo, Josune Hernantes, and Nicolas Ser-\nrano. Devops. IEEE software, 33(3):94–100, 2016.\n[29] Dominik Kreuzberger, Niklas Kühl, and Sebastian Hirschl. Machine\nlearning operations (mlops): Overview, definition, and architecture.\nIEEE access, 11:31866–31879, 2023.\n[30] Rongfei Zeng, Chao Zeng, Xingwei Wang, Bo Li, and Xiaowen\nChu. A comprehensive survey of incentive mechanism for federated\nlearning. arXiv preprint arXiv:2106.15406, 2021.\n[31] Nika Haghtalab, Mingda Qiao, and Kunhe Yang. Platforms for effi-\ncient and incentive-aware collaboration. In Proceedings of the 2025\nAnnual ACM-SIAM Symposium on Discrete Algorithms (SODA),\npages 2607–2628. SIAM, 2025.\n[32] Zhilin Wang, Qin Hu, Ruinian Li, Minghui Xu, and Zehui Xiong. In-\ncentive mechanism design for joint resource allocation in blockchain-\nbased federated learning. IEEE Transactions on Parallel and Dis-\ntributed Systems, 34(5):1536–1547, 2023.\n[33] Liang Gao, Li Li, Yingwen Chen, ChengZhong Xu, and Ming Xu.\nFgfl: A blockchain-based fair incentive governor for federated learn-\ning. Journal of Parallel and Distributed Computing, 163:283–299,\n2022.\n[34] Jiawen Kang, Zehui Xiong, Dusit Niyato, Shengli Xie, and Junshan\nZhang. Incentive mechanism for reliable federated learning: A joint\noptimization approach to combining reputation and contract theory.\nIEEE Internet of Things Journal, 6(6):10700–10714, 2019.\n[35] Tailin Zhou, Zehong Lin, Jun Zhang, and Danny HK Tsang.\nUn-\nderstanding and improving model averaging in federated learning on\nheterogeneous data. IEEE Transactions on Mobile Computing, 2024.\n[36] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson,\nand Blaise Aguera y Arcas.\nCommunication-efficient learning of\ndeep networks from decentralized data. In Artificial intelligence and\nstatistics, pages 1273–1282. PMLR, 2017.\n[37] Zhiming Zhao, Spiros Koulouzis, Riccardo Bianchi, Siamak Farshidi,\nZeshun Shi, Ruyue Xin, Yuandou Wang, Na Li, Yifang Shi, Joris\nTimmermans, et al. Notebook-as-a-vre (naavre): From private note-\nbooks to a collaborative cloud virtual research environment. Software:\nPractice and Experience, 52(9):1947–1966, 2022.\n[38] Laëtitia Launet, Yuandou Wang, Adrián Colomer, Jorge Igual, Cris-\ntian Pulgarín-Ospina, Spiros Koulouzis, Riccardo Bianchi, Andrés\nMosquera-Zamudio, Carlos Monteagudo, Valery Naranjo, et al. Fed-\nerating medical deep learning models from private jupyter notebooks\nto distributed institutions. Applied Sciences, 13(2):919, 2023.\n[39] Yuandou Wang, Spiros Koulouzis, Riccardo Bianchi, Na Li, Yifang\nShi, Joris Timmermans, W Daniel Kissling, and Zhiming Zhao. Scal-\ning notebooks as re-configurable cloud workflows. Data Intelligence,\n4(2):409–425, 2022.\n[40] Daniel J Beutel, Taner Topal, Akhil Mathur, Xinchi Qiu, Titouan\nParcollet, Pedro PB de Gusmão, and Nicholas D Lane.\nFlower:\nA friendly federated learning research framework.\narXiv preprint\narXiv:2007.14390, 2020.\n[41] Heiko Ludwig, Nathalie Baracaldo, Gegi Thomas, Yi Zhou, Ali An-\nwar, Shashank Rajamoni, Yuya Ong, Jayaram Radhakrishnan, Ashish\nVerma, Mathieu Sinn, et al. Ibm federated learning: an enterprise\nframework white paper v0. 1. arXiv preprint arXiv:2007.10987, 2020.\n[42] G Anthony Reina, Alexey Gruzdev, Patrick Foley, Olga Perepelkina,\nMansi Sharma, Igor Davidyuk, Ilya Trushkin, Maksim Radionov,\nAleksandr Mokrov, Dmitry Agapov, et al. Openfl: An open-source\nframework for federated learning. arXiv preprint arXiv:2105.06413,\n2021.\n[43] Alexander Ziller, Andrew Trask, Antonio Lopardo, Benjamin\nSzymkow, Bobby Wagner, Emma Bluemke, Jean-Mickael Nouna-\nhon, Jonathan Passerat-Palmbach, Kritika Prakash, Nick Rose, et al.\nPysyft: A library for easy federated learning.\nFederated Learning\nSystems: Towards Next-Generation AI, pages 111–139, 2021.\n[44] Kasun Indrasiri and Danesh Kuruppu.\ngRPC: up and running:\nbuilding cloud native applications with Go and Java for Docker and\nKubernetes. O’Reilly Media, 2020.\n[45] Anandan Krishnasamy, Yuandou Wang, and Zhiming Zhao. A col-\nlaborative framework for facilitating federated learning among jupyter\nusers. In 2024 IEEE 20th International Conference on e-Science (e-\nScience), pages 1–2. IEEE, 2024.\n[46] Shaoxiong Ji. A pytorch implementation of federated learning, March\n2018. URL https://doi.org/10.5281/zenodo.4321561.\n[47] Carole Goble, Sarah Cohen-Boulakia, Stian Soiland-Reyes, Daniel\nGarijo, Yolanda Gil, Michael R Crusoe, Kristian Peters, and Daniel\nSchober. Fair computational workflows. Data Intelligence, 2(1-2):\n108–121, 2020.\n[48] Yuandou Wang, Sheejan Tripathi, Siamak Farshidi, and Zhiming\nZhao. D-vre: From a jupyter-enabled private research environment\nto decentralized collaborative research ecosystem. Blockchain: Re-\nsearch and Applications, page 100244, 2024.\n[49] Huong Nguyen, Hong-Tri Nguyen, Lauri Lovén, and Susanna Pirt-\ntikangas. Stake-driven rewards and log-based free rider detection in\nfederated learning. In 2024 21st Annual International Conference on\nPrivacy, Security and Trust (PST), pages 1–10. IEEE, 2024.\n[50] Shuai Wang, Wenwen Ding, Juanjuan Li, Yong Yuan, Liwei Ouyang,\nand Fei-Yue Wang. Decentralized autonomous organizations: Con-\ncept, model, and applications. IEEE Transactions on Computational\nSocial Systems, 6(5):870–878, 2019.\n[51] Ningning Ding, Zhixuan Fang, and Jianwei Huang. Optimal contract\ndesign for efficient federated learning with multi-dimensional private\ninformation. IEEE Journal on Selected Areas in Communications, 39\n(1):186–200, 2020.\n[52] Zehui Xiong, Wei Yang Bryan Lim, Jiawen Kang, Dusit Niyato, Ping\nWang, and Chunyan Miao. Incentive mechanism design for mobile\ndata rewards using multi-dimensional contract. In 2020 IEEE Wire-\nless Communications and Networking Conference (WCNC), pages 1–\n6. IEEE, 2020.\n[53] Jerzy Neyman.\nOn the two different aspects of the representative\nmethod: the method of stratified sampling and the method of pur-\nposive selection. In Breakthroughs in statistics: Methodology and\ndistribution, pages 123–150. Springer, 1992.\n[54] Anders Hald. The compound hypergeometric distribution and a sys-\ntem of single sampling inspection plans based on prior distributions\nand costs. Technometrics, 2(3):275–340, 1960.\n[55] Rajendra K Jain, Dah-Ming W Chiu, William R Hawe, et al.\nA\nquantitative measure of fairness and discrimination.\nEastern Re-\nsearch Laboratory, Digital Equipment Corporation, Hudson, MA, 21:\n1, 1984.\n[56] Peter I Frazier. A tutorial on bayesian optimization. arXiv preprint\narXiv:1807.02811, 2018.\n[57] Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore.\nReinforcement learning: A survey. Journal of artificial intelligence\nresearch, 4:237–285, 1996.\n[58] Robert Dorfman. A formula for the gini coefficient. The review of\neconomics and statistics, pages 146–149, 1979.\nYuandou Wang et al.: Preprint submitted to Elsevier\nPage 16 of 16\n\n\n"}
{"text": "arXiv:2502.21226v2  [cs.AR]  3 Mar 2025\nRecurrent CircuitSAT Sampling for Sequential Circuits\nArash Ardakani\nUniversity of California, Berkeley\narash.ardakani@berkeley.edu\nKevin He\nUniversity of California, Berkeley\nkevinjhe@berkeley.edu\nJohn Wawrzynek\nUniversity of California, Berkeley\njohnw@berkeley.edu\nAbstract—In this work, we introduce a novel GPU-accelerated circuit\nsatisﬁability (CircuitSAT) sampling technique for sequential circuits. This\nwork is motivated by the requirement in constrained random veriﬁcation\n(CRV) to generate input stimuli to validate the functionality of digital\nhardware circuits. A major challenge in CRV is generating inputs for\nsequential circuits, along with the appropriate number of clock cycles\nrequired to meet design constraints. Traditional approaches often use\nBoolean satisﬁability (SAT) samplers to generate inputs by unrolling state\ntransitions over a ﬁxed number of clock cycles. However, these methods\ndo not guarantee that a solution exists for the given number of cycles.\nConsequently, producing input stimuli together with the required clock\ncycles is essential for thorough testing and veriﬁcation. Our approach\nconverts the logical constraints and temporal behavior of sequential\ncircuits into a recurrent CircuitSAT problem, optimized via gradient\ndescent to efﬁciently explore a diverse set of valid solutions, including\ntheir associated number of clock cycles. By operating directly on the\ncircuit structure, our method reinterprets the sampling process as a\nsupervised multi-output regression task. This differentiable framework\nenables independent element-wise operations on each tensor element,\nfacilitating parallel execution during learning. As a result, we achieve\nGPU-accelerated sampling with substantial runtime improvements (up\nto 105.1×) over state-of-the-art heuristic samplers. We demonstrate the\neffectiveness of our method through extensive evaluations on circuit\nproblems from the ISCAS-89 and ITC’99 benchmark suites.\nIndex Terms—Circuit Satisﬁability, Gradient Descent, Sequential Cir-\ncuits, Veriﬁcation, and Testing.\nI. INTRODUCTION\nSimulation-based functional veriﬁcation plays a crucial role in\ncontemporary digital design automation processes, though it is often\nquite time-intensive [1]. In this stage, the design undergoes thorough\ntesting by running numerous simulations with a wide range of input\nsignals to ensure that it complies with its intended functional require-\nments. For complex systems, each input typically spans numerous\nclock cycles, making exhaustive simulation impractical for real-world\ndesigns [2]. Therefore, generating high-quality stimuli that effectively\nexplore key functional scenarios, especially in corner cases, is essen-\ntial for achieving sufﬁcient coverage. Constrained random veriﬁcation\n(CRV) [3]–[6] addresses this challenge by allowing the user to deﬁne\nconstraints that guide the generation of valid input stimuli, ensuring\nthe design is tested in critical, bug-prone areas. CRV introduces\nrandomness in the input selection process while still satisfying the\ngiven constraints, improving the efﬁciency of the veriﬁcation process\nby increasing the likelihood of discovering hard-to-detect bugs [7]–\n[9].\nThe task of generating input stimuli in CRV for a given Boolean\ncircuit is known as circuit satisﬁability (CircuitSAT) sampling [10],\n[11]. CircuitSAT sampling involves transforming circuit problems\ninto Boolean satisﬁability (SAT) problems and using SAT samplers\nto generate random solutions that meet the speciﬁed constraints.\nCircuitSAT is essentially a specialized form of the SAT problem\n[12]–[16], where the circuit’s structure is represented explicitly as a\nnetwork of logic gates rather than in conjunctive normal form (CNF).\nCircuitSAT sampling for combinational circuits focuses on generating\ndiverse input patterns that satisfy the logic of the circuit, without\ninvolving any state or temporal dependencies. Since combinational\ncircuits are acyclic and consist purely of logic gates (e.g., AND, OR,\nNOT), the sampling process is relatively straightforward and well-\nestablished in the literature [10], [11], [17]. This process typically\nbegins by converting the circuit’s structure into CNF using methods\nsuch as the Tseitin transformation [18]. Any design constraints on\nthe circuit’s outputs can also be encoded in the CNF, after which\nSAT samplers can be applied to generate valid input stimuli for the\nCircuitSAT problem.\nUnlike combinational circuits, sequential circuits depend on mem-\nory elements and clock-driven state transitions, making test genera-\ntion signiﬁcantly more complex. To apply SAT sampling to sequential\ncircuits, the circuit must be “unrolled” over multiple time steps,\neffectively transforming it into a series of combinational steps that\ncapture the circuit’s state transitions [19]. However, this approach\nassumes that the required number of clock cycles for the given\ndesign constraints is known in advance, which is often unrealistic.\nConsequently, the CircuitSAT sampling process for sequential circuits\ninvolves not only generating input stimuli that satisfy the design\nconstraints but also determining the necessary number of clock cycles\nfor each input. This requirement makes CircuitSAT sampling for\nsequential circuits extremely challenging, yet invaluable if success-\nfully implemented, as it provides a powerful means of testing and\nveriﬁcation for complex, state-dependent systems.\nIn this work, we introduce a machine-learning-driven method to\ngenerate input stimuli for sequential circuits along with the necessary\nnumber of clock cycles for each sample. To this end, We formulate\nthe CircuitSAT sampling process for sequential circuits as a recurrent\nmulti-output regression task, where each logic gate is represented\nprobabilistically and each memory element (e.g., ﬂip-ﬂops) is treated\nas hidden state, enabling the use of gradient descent (GD) to learn\ndiverse solutions. This approach enables the parallel generation of\nindependent input stimuli including their associated required clock\ncycles while accelerating the sampling computations with GPUs. We\ndemonstrate the superior performance of our sampling method across\nall instances from the ISCAS-89 and ITC’99 benchmark suites.\nII. PRELIMINARIES\nA. CircuitSAT Sampling\nA digital circuit is an electronic system that processes binary\ninformation according to the rule of Boolean logic. Digital circuits are\nbuilt using interconnected digital components including logic gates\n(e.g., AND, OR, and NOT gates) and memory elements (e.g., ﬂip-\nﬂops). In digital circuits, variables are restricted to discrete binary\nvalues of either 0 or 1. The output of such circuits is produced based\non how these digital components operate on binary variables. There\nare two types of digital circuits: combinational and sequential circuits.\nIn combinational circuits, the output solely depends on current inputs,\nwhereas in sequential circuits, the output depends not only on the\ncurrent inputs but also on the history of inputs represented as the\ncurrent state.\nCircuitSAT sampling is the task of generating a sequence of a set of\nbinary-valued assignments to the inputs of a given digital circuit that\nmakes its output evaluate to desired value. The desired outputs are\nreferred to as output constraints to the CircuitSAT problem. Sampling\n\n\nsolutions from CircuitSAT instances in CRV is valuable as it enables\nefﬁcient, targeted generation of test cases that satisfy speciﬁc logical\nconstraints, ensuring that only valid and meaningful scenarios are\nexplored. This approach increases coverage and detects edge cases\nmore effectively than random sampling, focusing on diverse, rele-\nvant conﬁgurations that might reveal hidden bugs. CircuitSAT-based\nsampling also reduces the need for exhaustive testing, especially in\ncomplex circuits, by selectively exploring the solution space. Ulti-\nmately, this method improves veriﬁcation efﬁciency and reliability,\nmaking it valuable for high-quality digital design testing.\nA common method for CircuitSAT sampling is to use SAT\nsolvers equipped with sampling features. These solvers not only\ncheck whether a Boolean formula is satisﬁable but also sample\nspeciﬁc solutions from the set of all possible solutions. Efﬁcient\nSAT solving techniques include backtracking methods like the Davis-\nPutnam-Logemann-Loveland (DPLL) algorithm [20], stochastic local\nsearch approaches like WalkSAT [21], and conﬂict-driven clause\nlearning (CDCL) algorithms [22], [23]. In recent years, various\nalgorithms for SAT and CircuitSAT sampling have been introduced,\nincluding randomized methods, Markov chain Monte Carlo (MCMC)\napproaches, and heuristic-based sampling techniques [10], [24]–[27].\nThese techniques generally work by iteratively exploring possible\nsolutions, selecting candidates based on certain rules, and using\nprobabilistic criteria to accept or reject them.\nThe ﬁrst step in CircuitSAT sampling using SAT solvers is to model\nthe relationships among digital components and translate them into\na CNF representation. CNF is structured as a conjunction (AND)\nof multiple clauses, where each clause is a disjunction (OR) of\nliterals. Here, literals refer to Boolean variables or their negations.\nThe number of variables in the CNF is determined by the primary\ninputs, intermediate signals, and primary outputs of the digital circuit.\nConversely, the number of clauses corresponds to the logic operations\nin the circuit. The transformation into CNF offers SAT solvers\na standardized representation of the problem, preserving the core\nconstraints of the original circuit. The conversion to CNF is typically\nachieved using the Tseitin transformation [18].\nConverting combinational circuits to CNF is straightforward, as\neach logic gate can be mapped to corresponding CNF sub-clauses\nusing the Tseitin transformation. The size and intricacy of the\nresulting CNF formula can vary widely, inﬂuenced by factors like\nthe number of gates, circuit depth, and counts of inputs, outputs,\nand intermediate signals. Generally, the CNF representation requires\nsubstantially more bit-wise operations than the original circuit. This\nadded complexity increases the time SAT solvers need to ﬁnd\nsolutions due to the NP-complete nature of SAT problems. The\nchallenge is even greater for sequential circuits, which involve time-\ndependent behavior. Representing these temporal dependencies in\nCNF necessitates additional variables and constraints to maintain con-\nsistency across time steps, further complicating the transformation.\nAlthough theoretically possible, converting sequential circuits to CNF\ninvolves unique difﬁculties such as state encoding, temporal logic,\nand clock handling, making it particularly challenging for large or\ncomplex designs. The main challenge in converting sequential circuits\nto CNF is determining the speciﬁc number of clock cycles to unroll\nthe sequential circuit so that it can be treated as a combinational\ncircuit for conversion using the Tseitin transformation. However, it\nis generally impossible to know the number of clock cycles that will\nguarantee satisfying solutions for given output constraints, creating a\nunique challenge for sequential CircuitSAT sampling.\nCombinational\nCircuit (Fh)\nFlip-Flops\nCombinational\nCircuit (Fo)\nPt\nHt+1\nHt\nˆYt\nFigure 1: The general form of a sequential circuit.\nB. Multi-Output Regression Task\nA multi-output regression task involves predicting several target\nvariables simultaneously based on a set of input features [28]. The\ngoal is to build a model that effectively captures the connections\nbetween the inputs and each output variable. This model can be\ndeveloped through various approaches, such as linear regression or\nneural networks, and is trained on a dataset with known input-output\npairs. During training, the model’s parameters are optimized to reduce\nthe difference between its predictions and the actual targets, often\nusing mean squared error (MSE) or ℓ2 loss as a measure of accuracy.\nIII. METHODOLOGY\nIn contrast to combinational circuits, where outputs are determined\nsolely by their present inputs, the output in sequential circuits depends\non both the past behavior of the circuit and the present values of\ninputs. The temporal operations of sequential circuits are controlled\nby a clock signal. The contents of memory elements (i.e., ﬂip-ﬂops)\nrepresent the past behavior of such a circuit, which is commonly\nreferred to as the state of the circuit.\nA. Recurrent CircuitSAT Sampler for Sequential Circuits\nSolving CircuitSAT problems for sequential circuits presents a\nunique challenge as it requires ﬁnding a sequence of inputs that\nsatisﬁes the target output constraint over a series of clock cycles. To\ntackle such problems, we can leverage a novel technique inspired\nby recurrent neural networks (RNNs). In RNNs, backpropagation\nthrough time is utilized during the learning process, allowing for\nupdates to the network’s hidden state at each time step. Similarly,\nin the context of solving CircuitSAT problems, we perform forward\ncomputations to iteratively update the state values at each clock\ncycle. During backward computations, gradients are backpropagated\nthrough time, extending back to the initial time step (i.e., the ﬁrst\nclock cycle), to adjust the input sequence accordingly. While this\napproach draws parallels to RNN training, it is tailored to the unique\nchallenges posed by solving CircuitSAT problems for sequential\ncircuits.\nFig. 1 shows the general structure of a sequential circuit. We use\nthis structure to formulate the CircuitSAT problem for sequential\ncircuits to ﬁnd satisfying solutions. In this structure, there are two\ncombination circuits: one to update the state of the circuit (i.e., the\ncontent values of ﬂip-ﬂops) and the other one to generate the output.\nIt is worth mentioning that both of these combinational circuits take\nthe present values of ﬂip-ﬂops and primary inputs at the current time\nstep as their inputs. Given that these combinational circuits (i.e., Fh\nand Fo) are fundamentally discrete and non-differentiable, we need\nto relax the CircuitSAT problem into a continuous version that still\nmaintains the circuit’s core structure and behavior. To do this, we\nutilize the probability model of digital gates, as outlined in Table\nI. This probabilistic model is widely applied across various ﬁelds,\nincluding stochastic computing [29] and the estimation of dynamic\npower in digital circuits [30]. By applying these probabilities to model\neach gate, we produce a differentiable formulation of the circuit that\n\n\nRecurrent Cell\nFo\nFh\nHt\nHt+1\nˆYt\nPt\nRecurrent Cell\nPt\nˆYt\nHt+1\nFigure 2: Left: The proposed recurrent sequential model. Right: The\nproposed recurrent cell for sequential circuits.\npreserves its functionality. Importantly, for any binary input, this\nmodel remains equivalent to the original circuit in its discrete form.\nLet us represent the primary input variables at time step t as Vt ∈\nRb×n. We encode the primary input variables at each time step as\nlearnable parameters to an embedding layer followed by the sigmoid\nfunction to provide input probabilities at time t as Pt ∈[0, 1]p×n to\nthe combinational circuits, i.e.,\nPt = σ(Vt).\n(1)\nThe present output of the circuit (i.e., ˆYt ∈[0, 1]b×m) is computed\nas:\nˆYt = Fo(Pt, Ht),\n(2)\nwhere Fo and Ht\n∈[0, 1]b×r denote the functionality of the\ncombinational circuit generating outputs and the present values of\nﬂip-ﬂops at each time step, respectively. The number of ﬂip-ﬂops in\nthe circuit is represented by r. The state of the circuit for the next\ntime step is obtained as:\nHt+1 = Fh(Pt, Ht),\n(3)\nwhere the functionality of the combinational circuit updating the\nvalues of ﬂip-ﬂops is denoted by Fh. The ℓ2-loss function L can then\nbe constructed by measuring the distance between ˆYt at the desired\ntime step T and the target output valuation matrix Y ∈{0, 1}b×m\nas follows:\nL =\nX\nb,m\n\f\f\f\n\f\f\fY −ˆYT\n\f\f\f\n\f\f\f\n2\n2 .\n(4)\nWith such a formulation for sequential circuits, we can solve the\nCircuitSAT problem and provide b solutions. The general form of\nthe recurrent cell for sequential circuits is shown in Fig. 2, which is\nanalogous to the RNN cell.\nWith our formulation, backpropagation through time is used to\ncompute the gradients of the loss with respect to the primary input\nvariables (i.e., Vt). The objective is to ﬁnd the gradient of L with\nrespect to Vt. To derive this gradient, we apply the chain rule across\ntime steps. The gradient with respect to Vt requires accounting for\nhow the circuit’s states inﬂuence each other over time, computed as\nfollows:\n∂L\n∂Ht = ∂L\n∂ˆYt\n· ∂ˆYt\n∂Ht +\n∂L\n∂Ht+1 · ∂Ht+1\n∂Ht ,\n(5)\ngiven Eq. (2) and Eq. (3). The recursive formula above captures the\ninﬂuence of the circuit state at each time step on both the output and\nthe subsequent state. Backpropagation through time, as shown in Eq.\n(5), can now be used to calculate the gradients of L with respect to\nVt:\n∂L\n∂Vt = ∂L\n∂ˆYt\n· ∂ˆYt\n∂Pt · ∂Pt\n∂Vt +\n∂L\n∂Ht+1 · ∂Ht+1\n∂Pt\n· ∂Pt\n∂Vt .\n(6)\nThis formula enables updating the primary input variables by prop-\nagating the gradients back through time.\nTable I: Probability model and input derivatives of primary logic gates\nOperator\nOutput Variable\nDerivative w.r.t Input\nNOT(P1)\nPy = P1 = 1 −P1\n∂Py\n∂P1\n= −1\nAND(P1, P2)\nPy = P1 P2\n∂Py\n∂P1\n= P2, ∂Py\n∂P2\n= P1\nOR(P1, P2)\nPy = 1 −P1 P2\n∂Py\n∂P1\n= P2, ∂Py\n∂P2\n= P1\nXOR(P1, P2)\nPy = P1 P2 + P1 P2\n∂Py\n∂P1\n= 1 −2P2, ∂Py\n∂P2\n= 1 −2P1\nXNOR(P1, P2)\nPy = P1 P2 + P1 P2\n∂Py\n∂P1\n= 2P2 −1, ∂Py\n∂P2\n= 2P1 −1\nB. Theoretical Analysis\nProof of convergence is a crucial concept across various ﬁelds,\nincluding machine learning, economics, engineering, and operations\nresearch. Convergence in optimization is important because it en-\nsures stable solutions, enhances robustness to noise, and enables the\napplication of efﬁcient algorithms that leverage gradient information\nand duality principles. In our formulation for the CircuitSAT problem,\nconvergence plays a key role by simplifying the optimization process,\nthereby allowing our method to reliably ﬁnd the satisfying solutions.\nBefore presenting a proof of convergence, let us ﬁrst demonstrate\nthat the optimization problem in our formulation is non-convex. The\ncombinational parts of our formulation for the CircuitSAT problem\ncan be viewed as an AND-inverter graph (AIG), where the AND\noperation is deﬁned as\nAND(p1, p2) = p1 · p2,\n(7)\nwith p1, p2 ∈[0, 1] as the inputs. To test the convexity of the AND\nfunction, we analyze its Hessian matrix H of second derivatives,\nwhich is given by\nH =\n\n\n∂2AND(p1, p2)\n∂p2\n1\n∂2AND(p1, p2)\n∂p1∂p2\n∂2AND(p1, p2)\n∂p2∂p1\n∂2AND(p1, p2)\n∂p2\n2\n\n=\n\u00140\n1\n1\n0\n\u0015\n.\n(8)\nThe eigenvalues of H are ±1. Since one eigenvalue is negative, H\nis not positive semi-deﬁnite, and therefore, the AND function is non-\nconvex. As a result, the AIG containing multiple AND gates is also\nnon-convex.\nWhile our CircuitSAT problem formulation is a non-convex opti-\nmization, we prove the convergence of gradient descent to global\nminima for such a non-convex formulation using the Lipschitz\ncontinuity of the gradient [31]. Let us represent the sequence of\ninputs over T clock cycles as the matrix X ∈[0, 1]n×T and let\nF(X) →[0, 1]m be the non-convex function of the sequential circuit.\nSince X and F(X) are bounded within [0, 1], the ℓ2-loss (L(X)) of\nF(X) is also bounded by the number of constrained outputs (i.e.,\nL(X) ∈[0, m]) and, therefore, there exists a constant L > 0 such\nthat\n||∇L(X) −∇L(Z)|| ≤L||X −Z||\n∀X, Z ∈[0, 1]n×T ,\n(9)\nimplying that the Hessian of the function L(X) is bounded, i.e.,\nH(X) ≤L. The gradient descent update for a matrix X ∈[0, 1]n×T\nat the kth training iteration is as follows:\nXk+1 = Xk −α∇L(Xk),\n(10)\nwhere α > 0 denotes the learning rate.\n\n\nUsing the second-order Taylor expansion of L(Xk+1) around Xk,\nwe have:\nL(Xk+1) ≈L(Xk) + ∇L(Xk)T (Xk+1 −Xk)\n+ 1\n2(Xk+1 −Xk)T H(Xk)(Xk+1 −Xk).\n(11)\nUsing the Lipschitz bound on the Hessian, we can replace H(Xk)\nwith L to get the following inequality:\nL(Xk+1) ≤L(Xk) + ∇L(Xk)T (Xk+1 −Xk)\n+ L\n2 ||Xk+1 −Xk||2.\n(12)\nGiven Eq. (10), we can substitute Xk+1 −Xk with −α∇L(Xk) and,\ntherefore, rewrite the inequality as\nL(Xk+1) ≤L(Xk) −α||∇L(Xk)||2 + Lα2\n2 ||∇L(Xk)||2.\n(13)\nBy deﬁning µ = α −Lα2\n2\nfor 0 < α < 2\nL and µ > 0, we have\nL(Xk+1) ≤L(Xk) −µ||∇L(Xk)||2.\n(14)\nSince F(X) is bounded, we can sum this inequality over K ∈\n{0, 1, . . . , K −1} and accordingly get:\nL(XK) ≤L(X0) −µ\nK−1\nX\nk=0\n||∇L(Xk)||2.\n(15)\nBy rearranging the above inequality, we have\nµ\nK−1\nX\nk=0\n||∇L(Xk)||2 ≤L(X0) −L(XK) ≤L(X0).\n(16)\nSince L(X0) is bounded within [0, 1], we can conclude\nK−1\nX\nk=0\n||∇L(Xk)||2 < ∞.\n(17)\nThe above inequality implies that ||∇L(Xk)||2 →0 as k →∞,\nshowing the gradient descent algorithm converges to a stationary\npoint where ∇L(X) = 0. Since L(X) is bounded and non-convex,\ngradient descent guarantees convergence to a stationary point within\n[0, 1] for inputs, though this may be a global minimum, local\nminimum or saddle point.\nGiven our probabilistic formulation, ∇L(X) can only be zero in\ntwo cases. The ﬁrst occurs when the ℓ2-loss (i.e., L(X)) is zero. This\nimplies that the output constraints are satisﬁed for the given inputs,\nand the inputs to the circuit must have converged to either 0 or 1, as\nthe outputs can only take discrete binary values when the inputs are\nalso discrete binary values in our formulation. This guarantees that\nthe inputs associated with a loss value of zero are satisfying solutions\nand, as such, represent global minima.\nThe second case in which ∇L(X) = 0 occurs when the logic gates\ncause the gradients to be zero through their derivative functions with\nrespect to their inputs (see Table I). For example, a single AND gate\ncannot reach an output of 1 through gradient descent if both of its\ninputs are initialized to zero. This results in a non-zero loss while\n∇L(X) remains zero, which is referred to as a saddle point. However,\nthis scenario is unlikely (though possible) to occur since the inputs are\nrandomly initialized within [0, 1]. Moreover, even if it does happen,\nit can be mitigated by regularization techniques such as input decay\nand noise injection.\nAlgorithm 1 Pseudo Code of our Backtracking Algorithm\n1: Input: The upper bound for clock cycles η, the number of\ntraining iterations itr, the target output valuation matrix Y\n2: Output: List of satisfying primary inputs PI\n3: for cc = 1 to η do\n4:\nfor j = 1 to itr do\n5:\nfor t = 1 to cc do\n6:\nPt = σ(Vt)\n7:\nYt = Fo(Pt, Ht−1)\n8:\nHt = Fh(Pt, Ht−1)\n9:\nend for\n10:\nL = P\nb,m\n\f\f\f\n\f\f\fY −ˆYcc\n\f\f\f\n\f\f\f\n2\n2\n11:\nL.backward()\n12:\noptimizer.step()\n13:\nif P is satisﬁable then\n14:\nAppend P to PI\n15:\nend if\n16:\nend for\n17: end for\n18: Return PI\nC. Backtracking Algorithm\nGiven our differentiable recurrent sampling method described in\nSection III-A, we now introduce a backtracking algorithm that can\ngenerate satisfying input solutions of varying lengths based on\nspeciﬁc design constraints. All generated solutions are constrained\nto have their initial state values set to zero. We begin the sampling\nprocess by deﬁning the upper bound for the number of clock cycles,\ndenoted as η. The goal is to generate solutions where the required\nnumber of clock cycles for each sample is less than or equal to\nthis upper bound. Starting with the design constraint on the output\nof a sequential circuit, we run the differentiable recurrent sampling\nmethod for one clock cycle and generate potential solutions. If the\ngenerated solutions meet the design constraint, they are stored; other-\nwise, they are discarded. This process is repeated for 2 clock cycles,\ncontinuing the same procedure. The cycle-by-cycle sampling process\ncontinues incrementally until reaching the speciﬁed upper limit on\nclock cycles. By then, we will have solutions that require varying\nnumbers of clock cycles. It is possible, however, that for certain\nnumbers of clock cycles, no solutions are found, as the constraints\nmay not be satisﬁable for those speciﬁc cycle lengths. Without loss\nof generality, a lower bound can also be deﬁned for the number of\nclock cycles, ensuring that the generated solutions fall within a user-\nspeciﬁed range. Algorithm 1 summarizes the backtracking algorithm.\nIV. EXPERIMENTAL RESULTS\nIn this section, we showcase the effectiveness of our sampling\napproach. To this end, we implemented a prototype of our method\nusing PyTorch, an open-source library that merges Torch’s GPU-\noptimized backend with a Python-compatible interface. For a com-\nprehensive evaluation, we utilize the complete ISCAS-89 and ITC’99\nbenchmark suites, incorporating all components. Our experiments\nwere conducted on a system with an Intel Xeon E5-2698 processor\nrunning at 2.2 GHz and 8 NVIDIA V100 GPUs, each equipped\nwith 32 GB of memory. We assess the runtime performance of our\nmethod based on throughput, deﬁned as the number of unique and\nvalid solutions produced per second. GD was used as the optimizer\nduring training, with a learning rate of 50 and a total of 5 iterations.\nDepending on the size of each CircuitSAT instance, we adjust the\n\n\nTable II: The runtime performance of sampler is evaluated in terms of unique solution throughput for sequential circuits.\nCircuitSAT\n# Primary\n# Primary\n# Logic\n# DFF\n# Variables\n# Clauses\nThroughput (# Unique Solutions per Second)\nInstance\nInputs\nOutputs\nGates\n(CNF)\n(CNF)\nThis work\nCMSGEN\ns208.1\n10\n1\n104\n8\n7696\n7371\n114,242.4 (105.1×)\n1087.2\ns386\n7\n7\n159\n6\n12, 790\n12, 609\n35,849.5 (65.6×)\n546.6\ns526\n3\n6\n193\n21\n16, 772\n16, 692\n15,978.6 (32.6×)\n490.8\ns832\n18\n19\n287\n5\n25, 386\n24, 919\n5,700.4 (30.5×)\n186.9\ns1196\n14\n14\n529\n18\n38, 592\n38, 230\n5,651.1 (51.7×)\n109.4\ns4863\n49\n16\n2342\n104\n166, 035\n164, 796\n157.6 (75.0×)\n2.1\ns15850.1\n77\n150\n9772\n534\n606, 514\n604, 444\n14.5 (3.7×)\n3.9\ns38584\n12\n278\n19, 253\n1, 452\n1, 357, 767\n1, 357, 194\n1.7 (2.1×)\n0.8\nb01\n2\n2\n45\n5\n3, 205\n3, 154\n54,774.3 (18.2×)\n3, 012.6\nb06\n2\n6\n56\n9\n3, 271\n3, 216\n173,909.9 (66.5×)\n2, 614.3\nb10\n11\n6\n206\n17\n13, 963\n13, 683\n5,463.6 (13.8×)\n396.8\nb12\n5\n6\n1, 076\n121\n78, 559\n78, 429\n402.5 (4.1×)\n98.2\nb15\n36\n70\n8, 922\n449\n660, 026\n659, 059\n3.1 (15.5×)\n0.2\nb17\n37\n97\n32, 326\n1, 415\n2, 379, 165\n2, 378, 145\n0.2\nTO\nb19\n24\n30\n231, 320\n6, 642\n16, 819, 969\n16, 819, 339\n0.01\nTO\nb22\n32\n22\n29, 951\n735\n2, 183, 304\n2, 182, 485\n0.5 (5×)\n0.1\nbatch size between 100 and 1, 000, 000. It is also noteworthy that the\nmaximum sample count generated by our sampler is limited by the\nproduct of the batch size and the maximum number of clock cycles\n(i.e., η). For our experiments, we set the maximum clock cycle limit\nto η = 50.\nA. Runtime Performance\nIn sequential circuit sampling, the goal is to identify input se-\nquences of varying lengths (i.e., requiring different numbers of clock\ncycles) that satisfy given output constraints. To transform these\ncircuits into CircuitSAT sampling problems, we randomly assign\nspeciﬁc binary values to certain output nodes. Table II presents the\nsampling performance of our recurrent CircuitSAT sampler in terms\nof throughput, deﬁned as the number of unique input sequences\ngenerated per second, for a representative set of 16 sequential\nCircuitSAT instances from the ISCAS-89 and ITC’99 benchmark\nsuites. The generated samples are input sequences with clock cycles\nranging from 1 to 50. Producing valid input sequences of different\nlengths is essential for sequential CircuitSAT sampling, as speciﬁc\noutput constraints may preclude solutions at certain clock cycle\nlengths. Furthermore, samples with varying sequence lengths (i.e.,\nclock cycles) enable the detection of both static logic faults and\ndynamic issues related to timing and state transitions.\nTo compare runtime performance with previous works, we unrolled\nthe sequential circuits in the ISCAS-89 and ITC’99 benchmark suites\nover 25 clock cycles, converting the resulting combinational circuit to\nCNF using the Tseitin transformation. For this CircuitSAT sampling\nprocess, we applied the same output constraints used in our recurrent\nsampler experiments. These output constraints were initially set to\nensure the existence of solutions at 25 clock cycles. We selected 25\nclock cycles as it represents the average of all clock cycles (ranging\nfrom 1 to 50) used in our sampling experiments.\nWe compare the runtime performance of our recurrent CircuitSAT\nsampler with state-of-the-art SAT sampling methods, speciﬁcally\nUNIGEN3 [26], CMSGEN [27], and DIFFSAMPLER [17], each tasked\nwith generating at least 1, 000 solutions within a 4-hour timeframe.\nWhile these samplers work directly on the CNF representations of\nCircuitSAT instances, our sampler operates on the sequential circuits\nthemselves, preserving their original structure and temporal behavior.\nBoth UNIGEN3 and CMSGEN are optimized C++ implementations,\nwhile DIFFSAMPLER is a Python-based SAT sampler that leverages\nJAX for GPU acceleration. UNIGEN3 and CMSGEN were tested on a\nserver-grade AMD EPYC 9274F CPU with the maximum frequency\nof 4.3 GHz and 1 TB of RAM, whereas DIFFSAMPLER was evaluated\n0\n10\n20\n30\n40\n50\n10−2\n100\n102\n104\n106\nNumber of Clock Cycles\n[log scale] Throughput\n(# Unique Solutions per Second)\nISCAS-89\nITC’99\nFigure 3: A log plot showing the runtime performance of our recurrent\nsampler in terms of throughput, measured as the number of unique\nsatisfying solutions per second, versus the number of clock cycles\nacross all instances from the ISCAS-89 and ITC’99 benchmark suites.\nThe throughput values are rounded to two decimal places in this plot.\non a system with an Intel Xeon E5-2698 processor at 2.2 GHz and\n8 NVIDIA V100 GPUs, each with 32 GB of memory.\nOur experiments show that UNIGEN3 and DIFFSAMPLER can only\ngenerate solutions for the CircuitSAT instance “s27” with throughputs\nof 58, 180.0 and 64, 458.4, respectively. For this speciﬁc CircuitSAT\ninstance, our recurrent sampler outperforms both UNIGEN3 and\nDIFFSAMPLER by factors of 4.3× and 3.9×, respectively. On the\nother hand, CMSGEN generates solutions across the majority of\nCircuitSAT instances, as shown in Table II. According to Table II,\nour recurrent CircuitSAT sampler achieves up to a 105.1× increase\nin throughput compared to CMSGEN. Furthermore, our recurrent\nsampler consistently outperforms CMSGEN, achieving an average\nspeedup of 24.3× across all instances for which CMSGEN generated\nsolutions. Figure 3 illustrates the throughput of our sampler across\nvarious clock cycle counts for all instances from the ISCAS-89\nand ITC’99 benchmark suites, with throughput values rounded to\ntwo decimal places in the plot. This ﬁgure demonstrates that our\nsampler can produce input sequences of different lengths for a given\nCircuitSAT problem. Notably, some instances lack solutions that meet\nthe speciﬁed output constraints at certain clock cycles.\n\n\n0\n10\n20\n30\n40\n50\n0\n20\n40\n60\n80\n100\nNumber of Clock Cycles\nPercentage of Unique\nSolutions (%)\ns3271\nb02\nb08\nFigure 4: A plot illustrating the percentage of unique solutions\ngenerated by our recurrent sampler over 50 clock cycles for three\nrepresentative CircuitSAT instances. The batch size for this experi-\nment is set to 100, 000.\nFor conventional samplers, including CMSGEN, UNIGEN3, and\nDIFFSAMPLER, we ﬁxed the number of clock cycles at 25 to ensure\na fair runtime performance comparison. However, these samplers can\nalso be used to generate input sequences over varying clock cycles.\nThis approach, however, requires transforming the sequential circuit\ninto a separate CNF for each clock cycle, which leads to an increase\nin CNF size with the number of clock cycles. In contrast, our method\nrecursively computes forward computations for any desired number of\nclock cycles (see Algorithm 1) and calculates gradients with respect\nto inputs as described in Section III-A.\nB. Learning Dynamics\nHere, we present the learning dynamics of our recurrent sampler\nfor a few representative sequential CircuitSAT instances. We begin\nby analyzing the distribution of the generated input sequences across\n50 clock cycles. The randomly speciﬁed output constraints ensure the\nexistence of satisfying solutions at the 25th clock cycle for runtime\ncomparisons with prior works. However, there is no guarantee of so-\nlutions existing for other clock cycles. For example, Fig. 4 highlights\nthree representative sequential CircuitSAT instances, showcasing the\npercentage of valid and unique solutions generated by our sampler\n(calculated as the number of unique solutions divided by the batch\nsize) for speciﬁc clock cycles.\nFig. 5 shows the learning progress of our recurrent sampler,\nshowing the total number of unique solutions generated over ﬁve\niterations. The learning curves indicate that as the number of it-\nerations increases, the total number of unique solutions identiﬁed\nby our sampler over 50 clock cycles also grows. This aligns with\nour theoretical convergence proof, which shows that gradient descent\ncan reach a global minimum, achieving zero-valued loss in the\nnon-convex landscapes of our continuous formulation for sequential\nCircuitSAT problems. Of course, the convergence rate varies across\nCircuitSAT instances, inﬂuenced by the complexity and structure of\nthe sequential circuit, as well as the hyper-parameters selected for\nthe learning process.\nV. RELATED WORK\nOver the years, researchers have developed various Circuit-\nSAT/SAT sampling strategies. Among these, tools like UNIGEN3\naim to maintain approximate uniformity in sampling [32], while\nCMSGEN and QUICKSAMPLER [10] prioritize speed and efﬁciency.\nEfforts to exploit data-parallel hardware for SAT solving have largely\nrevolved around accelerating CDCL and heuristic-based methods\n[33], [34]. More recently, approaches such as MATSAT [35] and\nNEUROSAT [36] have reimagined SAT problems as constrained\noptimization tasks over continuous domains. However, these inno-\nvations have primarily targeted small, random benchmarks, leaving\n1\n2\n3\n4\n5\n104\n105\n106\n# Training Iterations\n# Unique\nSolutions\ns3271\nb02\nb08\nFigure 5: A log plot illustrating the number of unique solutions\ngenerated by our recurrent sampler over 5 training iterations for three\nrepresentative CircuitSAT instances.\ntheir potential for large-scale and standard benchmarks unrealized,\nparticularly with GPU-accelerated sampling. The introduction of\nDIFFSAMPLER [17] marks a turning point by demonstrating com-\npetitive performance in GPU-accelerated SAT sampling on widely\nused benchmarks, rivaling methods like UNIGEN3 and CMSGEN.\nComplementing this, DEMOTIC [11] offers a GPU-powered, sampling\nsolution tailored for combinational CircuitSAT in CRV, operating\ndirectly on hardware description language representations such as\nVerilog.\nDespite the high runtime performance of the aforementioned\nCircuitSAT samplers, they are primarily designed for combinational\ncircuits. Recently, a symbolic algorithm based on Algebraic Decision\nDiagrams, called TRACESAMPLER, has been introduced to sample\nbounded traces (i.e., sequences of states) in sequential circuits with\nguaranteed uniformity [19]. However, TRACESAMPLER focuses on\nuniform sampling of ﬁxed-length traces, typically starting from a\ndeﬁned initial state, without applying constraints on the circuit’s\nprimary outputs.\nUnlike the previous sampling approaches, our sampler takes a\nfundamentally different approach. Our formulation reframes the sam-\npling procedure as an optimization task that can be minimized using\ngradient descent under speciﬁed output constraints, with a theoretical\nproof of convergence to a stationary point. During optimization,\nforward and backward computations are performed recurrently over\nany number of clock cycles without unrolling the circuit (i.e.,\nconverting it to a combinational circuit). This allows sampling from\nthe solution space across varying clock cycles, which is crucial for\ndetecting timing-related and state-transition faults. Additionally, due\nto the parallel nature of this optimization process across different data\nbatches, our method can be accelerated using GPUs. Consequently,\nour sampler outperforms the state-of-the-art (i.e., CMSGEN) in terms\nof runtime performance across all CircuitSAT instances from the\nISCAS-89 and ITC’99 benchmark suites.\nVI. CONCLUSION\nIn this work, we introduced a novel GPU-accelerated CircuitSAT\nsampling approach tailored for sequential circuits, addressing key\nchallenges in CRV. By reframing the sampling process as a recurrent\noptimization problem and utilizing gradient descent, our method\nefﬁciently generates input stimuli along with the required clock\ncycles, ensuring compliance with design constraints. We provided\ntheoretical guarantees showing that minimizing this recurrent opti-\nmization process via gradient descent converges to a stationary point,\nwith a high likelihood of reaching a global minimum by incorporating\nregularization techniques to avoid saddle points. Operating directly on\nthe circuit structure without unrolling state transitions, our approach\nredeﬁnes sampling as a differentiable multi-output regression task, en-\nabling highly parallel computations and delivering substantial runtime\nimprovements. Extensive evaluations on benchmark suites such as\n\n\nISCAS-89 and ITC’99 demonstrate the effectiveness and scalability\nof our method, achieving up to 105.1× higher throughput compared\nto state-of-the-art samplers. This work lays the foundation for more\nefﬁcient veriﬁcation methods for sequential systems and paves the\nway for integrating differentiable sampling techniques into broader\nhardware design and validation processes.\nREFERENCES\n[1] H. D. Foster, “Trends in functional veriﬁcation: a 2014 industry study,”\nin Proceedings of the 52nd Annual Design Automation Conference, ser.\nDAC ’15. New York, NY, USA: Association for Computing Machinery,\n2015. [Online]. Available: https://doi.org/10.1145/2744769.2744921\n[2] L. Bening et al., Principles of Veriﬁable RTL Design, 2nd ed.\nUSA:\nKluwer Academic Publishers, 2001.\n[3] J. Bhadra et al., “A survey of hybrid techniques for functional\nveriﬁcation,” IEEE Des. Test, vol. 24, no. 2, p. 112–122, Mar. 2007.\n[Online]. Available: https://doi.org/10.1109/MDT.2007.30\n[4] N. Kitchen et al., “Stimulus generation for constrained random simula-\ntion,” in Proceedings of the 2007 IEEE/ACM International Conference\non Computer-Aided Design, ser. ICCAD ’07.\nIEEE Press, 2007, p.\n258–265.\n[5] Y. Naveh et al., “Constraint-based random stimuli generation for hard-\nware veriﬁcation,” in Proceedings of the 18th Conference on Innovative\nApplications of Artiﬁcial Intelligence - Volume 2, ser. IAAI’06.\nAAAI\nPress, 2006, p. 1720–1727.\n[6] J. Yuan et al., Constraint-Based Veriﬁcation, 1st ed. Springer Publishing\nCompany, Incorporated, 2010.\n[7] N. Kitchen et al., “Stimulus generation for constrained random simula-\ntion,” in 2007 IEEE/ACM International Conference on Computer-Aided\nDesign, 2007, pp. 258–265.\n[8] Y. Zhao et al., “Random stimulus generation with self-tuning,” in 2009\n13th International Conference on Computer Supported Cooperative\nWork in Design, 2009, pp. 62–65.\n[9] R. Naveh et al., “Beyond feasibility: Cp usage in constrained-random\nfunctional hardware veriﬁcation,” in Principles and Practice of Con-\nstraint Programming, C. Schulte, Ed.\nBerlin, Heidelberg: Springer\nBerlin Heidelberg, 2013, pp. 823–831.\n[10] R. Dutra et al., “Efﬁcient sampling of sat solutions for testing,” in Proc.\nof the International Conference on Software Engineering, 2018.\n[11] A. Ardakani et al., “DEMOTIC: A differentiable sampler for multi-level\ndigital circuits,” in Proceedings of the 30th Asia and South Paciﬁc Design\nAutomation Conference (ASP-DAC 2025), 2025.\n[12] A. Mishchenko et al., “Sat-based complete don’t-care computation for\nnetwork optimization,” in Proceedings of the Conference on Design,\nAutomation and Test in Europe - Volume 1, ser. DATE ’05.\nUSA:\nIEEE Computer Society, 2005, p. 412–417.\n[13] S. Tsai et al., “A false-path aware formal static timing analyzer consid-\nering simultaneous input transitions,” in Proceedings of the 46th Annual\nDesign Automation Conference, ser. DAC ’09.\nNew York, NY, USA:\nAssociation for Computing Machinery, 2009, p. 25–30.\n[14] A. R. Bradley, “Sat-based model checking without unrolling,” in Pro-\nceedings of the 12th International Conference on Veriﬁcation, Model\nChecking, and Abstract Interpretation, ser. VMCAI’11.\nBerlin, Hei-\ndelberg: Springer-Verlag, 2011, p. 70–87.\n[15] A. Mishchenko et al., “Improvements to combinational equivalence\nchecking,” in Proceedings of the 2006 IEEE/ACM International Con-\nference on Computer-Aided Design, ser. ICCAD ’06.\nNew York, NY,\nUSA: Association for Computing Machinery, 2006, p. 836–843.\n[16] H.-T. Zhang et al., “A circuit-based sat solver for logic synthesis,” in\n2021 IEEE/ACM International Conference On Computer Aided Design\n(ICCAD).\nIEEE Press, 2021, p. 1–6.\n[17] A. Ardakani et al., “Late breaking results: Differential and massively\nparallel sampling of sat formulas,” in Proceedings of the 61st ACM/IEEE\nDesign Automation Conference (DAC), 2024.\n[18] G. S. Tseitin, “On the complexity of derivation in propositional calcu-\nlus,” Automation of reasoning: 2: Classical papers on computational\nlogic 1967–1970, pp. 466–483, 1983.\n[19] S. Chakraborty et al., “On uniformly sampling traces of a transition\nsystem,” in Proceedings of the 39th International Conference on\nComputer-Aided Design, ser. ICCAD ’20.\nNew York, NY, USA:\nAssociation for Computing Machinery, 2020. [Online]. Available:\nhttps://doi.org/10.1145/3400302.3415707\n[20] M. Davis et al., “A machine program for theorem-proving,” Commun.\nACM, vol. 5, no. 7, p. 394–397, jul 1962.\n[21] B. Selman et al., “Local search strategies for satisﬁability testing.”\nCliques, coloring, and satisﬁability, vol. 26, pp. 521–532, 1993.\n[22] J. Marques Silva et al., “Grasp-a new search algorithm for satisﬁability,”\nin Proceedings of International Conference on Computer Aided Design,\n1996, pp. 220–227.\n[23] J. Marques-Silva et al., Chapter 4: Conﬂict-driven clause learning SAT\nsolvers, ser. Frontiers in Artiﬁcial Intelligence and Applications.\nIOS\nPress BV, 2021, pp. 133–182, publisher Copyright: © 2021 The authors\nand IOS Press. All rights reserved.\n[24] R. Impagliazzo et al., “Does Looking Inside a Circuit Help?” in 42nd\nInternational Symposium on Mathematical Foundations of Computer\nScience (MFCS 2017), ser. Leibniz International Proceedings in Infor-\nmatics (LIPIcs), vol. 83, 2017, pp. 1:1–1:13.\n[25] N. Kitchen et al., “A markov chain monte carlo sampler for mixed\nboolean/integer constraints,” in Computer Aided Veriﬁcation: 21st In-\nternational Conference, CAV 2009, Grenoble, France, June 26-July 2,\n2009. Proceedings 21.\nSpringer, 2009, pp. 446–461.\n[26] M. Soos et al., “Tinted, detached, and lazy cnf-xor solving and its\napplications to counting and sampling,” in Proceedings of International\nConference on Computer-Aided Veriﬁcation (CAV), 2020.\n[27] P. Golia et al., “Designing samplers is easy: The boon of testers,” in\nProc. of Formal Methods in Computer-Aided Design (FMCAD), 2021.\n[28] H. Borchani et al., “A survey on multi-output regression,” Wiley Inter-\ndisciplinary Reviews: Data Mining and Knowledge Discovery, vol. 5,\nno. 5, pp. 216–233, 2015.\n[29] A. Ardakani et al., “Vlsi implementation of deep neural network using\nintegral stochastic computing,” IEEE Transactions on Very Large Scale\nIntegration (VLSI) Systems, vol. 25, no. 10, pp. 2688–2699, 2017.\n[30] D. Harris et al., “Cmos vlsi design,” ed: Pearson Education, Inc, 2010.\n[31] A. A. Goldstein, “Optimization of lipschitz continuous functions,” Math-\nematical Programming, vol. 13, pp. 14–22, 1977.\n[32] Y. Pote et al., “On scalable testing of samplers,” in Advances in Neural\nInformation Processing Systems (NeurIPS), 2022.\n[33] C. Costa, “Parallelization of sat algorithms on gpus,” Technical report,\nINESC-ID, Technical University of Lisbon, Tech. Rep., 2013.\n[34] M. Osama et al., “Sat solving with gpu accelerated inprocessing,” in\nInternational Conference on Tools and Algorithms for the Construction\nand Analysis of Systems.\nSpringer, 2021, pp. 133–151.\n[35] T. Sato et al., “Matsat: a matrix-based differentiable sat solver,” arXiv\npreprint arXiv:2108.06481, 2021.\n[36] S. Amizadeh et al., “Learning to solve circuit-sat: An unsupervised\ndifferentiable approach,” in International Conference on Learning Rep-\nresentations, 2018.\n\n\n"}
{"text": "Short-rate Derivatives in a Higher-for-Longer Environment\nAram Karakhanyan ∗\nTakis Konstantopoulos †\nMatthew Lorig ‡\nEvgenii Samutichev §\nThis version: March 3, 2025\nAbstract\nWe introduce a class of short-rate models that exhibit a “higher for longer” phenomenon. Specifically, the\nshort-rate is modeled as a general time-homogeneous one-factor Markov diffusion on a finite interval. The lower\nendpoint is assumed to be regular, exit or natural according to boundary classification while the upper endpoint\nis assumed to be regular with absorbing behavior. In this setting, we give an explicit expression for price of a\nzero-coupon bond (as well as more general interest rate derivatives) in terms of the transition density of the\nshort-rate under a new probability measure, and the solution of a non-linear ordinary differential equation (ODE).\nWe then narrow our focus to a class of models for which the transition density and ODE can be solved explicitly.\nFor models within this class, we provide conditions under which the lower endpoint is regular, exit and natural.\nFinally, we study two specific models – one in which the lower endpoint is exit and another in which the lower\nendpoint is natural. In these two models, we give an explicit solution of transition density of the short-rate as a\n(generalized) eigenfunction expansion. We provide plots of the transition density, (generalized) eigenfunctions,\nbond prices and the associated yield curve.\nKeywords: short-rate model, interest rate, bond pricing, yield curve, spectral methods\n1\nIntroduction\nBetween January of 2021 to December of 2021, the annual rate of inflation in the United States rose from 1.4% to\n7.0% (source: BLS). Although Treasury Secretary Janet Yellin initially called the rapid rise in inflation “transitory,”\nit had become apparent by early 2022 that high levels of inflation had become entrenched. In an effort get inflation\ndown to its 2% target, between February of 2022 and August of 2023, the Federal Reserve raised its effective rate\nfrom roughly 0.1% to just over 5.3% (source: St. Louis Fed). Despite the historically fast pace of interest rate hikes,\nthe annual rate of inflation in September of 2023 was still sitting at 3.7%, leading Federal Reserve Chairman Jerome\nPowell to declare that interest rates would likely remain “higher for longer.” And, indeed, after topping out 5.32% in\nJuly of 2023, the federal overnight rate remained at this value until August of 2024.\nIn this paper, we present a class of one-factor short-rate models that capture this “higher for longer” phenomenon.\nSpecifically, we assume that the short-rate is modeled as a Markov diffusion on a finite interval. The upper end of\nthe interval is a regular (i.e., accessible) boundary according the Feller’s boundary classification, and is assumed to\n∗Department of Mathematics, University of Edinburgh. e-mail: aram6k@gmail.com\n†Department of Mathematics, Uppsala University. e-mail: takiskonst@gmail.com\n‡Department of Applied Mathematics, University of Washington. e-mail: mlorig@uw.edu\n§Department of Applied Mathematics, University of Washington. e-mail: e.samutichev@gmail.com\n1\narXiv:2502.21252v1  [q-fin.MF]  28 Feb 2025\n\n\nhave absorbing behavior. That is, when the short rate reaches the upper end of the interval, it remains there forever.\nAlthough in reality, the federal reserve will likely lower interest rates when inflation returns to the Federal Reserve’s\n2% target, the absorbing behavior captures the effect of interest rates remaining high for an extended period of time.\nThus, the class of models in this paper can be seen as valid up to some finite time-horizon, after which the short-rate\nmay be lowered. The class of models in this paper has the additional benefit of being analytically tractable, as the\nbond price can be written explicitly.\nThe rest of this paper proceeds as follows: in Section 2 we introduce a class of short-rate models described by a\none-factor Markov diffusion on a finite interval. In this setting, we use risk-neutral pricing to derive an expression\nfor the value of a financial derivative whose payoff depends on the terminal value of the short-rate. We additionally\nrelate the value of this financial derivative to the solution of a partial differential equation (PDE). In Section 3, we\nperform a change of variables that enables us to express the solution of the pricing PDE in terms of the solutions of a\nnonlinear ordinary differential equation (ODE) and generalized eigenvalue problem. Lastly, in Section 4, we consider\na class of short-rate models for which the ODE and generalized eigenvalue problem described in Section 3 can be\nsolved explicitly. We additionally perform a numerical study of bond prices and the resulting yield curves for two\nspecific examples within this class. The two examples are qualitatively different in that the first example has an exit\nboundary at the origin and the eigenvalue problem has a discrete set of solutions, while the second example has a\nnatural boundary at the origin and the eigenvalue problems has a continuous set of solutions.\n2\nShort-rate model and Pricing\nTo begin, we fix a time horizon T < ∞and consider a continuous-time financial market, defined on a filtered\nprobability space (Ω, F, F, P) with no arbitrage and no transaction costs. The probability measure P represents the\nmarket’s chosen pricing measure taking the money market account M = (Mt)0≤t≤T as numéraire. The filtration\nF = (Ft)0≤t≤T represents the history of the market.\nWe suppose that the money market account M is strictly positive, continuous and non-decreasing. As such, there\nexists a non-negative F-adapted short-rate process X = (Xt)0≤t≤T such that\ndMt = XtMt dt,\nM0 > 0.\nWe will focus on the case in which the dynamics of the short-rate X are described by a Stochastic Differential Equation\n(SDE) of the following form\ndXt = 1{Xt∈(0,L)}\n\u0010\nμ(Xt)dt + σ(Xt)dWt\n\u0011\n,\nX0 ∈[0, L],\n(1)\nwhere W is a (P, F)-Brownian motion. We assume that the drift μ : (0, L) →R and volatility σ : (0, L) →R+ are\nsuch that the origin is either a regular, exit or a natural boundary, and that L is a regular boundary, according to\nFeller’s boundary classification for scalar diffusions Feller (1952). We further assume that X has a unique strong\nsolution up until the first exit time of (0, L). Observe that, as soon as X exits (0, L), we have dXt = 0 due to the\nindicator function on the right-hand side of (1). And thus, the endpoint {0} is an absorbing state if it is a regular or\nexit boundary and endpoint {L} is an absorbing state.\nLet us denote by V = (Vt)0≤t≤T the value of a financial derivative that has a payoff g(XT) at time T ≤T, where\n2\n\n\ng : [0, L] →R. Using standard risk-neutral pricing, we have\nVt\nMt\n= E\n\u0010 VT\nMT\n\f\f\fFt\n\u0011\n,\n0 ≤t ≤T.\nSolving for Vt and using Mt = M0 exp(\nR t\n0 Xsds) and VT = g(XT), we obtain\nVt = E\n\u0010\ne– R T\nt\nXsdsg(XT)\n\f\f\fFt\n\u0011\n=: u(t, Xt),\n(2)\nwhere we have used the Markov property of X to express Vt as a function u : [0, T] × [0, L] →R of (t, Xt). By\nthe Feynman-Kac formula, the function u satisfies the following partial differential equation (PDE) and boundary\nconditions (BCs)\n(∂t + A – x)u = 0,\nu(T, x) = g(x),\n(3)\nu(t, 0) = g(0),\nif 0 is regular or exit,\n(4)\nu(t, L) = e–L(T–t)g(L),\n(5)\nwhere the operator A is the infinitesimal generator of X under P and is given explicitly by\nA = μ(x)∂x + 1\n2σ2(x)∂2\nx .\n(6)\nThroughout this paper we assume that Cauchy problem (3)-(5) has a unique classical solution.\nNotice that the function u defined in (2) depends on the maturity date T and the payoff function g. If we wish to\nemphasize the dependence of u on T and g, we may sometimes write u(t, Xt) ≡u(t, Xt; T, g). For example, denoting\nby BT = (BT\nt )0≤t≤T the price of a zero-coupon bond that pays one unit of currency at the maturity date T and by\nYT = (YT\nt )0≤t≤T the associated yield to maturity, we have\nBT\nt = u(t, Xt; T, 1),\nYT\nt = – log BT\nt\nT – t\n.\n(7)\n3\nSolving the pricing PDE\nIn this section, we will obtain an explicit expression for the solution u of Cauchy problem (3)-(5). The expression we\nobtain will depend on the solution of a first order, nonlinear, ordinary differential equation (ODE) as well as the\n(possibly improper) eigenfunctions and eigenvalues of a linear second-order differential operator. We begin with a\nshort lemma.\nLemma 1. Let u be the unique classical solution of Cauchy problem (3)-(5). Suppose f satisfies the following\nordinary differential equation\n0 = 1\n2σ2(x)(∂x f )2 – Af (x) – x,\n(8)\nwhere the operator A is given by (6). Define the function w : [0, T] × [0, L] →R as follows\nw(t, x) := ef (x)\u0010\nu(t, x) – e–x(T–t)g(x)\n\u0011\n.\n(9)\nThen the function w satisfies the following Cauchy problem\n(∂t + e\nA)w + q = 0,\nw(T, x) = 0,\n(10)\n3\n\n\nw(t, 0) = 0,\nif 0 is regular or exit,\n(11)\nw(t, L) = 0,\n(12)\nwhere the operator e\nA and the function q are given by\ne\nA = eμ(x)∂x + 1\n2σ2(x)∂2\nx\neμ(x) = μ(x) – σ2(x)f ′(x),\nq(t, x) = (∂t + e\nA)e–x(T–t)+f (x)g(x).\n(13)\nProof. First, we have from (3), (4) (5) and (9) that\nw(T, x) := ef (x)\u0010\nu(T, x) – e–x(T–T)g(x)\n\u0011\n= ef (x)\u0010\ng(x) – g(x)\n\u0011\n= 0,\nw(t, 0) := ef (0)\u0010\nu(t, 0) – e–0(T–t)g(0)\n\u0011\n= ef (0)\u0010\ng(0) – g(0)\n\u0011\n= 0,\nw(t, L) := ef (L)\u0010\nu(t, L) – e–L(T–t)g(L)\n\u0011\n= ef (L)\u0010\ne–L(T–t)g(L) – e–L(T–t)g(L)\n\u0011\n= 0,\nwhich establishes the BCs in (10), (11) and (12). Next, we have from (9) that\nu(t, x) = e–f (x)\u0010\nw(t, x) + e–x(T–t)+f (x)g(x)\n\u0011\n.\n(14)\nInserting (14) into (3), multiplying both sides by ef (x) and using the fact that f satisfies (8), we obtain\n0 = ef (x)(∂t + A – x)e–f (x)\u0010\nw(t, x) + e–x(T–t)+f (x)g(x)\n\u0011\n= ef (x)e–f (x)(∂t + e\nA)\n\u0010\nw(t, x) + e–x(T–t)+f (x)g(x)\n\u0011\n= (∂t + e\nA)w(t, x) + q(t, x).\nwhich establishes the PDE in (10).\nAt this point, it will be convenient to introduce the fundamental solution eΓ associated with the operator (∂t + e\nA).\nSpecifically, for any (T, y) ∈[0, T] × [0, L] we define eΓ( · , · ; T, y) : [0, T] × [0, L] →R+ as the unique classical solution\nof\n(∂t + e\nA)eΓ( · , · ; T, y) = 0,\neΓ(T, x; T, y) = δy(x),\n(15)\neΓ(t, 0; T, y) = 0,\nif 0 is regular or exit,\n(16)\neΓ(t, L; T, y) = 0.\n(17)\nRemark 2. Consider the following change of measure\ndeP\ndP := exp\n\u00101\n2\nZ T\n0\nσ2(Xt)(f ′(Xt))2dt –\nZ T\n0\nσ(Xt)f ′(Xt)dWt\n\u0011\n.\n(18)\nWe have from Girsanov’s Theorem that the process f\nW = (f\nWt)0≤t≤T, defined by\ndf\nWt := σ(Xt)f ′(Xt)dt + dWt,\nf\nW0 = 0,\n(19)\nis a (eP, F)-Brownian motion. From (1) and (19), we see that the dynamics of X under eP are\ndXt = 1{Xt∈(0,L)}\n\u0010\neμ(Xt)dt + σ(Xt)dWt\n\u0011\n,\nand the generator of X under eP is the operator e\nA given in (13). If follows that the solution eΓ of (15)-(17) is the\ntransition density of X under eP.\n4\n\n\nWe can now state the main result of this section.\nTheorem 3. The solution u of Cauchy problem (3)-(5) is given by\nu(t, x) = e–x(T–t)g(x) + e–f (x)\nZ T\nt\nds\nZ L\n0\ndξ eΓ(t, x; s, ξ)(∂s + e\nA)e–ξ(T–s)+f (ξ)g(ξ),\n(20)\nwhere the operator e\nA is given by (13) and the functions f and eΓ satisfy (8) and (15)-(17), respectively.\nProof. By Duhamel’s principle, we have from (10) that\nw(t, x) =\nZ T\nt\nds\nZ L\n0\ndξ eΓ(t, x; s, ξ)q(s, ξ),\nwhere eΓ satisfies (15)-(17). Thus, we have from (14) that\nu(t, x) = e–x(T–t)g(x) + e–f (x)\nZ T\nt\nds\nZ L\n0\ndξ eΓ(t, x; s, ξ)q(s, ξ).\n(21)\nEquation (20) follows by inserting expression (13) for q into (21).\nObserve from (21) that, to write the solution u of (3)-(5), we need an expression for a solution f of (8). The following\nProposition gives sufficient conditions on the coefficients μ and σ under which (8) has an explicit solution.\nProposition 4. Suppose μ and σ are given by\nμ(x) = –σ2(x)φ′(x)\n4φ(x)\n,\nσ(x) =\ns\n2x\nφ(x),\n(22)\nfor some non-negative function φ. Then\nf (x) := –\nZ p\nφ(x)dx + c,\n(23)\nis a solution of (8), where c is an arbitrary constant.\nProof. Define F := –f ′. Then we have from (8) that\n0 = F′ + F2 + 2μ\nσ2 F – 2x\nσ2 = F′ + F2 – φ′\n2φF – φ,\n(24)\nwhere the second equality follows from (22). It is easy to see by direction substitution that F = √φ satisfies (24).\nThus, we have from f = –\nR\nFdx that f is given by (23).\nWe note from (21) that, to write the solution u of (3)-(5), we need an expression for the solution eΓ of (15)-(17). To\nthis end, it will be helpful to write the operator e\nA in self-adjoint form. We have\ne\nA =\n1\nem(x)∂x\n1\nes(x)∂x ,\nes(x) := exp\n\u0010\n–\nZ\ndx 2eμ(x)\nσ2(x)\n\u0011\n,\nem(x) :=\n2\nσ2(x) exp\n\u0010 Z\ndx 2eμ(x)\nσ2(x)\n\u0011\n,\n(25)\nwhere we have introduced the scale density es and the speed density em of the operator e\nA. The scale and speed\ndensities will be needed later in this paper to determine the behavior of the short-rate X at the origin.\nAssumption 5. Henceforth, we assume the spectrum of e\nA, denoted σ( e\nA), is simple.\n5\n\n\nNow, consider the following eigenvalue problem\ne\nAψλ = λψλ,\n(26)\nψλ(0) = 0,\nif 0 is regular or exit,\n(27)\nψλ(L) = 0.\n(28)\nIn general, the spectrum of self-adjoint operator e\nA has the decomposition σ( e\nA) = σd( e\nA) ∪σc( e\nA), where σd( e\nA) and\nσc( e\nA) denote the discrete and continuous portions of σ( e\nA), respectively. We will denote by (λn)n∈N = σd( e\nA) the\nset of proper eigenvalues of e\nA and by (λ(ρ))ρ∈R+ = σc( e\nA) the set of improper eigenvalues of e\nA. Similarly, we will\ndenote by (ψn)n∈N the proper eigenfunctions of e\nA, normalized as follows\nψn := ψλn ,\nλn ∈σd(A),\n⟨ψn, ψk⟩e\nm :=\nZ L\n0\nψn(x)ψk(x) em(x)dx = δn,k,\n(29)\nand by (ψ(ρ, · ))ρ∈R+ the improper eigenfunctions of e\nA, normalized according to\nψ(ρ, · ) := ψλ(ρ),\nλ(ρ) ∈σc(A),\nZ L\n0\nψ(ρ, x)ψ(ρ′, x) em(x)dx = δ(ρ – ρ′),\n(30)\nwhere λ(ρ) = –cρ2 for some c > 0. Then, under Assumption 5, we have from (Hanson and Yakovlev, 2013, Chapter\n5) that the solution eΓ of (15)-(17) has the following (generalized) eigenfunction representation\neΓ(t, x; T, y) = em(y)\n\u0010 X\nn∈N\neλn(T–t)ψn(x)ψn(y) +\nZ ∞\n0\neλ(ρ)(T–t)ψ(ρ, x)ψ(ρ, y)dρ\n\u0011\n.\n(31)\nThus, we need only to solve eigenvalue problem (26)-(28) to construct the transition density eΓ. The following\nproposition, which is based on the transformation to Liouville normal form (DLMF, Chapter 1), can be helpful\nto find solutions of the eigenvalue equation (26) in the case where μ and σ are given by (22) for some non-negative\nfunction φ.\nProposition 6. Let φ be a non-negative function. Suppose μ and σ are given by (22). Then the scale density\nes(x), the speed density em(x), defined in (25), are given by\nes(x) = C\np\nφ(x) exp\n\u0010\n– 2\nZ\ndx\np\nφ(x)\n\u0011\n,\nem(x) =\np\nφ(x)\nCx\nexp\n\u0010\n2\nZ\ndx\np\nφ(x)\n\u0011\n,\n(32)\nwhere C is an arbitrary constant, and the operator e\nA is given by\ne\nA =\n\u0010\n2x\np\nφ(x)\n– xφ′(x)\n2φ2(x)\n\u0011\n∂x +\nx\nφ(x)∂2\nx ,\n(33)\nSuppose ψλ is a (possibly improper) eigenfunction of e\nA, which satisfies (26)-(28). Define functions p and ηλ\nthrough the following equations\np(x) :=\nZ\ndx\n1\nσ(x),\nψλ(x) = ηλ(p(x))\np\nσ(x)es(x),\n(34)\nThen ηλ satisfies\n1\n2η′′\nλ – Vηλ = ληλ,\nV(z) := U(p–1(z)),\nU(x) := φ′(x)\n8φ2(x) +\n3\n16xφ(x) + x,\n(35)\nηλ(p(0)) = 0,\nif 0 is regular or exit,\n(36)\nηλ(p(L)) = 0,\n(37)\nwhere p–1 is the inverse of p.\n6\n\n\nProof. The expressions for em, es and e\nA can be computed directly from (13), (22), (23) and (25). Next, using (26)\nand (34), we have\n0 =\n1\np\nσ(x)es(x)\n( e\nA – λ)ψλ(x) = 1\n2η′′\nλ(p(x)) – U(x)ηλ(p(x)) – ηλ(p(x)).\nSetting x = p–1(z), we obtain (35). The BCs (36)-(37) for ηλ follow from the BCs (27)-(28) for ψλ and (34).\n4\nA class of analytically tractable models\nIn this section, we analyze a class of models for the short rate X in which the drift μ and volatility σ are given by\n(22) with the function φ being given by\nφ(x) = 2\na2 x 2k–1,\na > 0.\n(38)\nInserting (38) into (22) we obtain\nσ(x) = ax 1–k,\nμ(x) = a2\u0010\n1\n4 – k\n2\n\u0011\nx 1–2k.\n(39)\nThus, we have from (1) and (39) that the dynamics of the short-rate X = (Xt)0≤t≤T are given by\ndXt = 1{Xt∈(0,L)}\n\u0010\na2(1\n4 – k\n2 )X1–2k\nt\ndt + aX1–k\nt\ndWt\n\u0011\n,\nX0 ∈[0, L],\n(40)\nRemark 7. k = 0 corresponds to a geometric Brownian motion with μ = a2\n4 and σ = a.\nIt then follows from (33) that the operator e\nA is given by\ne\nA =\n\u0010\na2\u0000 1\n4 – k\n2\n\u0001\nx 1–2k + a\np\n2x 3–2k\n\u0011\n∂x + 1\n2a2x 2–2k∂2\nx ,\n(41)\nand from (32) the scale density es and the speed density em are given by\nes(x) =\n\n\n\n\n\nCx k– 1\n2 exp\n\u0012\n–\n2\n√\n2\na(k+ 1\n2 )x k+ 1\n2\n\u0013\n,\nk ̸= – 1\n2\nCx –1– 2\n√\n2\na ,\nk = – 1\n2\n,\n(42)\nem(x) =\n\n\n\n\n\n2\nCa2 x k– 3\n2 exp\n\u0012\n2\n√\n2\na(k+ 1\n2 )x k+ 1\n2\n\u0013\n,\nk ̸= – 1\n2\n2\nCa2 x –2+ 2\n√\n2\na ,\nk = – 1\n2\n,\n(43)\nIn order to write spectral representation of the transition density (31) for the short-rate (40), we must determine the\nstructure of the spectrum σ( e\nA) of the diffusion generator e\nA (41), which depends on whether the origin is regular,\nexit or natural. This will also determine if a BC (16) is needed at the origin.\nProposition 8. Fix an interval (0, L) and consider the operator e\nA given by (41). For k ∈(–∞, 0] the origin is\nnatural, for k ∈(0, 1/2] the origin is exit, and for k ∈(1/2, ∞) the origin is regular.\nProof. See Appendix A.\nUsing (34) and (35) we have for k ̸= 0 that\nz = p(x) = x k/ak,\nU(x) = 1\n32x –2k \u0010\n32x 2k+1 + a2(4k + 1)\n\u0011\n,\nV(z) = (akz)1/k + 1 + 4k\n32k2z 2 ,\n(44)\n7\n\n\nand for k = 0 we have\nz = p(x) = 1\na log x,\nU(x) = x + a2\n32 ,\nV(z) = ez + a2\n32 .\n(45)\nThus, from (35), (36) and (37), we have for k ̸= 0 that (ηλ, λ) satisfy\n1\n2η′′\nλ(z) –\n\u0010\n(akz)1/k + 1 + 4k\n32k2z 2\n\u0011\nηλ(z) = ληλ(z),\n(46)\nηλ(0k/ak) = 0,\nif 0 is regular or exit,\nηλ(Lk/ak) = 0.\nFor k = 0 the origin is a natural boundary by Proposition 8. Hence, we have\n1\n2η′′\nλ(z) –\n\u0010\nez + a2\n32\n\u0011\nηλ(z) = ληλ(z),\nηλ(log L\na\n) = 0.\n(47)\nNotice that when k ≤0, after the Liouville transformation z = p(x) as in (44) and (45), the origin turns into –∞,\nwhich plays a role in the following decomposition of spectrum σ( e\nA).\nProposition 9. Fix an interval (0, L) and consider the operator e\nA given by (41). For k > 0 the spectrum is\npurely discrete, i.e. σ( e\nA) = σd( e\nA) ⊂(–∞, 0), for k < 0 the spectrum is purely continuous σ( e\nA) = σc( e\nA) = (–∞, 0],\nand for k = 0 the spectrum is mixed, with discrete portion σd( e\nA) ⊂(– a2\n32 , 0] clustering at – a2\n32 and continuous\nportion σc( e\nA) = (–∞, – a2\n32 ].\nProof. Notice that the right endpoint L is always regular. By Proposition 8 in case k > 0 the origin is either exit\nor regular. According to Linetsky (2007) regular and exit boundaries are always non-oscillatory. Therefore, both\nendpoints in this case are non-oscillatory and the operator e\nA is of the spectral category I (i.e., its spectrum is purely\ndiscrete and lies in (–∞, 0]).\nFor k ≤0, the origin is natural by Proposition 8. From (44) and (45) we have\nlim\nx→0+ p(x) = –∞,\ni.e., the origin is transformed into –∞by Liouville transformation (34). Then, we investigate the potential U(x) in\n(44) and (45). We have\nΛ := lim\nx→0 U(x) =\n\n\n\na2\n32 ,\nk = 0,\n0,\nk < 0.\nAs this limit is finite, the origin is an oscillatory boundary with cutoff –Λ. This, together with L being regular and\nthus non-oscillatory, implies that the operator e\nA is of the spectral category II. Therefore for Λ = 0 (k < 0) it has purely\ncontinuous spectrum in (–∞, 0], while for Λ = a2\n32 (k = 0) it has continuous spectrum σc( e\nA) = (–∞, –Λ] = (–∞, – a2\n32 ]\nand discrete spectrum in (– a2\n32 , 0] clustering at – a2\n32 .\nFor certain choices of k, we can express ηλ, the solution of (46) corresponding to eigenvalue λ in terms of special\nfunctions. We provide some examples here\nk = 1/2,\nηλ(z) = e– az2\n2\n√\n2\n√z\n\u0010\nc1U\n\u0010\nλ\n√\n2a , 0; az 2\n√\n2\n\u0011\n+ c2L(–1)\n–\nλ\n√\n2a\n\u0010\naz 2\n√\n2\n\u0011\u0011\n,\n(48)\n8\n\n\nk = 0,\nηλ(z) = c1I– 1\n2\n√\na2+32λ(2\n√\n2ez/2) + c2I 1\n2\n√\na2+32λ(2\n√\n2ez/2),\nk = –1/4,\nηλ(z) = c1ze–\n√\n2\nz\n\u0000 16\na2 +z 2i\n√\n–λ\u0001\nHeunD\n\u0010\n64\na2 i\n√\n–λ, –2i\n√\n–2λ, 32\na2\n√\n2, 2, –2i\n√\n–2λ; z\n\u0011\n+ c2ze\n√\n2\nz\n\u0000 16\na2 +z 2i\n√\n–λ\u0001\nHeunD\n\u0010\n64\na2 i\n√\n–λ, 2i\n√\n–2λ, – 32\na2\n√\n2, 2, 2i\n√\n–2λ; z\n\u0011\n,\nk = –1/2,\nηλ(z) = √z\n\u0010\nc1J 2\n√\n2\na\n\u0010\n– i\n√\n2λz\n\u0011\n+ c2Y 2\n√\n2\na\n\u0010\n– i\n√\n2λz\n\u0011\u0011\n,\n(49)\nwhere U(α, β; z) is the Tricomi’s confluent hypergeometric function, L(α)\nn (z) is the generalized Laguerre polynomial,\nHeunD(q, α, γ, δ, ε; z) is the double-confluent Heun function (Wolfram, HeunD function), Jα(z) and Yα(z) are Bessel\nfunctions, and Iα(z) is the modified Bessel function of the first kind. The constants c1 and c2 are chosen to satisfy\nthe BCs (36), (37) and (47) together with normalization (29) or (30) for (34) with respect to ⟨·, ·⟩e\nm where em is the\nspeed density given by (32).\n4.1\nExample: k = 1/2, the origin is exit\nFor k = 1/2, the SDE (40) of the short-rate X becomes\ndXt = 1{Xt∈(0,L)}a\np\nXtdWt.\n(50)\nIn Figure 2, using a standard Euler-Maruyama method, we plot four sample trajectories of the short-rate (50)\nunder P (corresponding to generator A in (6)) as well as the corresponding trajectories of the short-rate X under eP\n(corresponding to generator e\nA in (41)). As the origin is exit and L is regular, the short-rate X can hit both endpoints\n0 and L. However, the trajectories spend more time near the origin than they do near L before reaching either due to\nthe fact that σ(x) = a√x →0 as x →0.\nProposition 10. For k = 1/2 (dynamics of X given by (50)) the origin is exit, the spectrum σ( e\nA) = σd( e\nA) is\npurely discrete and the eigenfunctions ψn(x) := ψλn (x) (26), normalized as (29), have the following form\nψn(x) =\nr a\n2cn\nxM\n\u0010\n1 –\nλn\na\n√\n2, 2; – 2\n√\n2\na x\n\u0011\n,\n(51)\ncn :=\nZ a\n0\nxe\n2\n√\n2x\na\n\u0010\nM\n\u0010\n1 –\nλn\na\n√\n2, 2; – 2\n√\n2\na x\n\u0011\u00112\ndx,\n(52)\nwhere M(α, β; z) is a Kummer’s confluent hypergeometric function, that is (DLMF, Chapter 13)\nM(α, β; z) :=\n∞\nX\nk=0\nα(k)z k\nβ(k)k! ,\nα(k) := α(α + 1) . . . (α + k – 1),\nα(0) := 1,\n(53)\nwhile the eigenvalues λn are solutions of\nM\n\u0010\n1 –\nλn\na\n√\n2, 2; – 2\n√\n2\na L\n\u0011\n= 0.\n(54)\nThen, the transition density (15) admits the representation\neΓ(t, x; T, y) = e\n2\n√\n2y\na\n∞\nX\nn=1\neλn(T–t)\ncn\nxM\n\u0010\n1 –\nλn\na\n√\n2, 2; – 2\n√\n2\na x\n\u0011\nM\n\u0010\n1 –\nλn\na\n√\n2, 2; – 2\n√\n2\na y\n\u0011\n.\n(55)\nProof. See Appendix B\n9\n\n\nIn Figure 3, with L = 1, a = 1, we plot the first four normalized eigenfunctions (51), corresponding to λ1 =\n–2.16096, λ2 = –6.48742, λ3 = –13.2721, λ4 = –22.5243. Smaller λn is associated with a more oscillatory eigenfunction.\nIn Figure 4, we plot the transition density eΓ(t, x; T, y) (55) as a function of y with x = 0.5 and t = 0 fixed, and T\ngoing from 0.05 to 0.3 in steps of 0.05. The area under the density curve\nR 1\n0 eΓ(x, t; T, y)dy is less than 1 for T > 0\nbecause XT has a positive probability of being located at one of the endpoints.\nRemark 11. The transition density eΓ, which is needed to write the explicit solution (20) of the pricing PDE (3),\nis for the short-rate X under the equivalent probability measure eP (18). Under eP, from the expression (41) when\nk = 1/2, X has the following SDE\ndXt = 1{Xt∈(0,L)}\n\u0010\na\n√\n2Xtdt + a\np\nXtdWt\n\u0011\n.\n(56)\nTo compute bond prices (and the prices of other financial derivatives), we must compute an expression for f (x) (23).\nFrom (38) we have\nf (x) =\n\n\n\n–\n√\n8x 2k+1\na(2k+1) ,\nk ̸= – 1\n2,\n–\n√\n2\na log x,\nk = – 1\n2.\n(57)\nProposition 12. For k = 1/2 short-rate model (50) the bond price (7) has the following analytic expression\nBT\nt = e–x(T–t) + xe\n√\n2\na x\n∞\nX\nn=0\nM\n\u0010\n1 –\nλn\na\n√\n2, 2; – 2\n√\n2\na x\n\u0011\ncn\nZ L\n0\nh(t, T; ξ, λn)M\n\u0010\n1 –\nλn\na\n√\n2, 2; – 2\n√\n2\na ξ\n\u0011\ndξ,\n(58)\nh(t, T; ξ, λ) :=\na2ξe\nξ\n\u0010 √\n2\na –(T–t)\n\u0011 \u0010\n–((λ + ξ)(t – T)((λ + ξ)(t – T) – 2)) + 2e(λ+ξ)(T–t) – 2\n\u0011\n2(λ + ξ)3\n,\nwhere cn is defined as in (52), M(α, β; z) is the Kummer confluent hypergeometric function (53), and λn are\nsolutions of (54).\nProof. By definition (7), we let g(x) = 1 in (20). When k = 1/2, the expressions for (41) and (57) are\ne\nA = a\n√\n2x∂x + 1\n2a2x∂2\nx ,\nf (x) = –\n√\n2\na x.\n(59)\nPlugging (59) into (13) results in\nq(t, x) = 1\n2a2(T – t)2xe–x(T–t+\n√\n2\na ).\n(60)\nFinally, substituting (55) and (60) into (20) we get\nu(t, x) = e–x(T–t)\n+ a2\n2 xe\n√\n2x\na\n∞\nX\nn=0\nM\n\u0010\n1 –\nλn\na\n√\n2, 2; – 2\n√\n2\na x\n\u0011\ncn\nZ T\nt\nds\nZ L\n0\ndξe\n√\n2ξ\na\n+λn(s–t)–ξ(T–s)M\n\u0010\n1 –\nλn\na\n√\n2, 2; – 2\n√\n2\na ξ\n\u0011\n(T – s)2ξ.\nIntegrating with respect to s first results in (58).\nIn Figures 5 and 6 we fix parameters L = 1, a = 1 and plot both the bond price BT\n0 (58) and the corresponding yield\ncurve YT\n0 (7) with X0 = x ∈{ 1\n3, 1\n2, 2\n3}. Notice that the yield curve has an inverted pattern.\n10\n\n\n4.2\nExample: k = –1/2, the origin is natural\nFor k = –1/2, the SDE (40) of the short-rate X becomes\ndXt = 1{Xt∈(0,L)}\n\u0010\na2\n2 X2\nt dt + aX3/2\nt\ndWt\n\u0011\n.\n(61)\nIn Figure 7, with a = L = 1, we plot sample trajectories of the short-rate (61) under P (corresponding to generator\nA in (6)) as well as the corresponding trajectories for the short-rate X under eP (corresponding to generator e\nA in\n(41)). Note that the short-rate Xt can hit the right endpoint L, which is regular, while it cannot reach the origin,\nwhich is natural.\nProposition 13. For k = –1/2 model (61) the origin is natural, the spectrum of e\nA (41) is purely continuous in\n(–∞, 0) and the improper eigenfunctions ψ(ρ, x) := ψλ(ρ)(x) (26) normalized as (30) have the following form\nψ(ρ, x) :=\nq\nLρ\n2 π csc\n\u0010\n2\n√\n2π\na\n\u0011\n\f\f\f\fK 2\n√\n2\na\n\u0010\ni 2\n√\n2ρ\na\n\u0011\f\f\f\f x\n√\n2\na\n\u0012\nJ 2\n√\n2\na\n\u0010\n2\n√\n2ρ\na\n\u0011\nJ– 2\n√\n2\na\n\u0012\n2\n√\n2ρ\na\nq\nL\nx\n\u0013\n– J– 2\n√\n2\na\n\u0010\n2\n√\n2ρ\na\n\u0011\nJ 2\n√\n2\na\n\u0012\n2\n√\n2ρ\na\nq\nL\nx\n\u0013\u0013\n,\n(62)\nwhere Jα(x) is the Bessel function of the first kind and Kα(x) is the modified Bessel function of the second\nkind. Then, the transition density (15) admits the representation\neΓ(t, x; T, y) = 2\na2 y–2+ 2\n√\n2\na\nZ ∞\n0\ne–Lρ2(T–t)ψ(ρ, x)ψ(ρ, y)dρ,\n(63)\nProof. See Appendix D.\nRemark 14. Similar to Remark 11, we note that the transition density eΓ in (63) is that of the short-rate X (61), but\nunder the equivalent probability measure eP (18). Under eP, from the expression (41), when k = –1/2, the short-rate\nX has the following dynamics\ndXt = 1{Xt∈(0,L)}\n\u0010\n(1\n2a2 + a\n√\n2)X2\nt dt + aX3/2\nt\ndWt\n\u0011\n.\n(64)\nRemark 15. From the proof Proposition 13 together with (30) we can derive that for γ > 0, ρ0 > 0\nδ(ρ – ρ0) =\nLπ2 csc2(γπ)√ρρ0\n2|Kγ(iγρ)Kγ(iγρ0)|\n·\nZ L\n0\n1\nx γ\n\u0012\nJγ(γρ)J–γ(γρ\nq\nL\nx ) – J–γ(γρ)Jγ(γρ\nq\nL\nx )\n\u0013 \u0012\nJγ(γρ0)J–γ(γρ0\nq\nL\nx ) – J–γ(γρ0)Jγ(γρ0\nq\nL\nx )\n\u0013\ndx,\nwhere we have let γ := 2\n√\n2\na\nin (62). To our knowledge, this representation for the Dirac delta function is not present\nin contemporary literature.\nIn Figure 8, with a = l = 1 we plot four improper normalized eigenfunctions (62), where we use the natural log\nscale for the x axis. Notice that as x →0 eigenfunctions exhibit highly oscillatory behavior. In Figure 9 we plot the\ntransition density Γ(t, x; T, y) (63) as a function of y with x = 0.5 and t = 0 fixed, and T ranging from 0.05 to 0.3\nwith step 0.05. The transition density goes to 0 in the neighborhood of the origin, as expected due to the origin\nbeing natural. The area under the density curve\nR 1\n0 eΓ(x, t, T, y)dy is less than 1 for T > 0, because there is positive\nprobability that XT = L, which is regular endpoint.\n11\n\n\nProposition 16. For k = –1/2 corresponding to short-rate model (61), the bond price (7) has the following\nanalytic expression\nBT\nt = e–x(T–t) + L\n2 π2 csc2 \u0010\n2\n√\n2π\na\n\u0011 Z L\n0\ndξ\nZ ∞\n0\ndρ h(t, T, ξ, ρ)θ(ρ, x)θ(ρ, ξ)ρ,\n(65)\nθ(ρ, x) :=\n\f\f\f\fK 2\n√\n2\na\n\u0010\ni 2\n√\n2ρ\na\n\u0011\f\f\f\f\n–1 \u0012\nJ 2\n√\n2\na\n\u0010\n2\n√\n2ρ\na\n\u0011\nJ– 2\n√\n2\na\n\u0012\n2\n√\n2ρ\na\nq\nL\nx\n\u0013\n– J– 2\n√\n2\na\n\u0010\n2\n√\n2ρ\na\n\u0011\nJ 2\n√\n2\na\n\u0012\n2\n√\n2ρ\na\nq\nL\nx\n\u0013\u0013\n,\nh(t, T, ξ, ρ) := (Lρ2+ξ)eLρ2(t–T)–eξ(t–T)(L2ρ4(t–T)(ξt–ξT+1)+Lρ2(1–2ξ2(t–T)2)+ξ+ξ3(t–T)2+ξ2(T–t))\n(ξ–Lρ2)3\n,\nwhere Jα(x) is the Bessel function of the first kind and Kα(x) is the modified Bessel function of the second\nkind.\nProof. By definition (7), we let g(x) = 1 in (20). When k = –1/2, the expressions for (41) and (57) become\ne\nA =\n\u0010a2\n2 + a\n√\n2\n\u0011\nx 2∂x + a2\n2 x 3∂2\nx ,\nf (x) = –\n√\n2\na log x.\n(66)\nPlugging (66) into (13) results in\nq(t, x) = a2\n2 e–(T–t)x x 2–\n√\n2\na (t – T)(1 + tx – Tx).\n(67)\nFinally, substituting (63) and (67) into (20) we obtain\nu(t, x) = e–x(T–t) +\nZ T\nt\nds\nZ L\n0\ndξ\nZ ∞\n0\ndρ e–Lρ2(s–t)–(T–s)ξ(s – T)(1 + sξ – Tξ)x\n√\n2\na ψ(ρ, x)ξ\n√\n2\na ψ(ρ, ξ).\n(68)\nIntegrating with respect to s first in (68) and simplifying the expression results in (65).\nIn figures 10 and 11, with a = L = 1 fixed, we plot both the bond price BT\n0 (65) and the corresponding yield curve\nYT\n0 (7) where X0 = x ∈{1\n3, 1\n2, 2\n3}. For a higher value of x the yield curve has a humped shape at the beginning,\nbefore decreasing, while for a lower value of x it appears to be more flat.\nAcknowledgments\nAram Karakhanyan acknowledges partial support from EPSRC grant EP/S03157X/1 Mean curvature measure of free\nboundary.\nA\nProof of Proposition 8\nConsider the scale measure\neS[x, y] :=\nZ y\nx\nes(z)dz,\nx, y ∈(0, L),\n(69)\nwhere es(x) is a scale density (42), together with the limiting case at the origin\neS(0, z] = lim\nx→0+ S[x, z],\nz ∈(0, L).\n(70)\n12\n\n\nThe boundary classification of the origin depends on the following integrals\nI0 =\nZ ε\n0\neS(0, z] em(z)dz,\n(71)\nJ0 =\nZ ε\n0\neS[z, ε] em(z)dz,\n(72)\nwhere ε ∈(0, L) and em(x) is a speed density (43). In particular, according to Linetsky (2007), the origin is\n1. regular if I0 < ∞and J0 < ∞,\n2. exit if I0 < ∞and J0 = ∞,\n3. natural if I0 = ∞and J0 = ∞.\nThus, in order to classify the origin, it is sufficient to determine whether each of the integrals I0 and J0 converges or\ndiverges.\nFor simplicity, denote\nw(x) =\n√\n2\na(k + 1\n2)\nx k+ 1\n2 ,\nthen evaluating (69) we get the expression for the scale measure\neS[x, y] = Ca\n2\n√\n2\n\n\n\nexp(–2w(x)) – exp(–2w(y)),\nk ̸= – 1\n2,\nx – 2\n√\n2\na\n– y– 2\n√\n2\na ,\nk = – 1\n2.\n(73)\nNext, we compute eS(0, ε] as in (70)\neS(0, z] =\n\n\n\nCa\n2\n√\n2 (1 – exp (–2w(z))) ,\nk > – 1\n2,\n∞,\nk ≤– 1\n2.\n(74)\nThen by plugging (43) and (74) into (71) for k > – 1\n2 we get\nI0 =\n1\na\n√\n2\nZ ε\n0\nz k– 3\n2\n\u0012\nexp\n\u0012\n2\n√\n2\na(k+ 1\n2 )z k+ 1\n2\n\u0013\n– 1\n\u0013\ndz.\nUsing the asymptotic equivalence\nexp\n\u0012\n2\n√\n2\na(k+ 1\n2 )z k+ 1\n2\n\u0013\n– 1 ∼\n2\n√\n2\na(k+ 1\n2 )z k+ 1\n2\nas z →0,\nit follows that convergence of I0 coincides with convergence of\n2\na2(k+ 1\n2 )\nZ ε\n0\nz 2k–1dz\nbut the latter integral converges for k > 0 and diverges for k ∈(– 1\n2, 0]. Together with (74) for k ≤– 1\n2 this results in\n\n\n\nI0 < ∞,\nk > 0,\nI0 = ∞,\nk ≤0.\n(75)\n13\n\n\nPlugging (43) and (73) into (72) for k ̸= – 1\n2\nJ0 =\n1\na\n√\n2\nZ ε\n0\n\u0012\n1 – exp\n\u0012\n2\n√\n2\na(k+ 1\n2 )\n\u0010\nz k+ 1\n2 – εk+ 1\n2\n\u0011\u0013\u0013\nz k– 3\n2 dz\nNotice that since z k+ 1\n2 →0 as z →0 for k > – 1\n2, and z k+ 1\n2 →∞for k < – 1\n2, so we have asymptotic equivalence\n\u0012\n1 – exp\n\u0012\n2\n√\n2\na(k+ 1\n2 )\n\u0010\nz k+ 1\n2 – εk+ 1\n2\n\u0011\u0013\u0013\nz k– 3\n2 ∼Dz k– 3\n2\nas z →0,\nwhere\nD =\n\n\n\n\n\n\u0012\n1 – exp\n\u0012\n–\n2\n√\n2\na(k+ 1\n2 )εk+ 1\n2\n\u0013\u0013\n,\nk > – 1\n2,\n1,\nk < – 1\n2.\n.\nThus convergence of J0 (72) coincides with convergence of\nD\na\n√\n2\nZ ε\n0\nz k– 3\n2 dz,\nwhich converges for k > – 1\n2, so J0 < ∞in this case, and diverges for k < – 1\n2, so J0 = ∞. For k = – 1\n2\nJ0 =\n1\na\n√\n2\nZ ε\n0\n\u0012\nz – 2\n√\n2\na\n– ε– 2\n√\n2\na\n\u0013\nz –2+ 2\n√\n2\na dz =\n1\na\n√\n2\nZ ε\n0\n\u0012\nz –2 – ε– 2\n√\n2\na z –2+ 2\n√\n2\na\n\u0013\ndz = ∞,\nfrom asymptotic equivalence\nz –2 – ε– 2\n√\n2\na z –2+ 2\n√\n2\na\n= z –2\n\u0012\n1 – ε– 2\n√\n2\na z\n2\n√\n2\na\n\u0013\n∼z –2,\nas z →0.\nThus, we get\n\n\n\nJ0 < ∞,\nk > 1\n2,\nJ0 = ∞,\nk ≤1\n2.\nThis, combined with (75) proves the proposition.\nB\nProof of Proposition 10\nFrom Proposition 8 the origin is an exit boundary, and from Proposition 9 the spectrum is purely discrete. Plugging\nk = 1/2 into (39), (42), (43) with C = 1, and (44) we get\nσ(x) = a√x,\nes(x) = e– 2\n√\n2\na\nx ,\nem(x) = 2e\n2\n√\n2\na\nx\na2x\n,\np(x) = 2√x\na\n.\n(76)\nThus, from (46) and (76) we derive the following equation\nη′′\nλ(z) =\n1\n4z 2 (8λz 2 + 2a2z 4 + 3)ηλ(z),\nηλ(0) = 0,\nηλ\n\u0010\n2\n√\nL\na\n\u0011\n= 0.\nApplying the first BC to the analytic solution (48) we get that c1 = 0, and expanding L(–1)\n–\nλ\n√\n2a\n\u0010\naz 2\n2\n\u0011\n, the expression\nfor the eigenfunctions becomes\nηλ(z) = cz\n3\n2 e– az2\n2\n√\n2 M\n\u0010\n1 +\nλ\na\n√\n2, 2; az 2\n√\n2\n\u0011\n,\n(77)\n14\n\n\nwhere c is a constant coefficient. Before applying the second BC, we return back to the original variable x by plugging\n(76) and (77), into (34), resulting in\nψλ(x) = ce– 2\n√\n2\na\nx xM\n\u0010\n1 +\nλ\na\n√\n2, 2; 2\n√\n2\na x\n\u0011\n,\n(78)\nand using the Kummer’s transformation\nM(α, β; z) = ez M(β – α, β; –z),\n(78) becomes\nψλ(x) = cxM\n\u0010\n1 –\nλ\na\n√\n2, 2; – 2\n√\n2\na x\n\u0011\n.\nNow, applying the second BC (28)\nψλ(L) = cL · M\n\u0010\n1 –\nλ\na\n√\n2, 2; – 2\n√\n2\na L\n\u0011\n= 0.\nThus, the eigenvalues λn are solutions of this equation, or of (54) equivalently. The constant c for each ψn = ψλn is\ndetermined from normalization (29) with em (76).\n∥ψn∥2\ne\nm = ⟨ψn, ψn⟩e\nm = 2c2\na2\nZ L\n0\ne\n2\n√\n2\na\nx x\n\u0010\nM\n\u0010\n1 –\nλn\na\n√\n2, 2; – 2\n√\n2\na L\n\u0011\u00112\ndx = 1,\nso if we introduce cn (52), then\nc =\nr a\n2cn\n,\nwhich produces (51). Finally, since the spectrum is purely discrete, i.e. σ( e\nA) = σp( e\nA) and σc( e\nA) = 0, the expression\nfor the transition density eΓ(t, x; T, y) (55) follows from the general spectral representation (31).\nC\nSpectral representation for the natural boundary case\nIn practice, when the origin is natural and, consequently, the continuous spectrum is nonempty (see Proposition\n9), the eigenfunction representation is written by finding the Green’s function first (Borodin and Salminen, 2002,\nChapter 2)\nGλ(x, y) = ψλ(min(x, y))φλ(max(x, y))\nwλ\n,\nwλ :=\n1\nes(x)\n\u0000ψ′\nλ(x)φλ(x) – ψλ(x)φ′\nλ(x)\n\u0001\n,\n(79)\nwhere ψλ(x) and φλ(x) are solutions of (26) s.t. ψλ is increasing, while φλ is decreasing and satisfies (28). Both\nfunctions should satisfy some additional conditions at the origin to ensure Green’s function uniqueness, that is\nψλ(0+) := lim\nx→0+ ψλ(x) = 0,\nφλ(0+) := lim\nx→0+ φλ(x) = +∞,\nψ+\nλ (0+) :=\nlim\nh→0+\nψλ(0 + h) – ψλ(0+)\nS(0, h]\n= 0,\nφ+\nλ (0+) =\nlim\nh→0+\nφλ(0 + h) – φλ(0+)\nS(0, h]\n= –∞,\nwhere S(0, h] is defined as in (70). These conditions will typically be satisfied for ψλ and φλ under the prior\nassumptions, but when they are not, one must change ψλ and φλ accordingly. Under the aforementioned conditions\non ψλ(x) and φλ(x), the Wronskian wλ (79) will be independent of x which justifies the notation.\n15\n\n\nThen, applying inverse Laplace transform to (79) results in\neΓ(t, x; T, y) = em(y)\n2πi\nZ c+i∞\nc–i∞\neλ(T–t)Gλ(x, y)dλ,\n(80)\nwhich produces a discrete part of (31) when the Green’s function has isolated singularities, and a continuous part\nwhen there is a branch cut for λ.\nD\nProof of Proposition 13\nFrom Proposition 8 the origin is a natural boundary, and from Proposition 9 the spectrum is purely continuous in\n(–∞, 0). Plugging k = –1/2 into (39), (42), (43) with C = 1, and (44) we get\nσ(x) = ax 3/2,\nes(x) = x –1– 2\n√\n2\na ,\nem(x) = 2\na2 x –2+ 2\n√\n2\na ,\np(x) = –\n2\na√x .\n(81)\nFrom (46) we derive the following equation\nη′′ =\n1\n4z 2 (8λz 2 + 2a2z 4 + 3)η.\nThe general solution to this ODE is given by (49). First, we return back to the original variable x by plugging (49)\nand (81) (34). This results in the general solution of the eigenvalue equation (26)\nψλ(x) = c1x –\n√\n2\na J 2\n√\n2\na\n\u0012\n2i\n√\n2\na\nq\nλ\nx\n\u0013\n+ c2x –\n√\n2\na Y 2\n√\n2\na\n\u0012\n2i\n√\n2\na\nq\nλ\nx\n\u0013\n,\n(82)\nwhere Jα(z), Yα(z) are the Bessel functions. (82) is a linear combination of two independent solutions\nψ(1)\nλ\n= x –\n√\n2\na J 2\n√\n2\na\n\u0012\n2i\n√\n2\na\nq\nλ\nx\n\u0013\n,\nψ(2)\nλ\n= x –\n√\n2\na Y 2\n√\n2\na\n\u0012\n2i\n√\n2\na\nq\nλ\nx\n\u0013\n.\nFollowing the approach described in Appendix C, we need to construct two solutions ψλ(x) and φλ(x) of (26) s.t. for\nλ > 0 ψλ(x) is increasing, while φλ(x) is decreasing and satisfies the BC (28)\nψλ(x) = π\n2e\n(2\n√\n2+a)πi\n2a\nψ(1)\nλ (x) + π\n2e\n(2\n√\n2+a)πi\n2a\niψ(2)\nλ (x) = x –\n√\n2\na K 2\n√\n2\na\n\u0012\n2\n√\n2\na\nq\nλ\nx\n\u0013\n,\n(83)\nφλ(x) =\n\u0012\nK 2\n√\n2\na\n\u0012\n2\n√\n2\na\nq\nλ\nL\n\u0013\ne–\n√\n2πi\n2a\n– I 2\n√\n2\na\n\u0012\n2\n√\n2\na\nq\nλ\nL\n\u0013 π\n2e\n(2\n√\n2+a)πi\n2a\n\u0013\nψ(1)\nλ (x) – I 2\n√\n2\na\n\u0012\n2\n√\n2\na\nq\nλ\nL\n\u0013 π\n2e\n(2\n√\n2+a)πi\n2a\niψ(2)\nλ (x)\n= x –\n√\n2\na\n\u0012\nI 2\n√\n2\na\n\u0012\n2\n√\n2\na\nq\nλ\nx\n\u0013\nK 2\n√\n2\na\n\u0012\n2\n√\n2\na\nq\nλ\nL\n\u0013\n– I 2\n√\n2\na\n\u0012\n2\n√\n2\na\nq\nλ\nL\n\u0013\nK 2\n√\n2\na\n\u0012\n2\n√\n2\na\nq\nλ\nx\n\u0013\u0013\n,\n(84)\nwhere Iα(z), Kα(z) are the modified Bessel functions, and we have used the following connection formulas between\nregular and modified Bessel functions (DLMF, Chapter 10) which hold for all z ∈R\nIα(z) = e– απi\n2 Jα(z),\nKα(z) = π\n2e\n(α+1)πi\n2\n(Jα(iz) + iYα(iz))\nPlugging (81), (83) and (84) into the Wronskian (79) results in\nwλ = 1\n2K 2\n√\n2\na\n\u0012\n2\n√\n2\na\nq\nλ\nL\n\u0013\n.\n(85)\n16\n\n\nPlugging (83), (84) and (85) into (79) we compute the Green’s function\nGλ(x, y) =\n2\nx\n√\n2\na y\n√\n2\na K 2\n√\n2\na\n\u0012\n2\n√\n2\na\nq\nλ\nL\n\u0013\n·\n\n\n\n\n\n\n\nK 2\n√\n2\na\n\u0012\n2\n√\n2\na\nq\nλ\nx\n\u0013 \u0014\nK 2\n√\n2\na\n\u0012\n2\n√\n2\na\nq\nλ\nL\n\u0013\nI 2\n√\n2\na\n\u0010\n2\n√\n2\na\nq\nλ\ny\n\u0011\n– I 2\n√\n2\na\n\u0012\n2\n√\n2\na\nq\nλ\nL\n\u0013\nK 2\n√\n2\na\n\u0010\n2\n√\n2\na\nq\nλ\ny\n\u0011\u0015\n,\nx < y\nK 2\n√\n2\na\n\u0010\n2\n√\n2\na\nq\nλ\ny\n\u0011 \u0014\nK 2\n√\n2\na\n\u0012\n2\n√\n2\na\nq\nλ\nL\n\u0013\nI 2\n√\n2\na\n\u0012\n2\n√\n2\na\nq\nλ\nx\n\u0013\n– I 2\n√\n2\na\n\u0012\n2\n√\n2\na\nq\nλ\nL\n\u0013\nK 2\n√\n2\na\n\u0012\n2\n√\n2\na\nq\nλ\nx\n\u0013\u0015\n,\ny < x\n.\nFinally, to get the transition density, we must invert the Laplace transform (80) with em (81). Since T – t > 0 and\nGλ(x, y) has no poles, we close the contour in the left half-plane and apply the Jordan’s lemma. Then, using the\nsubstitution λ = –Lρ2 to account for the branch cut, we get\neΓ(t, x; T, y) = –L em(y)\nπi\nZ ∞\n0\ne–Lρ2(T–t) \u0010\nGLρ2eiπ(x, y) – GLρ2e–iπ(x, y)\n\u0011\nρdρ,\n(86)\nthe integrand can be simplified as\nGLρ2eiπ(x, y) – GLρ2e–iπ(x, y) = –\niπ3 csc2 \u0010\n2\n√\n2π\na\n\u0011\n2\n\f\f\fK\n\u0010\n2\n√\n2\na , i 2\n√\n2ρ\na\n\u0011\f\f\f\n2\nx\n√\n2\na y\n√\n2\na\n·\n\u0012\nJ 2\n√\n2\na\n\u0010\n2\n√\n2ρ\na\n\u0011\nJ– 2\n√\n2\na\n\u0012\n2\n√\n2ρ\na\nq\nL\nx\n\u0013\n– J– 2\n√\n2\na\n\u0010\n2\n√\n2ρ\na\n\u0011\nJ 2\n√\n2\na\n\u0012\n2\n√\n2ρ\na\nq\nL\nx\n\u0013\u0013\n·\n\u0012\nJ 2\n√\n2\na\n\u0010\n2\n√\n2ρ\na\n\u0011\nJ– 2\n√\n2\na\n\u0012\n2\n√\n2ρ\na\nq\nL\ny\n\u0013\n– J– 2\n√\n2\na\n\u0010\n2\n√\n2ρ\na\n\u0011\nJ 2\n√\n2\na\n\u0012\n2\n√\n2ρ\na\nq\nL\ny\n\u0013\u0013\n,\nplugging this into (86) and introducing ψ(ρ, x) (62) results in (63). Finally, we notice that (62) satisfy\n\u0012a2\n2 + a\n√\n2\n\u0013\nx 2∂x ψ(ρ, x) + a2\n2 x 3∂2\nx ψ(ρ, x) = –Lρ2ψ(ρ, x),\nwhich is (26) for e\nA (41) with k = –1/2, and since\n\u0012\nJ 2\n√\n2\na\n\u0010\n2\n√\n2ρ\na\n\u0011\nJ– 2\n√\n2\na\n\u0012\n2\n√\n2ρ\na\nq\nL\nL\n\u0013\n– J– 2\n√\n2\na\n\u0010\n2\n√\n2ρ\na\n\u0011\nJ 2\n√\n2\na\n\u0012\n2\n√\n2ρ\na\nq\nL\nL\n\u0013\u0013\n= 0\nψ(ρ, x) satisfy (28). Therefore ψ(ρ, x) are indeed improper eigenfunctions, and from the uniqueness of spectral\nrepresentation (31) we conclude that they satisfy the normalization (30).\nReferences\nBLS. US Bureau of Labor Statistics. https://data.bls.gov/timeseries/CUUR0000SA0?output_view=pct_12mths.\nAccessed: 2025-01-20. 1\nA. Borodin and P. Salminen. Handbook of Brownian Motion - Facts and Formulae. Probability and Its Applications.\nBirkhäuser Basel, 2002. ISBN 978-3-7643-67053. doi: 10.1007/978-3-0348-8163-0. 15\nDLMF. NIST Digital Library of Mathematical Functions. https://dlmf.nist.gov/, Release 1.2.1 of 2024-06-15.\nURL https://dlmf.nist.gov/. F. W. J. Olver, A. B. Olde Daalhuis, D. W. Lozier, B. I. Schneider, R. F. Boisvert,\nC. W. Clark, B. R. Miller, B. V. Saunders, H. S. Cohl, and M. A. McClain, eds. 6, 9, 16\n17\n\n\nW. Feller. The parabolic differential equations and the associated semi-groups of transformations. Annals of\nMathematics, 55:468–519, 1952. URL https://api.semanticscholar.org/CorpusID:123972208. 2\nG. W. Hanson and A. B. Yakovlev. Operator Theory for Electromagnetics: An Introduction. Springer New York,\nNY, 2013. ISBN 978-0-387-95278-9. doi: 10.1007/978-1-4757-3679-3. 6\nV. Linetsky. Chapter 6 spectral methods in derivatives pricing. In J. R. Birge and V. Linetsky, editors, Financial\nEngineering, volume 15 of Handbooks in Operations Research and Management Science, pages 223 – 299.\nElsevier, 2007. 8, 13\nSt. Louis Fed. Federal Reserve Bank of St. Louis. https://fred.stlouisfed.org/series/FEDFUNDS. Accessed:\n2025-01-20. 1\nWolfram. Wolfram Language and System Documentation Center. https://reference.wolfram.com/language/\nref/HeunD. Accessed: 2025-01-24. 9\nFigures\nSep'20\nJan'21\nMay'21\nSep'21\nJan'22\nMay'22\nSep'22\nJan'23\nMay'23\nSep'23\nJan'24\nMay'24\nSep'24\nDate\n0\n1\n2\n3\n4\n5\nOBFR, %\nFigure 1: Overnight bank funding rate (OBFR). Source: Federal Reserve Bank of New York\nhttps://www.newyorkfed.org/markets/reference-rates/obfr\n18\n\n\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 2: We plot 4 trajectories for the short-rate Xt in k = 1/2 model 4.1 with L = 1, a = 1, starting from X0 = 1/2,\nfor t ∈[0, 2]. Blue is under the original probability measure P with dynamics (50), while red is under the equivalent\nprobability measure eP as introduced in Remark 2 and dynamics (56).\n19\n\n\n0.2\n0.4\n0.6\n0.8\n1.0\n-0.2\n0.2\n0.4\nλ1\nλ2\nλ3\nλ4\nFigure 3: We plot the first four normalized eigenfunctions ψn (51) for L = 1 and a = 1, normalized as in (29).\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.5\n1.0\n1.5\n2.0\n2.5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.5\n1.0\n1.5\n2.0\n2.5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.5\n1.0\n1.5\n2.0\n2.5\nT = 0.05\nT = 0.10\nT = 0.15\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.5\n1.0\n1.5\n2.0\n2.5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.5\n1.0\n1.5\n2.0\n2.5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.5\n1.0\n1.5\n2.0\n2.5\nT = 0.20\nT = 0.25\nT = 0.30\nFigure 4: We plot the transition density eΓ(t, x; T, y) (55) for k = 1\n2 model (Section 4.1) as a function of y ∈[0, 1].\nThe fixed parameters are t = 0, x = 1\n2, L = 1, a = 1.\n20\n\n\n0\n2\n4\n6\n8\n10\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 5: The bond price BT\n0 (58) for k = 1\n2 model (Section 4.1). Red solid line corresponds to x = 1\n3, orange dashed\nline to x = 1\n2, blue dotted line to x = 2\n3. The fixed parameters are L = 1, a = 1.\n0\n2\n4\n6\n8\n10\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 6: The yield curve YT\n0 (7) for k = 1\n2 model (Section 4.1) with BT\n0 as in (58). Red solid line corresponds to\nx = 1\n3, orange dashed line to x = 1\n2, blue dotted line to x = 2\n3. The fixed parameters are L = 1, a = 1.\n21\n\n\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 7: We plot 4 trajectories for the short-rate Xt in k = –1/2 model 4.2 with L = 1, a = 1, starting from\nX0 = 1/2, for t ∈[0, 2]. Blue is under the original probability measure P with dynamics (61), while red is under the\nequivalent probability measure eP as introduced in Remark 2 and dynamics (64).\n22\n\n\n-2.5\n-2.0\n-1.5\n-1.0\n-0.5\n0.0\n-20\n-10\n10\n20\nρ 1\n2\nρ 1\nρ 3\n2\nρ 2\nFigure 8: We plot four improper normalized eigenfunctions ψ(ρ, ez ) (62) for k = –1/2 model with L = 1, a = 1, and\nz ∈[–3, 0] corresponding to x ∈[0.05, 1].\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n1\n2\n3\n4\n5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n1\n2\n3\n4\n5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n1\n2\n3\n4\n5\nT = 0.05\nT = 0.10\nT = 0.15\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n1\n2\n3\n4\n5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n1\n2\n3\n4\n5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n1\n2\n3\n4\n5\nT = 0.20\nT = 0.25\nT = 0.30\nFigure 9: We plot the transition density eΓ(t, x; T, y) (63) for k = – 1\n2 model (Section 4.2) as a function of y ∈[0, 1]\nfor different values of T. The fixed parameters are t = 0, x = 1\n2, L = 1, a = 1.\n23\n\n\n0\n2\n4\n6\n8\n10\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 10: The bond price BT\n0 (65) for k = – 1\n2 model (Section 4.2). Red solid line corresponds to x = 1\n3, orange\ndashed line to x = 1\n2, blue dotted line to x = 2\n3. The fixed parameters are L = 1, a = 1.\n0\n2\n4\n6\n8\n10\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 11: The yield curve YT\n0 (7) for k = – 1\n2 model (Section 4.2) with BT\n0 as in (65). Red solid line corresponds to\nx = 1\n3, orange dashed line to x = 1\n2, blue dotted line to x = 2\n3. The fixed parameters are L = 1, a = 1.\n24\n\n\n"}
{"text": "Effects of non-parallelism on standard and\nmagnetorheological measurements\nR. Rodrigues1,3, F.J. Galindo-Rosales2,3, and L. Campo-Deaño1,3†\n1CEFT - Centro de Estudos de Fenómenos de Transporte, Depto. de Engenharia Mecânica,\nFaculdade de Engenharia, Universidade do Porto, Rua Dr. Roberto Frias, 4200-465, Porto,\nPortugal\n2CEFT - Centro de Estudos de Fenómenos de Transporte, Depto. de Engenharia Química,\nFaculdade de Engenharia, Universidade do Porto, Rua Dr. Roberto Frias, 4200-465, Porto,\nPortugal\n3ALiCE - Laboratório Associado em Engenharia Química, Faculdade de Engenharia,\nUniversidade do Porto, Rua Dr. Roberto Frias, 4200-465, Porto, Portugal\n†Email: campo@fe.up.pt\nHuman blood has a complex composition and unique rheological properties, making\nit challenging to measure accurately. In addition to this, its mechanical properties\nmay be influenced by external magnetic fields, which, despite being a characteristic of\nsignificant interest in the development of new treatment therapies, remains relatively\nunexplored. To achieve an accurate magnetorheological description of blood, the\nemployed equipment must achieve accurate results taking into account its low viscous\nand elastic character. However, low and inconsistent apparent-viscosity values were\nobserved systematically in a rotational rheometer equipped with a magnetorheological\ncell, without the applied magnetic field. In this work, a parametric study was\nconducted, experimentally and numerically, to evaluate this error source. Steady\nshear measurements were carried out with low-viscosity Newtonian fluids with two\ngeometries: a parallel-plate, at different gap heights, and a cone-plate. An additional\nstandard bottom plate for non-magnetic testing was also employed for comparison.\nThe standard bottom plate returned constant viscosities near the expected values,\nwhereas the plate attached to the magnetorheological cell showed a clear decrease of\nmeasured viscosity with parallel-plate gap reduction and an increase in cone-plate-\nmeasured viscosity. Numerical results corroborated the experimental observations,\npointing towards an inclination of the bottom magnetic plate which can significantly\naffect the flow. Additional experimental and numerical work was conducted to\nevaluate the effects of the setup imperfection on magnetorheological measurements,\nunveiling magnetorheology’s deep dependence on the geometric characteristics.\nKeywords: Rotational rheometry; Steady shear; Gap-error; Magnetorheology;\nlow-viscous fluids\n1\narXiv:2502.20967v1  [physics.flu-dyn]  28 Feb 2025\n\n\n1 Introduction\nBlood is a complex fluid because of its composition and mechanical properties. It is\nessentially a suspension of blood cells in plasma. Blood cells can be characterised as\nthrombocytes (or platelets), leukocytes (white blood cells or WBCs) and erythrocytes\n(red blood cells or RBCs), the latter being the most predominant in blood, with volume\nconcentration, or haematocrit (Htc), between 40 to 54% in men and 36 to 48% in women,\ndepending on other factors such as diseased state, altitude, etc.1 Blood rheology highly\ndepends on plasma viscoelasticity, haematocrit and RBCs’ mechanical properties and\naggregation mechanisms. In terms of viscous response, blood is a shear-thinning fluid with\na relatively low high-shear viscosity2 (around 3 mPa·s). Moreover, RBCs are composed\nof haemoglobin, which incorporates iron. When not carrying oxygen (deoxyhaemoglobin),\nfree electrons make it paramagnetic (attracted to an external magnetic field), while\ncarrying oxygen (oxyhaemoglobin), it remains diamagnetic (slightly repulsed)3. This\nmakes RBCs and, as a consequence, whole blood susceptible to magnetic fields. In recent\nyears, the efforts of the scientific community to understand the effects of magnetic fields\non blood’s rheological properties have grown. Magnetic fields have been studied as means\nof guidance for microrobots and particles for a multitude of medical applications4–7 and\nalso to knowingly alter blood’s properties themselves8, making magnetohaemorheology\na field of significant interest. However, experimental data are still lacking, and a full\nmagnetorheological description of whole blood has yet to be achieved.\nMeasurements with low-viscosity fluids, such as blood, can be strenuous even when\nconsidering simple fluids. Rotational rheometers, equipped with a magnetorheological cell\nto generate a uniform magnetic field, are commonly used to perform magnetorheological\ncharacterisations of this kind of fluids. Therefore, addressing some common error sources\nin rotational rheometry that could negatively affect the results is essential. A more\nin-depth walkthrough of such experimental difficulties is given in the seminal work of\nEwoldt et al.. Relevant to this work, we shall focus on the flow of low-viscosity Newtonian\nfluids and suspensions under steady shear in planar geometries (parallel-plates, PP, and\ncone-plate, CP).\nThe steady shear viscosity, η, can be obtained through the shear stress, σ, and the\nshear ratei, ˙γ, as given in the following equation9:\nη = σ\n˙γ = FτT\nFγΩ,\n(1)\nwhere T is the torque, Ωis the rotational velocity and Fτ and Fγ are functions of\ngeometrical characteristics, which for PP and CP geometries are given by9:\nPP :\nFτ =\n2\nπR3\nand\nFγ = R\nh ,\n(2)\nCP :\nFτ =\n3\n2πR3\nand\nFγ = 1\nβ ,\n(3)\niEither can be the controlled input, depending on the instrument.\n2\n\n\nwhere R is the geometry radius, h is the gap between parallel-plates and β is the CP\ncone angle.\nSteady shear measurements are limited by the minimum and maximum torques of the\nrheometer, Tmin and Tmax, and the maximum angular velocity is Ωmax. Given that the\nviscosity results are usually plotted as a function of the shear rate, the valid range in the\nη(˙γ) space is limited by the following expressions:\nFτTmin\n˙γ\n< η < FτTmax\n˙γ\n,\n(4)\n˙γ < FγΩmax .\n(5)\nTo obtain viscosity curves, it is common to collect viscosity data at several shear rates\nin a single test, which implies a shear rate step between data points. This requires some\nattention to obtain quality data, first due to the non-infinite acceleration of the rheometer\nand, second, because the flow does not instantly regain a steady state. Thus, a constant\ntime interval is usually defined for the rheometer to wait between the shear rate step and\nthe measurement. Another approach is to define a time function that establishes this\nwaiting period, dependent on the shear rate, whether linear, exponential, logarithmic,\nor other. This latter method is helpful because, usually, the flow stabilises faster for\nlarger rotational velocities; thus, having the time step diminish with the shear rate may\nreduce the overall measurement time. In any case, it is usual to perform preliminary\nconstant-shear-rate measurements (at different shear rates) to evaluate the required time\nintervals. If this is not considered, one may be gathering unsteady flow data.\nCentrifugal forces at sufficiently high rotational velocities lead to an outward radial\nvelocity component at the moving boundary and, at the static plate, from mass conserva-\ntion, an inward radial component, i.e., secondary flows. This phenomenon leads to an\nincrease in measured torque and is responsible for an apparent shear-thickening at high\nshear rates. From an analysis of experimental data, Turian arrived at an expression which\nallows us to estimate the experimental region free from secondary flows. By adjusting\ntheir expression, we can obtain another experimental limit:\nη < ρ˙γR2\n4Fγ3 .\n(6)\nIncreasing the rotational velocity further, the sample may escape the geometry, leading\nto decreased sample volume and an abrupt drop in measured viscosity.\nConsidering the loading conditions of the sample, over- and underfilling can lead to\nsignificant errors. Overfilling the geometry may lead to additional torque depending on\nthe amount of extra sample volume and geometrical characteristics11. On the other hand,\nunderfilling will result in a lower apparent viscosity because the sample cannot completely\nfill the geometry. Additionally, the symmetry of the sample/air interface is paramount\nfor low shear measurements. Johnston and Ewoldt concluded from mathematical analysis\nthat an additional torque can arise from non-constant surface tension, or non-constant\ncontact angle, between the fluid sample and the plates. Essentially, asymmetries in\nthe contact line lead to surface tension forces that pull along the fluid/air interface,\n3\n\n\nresulting in an additional torque, which is more significant at low shear, thus leading\nto an apparent shear-thinning behaviour that vanishes at sufficiently high shear rates.\nSample evaporation and over/underfilling are also sources of surface tension torque12.\nThis issue can be mitigated by accurate sample volume, reducing sample evaporation\nand diminishing the fluid’s surface tension12. Moreover, placing the sample perfectly at\nthe centre of the geometry aids in reducing contact line asymmetry as far as the wetting\nconditions of the plates allow. Despite all the efforts to counteract this phenomenon, it\ncan still lead to torque artefacts up to two orders of magnitude larger than the rheometer’s\nminimum torque. Thus, it has been common practice to multiply the low-torque limit by\na safety coefficient, shifting it to a higher shear.\nGeometrical characteristics can also affect measurement quality. Deviations from the\nnominal dimensions and unwanted rougher surface finishing from fabrication errors or wear\ncan result in incorrectly measured viscosities. Apart from the geometries’ characteristics,\nerrors in the gap height between the plates (or the cone and plate) can also lead to\nsignificant errors. Gap-errors can arise from a multitude of factors13. Setting zero-gap is\nusually done by lowering the moving plate until an increase in normal force is sensed,\nsignalling solid-solid contact. However, if the threshold force for contact assumption\nis not sufficiently high (≲5 N), normal forces arising from air squeeze can incorrectly\nserve as the contact input, leading to a deviation of the zero-gap by a few micrometers14.\nDeviations in the surface finishing of either plate and non-parallelism of the geometry\nmay also lead to gap errors. In any case, the usual gap-error leads to the actual/average\ngap in the geometry being more significant than the commanded gap in the rheometer\ninterface around 10 to 50 µm9, which leads to experimental issues. Not accounting\nthe gap-error can result in sample underfilling if the correct geometry loading is not,\nor cannot be verified before initiating the measurements. Moreover, even with perfect\nloading, gap-errors must not be disregarded, particularly for small gaps. Having that\nthe error is constant for a set of experiments, it results in lower apparent viscosities for\nsmaller gaps, whereas, for progressively larger gaps, the results approach the fluid’s actual\nviscosity due to the decreased relevance of the gap-error. Now, considering a PP system\n(for CP geometries, the gap is fixed by default), having the real gap as the sum of the\ncommanded gap and a gap-error: hr = hc + ϵ, a divergence between the commanded and\napplied shear rates arises: ˙γc = (R/hc) Ω, ˙γr = (R/(hc + ϵ))Ω. With some manipulation,\nwe can obtain:\nhc\nηm\n=\n\u0012 1\nηr\n\u0013\nhc + ϵ\nηr\n,\n(7)\nwhere ηm and ηr are the measured and real viscosities, respectively. This expression is a\nvariation of the one given by Kramer et al. and allows for an estimation of the fluid’s true\nviscosity, ηr, and the gap-error, ϵ, by conducting measurements at multiple gap heights.\nIt is, however, noteworthy that through this analysis slip effects cannot be decoupled\nfrom a gap-error as these return a similar measured-viscosity decrease with gap height\nreduction16.\nRegarding suspensions, several phenomena must be taken into account. Here we focus\non dilute/semi-dilute suspensions of spherical particles (volume fractions below 0.25).\nBrownian motion, inertial effects and the overall ability of the particles to follow the\n4\n\n\nfluid flow are important and should be evaluated. The Peclet (Pe), (particle) Reynolds\n(Rep) and Stokes (St) numbers are used to quantify these respective effects which may\nbe generally disregarded if the following adimensional quantities are sufficiently small17:\n1\nPe =\nkB T\n6 π η (dp/2)3 ˙γ ,\n(8)\nRep = ρ (dp/2)2 ˙γ\nη\n,\n(9)\nSt = ρp d2\np ˙γ\n18 η\n.\n(10)\ndp and ρp are the particle diameter and density, respectively, kB is the Boltzmann\nconstant and T is the temperature. Additionally, gravitational effects can be relevant if\nthe density difference between the dispersed and continuous phases is significant, and\nparticle migration may occur either from shear rate gradients within the flow or particle\nsurface roughness and collisions17. This particle movement may lead to the formation\nconcentration gradients, originating depletion layers near the solid boundaries or even\nshear banding. Particle depletion near the geometry walls provokes an apparent slip of\nthe dispersed phase and is common in suspension rheology, resulting in a viscosity gap-\ndependence and may be corrected a posteriori or counteracted by increasing the surface\nroughness or detailing the geometry (milled or serrated, for example)18,19. Suspensions\nmay also display time-dependent behaviour as the microstructure takes time to adjust\nto the applied shear. Moreover, the rheological response is also dependent on the shear\nhistory, particularly for small shear rates or after a resting period where the suspension\nmay achieve a metastable state17.\nConcerning magnetorheological measurements, the application of an external magnetic\nfield induces magnetic dipoles in the individual particles, causing them to aggregate\ninto elongated chains aligned with the magnetic field lines17. As such, a reversible and\ncontrollable microstructure is enabled that can significantly alter the bulk rheological\nproperties20. Additional experimental concerns arise from the time-dependent nature\nof the structure formation/destruction mechanisms and heating from magnetic field\ngeneration, which can be significant for strong fields.\nAll these experimental limitations and phenomena can compromise measurement\nquality and, because whole blood is a precious fluid, it is vital to identify the possible\nerror sources and define an experimental window before tackling its magnetorheological\ncharacterisation. In this work, steady shear measurements were performed with different\nNewtonian fluids in two static bottom plates and two planar geometries (PP and CP), and\nthe experimental limits were defined and discussed. Measurements were also conducted\nwith a Newtonian fluid seeded with magnetic particles in different concentrations under\nan external magnetic field of varying intensity. Numerical simulations were conducted\nto evaluate the effects of a possible error source on standard and magnetorheological\nmeasurements.\n5\n\n\n2 Materials and methods\n2.1 Rheometer and accessories\nAn Anton Paar MCR302-e stress-controlled rotational rheometer, shown in Figure 1(a),\nwas used for the rheological characterisation. According to the supplier, the instrument’s\nlimitations were: Tmin = 1 nN·m, Tmax = 230 mN·m and Ωmax = 314 rad/s.\nTwo static bottom plates were used; one was designed explicitly for magnetorheological\nmeasurements (Magnetic Plate, or MP) and the other for standard testing (Standard\nPlate or SP), both are shown in Figure 1(b). The SP had a diameter of 50 mm and\nallowed for careful temperature control through a Peltier system. On the other hand,\nthe MP had a diameter of about 30 mm and was attached to a magnetorheological cell,\nwhich fit into the rheometer and was connected to a power source (being responsible for\nthe generation of the magnetic field) and to an external cooling system filled with water,\nwhich allowed to dissipate heat from the magnetic field generation, but keeping a precise\ntemperature control within the fluid sample still remained challenging. The MP itself\nwas fixed to the cell by two screws and centred by three radial fixtures, as can be seen in\nFigure 1(b2).\nTwo upper geometries were considered, one parallel-plate (PP20 MRD) and one trun-\ncated cone-plate (CP20 MRD), specially designed for magnetorheological measurements.\nBoth had a diameter of 20 mm, and the CP had a cone angle of 1.981◦and a truncation\nheight of 0.084 mm. Both geometries can be seen in Figure 1(c). Either MRD geometry\nhas an outer region consisting of two rings with different depths (both lesser than the\nplate itself) that hinder the visualisation of the loading quality. In the SP, we could\nstill visualise the sample’s profile with a high-contrast background (white walls in our\ncase). On the other hand, the MP also had an outer rim, which completely blocked our\nvision of the contact line in the MRD geometries, making evaluating the loading quality\nimpossible. Figure 2 shows a schematic of the PP20 MRD with either bottom plate.\nThe magnetorheological cell was able to generate a magnetic field perpendicular to the\nflow direction, measured with an FH 54 teslameter (MAGNET-PHYSIK) inserted into a\nslot underneath (depicted in Figure 2). The geometries (PP20 MRD and CP20 MRD)\nwere non-magnetic, which prevented radial forces from acting on the shafts, and the\nmagnetic circuit was closed with a yoke placed on top of the geometry21. In this study,\na working fluid was seeded with M-270 Carboxylic Acid Dynabeads™(Thermo Fisher\nScientific) paramagnetic particles with 2.8 µm diameter, density of 1600 kg/m3 and their\nmagnetisation curve is given by Grob et al. (saturation of 6.4 Am2/kg for B ⪆500 mT).\n2.2 Working fluids\nFour different Newtonian fluids were used: a calibration oil provided by Anton Paar (Cal.\nOil), two aqueous solutions of glycerol in mass concentrations of 75.20 and 93.05 wt%\n(75.20Gly and 93.05Gly, respectively), and an aqueous solution of 52 wt% of Dimethyl\nsulfoxide (DMSO) (Newtonian blood analogue23, NBa). Table 1 gives each working\nfluid’s compositions, expected densities, and viscosities at the experimental temperature\nof 20◦C.\n6\n\n\n(a) MCR302-e\n(b1) SP\n(b2) MP\n(c1) PP20 MRD\n(c2) CP20 MRD\nFigure 1: Experimental setup: (a) Anton Paar MCR302-e rotational rheometer, (b) SP and MP\nbottom plates, and (c) PP20 MRD and CP20 MRD geometries.\nFigure 2: Schematic of the PP20 MRD geometry on either bottom plate (not to scale) and\nassociated difficulties with evaluating the loading quality.\n7\n\n\nTable 1: Composition and expected viscosity, η, and density, ρ, of the working fluids (at 20◦C).\nCalibration oil data from the supplier (Anton Paar). Glycerol and water estimates from\nVolk and Kähler. DMSO data from Budeanu and Dumitrescu\nSample\nWater\n[wt%]\nGlycerol\n[wt%]\nCal. Oil\n[wt%]\nDMSO\n[wt%]\nρ [kg/m3]\nη [mPa·s]\nCal. Oil\n-\n-\n100\n-\n816.1\n3.66\n75.20Gly\n24.80\n75.20\n-\n-\n1194.9\n36.98\n93.05Gly\n6.95\n93.05\n-\n-\n1243.0\n365.68\nNBa\n48.00\n-\n-\n52.00\n1051\n3.32\n2.3 Experimental procedure\nBefore any measurement, the zero-gap was set, the inertia of both the drive and the\nmeasuring system was acquired, and a motor adjustment function was conducted according\nto the RheoCompass™(Anton Paar) procedure.\nThe set zero-gap and inertia call\noperations were repeated at every rheometer reset and bottom plate/geometry switch.\nAlso, both plates were thoroughly cleaned of any previous sample residues and dust\nparticles, cleansed with ethanol and dried with compressed air. The sample volume\n(previously determined and given as supplementary material) was accurately measured\nwith a VWR® standard line precision pipette and carefully applied at the centre of the\nbottom plate. Measurements with the calibration oil required the fluid sample to be\napplied on the top plate/cone instead, or a combination of both, due to the fluid’s low\nsurface tension. The rheometer head was then lowered with the predefined viscoelastic\nmovement profile. When possible, the loading quality was evaluated before measurement\nstarted, looking for any signs of under/overfilling and contact line asymmetry.\nThe viscosity was measured by imposing a shear rate logarithmic ramp-up with about\n10 points per decade, between 10-1 and 104 s-1. A constant waiting time of a few seconds\nwas defined between shear rate shift and data acquisition, guaranteeing steady flow. For\nall measurements, at least 5 tests were conducted for statistical robustness, posteriorly\naveraged, and the 95% confidence intervals calculated using Student’s t-distribution.\nWith the SP, the Peltier system was set to maintain a constant temperature (20◦C),\nwhereas no such fine temperature control was achievable with the MP. Our approach\nwas to conduct the measurements when the room temperature did not diverge too much\nfrom the analogous tests with the SP (maximum ± 5◦C) while keeping the average\ntemperature of the measurement set close to the targeted 20◦C.\nConcerning the magnetorheological measurements, prior to loading, the samples were\nthoroughly mixed to redisperse the sedimented particles and to break any possible\nmicrostructure that may have formed at rest. The steady shear viscosity data was\ngathered at a constant shear rate and 10 magnetic field density values were applied up to\nB ≤720 mT. The time interval between field density change and data acquisition was set\nto 20 s to guaranty a steady state, avoiding gathering data before the magnetic-induced\n8\n\n\nmicrostructure was formed.\n3 Results and discussion\nPreliminary experimental results pointed towards a non-negligible gap-error of the MP.\nLarger sample volumes are required to fill geometries on the MP than on the SP, and\na viscosity dependence on the gap height was found, with lower measured viscosities\nfor smaller gaps. The discussion on these preliminary measurements is presented as\nsupplementary material.\n3.1 Standard measurements\nWe set out to test the measurement quality with three Newtonian fluids of different\nviscosities (Cal. Oil, 75.20Gly and 93.05Gly) on both geometries (PP20 MRD and CP\nMRD) and both bottom plates (SP and MP). The PP20 MRD gap height was varied\nbetween 0.05 and 0.35 mm and Figure 3 shows the obtained viscosity curves. The\nmeasured viscosity was plotted along with the experimental limits associated with the\nrheometer’s specifications: torque, Tmax/min, and rotational velocity, Ωmax, and the\nprediction of secondary flow onset (expressions 4-6); if any are not visible it is because\nthey fall out of the experimental window and are, thus, not relevant for those particular\nmeasurements.\n10\n1\n10\n0\n10\n1\n10\n2\n10\n3\n10\n4\n [1/s]\n10\n0\n10\n1\n10\n2\n10\n3\n [mPa s]\nCP20 MRD\nPP20 MRD\nSP\n10\n1\n10\n0\n10\n1\n10\n2\n10\n3\n10\n4\n [1/s]\n10\n0\n10\n1\n10\n2\n10\n3\n [mPa s]\nMP\nPP20 MRD: 0.05 mm\nPP20 MRD: 0.10 mm\nPP20 MRD: 0.15 mm\nPP20 MRD: 0.20 mm\nPP20 MRD: 0.25 mm\nPP20 MRD: 0.30 mm\nPP20 MRD: 0.35 mm\nCP20 MRD\n70 × Tmin\nmax\nSec. Flow\nfluid @ 20°C\nFigure 3: Viscosity curves obtained with three Newtonian fluids: Cal. Oil (circular markers),\n75.20Gly (triangular markers) and 93.05Gly (cross markers), on either bottom plate:\n(left) SP and (right) MP. Data gathered with the CP20 MRD and PP20 MRD (gap\nheights between 0.05 and 0.35 mm).\n9\n\n\nThe results obtained with the SP show generally constant viscosities for all three tested\nfluids, with no significant differences between either geometry or PP gap heights. Large\nerrors can be seen at low shear with the two lower viscosity fluids, which should be\nassociated with low-torque issues. Because the rheometer’s minimum torque limit is\noutside the experimental window (at lower shear rates for the shown viscosity range), we\nbelieve these low-shear errors are probably due to surface tension torque from contact\nline asymmetry12. Applying an adjustment factor of 70 to the minimum-torque limit\n(70 × Tmin) seems to predict the low-shear errors accurately but significantly limits\nthe experimental window. Despite our effort to carefully evaluate the correct sample\nvolume, the design of the MRD geometries makes it a strenuous task, not allowing for a\nvisualisation of the contact line throughout the measurement, which may have allowed\nsmall loading errors to remain unnoticed. The adjusted torque 70 × Tmin = 70 nN·m\nhowever, is, despite significant, not shocking. Johnston and Ewoldt report measured\ntorques with water up to 1 µN·m at low shear from loading errors. Other works, despite\nemploying lesser adjustment factors also had less torque resolution at low shear26–28.\nAt high shear, a slight viscosity increase can be observed for the less viscous fluid on\nthe larger PP gaps and the CP, which is related to secondary flow onset and is accurately\npredicted by the respective limit10. The most viscous solution shows evidence of viscous\nheating, visible through an acute viscosity decrease at high shear (there was no evidence\nof sample loss). Even though the SP can guarantee a near-constant temperature via the\nPeltier system, the temperature sensor is not in direct contact with the fluid sample.\nThus, delays in temperature control are not surprising when dealing with very sharp\ntemperature variations in the sample. Additionally, with the 93.05Gly solution a slight\nviscosity decrease is noted throughout the whole viscosity curve. At low to medium shear\nthis should not be a symptom of viscous heating, as the imposed rotational velocity is\nrelatively low, but it might be due to water absorption over time, which may be more\nnoticeable for small gaps29. The same effect may not be as noticeable for the 75.20Gly\ndue to its lesser glycerol concentration.\nWith the MP, the adjusted low-torque and secondary flow limits also predict the\nlow-shear errors and high-shear measured viscosity increment and the viscous heating\neffects on the most viscous solution are not significantly more pronounced with the MP.\nCompared to the SP data, on the MP, the CP20 MRD returned slightly larger viscosities,\nwhile with the PP20 MRD, there is a clear viscosity reduction with a gap decrease.\nFocusing on the PP20 MRD results, we can conduct a more in-depth analysis by\nevaluating useful data at multiple gap-heights. To avoid experimental errors, the data\nselected from each fluid was limited to a particular shear rate range that avoided low-shear\nuncertainties and secondary flow/viscous heating issues at high shear: 20 ≤˙γCal.Oil ≤\n2000, 3 ≤˙γ75.20Gly ≤6000 and 0.2 ≤˙γ93.05Gly ≤3000 s-1. The data obtained at each\ngap height was averaged over the selected shear rate range, ηm, and compared to the\nexpected value, ηexp. (Table 1), through a relative error:\nR.E. = ηm −ηexp.\nηexp.\n,\n(11)\nwhich is shown, for each fluid, in Figure 4.\n10\n\n\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nhc [mm]\n40\n20\n0\nR. E.  [%]\nSP\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nhc [mm]\nMP\nCal. Oil\n75.20Gly\n93.05Gly\nFigure 4: Mean viscosity relative errors and respective 95% confidence intervals for all three\nfluids: Cal. Oil (circular markers), 75.20Gly (triangular markers) and 93.05Gly (cross\nmarkers), gathered on both geometries: PP20 MRD (filled markers) and CP20 MRD\n(empty markers), and on either bottom plate: (left) SP and (right) MP. Confidence\nintervals for the CP20 MRD always within marker size.\nThe measured viscosity dependence with PP gap height is evident with the MP. With\nthis bottom plate, for the lowest PP gap of 0.05 mm, the relative error is significant,\nbetween -45 and -30%, but it diminishes with increasing gap height to errors between\n-18 and -4% for hc = 0.35 mm. With the CP20 MRD, however, positive relative errors\nare noticed for the two least viscous fluids (2 and 7%), while the most viscous solution\n(93.05Gly) returned a negative error of -9%. It is important to remember that with the\n93.05Gly solution, we did notice a viscosity decrease along the curve, possibly due to\nwater absorption, which may be why this fluid shows more significant relative errors.\nAlthough the SP’s relative errors are not as striking as the MP’s, we can still notice a\nslight dependence of the measured viscosity with gap height. The Cal. Oil and 93.05Gly\ntend to a null relative error with gap increase, while the 75.20Gly solution presents a weak\ntendency towards a 7% positive error which could be due to a inaccurate preparation\nof the solution, where the glycerol concentration was slightly larger than intended. The\nCP20 MRD returned lesser errors than on the MP, with the two less viscous fluids\napproaching a null error.\nBecause this viscosity gap-dependence is not observed on the SP, we believe there is\na systematic gap-error on the MP and that slip effects are negligible. As such, fitting\nthis gap-varying data to Equation 7, we can estimate the gap-error, ε, and correct the\nmeasured viscosity to an estimated value15: ηr = ηm (1+ε/hc). The fitting procedure was\ndone using Python’s Statsmodels GLM functionalities, and Figure 5 shows the estimated\ngap-errors and corrected viscosities for each fluid.\nOn the SP, the 75.20Gly solution returned a practically null gap-error, while small\nvalues are observed for the Cal. Oil and the 93.05Gly. These non-zero estimated gap-errors\nmay have arisen from measurement uncertainty with the low-viscosity Cal. Oil, whereas\nfor the high-concentration glycerol solution it may be a result of the previously-mentioned\nwater absorption. From these results we assume hereafter that there is no gap-error on\n11\n\n\nCal. Oil\n75.20Gly\n93.05Gly\n0\n10\n20\n30\n [ m]\nSP\nCal. Oil\n75.20Gly\n93.05Gly\nMP\n0.05 0.10 0.15 0.20 0.25 0.30 0.35\nhc [mm]\n10\n0\n10\n1\n10\n2\n10\n3\n [mPa s]\n0.05 0.10 0.15 0.20 0.25 0.30 0.35\nhc [mm]\nCal. Oil\n75.20Gly\n93.05Gly\nFigure 5: Gap-error, ε, and true viscosity estimates, ηr (filled markers, with averaged measured\nviscosity, ηm, in open markers for comparison) for all three fluids: Cal. Oil (circular\nmarkers), 75.20Gly (triangular markers) and 93.05Gly (cross markers), on either bottom\nplate: (left) SP and (right) MP.\n12\n\n\nthe SP. Contrarily, on the MP the Cal. Oil and 75.20Gly point towards a gap error of\napproximately 22.8 µm, while the 93.05Gly solution again presents a larger estimation.\nThe corrections on the MP returned slightly lesser viscosities than on the SP, which could\nbe due to deviations of the mean room temperature from 20◦C during measurement (the\nrecorded mean temperatures were 21.1, 20.3 and 20.8◦C, for the Cal. Oil, 75.20Gly and\n93.05Gly), but overall the actual viscosity estimates do seem to effectively predict the\nfluids’ true viscosities.\nOnce an efficient method to estimate the fluids’ actual viscosities is validated, we can\ncorrect the experimental viscosity curves through two approaches. The first consists\nof correcting the viscosity by performing gap-error and true viscosity estimates at all\nshear rates which could return some statistically interesting data, but this method would\nalso be accounting for errors associated with the experimental limits when considering\nviscosity data obtained at extreme shear rates (either too low, from the rheometer’s\nminimum torque or surface tension torque from contact line asymmetry, or too high, from\nsecondary flow onset and viscous heating). The alternative is to employ the previously\nobtained MP gap-error (εMP ≈22.8 µm) to correct the measured viscosity at all shear\nrates (ηm (1 + ε/hc)). As the gap-error was estimated through viscosity data within the\nexperimental window, we opted for this second method and Figure 6 shows the corrected\nviscosity curves (the SP data remained unaltered).\n10\n1\n10\n0\n10\n1\n10\n2\n10\n3\n10\n4\n [1/s]\n10\n0\n10\n1\n10\n2\n10\n3\n [mPa s]\nCP20 MRD\nPP20 MRD\nSP\n10\n1\n10\n0\n10\n1\n10\n2\n10\n3\n10\n4\n [1/s]\n10\n0\n10\n1\n10\n2\n10\n3\n [mPa s]\nMP\nPP20 MRD: 0.05 mm\nPP20 MRD: 0.10 mm\nPP20 MRD: 0.15 mm\nPP20 MRD: 0.20 mm\nPP20 MRD: 0.25 mm\nPP20 MRD: 0.30 mm\nPP20 MRD: 0.35 mm\nCP20 MRD\n70 × Tmin\nmax\nSec. Flow\nfluid @ 20°C\nFigure 6: Viscosity curves corrected for the gap error (εSP = 0 and εMP ≈22.8 µm) obtained\nwith three Newtonian fluids: Cal. Oil (circular markers), 75.20Gly (triangular markers)\nand 93.05Gly (cross markers), on either bottom plate: (left) SP and (right) MP. Data\ngathered with the CP20 MRD and PP20 MRD (gap heights between 0.05 and 0.35\nmm).\nComparing to the raw viscosity curves (Figure 3), the MP results are drastically altered,\n13\n\n\nwith the correction rectifying the experimental data effectively. Thus, hereafter we will\nfocus on elucidating the cause of the gap-error and why the CP20 MRD seems immune\nto the measured viscosity reduction and prone to yielding larger viscosities.\nBecause the SP does not show such severe gap-error symptoms, issues regarding the\nsetting zero-gap procedure seem unlikely, as it was the same for either bottom plate. The\nsame can be said about surface finishing issues; even if the material for either plate is\ndifferent, both were acquired from the same supplier (Anton Paar) and, thus, should\nhave somewhat similar surface characteristics (the same reasoning for slip negligibility).\nAssuming that the MP is straight, as we hypothesise the SP is, the most simple cause\nshould be an inclination of the MP that leads to geometry non-parallelism.\n3.2 Numerical analysis of the non-parallelism on standard measurements\nTo validate our hypothesis, we set out to model both geometries (PP20 and CP20) with\nvarying inclinations of the bottom plate to study them numerically in a 3D simulation. The\nmodel of a non-parallel PP is very straightforward, but a CP requires some consideration.\nWhen executing the zero-gap procedure, the rheometer assumes a zero gap when contact\nis made between the cone and the bottom plate. In our case, we have a truncated cone\nwith an angle of 1.981◦and truncation height of 0.084 mm. If the inclination angle is\nlesser than the cone angle, φ < β, contact is made at the truncation radius, Rt. On the\nother hand, if φ > β, contact is made on the outer radius of the cone, R, just like a PP\ngeometry. When the inclination is equal to the cone angle, φ = β, both models are valid\nas contact is made along a radial segment. Figure 7 shows schematics of the modelled\nPP and CP geometries (not to scale). hc is the commanded gap, or the truncation height\nin the case of the CP (0.084 mm), and the radius for both geometries was R = 20 mm\n(PP20 and CP20).\nThe fluid flow was then calculated in COMSOL MultiPhysics, solving for the laminar,\nincompressible and isothermal flow of a Newtonian fluid of equal characteristics to the\nCal. Oil at 20◦C (Table 1). The steady state equations for conservation of mass and\nmomentum are solved:\nρ (u · ∇) u = −∇p + η∇2u + F ,\n(12)\n∇· u = 0 .\n(13)\nThe applied boundary conditions were no-slip at the top and bottom walls. A rotational\nvelocity, Ω, was imposed at the top wall (the measuring system) while the bottom wall\nremained fixed. The sample/air interface was simplified in two ways. First, the shape of\nthe interface was considered straight and vertical (Figure 7), while in real applications,\nwe aim to have a perfect convex meniscus shape, whose contact angle with the solid parts\nis dependent on fluid surface tension. Second, the fluid/air interface was modelled as a\nfree-slip condition for the sake of simplicity.\nThe mesh was constructed in COMSOL by defining it first on the top and outer\nwalls and then using a swept mesh operation to mesh the whole volume. The whole\nrevolution was divided into 5◦slices, and the outer wall (fluid/air interface) was defined\nby partitioning the gap height, along the z direction, into 20 equal-sized elements. The\n14\n\n\n(a) PP20\n(b) CP20\nFigure 7: Schematics (not to scale) of the modelled geometries with inclined bottom plate: (a)\nPP and (b) CP. The CP model differs depending on the magnitude of the inclination\nangle, φ, relative to the cone angle, β. Cylindrical coordinate system is assumed.\ntop wall was divided into two sections, with a threshold diameter of 0.5 mm. On the\nouter region, 0.5 mm ≤r ≤R, the radial direction was divided into 30 elements, being\nprogressively refined as we approach the outer rim of the geometry (r →R) to improve\nthe resolution where we expect maximum gradients (arithmetic progression with growth\nfactor of 5). The inner region, 0 ≤r ≤0.5 mm, was constructed by dividing it into\nangular sections of 60◦and filled with small triangular elements. Figure 8 shows the top\nand side views and a zoom-in on the inner region. More information regarding the mesh\nconstruction and analysis is given as supplementary material.\nThe models were validated by verifying the velocity profiles obtained with perfectly\nparallel geometries (φ = 0) at a shear rate range 10−1 ≤˙γ ≤104 s-1. The profiles never\ndeviated from the expected Couette shape except at high shear rates when secondary\nflows onset, which was also accurately captured by the models (data not shown here). The\nviscosity \"measured\" by our modelled rheometer was calculated, according to Equation 1,\nby evaluating the torque, which was computed by integrating the wall shear stress on\nthe top, rotating wall:\nT =\nZZ\nS\nτzθ r dS .\n(14)\nThe models returned constant measured viscosity equal to the defined fluid viscosity\nexcept when secondary flows onset at high shear, which led to a measured viscosity\nincrease (data presented as supplementary material).\nHaving evaluated the model’s capability, we introduce the non-parallelism of the\ngeometries by varying the inclination angle of the bottom plate, φ. To compare with\nour experimental data, the shear rate was fixed at ˙γ = 115 s-1 (data point within the\n15\n\n\n(a) Top View\n(b) Side view\n(c) Inner region\nFigure 8: Domain of the fluid sample model: (a) top view, (b) side view and (c) detail of the\ntop/bottom wall inner region.\nexperimental window) and twenty values for the inclination angle were tested, ten lesser\nthan the cone angle, 0.1◦≤φ ≤β, and ten larger, β ≤φ ≤5◦. In Figure 9 are shown the\nvelocity profiles (velocity components: ur, uθ and uz) at r = R/2 in opposite sides of the\ninclination (θ = 0 and θ = π) for five inclination angles: φ = 0, 0.1, 0.936, 1.981(= β) and\n5◦, on both geometries (only two PP20 gap heights are shown: hc = 0.05 and 0.35 mm).\nThe results show that the inclination of the bottom plate leads to divergences from\nthe canonical Couette flow (Figure 9). Whereas the CP20 and the larger PP20 gap seem\nrelatively unaffected by the lesser tested inclination (φ = 0.1◦), the flow on the lesser\nPP20 gap is clearly altered because of the relative significance of the inclination. Despite\nthe non-parallelism of PP geometries provoking only an increase of the local gap, the\nangular velocity profiles on either side of the 0.05 mm gap geometry are curved differently.\nNotably, aggravating the non-parallelism leads to interesting velocity profiles. On the one\nhand, at the lesser local gaps (θ = 0), there seems to be a velocity overshoot, noticed first\nfor the lesser PP20 gap and then for the CP20, while on the opposite location (θ = π)\nnegative rotational velocities are seen near the bottom wall, reaching very significative\nproportions particularly for the CP20 at a geometry inclination matching the cone angle\n(φ = β = 1.981◦). To further analyse the flow, Figures 10 and 11 show, respectively,\nthe velocity magnitude fields (at the vertical plane θ = 0) and the streamlines (at the\nhorizontal plane crossing the inclined bottom plate at r = Rt, θ = 0) for either geometry\nmodel and both PP20 gaps (hc = 0.05 and 0.35 mm), and three bottom plate inclinations.\nIt seems there are fundamental differences between the flow on either non-parallel\ngeometry.\nRegarding the PP20, from the velocity magnitude fields we can see the\ninclination of the bottom plate leads to a tilt of the flow’s axis of rotation, being more\npronounced for smaller gaps. This explains the negative angular velocities displayed on\nthe profiles (Figure 9) as they were gathered taking into account the geometry’s axis and\nnot the flow’s. On the other hand, the flow behaviour on the non-parallel CP is more\ncomplex and, for inclinations around and larger than β, near the bottom of the geometry\nthe flow counters the rotation imposed on the top wall. This phenomenon should be\n16\n\n\n \n0\n1\nz/h\n= 0.000\nr = R/2, \n= 0\n \n \nr = R/2, \n=\n \n0\n1\nz/h\n= 0.100\n \n \n \n0\n1\nz/h\n= 0.936\n \n \n \n0\n1\nz/h\n= 1.981\n \n \n0\n1\nu/(  R)\n0\n1\nz/h\n= 5.000\n0\n1\nu/(  R)\n \nu\nur\nuz\nPP20: 0.05 mm\nPP20: 0.35 mm\nCP20\nFigure 9: Velocity profiles obtained from the modelled flow of the Cal. Oil, along opposing vertical\nlines: θ = 0 and π for r = R/2, at several inclination angles (φ = 0, 0.1, 0.936, 1.981, 5◦)\nin the PP20 (hc = 0.05 and 0.35 mm) and the CP20.\n17\n\n\nFigure 10: Velocity magnitude fields (U =\np\nu2\nθ + u2r + u2z) gathered at the vertical plane θ = 0\non either geometry model and two PP gap heights (hc = 0.05 and 0.35 mm) for bottom\nplate inclinations of φ = 0.936◦, 1.981◦and 5.000◦. The colorbar range was limited\nto 40% of the data range (0 ≤U ≤0.4 ΩR) and the plots were stretched vertically\n(stretching factor of 5) to facilitate visualization.\ndue to the contraction-expansion provoked by the inclination, which has a maximum\nstrangulation when it matches the cone angle. Increasing the inclination past this point\nshifts the most narrow section from the truncation radius to the outer radius, widening\nthe contraction and, therefore, beginning to slowly dissipate the counter-rotating region.\nIn Figure 12 is shown the viscosity \"measured\" by our modelled rheometer (η∗\nm, relative\nto the input Cal. Oil viscosity, ηexp, given in Table 1) with the PP20 (0.05 ≤hc ≤0.35\nmm) and CP20 geometries with varying inclination (0 ≤φ ≤5◦). With the PP20, the\nmeasured viscosity decreases as the inclination is increased, tending to zero as φ →90◦\nwith a decreasing rate. Despite seemingly behaving as a usual gap-error on perfectly-\nparallel geometries, the gap-error formulation returned viscosity estimates lower than\nthe models’ input value, possibly due to the flow alterations displayed in Figures 10\nand 11. This can also explain the correction of experimental data undershooting for\nthe MP (Figure 5). Nonetheless, relatively acceptable errors (<10%) are still achievable\nthrough the gap-error formulation for inclinations φ ≲1◦. With the CP20, the measured\nviscosity is initially increased until reaching a maximum when the inclination matches\nthe cone angle, about 77% higher than the actual viscosity. This should be due to the\nprogressive \"pinching\" of the contraction-expansion. On one side of the geometry, θ = 0,\nthe increase in inclination reduces the flow depth, leading to enhanced velocity gradients\nand, therefore, larger torque contributions. Whereas on the other side, θ = π, there is an\nincrease in flow depth, but the counter-rotating region is formed near the bottom wall,\nallowing for the maintenance of significant velocity gradients and, thus, the overall torque\nis increased. Increasing the inclination past the cone angle, the contraction is progressively\nreduced, which naturally leads to, on one hand, a reduction of the velocity gradients at\nthe constricted section and, on the other hand, to the dissipation of the recirculation\n18\n\n\nFigure 11: Streamlines gathered at an horizontal plane (crossing the inclined bottom plate at\nr = Rt and θ = 0) on either geometry model and two PP gap heights (hc = 0.05 and\n0.35 mm) for bottom plate inclinations of φ = 0.936◦, 1.981◦and 5.000◦. Imposed\nrotational velocity on the top wall is counter-clockwise.\n19\n\n\nregion, which additionally reduces the velocity gradients, leading to a measured viscosity\ndecrease.\n0\n1\n2\n3\n4\n5\n [ ]\n100\n50\n0\n50\n100\n(\n*\nm\nexp. )/\nexp [%]\nPP20: 0.05 mm\nPP20: 0.1 mm\nPP20: 0.15 mm\nPP20: 0.2 mm\nPP20: 0.25 mm\nPP20: 0.3 mm\nPP20: 0.35 mm\nCP20\nPP20 - Corrected\nFigure 12: Modelled Cal. Oil numerical viscosity (η∗\nm, relative to the input value, ηexp, given\nin Table 1) with varying inclination of the bottom plate (0 ≤φ ≤5◦) on the PP20\n(0.05 ≤hc ≤0.35 mm) and CP20 geometries.\nThese results agree with the experimental ones that showed PP20-measured viscosity\ndecreasing with gap reduction and a slight increase in CP20-measured viscosity. A\nstandard gap-error on perfectly parallel geometries (arising, for example, from miss-\njudged zero-gap, due to air squeeze or other phenomena) would only lead to a viscosity\ndecrease (this was also evaluated for our models and is presented as supplementary\nmaterial). Thus, according to the previous experimental data we can infer that the\nbottom MP is likely slightly inclined. Fitting of the numerical viscosity, a gap-error of\nε = 22.8 µm should be estimated from an inclination between 0.10 < φ < 0.31, but, if\nso, according to Figure 12, the CP-viscosity increase should not be noticeable, contrary\nto our experimental observations. Other surface defects could have a similar effect on\nexperimental results; however, the issue may lie not with the quality of the plate’s surface\nbut with its coupling to the magnetorheological cell. Looking again at Figure 1(b2), we\ncan observe that the MP is fixed to the cell via two screws and either dimensional issues\nwith the screws themselves or surface defects on the bottom of the plate where contact is\nmade could be responsible for this inclination.\nGeometry non-parallelism can be predicted and corrected fairly well for PP geometries\nthrough the gap-error formulation and viscosity data at multiple gap heights. However,\nas far as we know, there is no equivalent approach for CP geometries. The flow dynamics\ncan become relatively complex, and the measurements can return unexpected results as\nwe generally assume geometry non-parallelism to result similarly to a general gap error.\n20\n\n\nFrom this numerical work, we can gather that reasonably acceptable errors (<10%) can\nbe obtained for standard measurements with a CP20 geometry for inclinations: φ ≲1◦.\nFor 3◦≲φ ≲4◦the CP also returns small errors, but the divergence from the canonical\nflow can significantly impact the behaviour of the used sample. Until now, we have\nfocused on single-phase Newtonian fluids, but measurements of more complex samples\nmay be affected differently. In the following Sections, we shall discuss the impacts of\ngeometry non-parallelism on magnetorheological measurements.\n3.3 Experimental magnetorheological measurements\nMeasurements were performed to evaluate the setup capability of generating a measurable\nmagnetorheological response and how the bottom plate and used geometry impacts the\nresults. To this end we employed the Newtonian blood analogue — NBa (composition\nand expected density and viscosity in Table 1).\nInitially, the analogue’s viscosity curves were obtained without particles and no magnetic\nfield application, using both geometries and bottom plates. Tests were conducted with\nfour different gap heights for the PP, ranging from 0.05 to 0.35 mm. The resulting\nviscosity curves are illustrated in Figure 13. Again, the data is similarly affected by\nthe gap-error, showing decreased PP-measured viscosity with gap height reduction and\nincreased CP-measured viscosity. On the other hand, the NBa seems more sensitive to\nlow-shear issues than the Cal. Oil, particularly for larger gaps on the MP, which could\nbe due to the analogue’s more significant surface tension.\n10\n1\n10\n0\n10\n1\n10\n2\n10\n3\n10\n4\n [1/s]\n10\n0\n10\n1\n10\n2\n [mPa s]\nCP20 MRD\nPP20 MRD\nSP\n10\n1\n10\n0\n10\n1\n10\n2\n10\n3\n10\n4\n [1/s]\n10\n0\n10\n1\n10\n2\n [mPa s]\nMP\nPP20 MRD: 0.05 mm\nPP20 MRD: 0.15 mm\nPP20 MRD: 0.25 mm\nPP20 MRD: 0.35 mm\nCP20 MRD\n70 × Tmin\nmax\nSec. Flow\nfluid@20 C\nFigure 13: Viscosity curves obtained with the Newtonian blood analogue (NBa), on either bottom\nplate: (left) SP and (right) MP. Data gathered with the CP20 MRD and PP20 MRD\n(gap heights between 0.05 and 0.35 mm). No magnetic field was applied (B = 0).\nThe magnetorheological steady shear measurements were conducted with the NBa\nseeded with three concentrations of the M270 particles: 5, 10 and 15 wt%, which\n21\n\n\nare equivalent to volume fractions of ϕ ≈3.4, 6.9 and 10.6 vol% (corresponding to\ndilute/semi-dilute suspensions17). The shear rate was kept constant at ˙γ = 500 s-1\n(within the experimental window of the unseeded NBa) and the magnetic field intensity\nwas varied up to B ≤720 mT. The measurements were conducted on the MP, with both\ngeometries (PP20 MRD and CP20 MRD). With the PP20 MRD, the fluid viscosity was\nmeasured at different gap heights: 0.05 ≤hc ≤0.35 mm, (left column of Figure 14), and\nthe data gathered at an intermediate gap, hc = 0.15 mm, was corrected for the gap-error\nε = 22.8 µm (right column of Figure 14).\n2\n5\n8\n [mPa s]\n0 wt%\n2\n5\n8\n [mPa s]\n5 wt%\n2\n5\n8\n [mPa s]\n10 wt%\n0\n200\n400\n600\nB [mT]\n2\n5\n8\n [mPa s]\n15 wt%\n0\n200\n400\n600\nB [mT]\nPP20 MRD 0.05 mm\nPP20 MRD 0.15 mm\nPP20 MRD 0.25 mm\nPP20 MRD 0.35 mm\nCP20 MRD \nPP20 MRD - Corrected\nBatchelor's estimation\nFigure 14: Viscosity data (at ˙γ = 500 s-1) of the NBa seeded with magnetic particles at different\nmass concentrations (0, 5, 10 and 15 wt%), on the MP with varying magnetic field\ndensity (B ≤720 mT). Data gathered with the CP20 MRD and PP20 MRD (gap\nheights between 0.05 and 0.35 mm). In the right column, the PP20 data gathered\nat hc = 0.15 mm is shown corrected for the gap-error (ε = 22.8 µm). Horizontal\nblack lines correspond to Batchelor’s estimated viscosity of the seeded NBa at 20◦C\n(Equation 16).\nBefore discussing the results, we address some possible experimental issues. Evaluating\nthe Peclet, Reynolds and Stokes numbers, the results to Equations 8-10 are, respectively:\n1/Pe ≈4.7 × 10−5, Rep ≈3.1 × 10−4 and St ≈1.0 × 10−4, which are ≪1, meaning\n22\n\n\nwe can disregard Brownian and inertial effects. Regarding particle sedimentation, the\nsettling velocity in a dilute system can be given by:\nVs = (1 −6.55 ϕ) (dp/2)2 ∆ρ g\n18η\n,\n(15)\nwhere the term (1 −6.55 ϕ) accounts for the hindered backflow of the continuous phase\ndue to the presence of the particles, ∆ρ is the density difference between dispersed and\ncontinuous phases and g is the gravitational acceleration30. The resulting sedimentation\nvelocities are, for 5, 10 and 15 wt% concentrations, respectively: Vs ≈0.55, 0.39 and 0.22\nµm/s, which may be significant given our small gap heights, even if the total measurement\ntime was relatively short (200 s). However, because the magnetic field is aligned with the\ngap, the magnetic dipole forces will counter the gravitational effects, further hindering\nthe particle sedimentation.\nLooking at the results obtained without magnetic field application, B = 0 there is an\nincrease in viscosity with magnetic particle concentration, which is reasonably predicted\nby Batchelor’s expression31:\nη∗= η (1 + 2.5ϕ + 6.2ϕ2) ,\n(16)\n(shown in Figure 14 as horizontal black lines) where ϕ is the particle volume fraction.\nRegarding the magnetic field effects, the results present the expected response: an\nincrease in viscosity is noted with enhanced magnetic field density, which is more significant\nthe larger the particle concentration. This is because the particle-chains formed from the\nmagnetic-induced dipole interactions are aligned (with the field) perpendicular to the\nflow direction, hindering it, provoking an additional torque and, therefore, an increase in\nmeasured viscosity. There is a discrepancy between the PP20 and CP20 results, with\nthe latter always presenting larger measured viscosities, and may seem dependent on the\nparticle concentration and the magnetic field density. Nevertheless, this could be the same\neffect of the bottom plate inclination as previously discussed, because the relative error\nbetween the two geometries remained relatively constant ((ηCP −ηPP)/ηCP ≈13.5%), it\njust becomes visually prominent due to the viscosity increase with magnetorheological\nenhancement.\nIt is worth mentioning that because we employed smooth geometries, our data is likely\naffected by apparent slip of the dispersed phase which would significantly reduced the\nmeasured magnetorheological response18,32. As such, these results serve only to verify that\nthe employed setup is able to generate a significant magnetorheological response and to\ncompare between the discussed geometries. We do note that inhibiting slip, either through\nroughened or detailed geometries, should return a much stronger magnetorheological\nbehaviour with measured viscosities perhaps up to orders of magnitude larger.\nLastly, and on a side note, heating effects from the magnetic field generation were\nobserved from a very slight temperature rise at the largest field densities, but it should\nnot have significantly affected the results (< 0.3◦C).\n23\n\n\n3.4 Numerical analysis of the non-parallelism on magnetorheological\nmeasurements\nAs the SP is not applicable for magnetorheology, decoupling the effects of the MP’s\ninclination on this type of measurement was not possible. Thus, the flow of magnetised\nparticles seeded in the NBa was evaluated numerically using the rheometer models. The\nexperimental procedure would ideally be replicated to assess the impact of non-parallelism\non bulk viscosity measurements and particle chain dynamics accurately. However, the\nnumber of modelled particles needed for this replication results in extensive simulations\nthat are, unfortunately, beyond our current computational capabilities.\nTaking a different approach, the effects of geometry non-parallelism on particle-chain\nbehaviour can be assessed by evaluating only a few chains with a small number of\nparticles placed strategically in the flow. We placed particle chains at half the geometry\nradius: r = R/2 at four equally-distanced angular positions: θ ∈[0, π/2, π, 3π/2].\nTo have coherency between the geometries, the effective gaps at r = R/2 were equal:\nhPP20\nc\n= hCP20|R/2 = (R/2) tan(β).\nCOMSOL’s particle tracing for fluid flow module was used. The forces considered to\nbe acting on the particles were the viscous drag due to the fluid flow and dipole-dipole\ninteractions due to particle magnetisation. Particle sedimentation has already been\ndiscussed in the previous Section and again addressing Brownian motion, Melle et al.\ndefined an adimensional ratio between magnetic and thermal energies:\nλ =\nµ0 µf m2\n16 π (dp/2)3 kB T ,\n(17)\nwhere kB is the Boltzmann constant and T is the temperature, which we defined as 20◦C.\nUsing the M270 saturation magnetisation22(mM270\nsat.\n= 6.4 × Vp ρp [Am2], where Vp is the\nparticle volume), we obtain λ ≈32000 ≫1, meaning the magnetic effects dominate over\nBrownian motion.\nViscous drag was computed using COMSOL’s in-built Stokes drag force:\nFd = 3 π η dp(uf −up) ,\n(18)\nwhere (uf −up) is the particle velocity (up) relative to the fluid (uf). The dipolar forces\nwere computed through particle-particle interactions following the expression given by\nMelle et al.:\nFa,ij = 3 µ0 µf m2\n4 π r4\nij\nh\u0010\n1 −5 ( ˆm · ˆrij)2\u0011\nˆrij + 2 ( ˆm · ˆrij) ˆm\ni\n,\n(19)\nwhere µ0 and µf are the vacuum and fluid permeabilities, respectively (the latter being\nthe relative permeability and considered µf = 1). This expression assumes all particles\nare equally affected by the external field, i.e., their magnetisation is identical and aligned\nwith the magnetic field, which, in this case, was considered uniform and perpendicular to\nthe flow direction, as in the magnetorheological cell. The magnetic force depends on two\n24\n\n\nvector components: the magnetic moment m and the inter-particle distanceii rij (with\nmagnitude m and rij, and adimensional directions ˆm and ˆrij, respectively).\nThe particles are considered hard spheres and, to approximate this behaviour, a\nrepulsive excluded-volume force was applied33,34:\nFr,ij = 2 3 µ0 µf m2\n4 π d4p\nexp\n\"\n−30\n \nrij\ndp\n−1\n!#\nˆrij .\n(20)\nAligned with the field, the excluded volume force perfectly balances the magnetic at-\ntraction when two particles are in mechanical contact (r = dp). Whereas, when the\ninter-particle distance is r = 1.1 dp, the excluded-volume force is approximately 14 times\nsmaller than the magnetic attraction force, not provoking unwanted forces in other\nsurrounding particles34. Both these forces were computed as particle-particle interactions,\nand, therefore, we define a general magnetic force acting on the particles as:\nFm,ij = Fa,ij + Fr,ij .\n(21)\nIn truth, the presence of the particles induces changes in the fluid flow. But, despite\nCOMSOL allowing for the coupling of different physics, this would further complicate\nthe model. Therefore, we opted only to evaluate the effects of the fluid on the particle\nchains and disregard the inverse phenomenon. Furthermore, as we have already simplified\nthe problem to analyse only a few chains, these effects on the total torque measurement\nwould be negligible and not representative of the impact on a real measurement. The\nparticle boundary condition forced the particles that collide with the geometry walls or\nthe simplified fluid/air interface to adhere to them with no slip, reducing their velocity to\nzero. As we are only interested in the behaviour of a general particle chain for a qualitative\nanalysis, we could alter the particle diameter to reduce the required number of particles\nper chain and their magnetisation, which, when decreased, allows lesser magnetic forces\nto facilitate convergence. To have a clear idea of the possible effects of the geometrical\nchanges on the particle chains, we wanted to set the system properties so that the chains\nremain unbroken in the perfectly parallel geometries while keeping the models relatively\nlight. Therefore, we had an interplay between particle size, particle magnetisation and\nflow velocity, where small particles, i.e., large chains34, low magnetisation and large shear\nrates led to chain break. We performed simulations of a single chain on the parallel\nCP20 to test the limits of these characteristics. The particle diameter was varied between\n35 and 16 µm, leading to chains of 4 to 10 particlesiii and the shear rate ranged from\n0.01 to 1 s-1, guaranteeing that the the particles are capable of effectively following the\nflow (for dp = 35 µm, from Equation 10: St ≈3.3 × 10−5 ≪1). The magnetisation was\nprogressively decreased using a factor, 1 ≤Cm ≤50, such that the tested magnetisation\nwas: m∗= mM270\nsat. /Cm. From these simulations, three behaviours were observed: a) the\nchain remained unbroken, b) the chain undergoes subdivision (including combinations\niiThe inter-particle distance is the vector between particle centres.\niiiThe number of particles in each chain was calculated by rounding down (to the nearest integer) the\nrequired particle number to span the local gap height: n = [h/dp], where dp is the modelled particle\ndiameter.\n25\n\n\nof minor chains and isolated particles), and c) the chain completely breaks down into\nindividual particles. Examples of these behaviours are showcased in Figure 15, and the\nresults can be seen in Figure 16.\n(a) Unbroken\n(b) Subdivision\n(c) Breakdown\n0\n1\n|vp|/(\nR/2)\nFigure 15: Magnetised-particle chain (dp = 16 µm, np = 10) on the CP geometry model (without\nbottom plate inclination) with imposed shear rate ˙γ = 1 s-1 with varying magnetisation\nfactor: (a) Cm = 5, (b) Cm = (left) 10 and (right) 20, (c) Cm = 30. The particles are\ninitially placed in a vertical uniform distribution (vertical line segment on the left, of\nlength, R/2 tan(β)) and the shown images were taken at t = 4 s. Flow is left to right.\nUsually, a Mason number, Mn, that describes the ratio between viscous and magnetic\neffects is used to identify the transitions in chain dynamics33, and a multitude of different\nformulations have been previously employed33–36. With our data, the Mason number\ndefined by de Gans et al. suitably predicts the complete breakdown of the chains when\napproaching unity:\nMn = 128 π2 (dp/2)6 η ˙γ\nµ0 µf m∗2\n= 1 ,\n(22)\nwhich can be seen, plotted in black, in Figure 16. From its definition, this Mason number\nseems dependent on the particle size, but having that:\nMM270\nsat.\n= 6.4 × ρp = mM270\nsat. /Vp [A/m] ,\n(23)\nand, therefore, M∗= MM270\nsat.\n/Cm, we can simplify the Mason number to:\nMn =\n72 η ˙γ\nµ0 µf M∗2 .\n(24)\n26\n\n\n1\n25\n50\nCm\n4 particles (dp = 35 m)\n1\n25\n50\nCm\n6 particles (dp = 25 m)\n1\n25\n50\nCm\n8 particles (dp = 20 m)\n10\n2\n10\n1\n10\n0\n [1/s]\n1\n25\n50\nCm\n10 particles (dp = 16 m)\nUnbroken\nSubdivision\nBreakdown\nFigure 16: Magnetised-particle chain behaviour under steady shear in a planar geometry with\nvarying particle diameter (number of particles per chain), particle magnetisation\n(through the scaling constant Cm) and shear rate (˙γ). In black, chain complete-\nbreakdown prediction through the Mason number defined by de Gans et al.: Mn = 1.\n27\n\n\nAs such, the chain complete breakdown is only dependent on the fluid properties, which\nwere set to the NBa’s, the shear rate and the particle magnetisation, independent of the\nnumber of particles and their size, as corroborated by the results in Figure 16. The same\ncannot be said about chain subdivision, which has a clear dependence on particle number,\nmaking larger chains easier to break. We have yet to find a second Mason number that\naccurately predicts our chain subdivision, but it is an issue that is out of the scope of\nthis work.\nWe could select a shear rate and particle magnetisation through the gathered data that\nminimised the computational effort while maintaining unbroken chains in the perfectly\nparallel geometries. It is pertinent to have the same defined shear rate (rim shear rate,\n˙γ(R)) for both geometries, similar to the experimental procedure and, having determined\nthe PP gap height as hc = R/2 tan(β):\n˙γ(R) = ˙γPP(R) = ˙γCP(R) ⇔\nR\nR/2 tan(β) ΩPP = 1\nβ ΩCP ⇔ΩPP ≈ΩCP\n2\n.\n(25)\nThus, at the defined location for chain deployment (R/2), the local shear rate on the PP\ngeometry:\n˙γPP(R/2) =\nR/2\nR/2 tan(β) ΩPP = ˙γ(R)\n2\n.\n(26)\nThis dictates a parameter choice where, for the chosen particle number and magnetisation\nparameter Cm, the chains remain unbroken at two local shear rates: ˙γ(R) and ˙γ(R)/2.\nWe opted for chains with only 4 particles (dp = 35 µm) and a rim shear rate of ˙γ(R) = 1\ns-1, having, thus, the maximum particle magnetisation factor before chain subdivision:\nCm = 10 (particle magnetisation: M∗= 6.4 ρ/Cm = 1024 A/m).\nBefore heading to the results, it is worth mentioning that the characteristics of the\ngeometries are significant to the chain dynamics. The CP’s constant shear rate along the\nradius (when disregarding the truncated region) corresponds to the maximum shear rate\ninduced on the PP (at the rim), therefore, for chains of equal length with an arbitrarily\nimposed rim shear rate, the CP is more likely to break the chains. On the other hand,\nthe CP gap height depends on the radial position, which leads to smaller, harder to\nbreak chains as we approach the centre. As such, an interplay between varying shear rate\n(on the PP) and chain length (on the CP) can diverge the magnetorheological results of\neither geometry.\nIn Figure 17 are shown the particle trajectories in both geometries (without bottom\nplate inclination) at time t = 650 s (the trajectories at other times, t = 70 and 200 s,\nare not included here for the sake of conciseness, but are provided as supplementary\nmaterial). The chains remain unbroken, travelling along the plane r = R/2 on either\ngeometry, but moving faster on the CP20, where they are subject to a larger rotational\nvelocity (ΩCP ≈2 ΩPP).\nIntroducing a bottom plate inclination of φ = 1◦(the maximum for acceptable viscosity\nmeasurements), the analogous results are shown in Figure 18. Before discussing the chain\ndynamics, the introduced geometry non-parallelism affects the number of particles in each\nchain. For the PP, it increases particle number from 4 in each chain to chains of 7, 9 and\n28\n\n\nφ = 0\nPP20\nCP20\nt = 650 s\n0\n1\n|vp|/(\nR/2)\nFigure 17: Magnetised-particle trajectories (dp = 35 µm and M ∗= 1024 A/m) on the (left) PP20\nand (right) CP20 models (without bottom plate inclination: φ = 0) with imposed\nshear rate ˙γ = 1 s-1 at time t = 650 s.\n12 particles. For the CP, because the inclination is lesser than the cone angle (φ < β),\nthe gap is reduced at the constrained region, θ = 0, (see Figure 7) before increasing as\nwe descend along the inclination, which leads to chains of 3, 6 and 8 particles.\nConsidering the PP results (left column of Figure 18), initially only the largest chain,\nof 12 elements, subdivides, consequently separating. Compared to the particle flow on the\nparallel geometry (Figure 17), the chains begin to diverge from the R/2 plane because\nthe inclination of the geometry also leads to an inclination of the rotational axis. This\nbecomes more evident as we move forward in time, particularly for t = 650 s, where the\ntrajectories of most chains overlap. There are some additional observations regarding\nwall interactions. Because the intermediate chains (of 9 particles at θ = π/2 and 3π/2)\ndo not subdivide, while travelling along the tilted geometry, they collide with the bottom\nwall and lose the particle that adheres to it. With a similar behaviour, the subdivided\nportion of the largest chain closest to the bottom wall also collides with it, but instead\nof further subdividing or losing the contact particle, because the local shear rate is not\nsufficiently strong, the whole chain is stuck to the wall.\nConsidering the CP (right column of Figure 18), only the smaller chain of 3 elements\nremains unbroken. As the imposed rotational velocity is larger in this geometry, the\nenlarged shear rate leads to their subdivision despite the intermediate chains being three\nelements smaller than on the PP. In this case, because the intermediate chains are also\nbroken, the three subdivided portions closest to the bottom wall stick to it when contact\nis made. Despite significantly altering the flow, this slight inclination does not reveal\nthe counter-rotating region near the bottom wall. However, increasing the inclination\nangle to φ = β = 1.981◦does generate a much stronger recirculation region (see Figure\n29\n\n\nφ = 1◦\nPP20\nCP20\nt = 70 s\nt = 200 s\nt = 650 s\n0\n1\n|vp|/(\nR/2)\nFigure 18: Magnetised-particle trajectories (dp = 35 µm and M ∗= 1024 A/m) on the (left\ncolumn) PP20 and (right column) CP20 models with a bottom plate inclination of\nφ = 1◦, with imposed shear rate ˙γ = 1 s-1 at different times (t = 70, 200 and 650 s).\n30\n\n\n9), which significantly affects the chain dynamics. For the sake of clarity, the particle\ntrajectories for φ = β are not shown here but are presented as supplementary material.\nThe bottom plate inclination calls for larger chains, which are easier to break, but\nthe subdivided portions may be close in size to the original chains we ought to have\nin the perfectly parallel geometries. However, the issue with the magnetorheological\nmeasurements is not the chain size directly but the consequent chain velocity. Near\nthe measuring geometry (top wall in this case), the closer the chain velocity is to the\nflow velocity, the lesser the influence on the measured torque and, therefore, the lesser\nthe magnetic influence is felt. The particle trajectories shown in Figures 17 and 18\nwere plotted with a colour scheme in which the particle velocity is scaled with the\nmaximum velocity at deployment (ΩR/2), which allows us to see that the particles near\nthe measuring plate/cone (on top) have a larger velocity on the tilted geometries than on\nthe parallel ones, which will reduce the magnetically-induced measured-viscosity increase.\n4 Conclusions\nIn this work we set out to evaluate the suitability of an experimental setup for steady\nshear magnetorheological measurements of whole blood. An experimental campaign\nwas conducted with Newtonian fluids in two planar geometries specially designed for\nmagnetorheological measurements, one parallel-plate (PP20 MRD) and one cone-plate\n(CP20 MRD), and on two bottom plates, one for standard measurements (SP) and one\nfor magnetic testing (MP).\nIt was found that the rheometer’s minimum torque multiplied by a 70× factor reasonably\ndelimited low shear errors and the apparent shear-thickening at high shear was accurately\npredicted by the onset of secondary flows. On the MP we encountered a dependence\nof the PP-measured viscosity with gap height, returning lower viscosities as the gap is\nreduced, and larger CP-measured viscosities with this bottom plate than with the SP.\nThe gap-error formulation15 effectively corrected the PP-measured viscosity data and\nthe error itself was found to be around ε ≈22.8 µm and most probably provoked by an\ninclination of the MP.\nNumerical models of non-parallel geometries corroborated the experimental results,\npointing towards an MP inclination between 0.10 < φ < 0.31◦. The numerical work\nalso revealed that the geometry non-parallelism can lead to notable flow alterations,\nparticularly for CP geometries which essentially give rise to contraction-expansion flow\nand may lead to the emergence of counter-rotating regions as the inclination approaches\nthe cone angle.\nMagnetorheological measurements were also conducted with a Newtonian blood ana-\nlogue seeded with magnetic particles and the results showed a viscosity increase with\nparticle concentration and magnetic field density. The flow of a few particle chains\nwas modelled and it was observed that geometry non-parallelism can significantly affect\nmagnetorheological measurements, seemingly diminishing the magnetic effects.\n31\n\n\nSupplementary material\nAs supporting material we present: preliminary measurements, a description and discus-\nsion on the mesh employed on the numerical work, numerical estimations of the effects of\na general gap-error on viscosity measurements with parallel geometries, and trajectories of\nmagnetised particles in perfectly parallel geometries and with a bottom plate inclination\nequal to the angle of the employed Cone-Plate (obtained numerically).\nAcknowledgements\nThis work was financially supported by national funds through the FCT/MCTES (PID-\nDAC), under the project PTDC/EME-APL/3805/2021 (DOI 10.54499/PTDC/EME-\nAPL/3805/2021), LA/P/0045/2020, UIDB/00532/2020 and UIDP/00532/2020, and the\nprogram Stimulus of Scientific Employment, Individual Support-2020.03203.CEECIND.\nReferences\n[1] H. H. Billett. Hemoglobin and Hematocrit. Clinical Methods: The History, Physical,\nand Laboratory Examinations. 3rd edition, 1990.\n[2] A. Z. Valant, L. Žiberna, Y. Papaharilaou, A. Anayiotos, and G. C. Georgiou. The\ninfluence of temperature on rheological properties of blood mixtures with different\nvolume expanders—implications in numerical arterial hemodynamics simulations.\nRheologica acta, 50:389–402, 2011.\n[3] M. Zborowski, G. R. Ostera, L. R. Moore, S. Milliron, J. J. Chalmers, and A. N\nSchechter. Red Blood Cell Magnetophoresis. Biophysical journal, 84(4):2638–2645,\n2003.\n[4] I. Khalil, H. Dijkslag, L. Abelmann, and S. Misra. MagnetoSperm: A microrobot\nthat navigates using weak magnetic fields. Applied Physics Letters, 104(22):223701,\n2014.\n[5] Q. Fu, S. Guo, Y. Yamauchi, H. Hirata, and H. Ishihara. A novel hybrid microrobot\nusing rotational magnetic field for medical applications. Biomedical microdevices, 17\n(2):1–12, 2015.\n[6] J. Mathieu and S. Martel. Aggregation of magnetic microparticles in the context of\ntargeted therapies actuated by a magnetic resonance imaging system. Journal of\napplied physics, 106(4):044904, 2009.\n[7] S. Shaw and P. Murthy. Magnetic Drug Targeting in the Permeable Blood Ves-\nsel—The Effect of Blood Rheology. Journal of Nanotechnology in Engineering and\nMedicine, 1(2), 2010.\n32\n\n\n[8] R. Tao and K. Huang. Reducing blood viscosity with magnetic fields. Physical\nReview E, 84(1):011905, 2011.\n[9] R. H. Ewoldt, M. T. Johnston, and L. M. Caretta. Experimental challenges of shear\nrheology: how to avoid bad data. In Complex fluids in biological systems, pages\n207–241. Springer, 2015.\n[10] R. M. Turian. Perturbation Solution of the Steady Newtonian Flow in the Cone and\nPlate and Parallel Plate Systems. Industrial & Engineering Chemistry Fundamentals,\n11(3):361–368, 1972.\n[11] R. Cardinaels, N. K. Reddy, and C. Clasen. Quantifying the errors due to overfilling\nfor Newtonian fluids in rotational rheometry. Rheologica Acta, 58:525–538, 2019.\n[12] M. T. Johnston and R. H. Ewoldt. Precision rheometry: Surface tension effects\non low-torque measurements in rotational rheometers. Journal of Rheology, 57(6):\n1515–1532, 2013.\n[13] G. A. Davies and J. R. Stokes. Thin film and high shear rheology of multiphase\ncomplex fluids. Journal of Non-Newtonian Fluid Mechanics, 148(1-3):73–87, 2008.\n[14] G. A. Davies and J. R. Stokes. On the gap error in parallel plate rheometry that\narises from the presence of air when zeroing the gap. Journal of Rheology, 49(4):\n919–922, 2005.\n[15] J. Kramer, J. T. Uhl, and R. K. Prud’Homme. Measurement of the viscosity of guar\ngum solutions to 50,000 s-1 using a parallel plate rheometer. Polymer Engineering\n& Science, 27(8):598–602, 1987.\n[16] G. Vleminckx and C. Clasen. On the Inseparability of Slip and Gap-Error. Journal\nof Rheology, 60(4):549–557, 2016.\n[17] J. Mewis and N. J. Wagner. Colloidal suspension rheology, volume 10. Cambridge\nUniversity Press, 2012.\n[18] Richard Buscall. Wall slip in dispersion rheometry. Journal of Rheology, 54(6):\n1177–1183, 2010.\n[19] A. Yoshimura and R. K. Prud’homme. Wall slip corrections for Couette and parallel\ndisk viscometers. Journal of Rheology, 32(1):53–67, 1988.\n[20] M. T. López-López, P. Kuzhir, S. Lacis, G. Bossis, F. González-Caballero, and\nJ. D. G. Durán. Magnetorheology for suspensions of solid particles dispersed in\nferrofluids. Journal of Physics: Condensed Matter, 18(38):S2803, 2006.\n[21] J. Laeuger, K. Wollny, H. Stettin, and S. Huck. A new device for the full rheological\ncharacterization of magneto-rheological fluids. International Journal of Modern\nPhysics B, 19(07n09):1353–1359, 2005.\n33\n\n\n[22] D. T. Grob, N. Wise, O. Oduwole, and S. Sheard. Magnetic susceptibility charac-\nterisation of superparamagnetic microspheres. Journal of Magnetism and Magnetic\nMaterials, 452:134–140, 2018.\n[23] L. Campo-Deaño, R. Dullens, D. G. Aarts, F. T. Pinho, and M. S. N. Oliveira. Vis-\ncoelasticity of blood and viscoelastic blood analogues for use in polydymethylsiloxane\nin vitro models of the circulatory system. Biomicrofluidics, 7(3), 2013.\n[24] A. Volk and C. J. Kähler. Density model for aqueous glycerol solutions. Experiments\nin Fluids, 59(5):75, 2018.\n[25] M. M. Budeanu and V. Dumitrescu. Densities, viscosities and excess properties for\ndimethyl sulfoxide with diethylene glycol and Methyldiethanolamine at different\ntemperatures. Applied Sciences, 12(1):116, 2021.\n[26] J. Soulages, M. S. N. Oliveira, P. C. Sousa, M. A. Alves, and G. H. McKinley.\nInvestigating the stability of viscoelastic stagnation flows in T-shaped microchannels.\nJournal of Non-Newtonian Fluid Mechanics, 163(1-3):9–24, 2009.\n[27] M. S. N. Oliveira, R. Yeh, and G. H. McKinley. Iterated stretching, extensional\nrheology and formation of beads-on-a-string structures in polymer solutions. Journal\nof non-Newtonian fluid mechanics, 137(1-3):137–148, 2006.\n[28] L.E. Rodd, T. P. Scott, D. V. Boger, J. J. Cooper-White, and G. H. McKinley. The\ninertio-elastic planar entry flow of low-viscosity elastic fluids in micro-fabricated\ngeometries. Journal of Non-Newtonian Fluid Mechanics, 129(1):1–22, 2005.\n[29] J. T. Ault, S. Shin, A. Garcia, A. Perazzo, and H. A. Stone. Viscosity measurements\nof glycerol in a parallel-plate rheometer exposed to atmosphere. Journal of Fluid\nMechanics, 968:A2, 2023.\n[30] G. K. Batchelor. Sedimentation in a dilute dispersion of spheres. Journal of fluid\nmechanics, 52(2):245–268, 1972.\n[31] G. K. Batchelor. The effect of Brownian motion on the bulk stress in a suspension\nof spherical particles. Journal of fluid mechanics, 83(1):97–117, 1977.\n[32] J. de Vicente, M. T. López-López, J. D. G. Durán, and F. González-Caballero. Shear\nflow behavior of confined magnetorheological fluids at low magnetic field strengths.\nRheologica acta, 44:94–103, 2004.\n[33] S. Melle, O. G. Calderón, M. A. Rubio, and G. G. Fuller. Microstructure evolution\nin magnetorheological suspensions governed by Mason number. Physical Review E,\n68(4):041503, 2003.\n[34] Y. Gao, M. A. Hulsen, T. G. Kang, and J. M. J. Den Toonder. Numerical and\nexperimental study of a rotating magnetic particle chain in a viscous fluid. Physical\nReview E—Statistical, Nonlinear, and Soft Matter Physics, 86(4):041503, 2012.\n34\n\n\n[35] B. de Gans, H. Hoekstra, and J. Mellema. Non-linear magnetorheological behaviour\nof an inverse ferrofluid. Faraday Discussions, 112:209–224, 1999.\n[36] D. J. Klingenberg, J. C. Ulicny, and M. A. Golden. Mason numbers for magnetorhe-\nology. Journal of Rheology, 51(5):883–893, 2007.\n35\n\n\n"}
{"text": "Published as a conference paper at ICLR 2025\nEVERYTHING, EVERYWHERE, ALL AT ONCE:\nIS MECHANISTIC INTERPRETABILITY IDENTIFIABLE?\nMaxime M´eloux, Franc¸ois Portet, Silviu Maniu, Maxime Peyrard\nUniversit´e Grenoble Alpes, CNRS, Grenoble INP, LIG, 38000 Grenoble, France\n{melouxm,portetf,manius,peyrardm}@univ-grenoble-alpes.fr\nABSTRACT\nAs AI systems are increasingly deployed in high-stakes applications, ensur-\ning their interpretability is essential. Mechanistic Interpretability (MI) aims to\nreverse-engineer neural networks by extracting human-understandable algorithms\nembedded within their structures to explain their behavior. This work systemati-\ncally examines a fundamental question: for a fixed behavior to explain, and under\nthe criteria that MI sets for itself, are we guaranteed a unique explanation? Draw-\ning an analogy with the concept of identifiability in statistics, which ensures the\nuniqueness of parameters inferred from data under specific modeling assumptions,\nwe speak about the identifiability of explanations produced by MI. We identify\ntwo broad strategies to produce MI explanations: (i) “where-then-what”, which\nfirst detects a subset of the network (a circuit) that replicates the model’s behavior\nbefore deriving its interpretation, and (ii) “what-then-where”, which begins with\ncandidate explanatory algorithms and searches in the activation subspaces of the\nneural model where the candidate algorithm may be implemented, relying on no-\ntions of causal alignment between the states of the candidate algorithm and the\nneural network. We systematically test the identifiability of both strategies us-\ning simple tasks (learning Boolean functions) and multi-layer perceptrons small\nenough to allow a complete enumeration of candidate explanations. Our experi-\nments reveal overwhelming evidence of non-identifiability in all cases: multiple\ncircuits can replicate model behavior, multiple interpretations can exist for a cir-\ncuit, several algorithms can be causally aligned with the neural network, and a\nsingle algorithm can be causally aligned with different subspaces of the network.\nWe discuss whether the unicity intuition is necessary. One could adopt a prag-\nmatic stance, requiring explanations only to meet predictive and/or manipulability\nstandards. However, if unicity is considered essential, e.g., to provide a sense of\nunderstanding, we also discuss less permissive criteria. Finally, we also refer to\nthe inner interpretability framework that demands explanation to be validated by\nmultiple complementary criteria. This work aims to contribute constructively to\nthe ongoing effort to formalize what we expect from explanations in AI.\n1\nINTRODUCTION\nInterpretability in machine learning spans diverse goals and methods (Molnar, 2022; Carvalho et al.,\n2019; Teney et al., 2022), from creating inherently interpretable models to applying post hoc tech-\nniques to explain model decisions. Mechanistic interpretability (MI) aims to reverse-engineer mod-\nels to reveal simple, human-interpretable algorithms embedded in neural network structure (Olah\net al., 2020). MI is focused on generating what we call computational abstractions, where complex\nneural networks’ behaviors are explained by simpler algorithms that track the internal computations\n(Olah et al., 2020). A computational abstraction – a mechanistic explanation – has two components:\n(a) what is the explanatory algorithm, and (b) where in the computational structure is this algo-\nrithm embedded? Given the intractability of exhaustively searching all possible algorithms across\nall subsets of a neural network, researchers have developed methods with different assumptions\nand trade-offs. We categorize these methods into two broad strategies. The first, which we call\nwhere-then-what, focuses on finding a subset of the network – a circuit – that captures most of the\ninformation flow from inputs to outputs. Once this circuit is identified, typically using heuristics,\n1\narXiv:2502.20914v1  [cs.LG]  28 Feb 2025\n\n\nPublished as a conference paper at ICLR 2025\nthe next step is to interpret its components (features) to derive the explanatory algorithm (Dunefsky\net al., 2024; Davies and Khakzar, 2024; Conmy et al., 2023). The second approach, which we name\nwhat-then-where, starts by identifying candidate algorithms and then searches subspaces in the neu-\nral network where the algorithm may be implemented. This is performed using causal alignment\nbetween the explanatory algorithm’s states and the network’s internal states and typically requires\napproximation algorithms (Geiger et al., 2022a;b). Each strategy relies on specific criteria to assess\ncandidate explanations. For instance, circuits can be evaluated by their circuit error, which quan-\ntifies how closely the circuit’s predictions match the full model ones (Conmy et al., 2023). In the\nwhat-then-where strategy, candidate algorithms are compared based on causal alignment measures\nlike intervention interchange accuracy (IIA), which assesses how well the algorithm’s states remain\naligned with the network’s internal states after counterfactual manipulations of the states.\nIn this work, we question a property of explanation that appears to be tacitly taken for granted: do\nMI criteria guarantee a unique explanation of a fixed behavior? The concept of identifiability is\nwell-established in statistics, where a model is identifiable if its parameters can be uniquely inferred\nfrom data under a given set of modeling assumptions (e.g., Rothenberg, 1971). By analogy, we\nextend this terminology to interpretability, defining the identifiability of explanation as the property\nwhere, under fixed assumptions of validity, a unique explanatory algorithm satisfies the criteria.\nSpecifically, we ask the following questions: In the where-then-what strategy, (i) is the circuit (the\n“where”) unique? (ii) Is a given circuit’s grounding interpretation (the “what”) unique? In the what-\nthen-where strategy, (iii) is the causally-aligned algorithm (the “what”) unique? (iv) For a given\nalgorithm, is there a unique subspace of the neural network (the “where”) that is causally aligned?\nWe stress-test the identifiability properties of current MI criteria by conducting experiments in a\ncontrolled, small-scale setting. Using simple tasks like learning Boolean functions and very small\nmulti-layer perceptrons (MLPs), we search for Boolean circuit explanations – aiming to discover\nwhich succession of logic gates is implemented by the MLPs. This setup allows us to exhaustively\nenumerate incompatible candidate explanations and test them with existing criteria. Our experiments\nreveal non-identifiability at every stage of the MI process. Specifically, we find that: (i) Multiple\ncircuits can perfectly replicate the model’s behavior (with a circuit error of zero), (ii) for a given\ncircuit, multiple valid interpretations exist, (iii) several algorithms can be perfectly causally aligned\nwith the neural computation (IIA of one), and (iv) for a given causally aligned algorithm, multiple\nsubspaces of the neural network can be equally aligned (IIA = 1).\nIn the discussion, we revisit whether unicity is necessary. We discuss alternative criteria and per-\nspectives that do not require modifying existing criteria, such as requiring explanations only to meet\npredictive and/or manipulability standards. However, if unicity is considered essential, e.g., to pro-\nvide a sense of understanding, we also discuss less permissive criteria. Finally, we also refer to the\ninner interpretability framework (Vilas et al., 2024) that requires an explanation to be validated by\nmultiple complementary criteria. We hope our work contributes constructively to the ongoing effort\nto develop rigorous definitions for what it means to explain a complex neural network.\n2\nBACKGROUND\n2.1\nMECHANISTIC INTERPRETABILITY\nMechanistic interpretability rests on the key assumptions that a neural network’s behavior can be\nexplained by a simpler algorithm than the full network, and that a sparse subset of the network\nexecutes this algorithm. Previous research has given support to these assumptions: pruning studies\n(Gale et al., 2019; Ma et al., 2023; Sun et al., 2024) and the lottery ticket hypothesis (Frankle and\nCarbin, 2019; Liu et al., 2024) suggest that networks are often overparameterized, and only a fraction\nof neurons and connections are critical to the final performance. Training sub-networks (Yuan et al.,\n2019) to approximate the full model (Liao and Kyrillidis, 2022), similar to dropout (Srivastava et al.,\n2014), supports the idea that sub-networks can approximate the full network’s behavior well.\nThis search for interpretable circuits is inspired by neuroscience, which has long sought to uncover\nneural circuits that explain observed behaviors (Yuste, 2008). Once the neural circuit is discovered,\nresearchers focus on interpreting the functional roles of each component in the brain (Yuste, 2008).\nResearch in computer vision has already shown that some nodes within neural networks compute\n2\n\n\nPublished as a conference paper at ICLR 2025\ninterpretable features (Olah et al., 2017). Connections between such features, also called circuits,\ncan be compact explanations of model behavior (Olah et al., 2020; Carter et al., 2019; Dreyer et al.,\n2024). Finally, recent work has applied mechanistic interpretability to LLMs (Elhage et al., 2021),\nespecially in transformer models (Templeton et al., 2024; Bricken et al., 2023; Vilas et al., 2023). For\nexample, Wang et al. (2022) identified a circuit responsible for Indirect Object Identification (IOI)\nin transformers, highlighting the potential for mechanistic explanations of complex LLM behaviors.\n2.2\nDEFINITIONS\nF\n0\n0\n1\nF\n1\n0\n1\nF\n2\na\nb\nc\nBase \nNeural Network\n1. Circuit\n0 1\n0 1\na\nb\nc\nI\nF0\nF1\nF2\n0\nComputational\nAbstraction\n2. Mapping 𝝉\nneural activations ↔feature values\nFigure 1: Illustration of the computational abstraction\ncomponents within a neural network. The circuit rep-\nresents a subgraph, and the mapping specifies the high-\nlevel features computed by the circuit, detailing how\ntheir values arise from low-level neural activations. To-\ngether, these form the computational abstraction (ex-\nplanation of the neural network). Here, feature F2 has\nthree possible values and is defined within the 2D ac-\ntivation space of two neurons. Features F0 and F1 are\nbinary variables, each assigned to a single neuron. F0\ncovers the entire activation space and F1 only maps\nspecific intervals, leaving some activations unassigned.\nA satisfactory mechanistic explanation of\na model’s behavior consists of two com-\nponents: the what, a high-level algorithm\nthat closely approximates the model’s be-\nhavior and tracks its internal computation,\nand the where, specifying how and where\nthis algorithm is embedded in the model’s\nlow-level neural computation.\nWe refer to the combination of an explana-\ntory algorithm and the mapping between\nthe high- and low-level states as a compu-\ntational abstraction. This is an abstraction\nas it simplifies the neural network’s com-\nputation, focusing on a subset of the com-\nputational graph and abstracting neural ac-\ntivations into simpler, high-level features.\nFor example, consider the mechanistic ex-\nplanation of how a vision algorithm recog-\nnizes rectangles. We might identify a com-\nputational abstraction where certain mod-\nules perform edge detection, others detect\nright angles, and a final component applies\nan AND logic gate to confirm the pres-\nence of four right angles. This abstraction\nspecifies the algorithm and how and where\nlow-level neural activations correspond to\nhigh-level features of the algorithm.\nIn\nthis work, we interchange the terms expla-\nnation and computational abstraction.\nFormally, we define a computational ab-\nstraction A as a tuple (S, τ), where S is the circuit, the subset of the neural network’s computational\ngraph responsible for the behavior of interest, and τ is the mapping between the states of the circuit\nand the states of the variables of the algorithm, which specifies how to interpret the computational\nfunction of the circuit’s components. We now proceed to define the circuit and mapping formally.\nDefinition 1 (Circuit). Let G = (V, E) represent the computational graph of a neural network,\nwhere V is the set of nodes (neurons) and E ⊆V × V is the set of edges (connections between\nneurons). A circuit S = (VS, ES) is a subgraph of G that contains at least one path from a subset\nof input nodes to a subset of output nodes.\nDefinition 2 (Mapping (τ)). A mapping between low-level values taken by neurons and high-level\nvalues taken by the variables of the explanatory algorithm consists of a set of K surjective maps,\none for each high-level variable. Each associates the neural network activations with the values of\nthe corresponding high-level variable. For a group of neurons Vj in the neural network, mapped to a\nhigh-level variable Aj with possible values {f0, . . . , fm}, the mapping τj : R|Vj| →{f0, . . . , fm}\nassigns a vector of activations to one of the possible values of Aj. Each mapping should be surjective\n\u0000∀fi, ∃h ∈R|Vj| : τj(h) = fi\n\u0001\nand with a non-empty pre-image\n\u0000∀fi, τ −1\nj\n(fi) ̸= ∅\n\u0001\nThese conditions ensure that all high-level values can be realized by some set of low-level activa-\ntions.\n3\n\n\nPublished as a conference paper at ICLR 2025\nIn practice, we are interested in mappings that satisfy a consistency requirement. Intuitively, consis-\ntency means that if we first perform part of the computation using the neural network and then apply\nthe mapping to get the state of a high-level variable, the outcome should be identical to applying the\nmapping and then performing the computation of the high-level algorithm. The computations in the\nneural network and the high-level algorithm should align consistently according to the mapping.\nDefinition 3 (Consistent Mapping). Let τ be a mapping between groups of low-level neurons {Vj}\nand their corresponding high-level variables {Aj}. The mapping τ is said to be consistent if for any\nhigh-level variable Aj, with parents PAj, the following diagram commutes:\nPAj\nAj\nR|VP Aj |\nR|Vj|\nAlg.\nNN\nτP Ai\nτj\nHere: τP Aj represents the application of τ to each variable in PAj; NN refers to the computation\nbetween the low-level neural network states; and Alg. refers to the computation between high-level\nvariables governed by the explanatory algorithm.\nPrevious works have explored various types of high-level features and representational abstractions,\nincluding mappings based on “directions in activation space” or specific points within activation\nsubspaces (Olah et al., 2020; 2018; Bereska and Gavves, 2024). This work focuses on explana-\ntory algorithms represented as Boolean circuits, where high-level features are binary (0 or 1). The\nmappings specify which activations correspond to 0 and 1. Boolean circuits are computationally\nuniversal and thus sufficient to demonstrate identifiability issues in existing MI criteria.\n2.3\nAPPROACHES TO CIRCUIT DISCOVERY\nWe identify and describe two strategies for reverse-engineering neural networks: the where-then-\nwhat and what-then-where approaches.\nWHERE-THEN-WHAT\nMethods from this strategy first aim to identify a circuit that replicates the behavior of the full model\nwell. Once a circuit is found, the next step is to interpret its components to uncover the high-level\nalgorithm being implemented (Dunefsky et al., 2024; Davies and Khakzar, 2024). The evaluation\ncriteria for circuits is how well they replicate the full model’s behavior for the input of interest.\nDefinition 4 (Circuit Error). Let S be the function computed by a circuit and g the function computed\nby the model on which the circuit is defined. For the input set x, the error of the circuit S is:\n1 −\n1\n|x|\nP\nx∈x 1[S(x) = g(x)]\nIn the case of perturbed inputs, it can also be defined via the KL divergence between the logits of\nthe circuit and the model (Conmy et al., 2023).\nIn practice, circuit search relies on causal mediation analysis, which seeks to isolate the subset of\nthe network that carries the information from the inputs to the output. Since it is computationally\nintractable to enumerate all possible circuits in complex models (Adolfi et al., 2024a), existing meth-\nods focus on computing mediation formulas for individual components to decide their inclusion in\nthe circuit (Vig et al., 2020; Meng et al., 2022; Monea et al., 2024; Kram´ar et al., 2024; Conmy et al.,\n2023; Geva et al., 2023; Syed et al., 2023).\nA combination of data analysis and human input is typically used to interpret candidate circuits.\nFor example, activation maximization identifies inputs that maximally activate a component, which\nhelps clarify its function (Zhou et al., 2016; Zeiler and Fergus, 2014; Simonyan et al., 2014). This\ntechnique has been extended to modern LLMs (Peyrard et al., 2021; Jawahar et al., 2019; Dai et al.,\n2022). However, polysemantic neurons, which encode multiple concepts simultaneously (Templeton\net al., 2024; Bricken et al., 2023), complicate the interpretation of LLMs components. For a broader\noverview of these challenges, we refer readers to the following surveys: Sajjad et al. (2022); Khakzar\net al. (2021). In this work, we use the concept of consistent mapping as the objective evaluation of\nthe quality of an interpretation.\n4\n\n\nPublished as a conference paper at ICLR 2025\nWHAT-THEN-WHERE\nMethods from this strategy first hypothesize a candidate high-level algorithm and then search for\nmappings between the states of this algorithm and subspaces of the neural activations. The goal is\nto identify mappings where the high-level and low-level states are causally aligned, meaning they\nrespond similarly under interventions.\nGiven a candidate high-level algorithm A, neural activations H, and a mapping τ defined between\nthem, counterfactual interventions are performed on the inner variables of A, and corresponding\ninterventions are applied to H via τ. Intervention interchange accuracy (IIA) (Geiger et al., 2022b)\nis then defined for each high-level variable and measures the similarity of outputs in A and H after\nintervening (metric for causal alignment). We give in Appendix A a complete, formal definition.\nA perfect IIA score (1) for all variables indicates that all possible interventions produce the same ef-\nfect in low-level and high-level models. In practice, exhaustive enumeration is often impractical, and\nIIA is approximated using randomly sampled inputs (Geiger et al., 2022b). Similarly to the map-\nping consistency defined above, perfect causal alignment requires diagram commutation between\nlow- and high-level models under interventions.\nSearching for causal alignment between high-level models and neural activations can be computa-\ntionally expensive, as it often requires testing many potential mappings. The Distributed Align-\nment Search method (Geiger et al., 2024) addresses this challenge by employing gradient de-\nscent to search for alignments efficiently. This approach also allows for distributed representa-\ntions, where multiple neurons represent a single high-level variable. Indeed, an underlying assump-\ntion of IIA is that the neural activations corresponding to distinct high-level variables are disjoint:\n∀x, y ∈V, τ −1(x) ∩τ −1(y) = ∅, which may not occur in real-world examples (Olah et al., 2020).\nCurrently, no systematic method exists for choosing which candidate algorithms to test. Previous\nwork (Wu et al., 2023) has manually proposed a few candidates, but the vast space of possible\nalgorithms makes this an open challenge.\n2.4\nEXPLANATION “IDENTIFIABILITY”\nThe assumption of explanatory unicity – the idea that there exists a single, unique explanation for\na given phenomenon – is not only implicit in the practice of mechanistic interpretability (see rel-\nevant citations in Appendix C) but also rooted in human cognitive and psychological tendencies\n(Trout, 2007; Waskan, 2024; Gopnik, 2000). Humans demonstrate a cognitive preference for coher-\nent explanations that integrate disparate observations into a unified narrative (e.g., Friedman, 1974;\nKitcher, 1962; 1981; Schurz, 1999; Kveraga et al., 2007). This preference aligns with the psy-\nchological need for cognitive closure, defined as the desire for a definitive conclusion (Kruglanski,\n1989). Multiple incompatible explanations disrupt coherence, leading to ambiguity and a sense of\nunresolved understanding.\nConversely, in the philosophy of science, explanatory pluralism acknowledges that the world is\ntoo complex to be fully described by a single comprehensive explanation (Kellert et al., 2006; Po-\ntochnik, 2017). Multiple explanations often coexist without conflict because they address different\nexplanatory goals (e.g., explaining distinct behaviors) or employ different simplification strategies\n(e.g., differing levels of abstraction Marr and Poggio, 1976). However, in this work, we deliber-\nately search for conflicting explanations by fixing both the explanatory goal and the simplification\nstrategies, as the ones defined by MI criteria.\nIdentifiability and incompatible explanations.\nAs mentioned in Section 1, we borrow the term of identifiability from the field of statistics (Rothen-\nberg, 1971), defining identifiability of explanation as the property where a unique explanation is\nvalid under fixed standards of validity. An MI strategy is not identifiable if its standards of validity\ndo not discriminate between two incompatible explanations.\nWe define two explanations as incompatible or conflicting if they share the same explanatory goal\nand simplification strategy, but posit different computational abstractions. In our context, the ex-\nplanatory goal is fixed: explaining the specific input-output behavior of a trained MLP. The simplifi-\ncation strategy is also fixed, corresponding to one of two predefined strategies to find computational\nabstractions: the what-then-where or the where-then-what defined above.\n5\n\n\nPublished as a conference paper at ICLR 2025\nBase Neural Network\nTrain an MLP to implement XOR\n𝐴= 0|1 + 𝒩(0, ℰ) \n𝐵= 0|1 + 𝒩(0, ℰ) \n𝐶= 𝑟𝑜𝑢𝑛𝑑𝐴⊕ 𝑟𝑜𝑢𝑛𝑑(𝐵) \n𝐴\n𝐵\n𝐶\nWhere-then-what: searching circuits \nthat support behavior and then \ninterpreting their features via grounding\n…\nCircuits\nComputational Abstractions\n…\n85 unique circuits with perfect accuracy\n25 unique abstractions with exact grounding \nbetween neural activations and gate definitions\nWhat-then-where: searching for different \ncandidate algorithms causally aligned in the \nactivations of the base neural network\nCandidate Algorithms\nA\nB\n𝐀𝐍𝐃\n𝐍𝐀𝐍𝐃\n𝐎𝐑\n𝐎𝐑\nA\nB\n¬𝐀 𝐀𝐍𝐃 𝐁\n𝐀 𝐀𝐍𝐃 ¬𝐁\n2 candidates are tested\nComputational Abstractions\n159 perfect mappings (IIA=1)\n…\nExample of 2 perfect mappings for one algorithm\nFigure 2: Illustration of identifiability problems using the XOR example. We train a small\nMLP with two hidden layers of size 3 to compute the XOR function perfectly. The figure shows\nthe outcome of stress-testing the two reverse-engineering strategies: Top: For the what-then-where\nstrategy, we enumerate all subsets of neurons searching for subsets causally aligned with interme-\ndiate variables of candidate algorithms, with alignment measured by IIA. Even testing only two\ncandidate algorithms, we find perfect implementations of both in the model. Multiple mappings\n(localizations) for each algorithm were identified, showing that neither the algorithm (what) nor its\nlocation in the network (where) is unique. Bottom: For the where-then-what strategy, we enumerate\ncircuits (sub-networks) and test whether each computes the XOR independently. For each circuit,\nwe search for possible feature interpretations of the selected neurons, identifying intermediate logic\ngates whose values can be mapped consistently with the neurons’ activations. Consistency is defined\nas in 3. We find many different perfect circuits (the where is not unique) and for any given circuit,\nwe find multiple valid interpretations (the what is not unique).\nIncompatibility of two explanations can occur if (1) the two explanations posit different algorithms\nfor the same behavior, or (2) the same algorithm is embedded in different subspaces of the neural\nnetwork. Both scenarios entail different internal representations and causal pathways linking inputs\nto outputs. In Figure 2, we report examples of incompatible explanations in trained MLPs.\nOur experiments show that even the strict causal criteria of MI allow many incompatible computa-\ntional abstractions. In the discussion (Section 5), we revisit whether this expectation of unicity is\nnecessary or even achievable.\n3\nILLUSTRATING POTENTIAL IDENTIFIABILITY ISSUES\nThis section highlights identifiability counter-examples for a small MLP trained to compute the XOR\nfunction. It is well-known that an MLP requires at least two layers to compute the XOR function.\nOnce the network can do so, the interpretability exercise becomes: how is the XOR implemented?\nFor a mechanistic explanation, the answer must have two components: what algorithm is being used,\nsuch as which combination of logic gates transforms the inputs into the XOR truth table, and where\nthese intermediate logic gates are located within the neural network’s computation—i.e., where the\nalgorithm is executed within the MLP.\nTo stress-test the two main MI strategies (where-then-what and what-then-where), we chose an MLP\nsmall enough to allow exhaustive enumeration of all circuits and extensive search over mappings.\nThe MLP is trained on binary inputs with a single logit output to produce the XOR behavior. The\ninputs are 0 or 1 and can have a randomly sampled Gaussian noise of a fixed standard deviation.\n6\n\n\nPublished as a conference paper at ICLR 2025\nOur methods to test the different criteria defined in the previous section are as follows:\nCircuit search: We enumerate all possible circuits, and then execute the validation data of the XOR\non each circuit as if it were a standalone neural network, effectively removing from the computation\neach node and edge that is not part of the circuit. If a circuit achieves perfect accuracy (zero circuit\nerror), we label it a perfect circuit, as it exactly replicates the model’s behavior. This search tests the\nidentifiability property of the circuit error criteria.\nInterpretation search: For each perfect circuit, we attempt to interpret the activations of the in-\ncluded neurons based on XOR validation data. As the scope of interpretations is limited to logic\ngates, we search, for each neuron, a logic gate whose values are consistent with that neuron’s acti-\nvation. The method proceeds recursively, layer by layer, based on a given neuron’s relationship with\nits parents in the circuit. The parents already have an interpretation (mapping their activations to 0\nor 1). We enumerate all possible inputs from the parents and examine how they are mapped into the\nneuron’s activation by the model. We then list all possible ways to separate these inputs and label\nthe resulting logic gate. If we find no valid interpretation for a given neuron (e.g., all inputs overlap\nin the output activation and no separation is possible), we end this candidate interpretation of the cir-\ncuit. If we find multiple, we expand the tree of possible candidate interpretations for the circuit. To\navoid trivial over-counting, we ignore value relabeling (e.g., swapping 0 and 1) and, by convention,\nassign 1 to the larger intervals and 0 to the smaller ones. The outcome is a computational abstrac-\ntion, a Boolean circuit computing the XOR function whose internal logic gates are mapped to some\nneural network components. Note that this method undercounts possible interpretations because it\ndoes not consider cases where the high-level logic gates are mapped on multiple low-level neurons.\nThis search tests the identifiability property of the mapping consistency criteria.\nAlgorithm search: We enumerate all algorithms implementing the XOR gate by considering\nBoolean formula parse trees. We assume that each neuron’s activations encode either the iden-\ntity, AND, or OR gate, a network of depth d can implement only formulas with a parse tree of depth\n≤d. We therefore recursively enumerate all unique formulas of depth d or less using AND, OR,\nand negation. For d = 3, this results in 56 XOR-equivalent algorithms.\nMapping search: For a given candidate algorithm with specified intermediate logic gates, we ex-\nplore all possible neuron subsets and mappings between these subsets and the algorithm’s interme-\ndiate gates. We then measure the causal alignment of the mapping using IIA. If a mapping achieves\nperfect IIA, we call it a perfect mapping. If there is no other mapping with larger images (set\ninclusion-wise), we also call this mapping minimal. In the example described in this section, we\nmanually test two candidate algorithms, while the next section enumerates algorithms that imple-\nment the target function, excluding trivial variations (e.g., negating gates). This search tests the\nidentifiability property of the IIA criteria.\nWe depict in Figure 2 counter-examples for each criterion in one small MLP. In this example, for the\nwhat-then-where strategy, we only test two candidate algorithms but find 159 perfect minimal map-\npings within the neural network activations, with perfect mappings for both algorithms. Therefore,\nthe algorithm is not unique and, for a given algorithm, its localization is not unique. For the where-\nthen-what strategy, we find 85 unique circuits with perfect accuracy, with an average of 535.8 logic\ngate interpretations (consistent mappings) per circuit. Therefore, the localization is not unique, and\nfor a given circuit, the interpreted algorithm is not unique. Overall, in this example, we obtain 159 +\n45,543 computational abstractions, most of which are incompatible. This is a serious identifiability\nproblem as there is no clear and consensual criterion to decide among all these explanations.\n4\nEXPERIMENTS\n4.1\nQUANTITATIVE ANALYSIS\nWe now repeat the experiment used for the XOR example with different seeds, while varying the\narchitecture size and the complexity of the global behavior.\nThe basic setup is consistent across all experiments. We choose n 2-input logic gates L1, . . . , Ln,\ngenerate a multilayer perceptron (MLP) N with layer sizes (2, k, k, n), and train N to implement\nthe gates L1, . . . Ln. Similarly to the previous section, training is performed on binary samples with\nadded Gaussian noise and continues until the network’ mean squared loss is lower than n × 10−3.\n7\n\n\nPublished as a conference paper at ICLR 2025\nWe then quantify the identifiability issues again. For the circuit-first search (where-then-what strat-\negy), we count perfect circuits for L in N and valid interpretations for each circuit.\nFor the\nalgorithm-first search (what-then-where strategy), we count perfectly aligned algorithms for L in\nN and perfect minimal mappings for each algorithm.\nIn the circuit-first search, we only search for circuits containing two inputs and one output for each\ntarget gate. Furthermore, due to the combinatorial explosion in circuit enumeration, we only test\ncircuits with a sparsity greater than 0.3, where sparsity is measured as the fraction of components\nexcluded from the circuit. This number is chosen as the smallest sparsity that remains manageable\nfor the size of MLPs that we consider. As a result, the reported number of circuits should be consid-\nered a lower bound. Furthermore, the number of potential interpretations for a single circuit grows\nexponentially with its size. Since we count interpretations for only the sparser circuits, the reported\nnumber of interpretations is also significantly lower than an exhaustive search would yield.\n4.1.1\nARCHITECTURE SIZE\nIncreasing the neural network’s size may impact the number of computational abstractions found\nin networks. Although a larger architecture may create more computational abstractions due to the\nincreased search space, it could also lead to greater overparameterization, meaning a smaller subset\nof the network may suffice to implement the target gate. This, in turn, could reduce the number of\nvalid abstractions if most of the network is inactive during inference.\nIn the left side of Figure 3, we report the total number of explanations found when the architecture\nsize ranges from k = 2 to k = 5. We exclude from this figure the networks for which no valid\nmapping or interpretation was found, which we give in Appendix D.2 along with additional plots.\nFigure 3: Number of computational abstractions found in the circuit-first approach (circuit interpre-\ntations) and the algorithm-first approach (perfect minimal mappings), as a function of architecture\nsize k (left) or of the number n of gates the model is trained on (right, averaged over all target gates).\nOne point per neural network.\nIn both cases, we observe that the number of computational abstractions found significantly in-\ncreases with network size, with median values growing from 38 to 910,000 in the circuit-first method\nand from 8 to 3,700 in the algorithm-first approach. Less than 2% of the trained networks contain\nexactly one valid minimal mapping, and no network contains exactly one circuit interpretation.\n4.1.2\nMULTIPLE TASKS\nWe also investigate the effect of global behavior complexity. The model is trained to implement\na single logic gate in the basic setup. What happens when the network is trained in a multi-task\nsetting? As the number of target tasks increases, we expect the network to use its activations more\nefficiently, possibly relying on a smaller subset of its structure for each task. To explore this, we fix\nk = 3 and vary n from 1 to 6, sampling n logic gates (without replacement) from the same list as\nabove, extended to include the negation of the gates (NOR, NAND, NIMP, and XNOR). The neural\nnetwork N is then trained to implement these gates in parallel, using two shared input neurons and\nn output neurons (one per gate). We repeat this procedure using different random seeds.\n8\n\n\nPublished as a conference paper at ICLR 2025\nThe right side of Figure 3 contains the total number of computational abstractions obtained when\ntraining each neural network on a different number of logic gates in parallel, ranging from 1 to 6.\nMore detailed plots are available in Appendix D.3.\nIn both approaches, the number of interpretations significantly decreases with the number of training\ntasks (p = 0.05), up to 4 tasks. Past that point, the variation is no longer statistically significant.\n4.2\nTRAINING DYNAMICS\nA possible explanation for the high number of computational abstractions we find in trained net-\nworks is that our networks do not perfectly implement the target logic gates, since training is stopped\nwhen a low but non-zero loss value is reached. We explored this effect by varying the loss cutoff in\nthe basic setup. The results are given in Appendix D.4. More generally, the influence of the training\ndistribution on the number of valid computational abstractions is investigated in Appendix D.6. In\naddition, we find that adding noise to binary samples during training has no significant effect on the\nresults obtained in the algorithm-first method, but decreases the number of circuits while increas-\ning the overall number of interpretations found in the circuit-first method (results are described in\nAppendix D.5). Training dynamics and generalization abilities of a model may therefore reduce the\nnumber of available abstractions, but this effect alone is unlikely to mitigate the issue entirely.\n4.3\nTOWARDS LARGER MODELS\nWhile enumerating circuits or mappings is infeasible in large networks, it is still possible to find\ncounterexamples in which multiple circuits exist. For example, we trained a larger MLP on a subset\nof the MNIST dataset (Deng, 2012), filtered to contain only the digits 0 and 1. We obtained a\nregression model with layer sizes (784, 128, 128, 3, 3, 3, 1). After training, we extracted the last\nlayers of the model to form two sub-networks: one of size (784, 128, 128, 3) and one of size (3, 3, 3,\n1). We fed the training samples through the larger sub-network, generating a new dataset comprised\nof partial computations of the overall model.\nApplying the circuit search method to the smaller sub-network using this new dataset yielded 3,209\nvalid circuits. While we cannot enumerate circuits in the first half of the network, any such valid\ncircuit can include one of the circuits of the second half as its continuation. Two situations may\narise: If the first half of the MLP does not contain any valid circuits, then no valid circuit exists for\nthe full network; if valid circuits exist in the first half of the MLP, then a minimum of 3,209 valid\ncircuits exist in the full network. This shows, at least in the case of circuits, that the problem does\nnot seem to disappear with significantly larger scale and more complex data distributions.\n5\nWHAT DOES IT MEAN FOR INTERPRETABILITY?\nOur findings challenge the strong intuition that a unique mechanistic explanation exists for a given\nbehavior under fixed explanatory goals and validity criteria. Even when employing the strict causal\nrequirements of MI, we find that many incompatible explanations can coexist. While predictive\nof behavior and causally aligned with the neural network’s states, these explanations differ in the\ncomputational algorithms they postulate or how they are embedded in the network’s subspaces. We\nnow discuss several ways to move forward from this striking observation.\n5.1\nDOES LACK OF UNICITY MATTER?\nWhether multiple “valid” explanations pose a real problem is worth considering. From a pragmatic\nstance, one could argue that unicity is not essential if the explanations meet functional goals such\nas predictivity, controllability, or utility in decision-making (Van Fraassen, 1988; Achinstein, 1984).\nThis perspective emphasizes crafting practical criteria to evaluate explanations based on their utility,\nrather than their ontological closeness to the truth. Stating explicitly the pragmatic goals of an\nexplanation can also clarify what is expected of an explanation (Woodward and Ross, 2021). For\nexample, in the recent debate about interpretability illusion, Makelov et al. (2023) mention problems\nabout interventions that can potentially activate dormant pathways leading the resulting explanation\nto misrepresent the mechanisms at play. In their response, one of the arguments advanced by Wu\net al. (2024) is to point out that the explanation produced by their method (DAS), still meets the\n9\n\n\nPublished as a conference paper at ICLR 2025\npragmatic goals of predictivity and manipulability, ensuring its usefulness. The debate is resolved\nby clarifying the epistemic goals of the explanation.\n5.2\nIF YES, HOW CAN WE AIM TO RESOLVE IT?\nIf we decide that identifiability of explanations is important, our work demonstrates that current\nMI criteria are insufficient to guarantee it. One potential approach to resolving this issue involves\nintroducing additional heuristics, such as prioritizing the sparsest circuits. However, Occam’s razor\nalone is unlikely to solve the problem. Should we dismiss an entirely different candidate explanation\nsimply because it involves one additional node than another? In our experiments, simplicity or\nsparsity cannot single out one explanation.\nTo address these challenges, we believe that ideas from causal abstraction (Beckers and Halpern,\n2019; Beckers et al., 2020; Rubenstein et al., 2017) can be helpful (Geiger et al., 2022a). Although\nIIA is directly inspired by causal abstraction, it does not fully implement it in its current form. Unlike\ncurrent MI frameworks, causal abstraction requires that all lower-level model states are accounted\nfor in higher-level representations. Furthermore, if components are excluded from an explanation,\ntheir absence must be justified causally. This intuition has been formalized recently through the\nconcept of faithfulness, which evaluates how well a circuit replicates the model’s behavior and the\n(lack of) impact of excluded elements (Hanna et al., 2024).\nAlternatively, one can look at broader approaches and not focus on searching for explanations that\noptimize a single criterion. Interestingly, Vilas et al. (2024) propose an inner interpretability frame-\nwork based on lessons from cognitive neuroscience. In this framework, the authors emphasize the\nimportance of building multi-level mechanistic explanations and stress-testing these explanations\nwith proper hypotheses testing. An explanation validated by many criteria and which exhibits differ-\nent properties (e.g., invariances) becomes more trustworthy. This framework provides a promising\npath to establish MI as a natural science akin to neuroscience or biology.\n5.3\nIS IDENTIFIABILITY EVEN ACHIEVABLE?\nIn some domains of science, competing theories coexist despite being ontologically incompatible.\nFor instance, the Lagrangian and Hamiltonian formulations of classical mechanics posit different\nunderlying entities but yield identical experimental predictions. Such scenarios are examples of con-\ntrastive underdetermination, a philosophical debate about whether empirical evidence can or should\nuniquely determine scientific explanations (Stanford, 2023). The intrinsic computational hardness\nof interpretability queries (Adolfi et al., 2024a;b) suggests that MI may have fundamental limits,\nleaving it possibly underdetermined. More broadly, the search for MI explanations can be seen as\na type of causal representation learning, where identifiability may depend on strong constraints that\nmight not be realistic Morioka and Hyv¨arinen (2024).\n5.4\nFROM TOY MODELS TO REAL MODELS\nOur experiments focus on toy MLPs trained on toy tasks, which differ drastically from large lan-\nguage models (LLMs) trained on vast, complex datasets using Transformer architectures. This raises\nthe possibility that the issues in small models may not apply to larger, more sophisticated models.\nHowever, if this is true, why the problems would disappear at larger scales must be demonstrated.\nUnderstanding why current criteria function well in some regimes but not in others would also lead\nto refined criteria and definitions.\n5.5\nCONCLUSION\nOur results should encourage the community to reflect on the role of unicity when searching for and\ncommunicating about mechanistic explanations found in neural networks. We believe that exploring\nstricter criteria based on causal abstraction, explicitly formulating pragmatic goals of explanations\nand embracing broader frameworks such as the inner interpretability one are all promising directions.\nThe code and parameters used to conduct this paper’s experiments can be found on GitHub.\n10\n\n\nPublished as a conference paper at ICLR 2025\nACKNOWLEDGEMENTS\nThis work was conducted within French research unit UMR 5217 and was partially supported by\nCNRS (grant ANR-22-CPJ2-0036-01) and by MIAI@Grenoble-Alpes (grant ANR-19-P3IA-0003).\nIt was granted access to the HPC resources of IDRIS under the allocation 2025-AD011014834\nmade by GENCI.\n11\n\n\nPublished as a conference paper at ICLR 2025\nREFERENCES\nPete Achinstein. 1984. The pragmatic character of explanation. In PSA: Proceedings of the Bien-\nnial Meeting of the Philosophy of Science Association, volume 1984, pages 274–292. Cambridge\nUniversity Press.\nFederico Adolfi, Martina G. Vilas, and Todd Wareham. 2024a. Complexity-Theoretic Limits on the\nPromises of Artificial Neural Network Reverse-Engineering. Proceedings of the Annual Meeting\nof the Cognitive Science Society, 46(0).\nFederico Adolfi, Martina G. Vilas, and Todd Wareham. 2024b. The computational complexity of\ncircuit discovery for inner interpretability.\nSander Beckers, Frederick Eberhardt, and Joseph Y. Halpern. 2020. Approximate causal abstrac-\ntions. In Proceedings of The 35th Uncertainty in Artificial Intelligence Conference, volume 115\nof Proceedings of Machine Learning Research, pages 606–615. PMLR.\nSander Beckers and Joseph Y. Halpern. 2019. Abstracting causal models. Proceedings of the AAAI\nConference on Artificial Intelligence, 33(01):2678–2685.\nLeonard Bereska and Efstratios Gavves. 2024. Mechanistic interpretability for ai safety – a review.\nTrenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Con-\nerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu,\nShauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex\nTamkin, Karina Nguyen, Brayden McLean, Josiah E Burke, Tristan Hume, Shan Carter,\nTom Henighan, and Christopher Olah. 2023.\nTowards monosemanticity: Decomposing lan-\nguage models with dictionary learning.\nTransformer Circuits Thread.\nHttps://transformer-\ncircuits.pub/2023/monosemantic-features/index.html.\nNick Cammarata, Gabriel Goh, Shan Carter, Chelsea Voss, Ludwig Schubert, and Chris Olah. 2021.\nCurve Circuits. Distill, 6(1):e00024.006.\nShan Carter, Zan Armstrong, Ludwig Schubert, Ian Johnson, and Chris Olah. 2019. Activation atlas.\nDistill. Https://distill.pub/2019/activation-atlas.\nDiogo V. Carvalho, Eduardo M. Pereira, and Jaime S. Cardoso. 2019.\nMachine learning inter-\npretability: A survey on methods and metrics. Electronics, 8(8).\nArthur Conmy, Augustine Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adri`a Garriga-\nAlonso. 2023. Towards automated circuit discovery for mechanistic interpretability. In Advances\nin Neural Information Processing Systems, volume 36, pages 16318–16352. Curran Associates,\nInc.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. 2022. Knowledge\nneurons in pretrained transformers. In Proceedings of the 60th Annual Meeting of the Associ-\nation for Computational Linguistics (Volume 1: Long Papers), Dublin, Ireland. Association for\nComputational Linguistics.\nAdam Davies and Ashkan Khakzar. 2024. The cognitive revolution in interpretability: From ex-\nplaining behavior to interpreting representations and algorithms.\nLi Deng. 2012. The mnist database of handwritten digit images for machine learning research. IEEE\nSignal Processing Magazine, 29(6):141–142.\nMaximilian Dreyer, Erblina Purelku, Johanna Vielhaben, Wojciech Samek, and Sebastian La-\npuschkin. 2024. Pure: Turning polysemantic neurons into pure features by identifying relevant\ncircuits.\nJacob Dunefsky, Philippe Chlenski, and Neel Nanda. 2024. Transcoders find interpretable llm fea-\nture circuits.\n12\n\n\nPublished as a conference paper at ICLR 2025\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann,\nAmanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep\nGanguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt,\nKamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and\nChris Olah. 2021. A mathematical framework for transformer circuits. Transformer Circuits\nThread. Https://transformer-circuits.pub/2021/framework/index.html.\nJonathan Frankle and Michael Carbin. 2019. The lottery ticket hypothesis: Finding sparse, trainable\nneural networks. In ICLR. OpenReview.net.\nMichael Friedman. 1974. Explanation and scientific understanding. Journal of Philosophy, 71(1):5–\n19.\nTrevor Gale, Erich Elsen, and Sara Hooker. 2019. The state of sparsity in deep neural networks.\nAtticus Geiger, Zhengxuan Wu, Karel D’Oosterlinck, Elisa Kreiss, Noah D. Goodman, Thomas\nIcard, and Christopher Potts. 2022a. Faithful, interpretable model explanations via causal ab-\nstraction. Stanford AI Lab Blog.\nAtticus Geiger, Zhengxuan Wu, Hanson Lu, Josh Rozner, Elisa Kreiss, Thomas Icard, Noah Good-\nman, and Christopher Potts. 2022b. Inducing causal structure for interpretable neural networks.\nIn Proceedings of the 39th International Conference on Machine Learning, volume 162 of Pro-\nceedings of Machine Learning Research, pages 7324–7338. PMLR.\nAtticus Geiger, Zhengxuan Wu, Christopher Potts, Thomas Icard, and Noah Goodman. 2024. Find-\ning alignments between interpretable causal variables and distributed neural representations. In\nProceedings of the Third Conference on Causal Learning and Reasoning, volume 236 of Pro-\nceedings of Machine Learning Research, pages 160–187. PMLR.\nMor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. 2023. Dissecting recall of fac-\ntual associations in auto-regressive language models. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing, pages 12216–12235, Singapore. Associa-\ntion for Computational Linguistics.\nAlison Gopnik. 2000. Explanation as orgasm and the drive for causal knowledge: The function,\nevolution, and phenomenology of the theory formation system. In Explanation and cognition.,\npages 299–323. The MIT Press, Cambridge, MA, US.\nMichael Hanna, Sandro Pezzelle, and Yonatan Belinkov. 2024. Have faith in faithfulness: Going\nbeyond circuit overlap when finding model mechanisms.\nGanesh Jawahar, Benoˆıt Sagot, and Djam´e Seddah. 2019. What does BERT learn about the structure\nof language? In Proceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics, Florence, Italy. Association for Computational Linguistics.\nStephen H. Kellert, Helen Longino, and C. Kenneth Waters. 2006. Introduction: The pluralist stance.\nIn Stephen H. Kellert, Helen Longino, and C. Kenneth Waters, editors, Scientific Pluralism, pages\nvii–xxix. University of Minnesota Press.\nAshkan Khakzar, Soroosh Baselizadeh, Saurabh Khanduja, Christian Rupprecht, Seong Tae Kim,\nand Nassir Navab. 2021. Neural response interpretation through the lens of critical pathways. In\n2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 13523–\n13533.\nPhilip Kitcher. 1962. Explanatory unification and the causal structure of the world. In Philip Kitcher\nand Wesley C. Salmon, editors, Scientific Explanation, pages 410–505. Univ of Minnesota Pr.\nPhilip Kitcher. 1981. Explanatory unification. Philosophy of Science, 48(4):507–531.\nJ´anos Kram´ar, Tom Lieberum, Rohin Shah, and Neel Nanda. 2024. Atp*: An efficient and scalable\nmethod for localizing llm behaviour to components.\n13\n\n\nPublished as a conference paper at ICLR 2025\nArie W. Kruglanski. 1989.\nLay epistemics and human knowledge: Cognitive and motivational\nbases. Lay epistemics and human knowledge: Cognitive and motivational bases. Plenum Press,\nNew York, NY, US.\nKestutis Kveraga, Avniel S. Ghuman, and Moshe Bar. 2007. Top-down predictions in the cognitive\nbrain. Brain and Cognition, 65(2):145–168.\nFangshuo Liao and Anastasios Kyrillidis. 2022. On the convergence of shallow neural network\ntraining with randomly masked neurons.\nBohan Liu, Zijie Zhang, Peixiong He, Zhensen Wang, Yang Xiao, Ruimeng Ye, Yang Zhou, Wei-\nShinn Ku, and Bo Hui. 2024. A survey of lottery ticket hypothesis.\nXinyin Ma, Gongfan Fang, and Xinchao Wang. 2023. Llm-pruner: On the structural pruning of\nlarge language models. In Advances in Neural Information Processing Systems.\nAleksandar Makelov, Georg Lange, and Neel Nanda. 2023. Is this the subspace you are looking for?\nan interpretability illusion for subspace activation patching.\nSamuel Marks, Can Rager, Eric J. Michaud, Yonatan Belinkov, David Bau, and Aaron Mueller.\n2024. Sparse feature circuits: Discovering and editing interpretable causal graphs in language\nmodels.\nD. Marr and T. Poggio. 1976. From understanding computation to understanding neural circuitry.\nTechnical report, USA.\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022.\nLocating and edit-\ning factual associations in GPT.\nAdvances in Neural Information Processing Systems, 36.\nArXiv:2202.05262.\nChristoph Molnar. 2022. Interpretable Machine Learning, 2 edition.\nGiovanni Monea, Maxime Peyrard, Martin Josifoski, Vishrav Chaudhary, Jason Eisner, Emre\nKıcıman, Hamid Palangi, Barun Patra, and Robert West. 2024. A glitch in the matrix? locat-\ning and detecting language model grounding with fakepedia.\nHiroshi Morioka and Aapo Hyv¨arinen. 2024. Causal representation learning made identifiable by\ngrouping of observational variables.\nIn Proceedings of the 41st International Conference on\nMachine Learning, ICML’24. JMLR.org.\nChris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter.\n2020. Zoom in: An introduction to circuits. Distill. Https://distill.pub/2020/circuits/zoom-in.\nChris Olah, Alexander Mordvintsev, and Ludwig Schubert. 2017. Feature visualization. Distill.\nHttps://distill.pub/2017/feature-visualization.\nChris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine\nYe, and Alexander Mordvintsev. 2018.\nThe building blocks of interpretability.\nDistill.\nHttps://distill.pub/2018/building-blocks.\nJudea Pearl. 2009. Causality: Models, Reasoning and Inference, 2nd edition. Cambridge University\nPress, USA.\nMaxime Peyrard, Beatriz Borges, Kristina Gligori´c, and Robert West. 2021. Laughing heads: Can\ntransformers detect what makes a sentence funny? In Proceedings of the Thirtieth International\nJoint Conference on Artificial Intelligence, IJCAI-21, pages 3899–3905. International Joint Con-\nferences on Artificial Intelligence Organization. Main Track.\nAngela Potochnik. 2017.\nIdealization and the Aims of Science.\nUniversity of Chicago Press,\nChicago.\nThomas J. Rothenberg. 1971. Identification in parametric models. Econometrica, 39(3):577–591.\n14\n\n\nPublished as a conference paper at ICLR 2025\nP. K. Rubenstein, S. Weichwald, S. Bongers, J. M. Mooij, D. Janzing, M. Grosse-Wentrup, and\nB. Sch¨olkopf. 2017. Causal consistency of structural equation models. In Proceedings of the 33rd\nConference on Uncertainty in Artificial Intelligence (UAI), page ID 11. *equal contribution.\nHassan Sajjad, Nadir Durrani, and Fahim Dalvi. 2022. Neuron-level Interpretation of Deep NLP\nModels: A Survey. Transactions of the Association for Computational Linguistics, 10:1285–\n1303.\nGerhard Schurz. 1999. Explanation as unification. Synthese, 120(1):95–114.\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2014. Deep inside convolutional net-\nworks: Visualising image classification models and saliency maps.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\n2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research, 15(56):1929–1958.\nKyle Stanford. 2023. Underdetermination of Scientific Theory. In Edward N. Zalta and Uri Nodel-\nman, editors, The Stanford Encyclopedia of Philosophy, Summer 2023 edition. Metaphysics Re-\nsearch Lab, Stanford University.\nMingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter. 2024. A simple and effective pruning\napproach for large language models.\nAaquib Syed, Can Rager, and Arthur Conmy. 2023. Attribution patching outperforms automated\ncircuit discovery.\nAdly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen,\nAdam Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones, Hoagy Cunningham, Nicholas L\nTurner, Callum McDougall, Monte MacDiarmid, C. Daniel Freeman, Theodore R. Sumers, Ed-\nward Rees, Joshua Batson, Adam Jermyn, Shan Carter, Chris Olah, and Tom Henighan. 2024.\nScaling monosemanticity: Extracting interpretable features from claude 3 sonnet. Transformer\nCircuits Thread.\nDamien Teney, Maxime Peyrard, and Ehsan Abbasnejad. 2022. Predicting is not understanding:\nRecognizing and addressing underspecification in machine learning. In European Conference on\nComputer Vision, pages 458–476. Springer.\nJ. D. Trout. 2007. The psychology of scientific explanation. Philosophy Compass, 2(3):564–591.\nBas Van Fraassen. 1988. The pragmatic theory of explanation. Theories of explanation, 8:135–155.\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and\nStuart Shieber. 2020. Investigating gender bias in language models using causal mediation anal-\nysis. In Advances in Neural Information Processing Systems, volume 33, pages 12388–12401.\nCurran Associates, Inc.\nMartina G. Vilas, Federico Adolfi, David Poeppel, and Gemma Roig. 2024. Position: An inner\ninterpretability framework for ai inspired by lessons from cognitive neuroscience.\nMartina G. Vilas, Timothy Schauml¨offel, and Gemma Roig. 2023. Analyzing vision transformers\nfor image classification in class embedding space. In Advances in Neural Information Processing\nSystems, volume 36, pages 40030–40041.\nKevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. 2022.\nInterpretability in the wild: a circuit for indirect object identification in gpt-2 small.\nJonathan Waskan. 2024. Experimental Philosophy of Science: Scientific Explanation, pages 237–\n262. De Gruyter, Berlin, Boston.\nJames Woodward and Lauren Ross. 2021. Scientific Explanation. In Edward N. Zalta, editor, The\nStanford Encyclopedia of Philosophy, Summer 2021 edition. Metaphysics Research Lab, Stanford\nUniversity.\n15\n\n\nPublished as a conference paper at ICLR 2025\nZhengxuan Wu, Atticus Geiger, Jing Huang, Aryaman Arora, Thomas Icard, Christopher Potts, and\nNoah D. Goodman. 2024. A reply to makelov et al. (2023)’s ”interpretability illusion” arguments.\nZhengxuan Wu, Atticus Geiger, Thomas Icard, Christopher Potts, and Noah Goodman. 2023. Inter-\npretability at scale: Identifying causal mechanisms in alpaca. In Advances in Neural Information\nProcessing Systems, volume 36, pages 78205–78226. Curran Associates, Inc.\nBinhang Yuan, Anastasios Kyrillidis, and Christopher M. Jermaine. 2019. Distributed learning of\ndeep neural networks using independent subnet training. CoRR, abs/1910.02120.\nRafael Yuste. 2008. Circuit neuroscience: the road ahead. Frontiers in neuroscience, 2:1038.\nMatthew D. Zeiler and Rob Fergus. 2014. Visualizing and understanding convolutional networks.\nIn Computer Vision - ECCV 2014 - 13th European Conference, Zurich, Switzerland, September\n6-12, 2014, Proceedings, Part I, volume 8689 of Lecture Notes in Computer Science, pages 818–\n833. Springer.\nB. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. 2016. Learning deep features for dis-\ncriminative localization. In 2016 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 2921–2929, Los Alamitos, CA, USA. IEEE Computer Society.\n16\n\n\nPublished as a conference paper at ICLR 2025\nA\nFORMAL DEFINITION OF IIA\nThe definitions in this section are adapted from Geiger et al. (2022b). We begin by setting notation\nconventions.\nLet N be a neural network, and A be a high-level algorithm.\nLet τ be a mapping between low-level neuron groups {Vj} of N and the values of their correspond-\ning high-level variables {Aj} in A.\nLet Vin (resp. Vout) be the neuron groups corresponding to the inputs (resp. outputs) of N.\nLet Ain (resp. Aout) be the variables in A with no parents (resp. no children).\nNotation (Value reading). Let vin ∈R|Vin| be some possible input values of the network N. We note\nN[vin, Vj] the values of the activations of Vj that are obtained when setting the values of Vin to vin\nand running the computation graph of N.\nSimilarly, let ain ∈R|Ain| be some possible values of the variables of A with no parents. We note\nA[ain, Aj] the values of the variable Aj that are obtained when setting the values of Ain to ain and\nrunning the algorithm A.\nNotation (Intervention). Let vj ∈R|Vj| be some possible activations of the variable Vj in the\nnetwork N. We note NVj←vj a copy of N, in which the activations of Vj are forcibly set to the\nvalue vj during the computation.\nSimilarly, let aj ∈R|aj| be a possible value of the variable Aj in the algorithm A. We note AAj←aj\na copy of A, in which the value of Aj is forcibly set to the value aj when the algorithm is run.\nThis notion is aligned with the do-operator (Pearl, 2009).\nDefinition 5 (Intervention interchange). Let basel,in, sourcel,in ∈R|Vj| be some possible input values\nof the network N. We call low-level intervention interchange the quantity:\nIIlow(N, basel,in, sourcel,in, Vk) = (NVk←N[sourcel,in,Vk])[basel,in, Vout]\nSimilarly, let baseh,in, sourceh,in ∈R|Aj| be some possible values of the variables in A with no\nparent. We call high-level intervention interchange the quantity:\nIIhigh(N, baseh,in, sourceh,in, Ak) = (AAk←N[sourceh,in,Ak])[baseh,in, Aout]\nThis corresponds to the notion of counterfactual intervention: After running the algorithm (or net-\nwork) on a set of inputs (source) and recording the value of a given variable, we execute the algo-\nrithm again on a different set of inputs (base) but restore the value of the variable from the first run\nduring the computation. The system is now in a counterfactual state, and we measure its new output.\nDefinition 6 (IIA). Let Val(Aj) be the set of possible values of Aj, and Val(Ain) = Q\nV ∈Vin Val(V )\nbe the set of possible combinations of values of the variables in A with no parents. Let Ak be a\nhigh-level variable of A.\nThe intervention interchange accuracy of the mapping τ for the variable Ak is the quantity:\nIIA(N, A, Ak, τ) =\n1\n|Val(Ain)|2\nX\nb,s∈Val(Ain)\n1 [IIhigh(A, b, s, Ak) = IIlow(N, b, s, Vk)]\nB\nOUTPUT EXAMPLES\nThis section contains additional examples of computational abstractions found by both strategies.\nSpecifically, we report abstractions found in a single neural network trained on the XOR gate with\nk = 3 and n = 1 with a loss cutoff of 10−3.\n17\n\n\nPublished as a conference paper at ICLR 2025\nB.1\nCIRCUIT-FIRST APPROACH\nAn exhaustive pass of the circuit-first approach (with no minimal sparsity threshold) yielded 59\ncircuits (*where*). We depict in Figure 4 the 12 most sparse circuits found.\nFigure 4: The 12 most sparse circuits found in the example network.\nWhen searching for interpretations (*what*) in all 59 circuits, we find 114,230 interpretations. We\nthen focus on circuit 8 (second row, last column in Figure 4, chosen for illustration purposes as it\nyields 24 valid interpretations, which we fully list in Table 1. Each of these interpretations leads\nto a different explanation of the network; for example, interpretation 1 corresponds to the formula\n¬(¬(A ∧B) →¬(A ∨B)), while interpretation 2 corresponds to ¬((¬(A ∧B) ∨¬(A ∨B)) →\n(¬(A ∧B) →¬(A ∨B))).\nB.2\nALGORITHM-FIRST APPROACH\nIn the algorithm-first approach, exhaustive enumeration yields 56 possible logic formulas for the\nXOR gate with a depth of 3 (excluding commutative-invariant formulas). Four of these formulas\nproduce valid mappings for the neural network. Those mappings are listed in Table 2.\nC\nIDENTIFIABILITY IN CIRCUIT LITERATURE\nIdentifiability is, to the best of our knowledge, never stated as an explicit assumption in existing\nworks about circuits. In this section, we list examples from the literature that indicate that it is\nnonetheless typically taken for granted:\n• Cammarata et al. (2021): ”the curve circuit” (multiple occurrences)\n• Wang et al. (2022): ”discover the circuit”, ”discovering the circuit”, ”uncover the circuit”\n• Kram´ar et al. (2024): ”we investigate the circuit underlying multiple-choice question-\nanswering”\n• Conmy et al. (2023): ”Choosing a clearly defined behavior means that the circuit will be\neasier to interpret than a mix of circuits corresponding to a vague behavior”, ”ACDC [...]\nfully recovers the circuit of toy model”.\n• Hanna et al. (2024): ”We next search for the circuit responsible for computing this task”\n• Marks et al. (2024): ”The circuit for agreement across a prepositional phrase (Figure 12)”\n18\n\n\nPublished as a conference paper at ICLR 2025\nNeuron (1, 0)\nNeuron (1, 1)\nNeuron (2, 0)\nNeuron (2, 1)\nNeuron (3, 0)\n#\nGate\nSep.\nGate\nSep.\nGate\nSep.\nGate\nSep.\nGate\nSep.\n1\n-0.003\nNAND\n0.693\nNOR\n0.151\nIMP\n0.777\nOR\n0.5\nNOT A\n2\n-0.003\nNAND\n0.693\nNOR\n0.151\nIMP\n0.777\nOR\n0.5\nRNIMP\n3\n-0.003\nNAND\n0.693\nNOR\n0.151\nIMP\n0.777\nA\n0.5\nNOT A\n4\n-0.003\nNAND\n0.693\nNOR\n0.151\nIMP\n0.777\nA\n0.5\nRNIMP\n5\n-0.003\nNAND\n0.693\nNOR\n0.151\nIMP\n0.987\nNIMP\n0.5\nIMP\n6\n-0.003\nNAND\n0.693\nNOR\n0.151\nIMP\n0.987\nNIMP\n0.5\nNOT A\n7\n-0.003\nNAND\n0.693\nNOR\n0.151\nIMP\n0.987\nNIMP\n0.5\nB\n8\n-0.003\nNAND\n0.693\nNOR\n0.151\nIMP\n0.987\nNIMP\n0.5\nRNIMP\n9\n-0.003\nNAND\n0.693\nNOR\n0.397\nNOT A\n0.987\nNIMP\n0.5\nB\n10\n-0.003\nNAND\n0.693\nNOR\n0.397\nNOT A\n0.987\nNIMP\n0.5\nRNIMP\n11\n-0.003\nNAND\n0.693\nNOR\n0.397\nNOR\n0.987\nNIMP\n0.5\nB\n12\n-0.003\nNAND\n0.693\nNOR\n0.397\nNOR\n0.987\nNIMP\n0.5\nRNIMP\n13\n0.321\nNOR\n0.230\nNAND\n0.151\nRIMP\n0.777\nOR\n0.5\nNOT A\n14\n0.321\nNOR\n0.230\nNAND\n0.151\nRIMP\n0.777\nOR\n0.5\nRNIMP\n15\n0.321\nNOR\n0.230\nNAND\n0.151\nRIMP\n0.777\nB\n0.5\nNOT A\n16\n0.321\nNOR\n0.230\nNAND\n0.151\nRIMP\n0.777\nB\n0.5\nRNIMP\n17\n0.321\nNOR\n0.230\nNAND\n0.151\nRIMP\n0.987\nRNIMP\n0.5\nIMP\n18\n0.321\nNOR\n0.230\nNAND\n0.151\nRIMP\n0.987\nRNIMP\n0.5\nNOT A\n19\n0.321\nNOR\n0.230\nNAND\n0.151\nRIMP\n0.987\nRNIMP\n0.5\nB\n20\n0.321\nNOR\n0.230\nNAND\n0.151\nRIMP\n0.987\nRNIMP\n0.5\nRNIMP\n21\n0.321\nNOR\n0.230\nNAND\n0.397\nNOT B\n0.987\nRNIMP\n0.5\nB\n22\n0.321\nNOR\n0.230\nNAND\n0.397\nNOT B\n0.987\nRNIMP\n0.5\nRNIMP\n23\n0.321\nNOR\n0.230\nNAND\n0.397\nNOR\n0.987\nRNIMP\n0.5\nB\n24\n0.321\nNOR\n0.230\nNAND\n0.397\nNOR\n0.987\nRNIMP\n0.5\nRNIMP\nTable 1: The list of interpretations found for circuit 8 of 59 in the example network. For each\ninterpretation, the four intermediate neurons and the output one are assigned a logic gate and a\nseparation boundary. Each neuron is represented by its layer and position in the layer (indexed from\n0), as read from left to right and from top to bottom in Figure 4. IMP refers to the implication gate\n(A implies B), NIMP to its negation, and RIMP and RNIMP refer to the reversed implication gate\n(B implies A) and its negation.\nFormula 1: ¬(A ∧B) ∧(A ∨B)\nMapping\nA ∨B\n¬(A ∧B)\n1\nNeuron (1, 2)\nNeuron (1, 1)\n2\nNeuron (1, 0)\nNeuron (1, 1)\nFormula 2: ¬((A ∧B) ∨¬(A ∨B))\nMapping\nA ∧B\n¬(A ∨B)\n1\nNeuron (1, 1)\nNeuron (1, 2)\n2\nNeuron (1, 1)\nNeuron (1, 0)\nFormula 39: ¬((A ∧B) ∨¬(A ∨B)) ∧(A ∨B)\nMapping\nA ∧B\nA ∨B\n¬((A ∧B) ∨¬(A ∨B))\n¬(A ∨B)\n1\nNeuron (1, 1)\nNeuron (1, 2)\nNeuron (2, 0)\nNeuron (1, 0)\n2\nNeuron (1, 1)\nNeuron (1, 2)\nNeuron (2, 1)\nNeuron (1, 0)\n3\nNeuron (1, 1)\nNeuron (1, 0)\nNeuron (2, 0)\nNeuron (1, 2)\n4\nNeuron (1, 1)\nNeuron (1, 0)\nNeuron (2, 1)\nNeuron (1, 2)\nFormula 40: ¬(((A ∧B) ∨¬(A ∨B)) ∨¬(A ∨B))\nMapping\n(A ∧B) ∨¬(A ∨B)\nA ∧B\n¬(A ∨B) (left)\n¬(A ∨B) (left)\n1\nNeuron (2, 1)\nNeuron (1, 1)\nNeuron (1, 0)\nNeuron (1, 2)\n2\nNeuron (2, 1)\nNeuron (1, 1)\nNeuron (1, 2)\nNeuron (1, 0)\n3\nNeuron (2, 0)\nNeuron (1, 1)\nNeuron (1, 0)\nNeuron (1, 2)\n4\nNeuron (2, 0)\nNeuron (1, 1)\nNeuron (1, 2)\nNeuron (1, 0)\nTable 2: The list of valid minimal mappings for the example network. For each intermediate node\nof each formula, we specify which neuron corresponds to that node.\n19\n\n\nPublished as a conference paper at ICLR 2025\nD\nADDITIONAL PLOTS\nD.1\nTARGET GATE VARIATION\nFigure 5 contains the total number of computational abstractions obtained after fixing k = 3 and\nn = 1 and sampling the target gate from the following list: AND, OR, XOR, IMP.\nFigure 5: Total number of interpretations found in the circuit-first approach and of mappings found\nin the algorithm-first approach, grouped by target gate.\nFigure 6 contains the results of the same experiment but displays separate plots for the number\nof circuits and interpretations per circuit (resp. algorithms and mappings per algorithm) for each\nnetwork.\nFigure 6: Left: Number of circuits and average interpretations per circuit found in the circuit-first\napproach. Right: Number of algorithms and average mappings per algorithm found in the algorithm-\nfirst approach (right). Results are grouped by target gate over 100 experiments.\nD.2\nARCHITECTURE SIZE\nFigure 7 contains additional plots for the experiment described in 4.1.1, in which we vary the archi-\ntecture size.\nD.3\nMULTI-TASK TRAINING\nWe report in Figure 8 additional plots for the experiment in which we vary the number of gates the\nmodel is being trained on (described in 4.1.2).\n20\n\n\nPublished as a conference paper at ICLR 2025\nFigure 7: Number of abstractions found in the circuit-first approach (left) and the algorithm-first\napproach (right) as a function of the architecture size.\nFigure 8: Number of abstractions found in the circuit-first approach (left) and the algorithm-first\napproach (right) as a function of the number of training tasks.\n21\n\n\nPublished as a conference paper at ICLR 2025\nD.4\nLOSS CUTOFF\nWe report in Figure 9 plots for the experiment in which we apply the basic setup with n = 1 and k3\non a set of networks, trained while varying the loss cutoff from 10−1 to 10−6. For the algorithm-first\napproach, a two-sample t-test indicates a modest but significant decrease in the number of algorithms\nfound when the loss cutoff is lower or equal to 10−5. In contrast, the number of mappings per\nalgorithm does not statistically vary. For the circuit-first approach, significantly fewer circuits and\ninterpretations per circuit are found when the loss cutoff is high (0.1), but values do not otherwise\nvary for lower loss values. In addition, we found that multiple computational abstractions can still\nbe identified in randomly initialized networks that happen to implement a logic gate (i.e. without\ntraining).\nFigure 9: Number of abstractions found in the circuit-first approach (left) and the algorithm-first\napproach (right) as a function of the neural network’s training loss cutoff.\nD.5\nEFFECT OF NOISE\nWe report in Table 3 the number of abstractions found in the basic setup (without noisy inputs) for\nboth approaches, and compare it to the results found when repeating the same setup while adding\nGaussian noise from N(0, 0.1) to the inputs at training time. We also report the p-value obtained\nwith Welch’s t-test.\nCriterion\nBasic setup\nNoisy setup\np-value\nAlgorithms\n2.70\n2.56\n0.766\nMinimal mappings/algorithm (avg)\n2.84\n3.05\n0.617\nTotal mappings\n9.86\n9.89\n0.959\nCircuits\n52.3\n28.6\n< 0.001\nInterpretations/circuit (avg)\n2,270\n16,400\n< 0.001\nTotal interpretations\n107,000\n285,000\n< 0.001\nTable 3: Number of abstractions found for both approaches in each setup (averaged over all net-\nworks).\nD.6\nTRAINING DISTRIBUTION\nThe influence of the training distribution was investigated through the following procedure:\n1. Sample a random neural network NN and target gate with n = 1 and k = 3\n2. Draw x1, . . . , x4 from U[0,1]\n3. Train NN on a skewed input distribution, with weights\nxi\nP\ni xi for each input i, and a loss\ncutoff of 10−3.\n4. Exhaustively enumerate all circuits, interpretations, algorithms, and mappings as in the\nbasic setup.\n22\n\n\nPublished as a conference paper at ICLR 2025\n5. Repeat from step 1.\nWe repeated those steps 100 times, resulting in varying training distributions with joint entropy\nvarying from 0.8 to 2.0 bits. We then performed a linear regression of the resulting counts as a\nfunction of the distribution’s joint entropy. We give in Table 4 the results for this experiment, which\nshow that the results of the algorithm-first approach do not significantly depend on the training\ndistribution. On the other hand, having an unbalanced distribution causes an increase in the number\nof circuits and total interpretations found in the circuit-first approach, but a decrease in the number\nof interpretations per circuit.\nCriterion\nSlope\nIntercept\np-value\nAlgorithms\n-0.21\n2.86\n0.857\nMinimal mappings/algorithm (avg)\n-0.14\n1.42\n0.807\nTotal mappings\n-0.17\n8.82\n0.974\nCircuits\n-326\n556\n0.001\nInterpretations/circuit (avg)\n1,235\n139\n0.029\nTotal interpretations\n-174,000\n367,000\n0.018\nTable 4: Linear regression performed on the number of computational abstractions as a function of\nthe training distribution’s joint entropy (in bits).\n23\n\n\n"}
{"text": "Reducing Reward Dependence in RL Through Adaptive\nConfidence Discounting\nMuhammed Yusuf Saticia,* and David L. Robertsb\naDepartment of Computer Science, NCSU\nbDepartment of Computer Science, NCSU\nAbstract.\nIn human-in-the-loop reinforcement learning or environ-\nments where calculating a reward is expensive, the costly rewards can\nmake learning efficiency challenging to achieve. The cost of obtain-\ning feedback from humans or calculating expensive rewards means\nalgorithms receiving feedback at every step of long training sessions\nmay be infeasible, which may limit agents’ abilities to efficiently\nimprove performance. Our aim is to reduce the reliance of learning\nagents on humans or expensive rewards, improving the efficiency of\nlearning while maintaining the quality of the learned policy. We offer\na novel reinforcement learning algorithm that requests a reward only\nwhen its knowledge of the value of actions in an environment state\nis low. Our approach uses a reward function model as a proxy for\nhuman-delivered or expensive rewards when confidence is high, and\nasks for those explicit rewards only when there is low confidence in\nthe model’s predicted rewards and/or action selection. By reducing\ndependence on the expensive-to-obtain rewards, we are able to learn\nefficiently in settings where the logistics or expense of obtaining re-\nwards may otherwise prohibit it. In our experiments our approach\nobtains comparable performance to a baseline in terms of return and\nnumber of episodes required to learn, but achieves that performance\nwith as few as 20% of the rewards.\n1\nIntroduction\nHaving the ability to learn from sparse rewards is a crucial aspect\nof real-world applications of reinforcement learning (RL) since, as\nopposed to the proxy reward functions defined in computer simu-\nlations, the real world events do not always return immediate and\naccurate feedback signals to the learner. These sparse rewards present\nchallenges to the RL agents due to their irregular and imperfect distri-\nbution. Many sparse-reward RL algorithms introduce artificial rewards\nto densify the reward function in the hope of facilitating the learning\nprocess and overcoming the sporadic nature of the reward signal. We\ntake the opposite approach in this research. We assume the agent has\nthe ability to request reward signals at will from the environment.\nWe start with relatively dense reward signals at the beginning of the\ntraining and make the reward signals sparser in line with the improve-\nments in the agent’s learning model. In this way, we prevent the agent\nfrom receiving rewards in the parts of the state space where the agent\nalready has a good understanding of its policy and gather reward\nsignals only for the states that the agent has a low confidence in its\naction selection. The agent depends on the external feedback at the\n∗Corresponding Author. Email: msatici@ncsu.edu.\nearly stages of its learning but does not require frequent feedback\nsignals once it obtains a certain amount and variety of experiences.\nOur algorithm, at a high level, measures the learner’s confidence\nusing the output distribution(s) of the agent’s model(s). We assume\nthe agent has the ability to request feedback from the environment for\nthe given state and train a reward function model using the results of\nthose requests. The agent is trained using regular deep RL algorithms,\nbut for the states with no requested feedback value, the agent uses the\noutput of the reward function model as the feedback. In this way, the\nagent skips the feedback from the environment for the states it has\nhigh confidence.\nWe compare two different formulations for measuring confidence.\nThe first uses the entropy derived from the output probability dis-\ntribution of the actor model (or the Q-value distribution for DQN\narchitectures)—low entropy suggests high confidence in action selec-\ntion. The second uses entropy derived from the output distribution\nof the reward function model in addition to the entropy of the actor\nmodel. This combination of reward and action entropies measures\nthe inaccuracies in both the agent’s learning and learning the re-\nward function. We also compare two different regularization terms\nto lessen the reduction in environment rewards. We test all of our\napproaches in three domains: a discrete-space sparse-reward grid-\nworld, a continuous-space sparse-reward robotics environment, and a\ncontinuous-space dense-reward highway environment. We evaluate\nour approach against deep Q-network (DQN) [14], actor-critic net-\nwork (A2C) [15], and hindsight experience replay (HER) [1]. Results\nshow that our approach at worst matches the cumulative reward of the\nbaselines while greatly reducing the number of environment rewards.\n2\nRelated Work\nOne challenge in applying RL to real-world problems is the cost of\nsample collection. Sample efficient RL algorithms have been proposed\nin recent years to lower the sample complexity of deep RL algorithms\nin real world scenarios [3, 13, 24]. These approaches aim to mitigate\nthe challenges posed by the high costs associated with collecting real-\nworld samples for applications of RL. In the case of our algorithm,\ninstead of focusing on the cost of interacting with the environment,\nwe investigate the problem of improving the feedback efficiency by\nmaking the training process as independent as possible from the\nrewards returned by the environment. The feedback efficiency differs\nfrom the sample efficiency in that feedback efficient methods afford\nrequesting and training on many transitions from the environment\nas opposed to sample efficient algorithms but they cannot afford\narXiv:2502.21181v1  [cs.LG]  28 Feb 2025\n\n\nasking guidance from a human expert due to the high cost of feedback\nretrieval associated with it. There have been few deep-RL algorithms\naiming to learn with limited feedback derived from human preferences\n[7, 9, 17] but none of those approaches offer a framework that does\nnot depend on human feedback elicitation. We formulate the feedback\nretrieval problem as an implicit curriculum and try to offer a general\nframework that works on both human-designed and artificial feedback.\nAlgorithms that deal with sparse rewards focus on encouraging\nexploration to increase the agent’s chance of attaining useful reward\nsignals. One common approach defines a curiosity function to measure\nthe novelty of the visited states and provides the agent additional\nreward signals if it visits a novel state of the environment [18]. Another\napproach uses the error of the neural network as an exploration bonus\nadded to the reward function to encourage the agent to visit the areas\nof the state space with high error in the prediction of the observation\nfeatures [4]. A different approach employs a HER buffer to sample\nadditional transitions containing the future state of the agent as the\ngoal for the transition at hand [1]. These methods try to address the\nsparsity of rewards as an issue of limited exploration and aims to\nimprove the agent’s performance by bringing it closer to unknown\nregions of state space. They do not change the number of rewards,\nbut try to put the agent in a position where it could gather the most\nnovel feedback. Our algorithm does not encourage exploration; rather,\nit prevents the agent from receiving unnecessary rewards.\nThere are also human-guided RL algorithms that attempt to model\nthe human feedback using a reward predictor and use this reward\nmodel to train the agent without having the need to design a proxy\nreward function. These algorithms are typically used for complex\nreal-world environments where it would be difficult to craft a re-\nward function that encapsulates all aspects of the RL objective. Deep\nTAMER learns a reward function from human-guided feedback and\nuses this reward function in training the behavioral policy of the\nagent [23]. Similarly, Christiano et al. trains a reward function pre-\ndictor using human-guided and synthetic feedback [6]. Liang et al.\nuses disagreement between reward functions to increase exploration\nin human preference-based rl algorithms [11] and Liu et al. optimizes\nthe reward model, and the agent in parallel in a bi-objective optimiza-\ntion process rather than learning a reward model before the agent\ntraining, which is the case for the majority of the human-guided RL\nalgorithms [12]. These approaches show that it is possible to derive\na useful reward function model through human feedback elicitation\nbut they make no attempt at reducing the agent’s dependency on the\nfeedback coming from the environment. They also only depend on\nthe feedback from the reward predictor in training their agent. On\nthe other hand, we use both the reward from the environment and the\nfeedback from the reward function model in training and show it is\npossible to dramatically reduce the environment rewards used during\ntraining.\nFinally, there are inverse RL (IRL) algorithms that build a model\nof the reward function from demonstrations. These algorithms share\ncommon elements with the sparse-reward RL in that they both aim\nto obtain a reward predictor. However, IRL algorithms address the\nproblem of inferring a good reward function from demonstrations\nwhereas sparse-reward RL uses a reward model for making the reward\nfunction less sparse or more efficient. Although IRL algorithms could\nbe used to deal with sparse-reward learning, they cannot attain that\ngoal without re-purposing their inferred reward model. See Arora et\nal. [2] for a detailed analysis of IRL.\n3\nProblem Formulation\nHere we provide background on the RL method and formally define\nthe feedback diminution problem.\n3.1\nReinforcement Learning\nWe model learning as a MDP M = < S, A, T, R, si, Sg >, a tuple\nconsisting of a set of states S, a set of actions A, a transition function\nT, a reward function R, an initial state si, and a set of terminal states\nSg. The transition function T : S×A×S →[0, 1] corresponds to the\nprobability of transitioning from a state s ∈S to another state s′ ∈S\nusing a valid action a ∈A at state s. All of the environments we use\nare deterministic, so taking action a in state s always transitions into\nthe same resulting state s′—although that is not a requirement for our\nalgorithm. The reward function R : S × A × S →F maps a state,\naction, state tuple to the real-valued reward the learner receives. The\npolicy π : S →A maps states to actions. The cumulative reward G\nat time t is the discounted sum of all rewards the agent receives from\nt until it reaches a terminal state, Gt = Pτ−t\nk=0 δk ∗Rt+k, where δ is\nthe discount factor, τ is the time to reach a terminal state, and Rt+k\nis the feedback received in state st+k [20]. The agent’s objective is to\nlearn the optimal policy π∗\nM that maximizes G.\nWe train using deep Q-networks (DQN) [14] for discrete space\nenvironments. We use two four-layered, fully-connected, feed-forward\nDQNs. The networks take the states as input and output Q-value\nestimates for each available action. One neural network serves as the\nlearned model, and the other provides target Q-value estimates. The\nloss function is\nL(θ) = Es,a,r,trm,s′∼RB[(r + δ∗maxa′Q(s′, a′; θ−)\n−Q(s, a; θ))2],\n(1)\nwhere θ is the learned model weights, θ−is the target model weights,\nand δ is the discount factor [14].\nFor continuous action spaces, we employ an actor-critic architecture\nsimilar to [15]. We use a four-layered, fully-connected, feed-forward\nnetwork for the actor and critic. The critic receives the state as its input\nand outputs the value function estimate. The actor network outputs\ntwo real vectors which we treat as the mean and standard deviation of\nthe multi-dimensional normal distribution that we sample the actions\nfrom. We use the advantage loss defined in [15] to train the actor,\nand we train the critic network using the mean square error given in\nEq. 1. In the case of the robotics environment that we discuss in the\nresults section, in addition to the given actor-critic architecture, we\nalso employ a hindsight experience replay buffer (HER) to up-sample\nthe positive reinforcement transitions. We use the future strategy with\nk = 4 for the HER algorithm since that strategy is shown to produce\nthe best results in [1]. We also use the same loss function as [1] in\nthe robotics environment while retaining the aforementioned actor\nnetwork for the sampling of the action values.\nAt each training step, the agent takes a single action in the envi-\nronment, records the tuple (s, a, r, trm, s′) into a replay buffer RB\n(where trm indicates whether s is a terminal state), randomly samples\ntuples from the RB, and performs a single batch update on the learned\nmodel. The target model weights are updated using the weights of the\nlearned model at the end of each episode.\n3.2\nFeedback Diminution Problem\nIn environments where it is difficult to retrieve rewards but relatively\neasy to take actions and observe new states, there is a need for using\n\n\nfeedback efficient RL algorithms. Real world applications of reinforce-\nment learning such as disease outbreak simulations require extensive\ncomputations to assess the quality of an action being taken where\nallocation of resources to certain locations to mitigate the spread of\nthe disease is necessary [5, 21]. In these scenarios, it becomes trivial\nto take an action in the environment and observe the change in the\nenvironment state for a single timestep but assessing the outcome\nof the action taken involving interactions between many locations\npossibly over the span of a long period of time proves time consum-\ning. Feedback efficient RL agents show potential in addressing the\nshortcomings of designing good reward functions for these complex\nreal world environments by calculating the reward only for certain\nstates of the environment where the agent itself determines what states\nwould require an associated reward value.\nThe feedback diminution problem asks how an agent could learn\nthe optimal policy for a given task while requesting as little feedback\nas possible from the environment. Traditional RL agents receive a\nsingle feedback signal for every environment interaction. In the case\nof feedback diminution, the agent develops a model of the feedback\nfunction and uses this model to sparsify the feedback signals. The\nsparsification of the feedback is accomplished by allowing the agent\nto request a reinforcement only when it deems necessary. By solely\nlooking at its own understanding of the target task, whether that\nknowledge comes from the Q-values or the reward function model,\nthe agent solves a bi-objective optimization problem that reduces\nthe number of reward signals coming from the environment while\nmaximizing the cumulative reward objective as is the case with the\nregular RL agent.\nWe evaluate the performance of the RL agent based on the total\nreward it achieves with and without feedback diminution at a given\ntime in the training (asymptotic performance). We also measure the\nperformance based on the number of feedback signals the agent needs\nto converge to the maximum cumulative reward Gmax (time to con-\nvergence). The second measure does not look at how much training\nthe agent performs to converge since all of our algorithms perform\nthe same amount of training; rather, it attempts to measure the cost\nof receiving rewards during the training process. Our experiments\ncompare feedback diminution algorithms based on these two metrics.\n4\nEntropy Approaches for Feedback Diminution\nWe present confidence measures, pseudo-code for the high-level de-\nscription of our feedback diminution algorithm, and define the regu-\nlarization terms for the entropy calculation.\n4.1\nMeasuring Confidence\nWe construct our confidence criteria using the output distributions of\nthe actor and reward models. The probability distribution here could\nbe defined over a discrete or continuous variable depending on how\nthe neural networks treat output parameters. For the discrete case, we\ndefine the entropy as\nH(π(·|s)) = −\nX\na∈A\nP1(s, a) ∗lg(P1(s, a)),\n(2)\nwhere H(π(·|s)) is the entropy of the action policy π for the state s us-\ning the probabilities P1. We calculate P1 as P1(s, a) =\neQ(s,a)\nP\na∈A eQ(s,a) ,\nfor the DQN architecture where Q(s, a) is the Q-value for the state-\naction pair (a, s), and P1(s, a) is the softmax of Q-values. For the\ncontinuous case, we define the differential entropy as\nH(R(s, a)) = −\nZ\nR(s,a)\nP2(R(s, a)) ∗lg(P2(R(s, a)) ∗dR, (3)\nwhere H(R(s, a)) is the entropy of a single state, action pair w.r.t the\nreward function R modeled by a four-layered fully-connected neural\nnetwork taking state and action as input and outputting the mean\nand standard deviation of the Gaussian distribution for probabilities\nP2. P2 differs from P1 in the sense that it does not use softmax\nto calculate probability values since the output of R is already a\nGaussian distribution that we sample the probability values for P2.\nWe use Eq.3 to calculate the reward entropy of the reward model R\nand use Gaussian negative log likelihood loss in reward model training.\nFor actor models that output continuous actions instead of Q-value\nestimates, we use the entropy calculation given in Eq. 3 for the action\nentropy where R(s, a) is replaced by the output of the action policy π,\nwhich is again a Gaussian distribution described in Section 3.1. Since\ndifferential entropy is not bound to a specific range as opposed to the\ndiscrete entropy, we clip the differential entropy values to [0,10] range\nand then normalize them to obtain a confidence measure that is at the\nsame scale as the discrete entropy, which is always between [0,1]. In\nvast majority of the states, the differential entropy lies in [0, 10] so the\nclipping doesn’t cause a significant change in the entropy calculation.\nWe define the confidence of the agent as Conf(s) = 1 −H(R(s, a))\nfor the reward model and Conf(s) = 1 −H(π(·|s)) for the action\nmodel. We take the harmonic mean of the two confidence values to\nobtain the final confidence value for state s.\n4.2\nFeedback Diminution Algorithm\nOur algorithm calculates the entropy of the action and reward models\nfor the current state, tells the environment whether it wishes to skip\nthe reward based on the agent’s confidence level, and trains the agent\nusing the reward values sampled from the reward model if a transition\ndoes not have a feedback assigned to it. We use two reward models to\nimprove the stability of the feedback prediction. One model serves as\nthe learning model and the other the target. We use the target model\nto predict the missing reward values for the training of the agent. We\ntrain the learning model at every iteration and copy it to the target\nreward model after each episode. We keep two separate buffers: the\nreplay buffer contains all transitions for training the agent and the\nfeedback buffer stores only the transitions containing an associated\nreward value. Algorithm 1 is pseudo-code for the training process.\nAlgorithm 1 first initializes the agent model (Line 4) which is the\nneural network we use for the policy, including the target network.\nThen, Algorithm 1 initializes the reward models (Line 5) and begins\ntraining from an initial state of the environment (Line 7). It performs ϵ-\ngreedy action selection from the agent model (Line 11) and observes\nthe next state (Line 13). It calculates confidence using Eq. 2 & 3\n(Line 12) and if confidence is below the threshold (Line 14), it receives\na reward from the environment (Line 15). It stores (s, a, r, trm, s′)\ntuples in FB for the training of the reward model (Line 16) and in RB\nfor the training of the agent (Line 18). Then, it samples a minibatch of\ntransitions from FB (Line 19) and performs an optimization step on\nthe learning reward model (Line 20). It also samples a minibatch of\ntransitions from RB (Line 21) and replaces the missing reward values\nwith the predictions from the target reward model (Line 24). Finally,\nit performs an optimization step on the agent using the minibatch\nfrom RB with the mixture of actual and predicted rewards values\n(Line 27). It repeats the training process until the agent reaches the\nend of episode (Line 29 &Line 10). Then, it copies the learning reward\n\n\n1: INPUTS: environment: ENV ; convergence criteria: conv.\n2: CONSTANTS: set of terminal states: Sg; initial state: si;\nconfidence threshold: CTHRESH.\n3: VARS: replay buffer: RB; feedback buffer: FB; state: s;\nterminal condition: trm; reward: r.\n4: Initialize agent model: NNagent;\n5: Initialize reward model: NNlearn\nrew\nand NNtarget\nrew\n6: while conv is not satisfied do\n7:\ns ←si of ENV\n8:\ntrm ←s ∈Sg\n9:\nr ←null\n10:\nwhile trm is False do\n11:\nSelect a using ϵ-greedy policy on NNagent(s)\n12:\nCalculate confidence Conf on NNagent and NNtarget\nrew\n13:\nTake action a in ENV and observe s′\n14:\nif Conf ≤CTHRESH then\n15:\nobserve r from ENV\n16:\nStore (s, a, r, trm, s′) in FB\n17:\nend if\n18:\nStore (s, a, r, trm, s′) in RB\n19:\nSample a minibatch B from FB\n20:\nPerform a batch update on NNlearn\nrew\nusing B\n21:\nSample a minibatch B from RB\n22:\nfor (s, a, r, trm, s′) in B do\n23:\nif r == null then\n24:\nr ←NNtarget\nrew\n(s, a)\n25:\nend if\n26:\nend for\n27:\nPerform a batch update on NNagent using B\n28:\ns ←s′\n29:\ntrm ←s ∈Sg\n30:\nend while\n31:\nCopy NNlearn\nrew\nto the NNtarget\nrew\n32: end while\nAlgorithm 1: High-level Algorithm\nmodel to the target reward model (Line 31) and continues until the\nconvergence criteria is met (Line 6).\n1: INPUTS: learning model: NNagent; reward model: NNtarget\nrew\n;\nstate: s; action: a.\n2: VARS: Q-values for state s: Qs; probabilities for agent and\nreward models: P agent & P reward; entropies for agent and\nreward: H(A) & H(R).\n3: OUTPUTS: agent’s confidence for state s: Conf.\n4: Qs\nagent ←get Q-values from NNagent(s)\n5: Pagent ←softmax(Qs\nagent)\n6: Preward ←get µ, σ from NNtarget\nrew\n(s, a)\n7: H(A) ←entropy(P s\nagent)\n8: H(R) ←entropy(P s\nreward)\n9: Conf ←HarmonicMean(1 −H(A), 1 −H(R))\n10: return Conf\nAlgorithm 2: Confidence Calculation with Discrete Action and\nContinuous Reward Entropies\nAlgorithm 2 provides the details for the entropy calculation given\nin Algorithm 1 at Line 12. It takes the agent and reward models, and\nthe current state-action pair as inputs. It takes the Q-values for the\nagent model (Line 4), applies softmax to obtain probability values\n(Line 5), and uses the probability estimates from the reward model\n(Line 6). It then calculates the action entropy (Line 7; H(A)), and the\nreward entropy (Line 8; H(R)), and converts them to confidence values\n(Line 9). It combines these confidence values using the harmonic\nmean (Line 9) since the harmonic mean tends to be closer to the\nlower of the two confidence values making the algorithm less greedy\nin skipping rewards. In our experiments, we refer to this entropy\ncalculation as action entropy (AE) + reward entropy (RE) since it\nattempts to capture the confidence of the agent in both its ability to\nlearn the reward function and its accuracy in the action policy. We\nalso use a simplified version of this algorithm where we only consider\nthe entropy of the agent without any guidance from the reward model.\nWe refer to the second version of the entropy calculation as action\nentropy (AE) since it does not use the harmonic mean to combine two\nentropy values.\n4.3\nRegularization of Confidence\nThe feedback diminution algorithm is a greedy process that skips the\nreward for every state where the agent’s confidence is predicted to be\nhigh based on entropy calculations. During training, the agent does\nnot always produce reliable estimates for its confidence since it has no\naccess to the optimal policy and the entropy only serves as a heuristic\nassessment of confidence. Skipping the reward in states where the\nagent’s confidence is misplaced may lead to suboptimal performance.\nTo prevent this type of phenomena, we regularize the confidence of\nthe agent.\nWe offer two regularization terms, namely, exponential and hyper-\nbolic regularization. We define exponential regularization as e−ν∗n,\nand the hyperbolic regularization as\n1\n1+ν∗n, where ν is the tempera-\nture parameter and n is the number of steps taken without any reward\nfrom the environment (i.e., we reset n to zero when the agent receives\na reward from the environment). We multiply the confidence term\nwith the regularization term at every step to reduce the likelihood\nof the agent not receiving any reward for long periods of training.\nThe exponential regularization performs a steeper reduction in the\nconfidence values. Hyperbolic regularization provides a softer decay\nthan exponential and resembles the human psychological decay for\ndelayed rewards [22]. We use a constant temperature parameter ν\nwhich is 0.5 for exponential and 1 for hyperbolic regularization. We\ndid manual hyperparameter tuning in the range of [0.5, 0.75, 1] and\npicked the best performing value for the temperature parameter.\n5\nEvaluation Methods\nHere we describe the test environments, experimental setup, and state\nrepresentation for each domain.\n5.1\nEnvironments\nWe use a 20 × 20 2D grid and two continuous state-action space\ndomains to test the algorithms.\n5.1.1\nKey-Lock\nThe Key-Lock domain contains keys, locks, pits, and obstacles similar\nto [8] and [16]. The agent’s task is to pick up the key and unlock\nthe lock while avoiding the pits and obstacles. Each key picked up\ngives a reward of 500 and each lock unlocked gives a reward of 1,000.\nFalling into a pit receives -400. All other actions including moving\ninto an obstacle receive -10. Moving into an obstacle results in no\nstate transition. The learner can only move in cardinal directions and\nis assumed to have obtained the key or unlocked the lock if its location\nmatches the location of the key or it has obtained the key and matches\nthe location of the lock. An episode terminates when the agent obtains\nall the keys and unlocks all the locks, falls into a pit, or reaches 100\ntime steps. Since the agent only receives rewards when it is exactly at\nthe same position as the key or the lock, this is a sparse-reward task. A\nstate in the key-lock environment is represented as a vector, including\nthe Euclidean distance from the learner in all cardinal directions to\nthe nearest key and lock, four binary parameters indicating if there is\n\n\nan obstacle in the neighboring cells, four binary parameters indicating\nif there is a key or lock in the neighboring cells, and eight binary\nparameters indicating if there is a pit in the two adjacent cells in all\nfour directions. Lastly, two integers indicate the number of keys and\nlocks captured so far.\n5.1.2\nFetch-Push\nThe Fetch-Push environment from the gymnasium robotics li-\nbrary [19] is a robotics control domain where a mobile manipulator\nrobot must move a block to a target position using its gripper. The\nposition of the robotic gripper and the block are randomly determined\nat the beginning of each episode. The agent takes three continuous\nactions that change the displacement of the gripper in 3D space. All\nof the actions are defined in the range of [-1, 1]. The reward the agent\nreceives is 0 for having the block within the target position and -1\notherwise. Episodes don’t terminate, instead they are truncated after\n50 steps. A state is a vector, including the position, velocity and dis-\nplacement of the gripper and the position, velocity and rotation of the\nblock. The input to the neural network is the concatenation of the state\nand the goal information since the environment uses a goal-aware\nobservation space.\n5.1.3\nParking\nThe parking environment [10] consists of 30 parking spots, an agent\nand a goal item randomly positioned in one of the spots. The agent\nalways starts in the same position but its initial orientation changes\nrandomly and is tasked to reach the goal and orient itself in the right\ndirection. There are two continuous actions: velocity and angular\nvelocity both in [-1,1]. An episode terminates when the agent reaches\nthe goal and orients itself in the correct direction or when the episode\nlength reaches 100 time steps. The environment allows the agent to\nwander outside of the parking lot and does not provide any boundaries.\nThe agent receives a punishment proportional to its distance to the\ngoal w.r.t. position and orientation. A state is a vector, including the\nagent’s position, velocity, angular velocity, and the goal’s position and\norientation. The input to the neural network is the concatenation of the\nstate and goal information since the environment uses a goal-aware\nobservation space.\n5.2\nComparison Algorithm\nWe compare our algorithm to a DQN [14] in the key-lock domain, a\nvariant of advantage actor-critic network (A2C) [15] in the parking\nenvironment and the hindsight experience replay (HER) [1] algorithm\nin the robotics domain. Our approach to reducing environment rewards\nmakes the task harder for the agent to learn as time passes, which in\nreturn creates a soft curriculum without providing a clear sequence of\ncurriculum tasks. HER also serves as an implicit curriculum learning\nalgorithm as it makes reaching the goal simpler for the agent. However,\nour algorithm deals with reducing the feedback input to the agent\nwhereas HER modifies the already existing transitions in the replay\nbuffer. For this reason, we ran our approach on top of HER to get the\nbenefit of both as they do not hinder or interfere with one another.\nFor additional comparison, we also use a randomly generated entropy\nvalue between 0 and 1 as a random feedback diminution method\nand we use the regularization term without any entropy as a constant\nfeedback diminution approach.\n5.3\nExperimental Setup and Hyperparameters\nWe compare performance based on the total reward obtained while\nlearning the task and the convergence times, and include 95% confi-\ndence intervals. All algorithms share the same hyperparameters for\nthe neural network and train using the same environment-specific\nparameters.\nTable 1.\nHyperparameters for the Feedback Diminution Algorithm.\nParam\nValue\nParam\nValue\nϵ\n1\nOptimizer\nAdamax\nEps. Decay\n0.995\nLoss\nMean Squared\nMin. ϵ\n0.01\nBuffer Size\n40,000\nα\n0.005\nBatch Size\n16\nδ\n0.99\nτ\n0.99\nConf. Thresh.\n0.25\nAveraging\nHarmonic Mean\nThere are two types of hyperparameters: 1) neural network (in-\ncluding the convergence criteria) and 2) entropy calculation. Table 1\ncontains the parameters used in our experiments. These parameters\nare not tuned for any specific algorithm and all algorithms we use\nshare these parameters for all of the experiments run in the same do-\nmain. The last row represents the parameters specific for the entropy\ncalculation. All other parameters are for the neural network. ϵ is the\nϵ-greediness of the algorithm, which decays during training. α and δ\nare the learning rate and discount factor respectively. τ is the rate of\nupdate for the target network. Confidence threshold is the value that\ndetermines if an environment reward is obtained. Harmonic Mean\nrefers to the averaging we perform for combining the action entropy\nand reward entropy.\n6\nResults and Analysis\nWe perform multiple runs of each algorithm and report the average.\nEach run uses a different seed for the random initialization of the\nnetwork weights and environment episodes. Using 24 CPU cores,\nexecution times ranged from 5 hours for key-lock to 10 hours for\nfetch-push to 15 hours for parking environment. All experiments\nperform the same number of batch updates for the same number of\nsteps taken.\n6.1\nResults for Key-Lock Domain\nFigure 1(a) contains the number of training steps across 25 runs\nfor our action entropy and reward entropy algorithms in relation to\nDQN and random feedback diminution baselines in the key-lock\ndomain. Constant reg. is the baseline reward diminution algorithm\napplying exponential regularization to a constant entropy value of 1.\nThe random entropy uniformly samples an entropy value between 0\nand 1. Figure 1(b) is a box plot representing the number of rewards\nthe agent required to converge to the highest score. The highest score\nin this case is calculated as the average score of the first five episodes\nthat reach a performance within 5% of the highest possible score in\nthe environment. The vertical box edges represent the 25th and 75th\nquantiles for the highest score while the horizontal box edges denote\nthe 25th and 75th quantiles for the number of rewards required to\nconverge. Similarly, the whiskers display the minimum and maximum\nvalues excluding outliers. The horizontal and vertical lines within\nthe box represent the median highest score and the median number\nof rewards required to converge, respectively. The farther the box\nis to the upper left corner of the plot the better the performance.\nFigure 1(c) compares the performance of AE + RE with Hyperbolic\nRegularization on different sizes of neural networks to get an idea as\n\n\n(a) Asymptotic Performance\n(b) Rewards Required to Learn\n(c) Model Size vs. Performance\nFigure 1.\nPerformance of Our Feedback Diminution Algorithms in Key-Lock Domain.\nto how the performance of the reward model changes depending on\nthe neural network architecture. Finally, Table 2 shows the summary\nstatistics for all algorithms. The median score and the rewards required\nto converge refer to the highest score of the median run and the number\nof rewards needed to learn the task by the median run respectively,\nwhich is equivalent to the vertical median lines of the box plot.\nAE + RE reaches the highest score during training while requiring\nslightly more than 100,000 rewards to converge (Figures 1(a) & 1(b)),\nmaking it the best performing algorithm in this domain. AE + RE with\nhyperbolic regularization reaches a similar highest score as AE + RE\nwhile requiring around 123,000 rewards to converge, making it the sec-\nond best performing method for this domain (Figure 1(b) & Table 2).\nHowever, the asymptotic performance of AE + RE with hyperbolic\nregularization falls slightly behind AE + RE due to one outlier run\ndragging the average down for AE + RE with hyperbolic regulariza-\ntion (Figure 1(a)). The DQN baseline requires the highest number of\nrewards to converge while getting relatively similar performance to\nAE + RE with exponential regularization (Figure 1(b)).\nTable 2.\nSummary Results for Key-Lock Environment.\nAlgorithm\nMedian Score\n# of Rewards\nDQN\n996\n226,000\nRandom Entropy\n847\n280,000\nConstant Reg.\n900\n180,000\nAction Entropy\n659\n233,000\nAE with Exp. Reg.\n961\n197,000\nAE with Hyper. Reg.\n888\n191,000\nAE + RE\n997\n116,000\nAE + RE with Exp. Reg.\n977\n165,000\nAE + RE with Hyper. Reg\n967\n123,000\nThe constant reg. baseline, despite requiring fewer rewards to learn\ncompared to DQN, does not manage to reduce the number of rewards\nas much as AE + RE with hyperbolic regularization, showing that\nthe entropy selection method is necessary to provide better reduction\nin external rewards (Figures 1(a)). AE performs poorly due to not\nhaving access to the confidence of the reward model or any type of\nregularization and gets stuck at a local optima (Figures 1(a)). Using a\nlarge DQN with a small reward model results in the best performance\nwhile the large reward models fail to learn the task (Figures 1(c)).\nThe large reward models seem to be overfitting to the earlier rewards,\nmaking the model overly confident of its policy and not asking for\nenough environment rewards. Our tuning on the model size indicates\nit is necessary to have a reward model that is smaller in size to the\naction model to obtain feedback diminution benefits.\n6.2\nResults for Fetch-Push Domain\nWe perform 5 runs of each algorithm in the fetch-push domain and\nreport the average. We use the best performing neural network archi-\ntecture from (Figures 1(c)). The confidence intervals appear larger\n(a) Success Rate\n(b) Rewards Required to Learn\nFigure 2.\nPerformance in the Fetch-Push Domain.\nin this domain mainly because of the reduction in the number of\nruns we execute. Figure 2(a) shows the asymptotic performance of\nour algorithms excluding AE and AE + RE. Similar to the key-lock\ndomain, AE performs poorly and fails to reach a high success rate\neven after 1 million training steps. To make the graphs more readable,\nwe do not show AE or AE + RE as they remain below 0.2 success rate\nthroughout the training. Figure 2(b) is the box plot for the number of\nrewards required to convergence.\nAE + RE with hyperbolic regularization shows the best asymptotic\nperformance, obtains the highest score, and requires the fewest exter-\nnal rewards, making it the best performing algorithm in this domain\n(Figures 2(a) & 2(b)). It also shows the results we obtain in two dif-\nferent environments are consistent with one another since the same\nalgorithm results in near best performance in both domains. All the\nother algorithms have similar performance to the HER baseline in\nterms of return but they still reduce the number of external rewards\nby at least 50% (Figure 2(b)). AE with exponential regularization\nobtains the highest median score although it is only slightly better\nthan the variants of AE + RE and the difference could be attributed to\n\n\nTable 3.\nSummary Results for Fetch-Push Environment.\nAlgorithm\nMedian Score\n# of Rewards\nHER\n-9.7\n961,000\nAE Exp. Reg.\n-8.85\n450,000\nAE Hyper. Reg.\n-9.5\n306,000\nAE + RE Exp. Reg.\n-9.15\n411,000\nAE + RE Hyper. Reg.\n-9.0\n216,000\nthe variance in the execution (Table 3).\n6.3\nResults for Parking Domain\nWe report the average of 25 runs in the parking domain, using the\nbest performing neural network architecture from (Figure 1(c)). Fig-\nure 3(a) shows the asymptotic performance against advantage actor-\ncritic (A2C). Figure 3(b) is the box plots for the number of rewards\nrequired to converge.\n(a) Asymptotic Performance\n(b) Rewards Required to Learn\nFigure 3.\nPerformance in the Parking Domain.\nAE with exponential reg. gives the best average score (although\nit is only slightly better than the A2C baseline) (Figure 3(a)), but\nrequires more environment rewards to learn the task compared to\nthe variants of AE + RE (Figure 3(b)). AE + RE with hyperbolic\nregularization produces the highest score while needing a very low\nnumber of environment rewards to converge (Figure 3(b)), arguably\nmaking it the best performing algorithm again. AE + RE and AE\nmanage to reduce the number of rewards needed to converge at the\nexpense of performance. They show poor asymptotic performance\nsimilar to prior domains (Figure 3(a)) and show a large variation in\nthe highest score they obtain (Figure 3(b)). AE only requires 24,000\nrewards to learn, providing close to 95% improvement and suggesting\nthat denser external rewards might be more prone to reward skipping\nby the agent (Table 4). The baseline algorithm obtains a similar highest\nscore as the regularized AE + RE but it requires more than four times\nthe number of environment rewards to converge (Figure 3(b)). The\nperformance of AE + RE with hyperbolic regularization and AE with\nexponential reg. remain consistent with the other domains despite the\nparking environment having a denser external reward function. This\nfurther supports reducing the need for environment rewards across\nmultiple network architectures over three domains.\nWe also see that some of the feedback diminution algorithms reach\nhigher scores than the baselines having access to true rewards. Only\nthe feedback diminution algorithms making use of hyperbolic or expo-\nnential regularization manage to outperform the baseline algorithms\nin some domains, which makes us reason that the effect of the regular-\nization allows the algorithm to avoid suboptimal policies by providing\ninterleaved feedback which results in the feedback diminution method\noutperforming the baselines.\nTable 4.\nSummary Results for Parking Environment.\nAlgorithm\nMedian Score\n# of Rewards\nA2C\n-5.53\n431,000\nAE\n-8.88\n24,000\nAE Exp. Reg.\n-5.55\n174,000\nAE Hyper. Reg.\n-7.50\n99,000\nAE + RE\n-9.15\n27,000\nAE + RE Exp. Reg.\n-6.13\n150,000\nAE + RE Hyper. Reg.\n-5.51\n101,000\n6.4\nChoosing the Best Performing Approach\nHaving a feedback diminution method that can provide consistent\nimprovements across multiple domains is crucial since it would be\nimpractical to try all regularization methods and entropy metrics\nevery time the algorithm is adapted to a new domain or framework.\nDue to the variations in performance of the feedback diminution\nalgorithms on different domains, there emerges the need for selecting\nthe proper approach that would be suitable for most domains and real-\nworld applications. If we exclude the non-regularized methods due to\ntheir poor scores in some environments, we see that AE + RE with\nhyperbolic regularization produces the best performance in the key-\nlock and robotics domains in terms of the rewards required to converge\nwhile being close to the best performing algorithm in the parking\ndomain. If the purpose is to reduce the feedback dependence while\nnot compromising the performance of the RL algorithm, AE + RE\nwith hyperbolic regularization presents the best option for applying\nfeedback diminution in RL frameworks.\n7\nConclusion\nWe presented a novel approach to reducing the need to sample environ-\nment rewards using entropy as a heuristic. We detailed two versions of\nthe approach, one using only the entropy of the action model and an-\nother that incorporates both action and reward entropies. We adapted\nvariants of DQN, A2C, and HER algorithms to compare against our\nalgorithm and evaluated performance on the key-lock, robotics, and\nparking domains using discrete and continuous action spaces. Our\nresults show that our approach was able to match the performance\nof a randomly generated entropy baseline, a constant regularization\nbaseline, and the comparison criteria while requiring fewer environ-\nmental rewards to learn in all cases. In the future, we intend to analyze\nin what parts of the state space the agent is more likely to request\nrewards to obtain a better understanding of how the characteristics of\nthe environment affect the feedback diminution process.\n\n\nReferences\n[1] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder,\nB. McGrew, J. Tobin, P. Abbeel, and W. Zaremba. Hindsight experience\nreplay, 2018.\n[2] S. Arora and P. Doshi. A survey of inverse reinforcement learning:\nChallenges, methods and progress, 2018. URL https://arxiv.org/abs/\n1806.06877.\n[3] J. Buckman, D. Hafner, G. Tucker, E. Brevdo, and H. Lee. Sample-\nefficient reinforcement learning with stochastic ensemble value ex-\npansion.\nIn S. Bengio, H. Wallach, H. Larochelle, K. Grauman,\nN. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural In-\nformation Processing Systems, volume 31. Curran Associates, Inc.,\n2018. URL https://proceedings.neurips.cc/paper_files/paper/2018/file/\nf02208a057804ee16ac72ff4d3cec53b-Paper.pdf.\n[4] Y. Burda, H. Edwards, A. Storkey, and O. Klimov. Exploration by ran-\ndom network distillation, 2018. URL https://arxiv.org/abs/1810.12894.\n[5] S. Bushaj, X. Yin, A. Beqiri, D. Andrews, and E. Buyuktahtakin. A\nsimulation-deep reinforcement learning (sirl) approach for epidemic\ncontrol optimization. Annals of Operations Research, 328:1–33, 09\n2022. doi: 10.1007/s10479-022-04926-7.\n[6] P. Christiano, J. Leike, T. B. Brown, M. Martic, S. Legg, and D. Amodei.\nDeep reinforcement learning from human preferences, 2023.\n[7] D. Kong and L. F. Yang. Provably feedback-efficient reinforcement\nlearning via active reward learning, 2023.\n[8] G. Konidaris and A. Barto. Building portable options: Skill transfer in\nreinforcement learning. In Proceedings of the 20th International Joint\nConference on Artificial Intelligence, pages 895–900, 2007.\n[9] K. Lee, L. Smith, and P. Abbeel. Pebble: Feedback-efficient interactive\nreinforcement learning via relabeling experience and unsupervised pre-\ntraining, 2021.\n[10] E. Leurent. An environment for autonomous driving decision-making.\nhttps://github.com/eleurent/highway-env, 2018.\n[11] X. Liang, K. Shu, K. Lee, and P. Abbeel. Reward uncertainty for explo-\nration in preference-based reinforcement learning, 2022.\n[12] R. Liu, F. Bai, Y. Du, and Y. Yang. Meta-reward-net: Implicitly dif-\nferentiable reward learning for preference-based reinforcement learn-\ning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho,\nand A. Oh, editors, Advances in Neural Information Processing\nSystems, volume 35, pages 22270–22284. Curran Associates, Inc.,\n2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/\n8be9c134bb193d8bd3827d4df8488228-Paper-Conference.pdf.\n[13] V. Mai, K. Mani, and L. Paull. Sample efficient deep reinforcement\nlearning via uncertainty estimation, 2022.\n[14] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,\nS. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran,\nD. Wierstra, S. Legg, and D. Hassabis. Human-level control through\ndeep reinforcement learning. Nature, 518(7540):529–533, Feb. 2015.\nISSN 00280836. URL http://dx.doi.org/10.1038/nature14236.\n[15] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley,\nD. Silver, and K. Kavukcuoglu. Asynchronous methods for deep rein-\nforcement learning, 2016.\n[16] S. Narvekar, J. Sinapov, and P. Stone. Autonomous task sequencing for\ncustomized curriculum design in reinforcement learning. In Proceedings\nof the 26th International Joint Conference on Artificial Intelligence,\nIJCAI’17, pages 2536–2542. AAAI Press, 2017. ISBN 978-0-9992411-\n0-3. URL http://dl.acm.org/citation.cfm?id=3172077.3172241.\n[17] J. Park, Y. Seo, J. Shin, H. Lee, P. Abbeel, and K. Lee. Surf: Semi-\nsupervised reward learning with data augmentation for feedback-efficient\npreference-based reinforcement learning, 2022.\n[18] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell. Curiosity-driven\nexploration by self-supervised prediction, 2017. URL https://arxiv.org/\nabs/1705.05363.\n[19] M. Plappert, M. Andrychowicz, A. Ray, B. McGrew, B. Baker, G. Pow-\nell, J. Schneider, J. Tobin, M. Chociej, P. Welinder, V. Kumar, and\nW. Zaremba. Multi-goal reinforcement learning: Challenging robotics\nenvironments and request for research, 2018.\n[20] R. S. Sutton and A. G. Barto. Reinforcement learning - an introduction.\nAdaptive computation and machine learning. MIT Press, 1998. ISBN\n0262193981. URL http://www.worldcat.org/oclc/37293240.\n[21] A. L. Sykes, J. A. Galvis, K. C. O’Hara, C. Corzo, and G. Machado.\nEstimating the effectiveness of control actions on African swine fever\ntransmission in commercial swine populations in the United States. Pre-\nventive Veterinary Medicine, 217:105962, Aug. 2023. ISSN 01675877.\ndoi: 10.1016/j.prevetmed.2023.105962. URL https://linkinghub.elsevier.\ncom/retrieve/pii/S0167587723001265.\n[22] T. Takahashi, S. Tokuda, M. Nishimura, and R. Kimura.\nThe q-\nexponential decay of subjective probability for future reward: A psy-\nchophysical time approach. Entropy, 16(10):5537–5545, 2014. ISSN\n1099-4300. doi: 10.3390/e16105537. URL https://www.mdpi.com/\n1099-4300/16/10/5537.\n[23] G. Warnell, N. Waytowich, V. Lawhern, and P. Stone. Deep tamer:\nInteractive agent shaping in high-dimensional state spaces, 2018.\n[24] J. Zhang, J. Kim, B. O’Donoghue, and S. Boyd. Sample efficient rein-\nforcement learning with reinforce, 2020.\n\n\n"}
{"text": "Distribution Prototype Diffusion Learning for Open-set Supervised\nAnomaly Detection\nFuyun Wang, Tong Zhang, Yuanzhi Wang, Yide Qiu, Xu Guo\nSchool of Computer Science and Engineering\nNanjing University of Science and Technology\nfyw271828@njust.edu.cn, tong.zhang@njust.edu.cn, yuanzhiwang@njust.edu.cn,\n121106010824@njust.edu.cn, guo.xu@njust.edu.cn\nXin Liu\nSeetaCloud Technology\nNanjing, China\nxin.liu@seetacloud.com\nZhen Cui\nSchool of Artificial Intelligence\nBeijing Normal University\nzhen.cui@bnu.edu.cn\nAbstract\nIn Open-set Supervised Anomaly Detection (OSAD), the\nexisting methods typically generate pseudo anomalies to\ncompensate for the scarcity of observed anomaly samples,\nwhile overlooking critical priors of normal samples, lead-\ning to less effective discriminative boundaries. To address\nthis issue, we propose a Distribution Prototype Diffusion\nLearning (DPDL) method aimed at enclosing normal sam-\nples within a compact and discriminative distribution space.\nSpecifically, we construct multiple learnable Gaussian pro-\ntotypes to create a latent representation space for abun-\ndant and diverse normal samples and learn a Schr¨odinger\nbridge to facilitate a diffusive transition toward these pro-\ntotypes for normal samples while steering anomaly sam-\nples away.\nMoreover, to enhance inter-sample separa-\ntion, we design a dispersion feature learning way in hyper-\nspherical space, which benefits the identification of out-of-\ndistribution anomalies. Experimental results demonstrate\nthe effectiveness and superiority of our proposed DPDL,\nachieving state-of-the-art performance on 9 public datasets.\n1. Introduction\nAnomaly detection (AD) [15, 21, 42, 44] aims to iden-\ntify outliers significantly diverging from the prevailing sam-\nples in a dataset, and has a wide range of applications like\nindustrial inspection, medical image analysis, and scien-\ntific discovery, etc.\nRecently, unsupervised anomaly de-\ntection (UAD) [6, 11, 26, 28] and few-shot anomaly de-\ntection (FSAD) [12, 21, 22] have emerged as prominent\nresearch paradigms, emphasizing the modeling of normal\nsample distributions to discern anomalies effectively. Yet,\nthese methods often neglect prior knowledge from limited\nanomaly samples, resulting in imprecise delineation of nor-\nmal sample boundaries and reduced efficacy in differenti-\nating normal from anomaly instances. On the contrary, su-\npervised anomaly detection (SAD) [2, 20, 41] leverages a\nlimited subset of anomaly samples as prior knowledge, im-\nproving detection performance. However, this reliance on\nseen anomalies poses a risk of overfitting and hampers gen-\neralization to unseen anomalies in real-world settings.\nTo mitigate the challenge of limited generalization in-\nherent in closed-set training, we focus on open-set super-\nvised anomaly detection (OSAD) [1, 27, 45, 46], which\nutilizes a small set of known anomaly classes during train-\ning to identify unseen anomalies from open-set classes. By\nleveraging prior knowledge from observed samples, OSAD\nmethods could reduce false positive errors. To improve the\ngeneralized detection of unseen anomalies, DRA [9] lever-\nages data augmentation and outlier exposure to learn a de-\ncomposed anomaly representation comprising seen anoma-\nlies, pseudo-anomalies, and potential residual anomalies.\nBGAD [41] leverages decision boundaries derived from\nnormalized flow models to capture and model anomaly in-\nformation.\nRecently, AHL [45] simulates heterogeneous\nanomaly distributions and performs collaborative differen-\ntiable learning to further enhance the model’s generality.\nWhile data augmentation and outlier exposure tech-\nniques [9, 19, 33] have demonstrated considerable success\nin anomaly detection, they fall short in generating com-\nprehensive pseudo anomalies, capturing only a fraction of\npotential unseen anomalies.\nThis limitation arises from\noverlooking the intricate nature of real-world anomaly dis-\ntributions, thereby hindering the model’s ability to gener-\nalize to novel anomaly types.\nAlthough AHL [45] has\narXiv:2502.20981v1  [cs.CV]  28 Feb 2025\n\n\nmade strides in addressing this issue by simulating hetero-\ngeneous anomaly distributions, it still relies on approximat-\ning unknown out-of-distribution anomalies using known in-\ndistribution anomalies for generalization. Yet, three critical\nissues persist: i) the simulation mechanism cannot cover all\nanomaly distribution patterns due to the varied scales and\nstructures of anomaly distributions; ii) simulated anoma-\nlies inherit in-distribution data biases, leading to subopti-\nmal performance on out-of-distribution anomalies; iii) the\ndiversity of normal samples presents dilemmas for existing\nmethods, complicating the differentiation between normal\nand anomaly boundaries. These issues prompt a fundamen-\ntal question: Instead of generating pseudo and uncertain\nanomaly samples, how can we accurately characterize com-\npact distribution boundaries amidst a range of diverse nor-\nmal samples and achieve robust generalization for unknown\nout-of-distribution anomalies?\nTo address the aforementioned issue, in this work,\nwe propose a Distribution Prototype Diffusion Learning\n(DPDL) method for open-set supervised anomaly detection.\nConsidering abundant and diverse normal samples but very\nlimited anomaly data, our method involves learning latent\ndistribution prototypes, specifically multiple Gaussian dis-\ntributions, onto which all observed normal samples can be\neffectively projected. To facilitate the mapping of normal\nsamples into the prototype space, we leverage Schr¨odinger\nbridge (SB) framework, which enables a diffusive transi-\ntion by aligning the distributions of these samples with the\nprototypes.\nThe SB-based diffusion way could mitigate\nthe out-of-distribution issue for normal samples to some\nextent. Within the distribution prototype space, we push\nobserved anomaly samples away from normal samples to\nenhance discriminative capacity. Notably, both the proto-\ntypes and the diffusive bridge are learned jointly, resulting\nin a robust embedding space for normal samples. More-\nover, to enhance generalization across unseen anomaly do-\nmains, we introduce a dispersion feature learning mecha-\nnism that maps intermediate features to a hyperspherical\nspace, leveraging a mixture of von Mises-Fisher (vMF) dis-\ntributions. This approach bolsters directional feature ex-\ntraction and promotes robust inter-sample separation, facili-\ntating effective identification of out-of-distribution samples.\nExperiments demonstrate that our method greatly improves\ndetection capabilities for unseen anomalies. In the single-\nanomaly training setting, DPDL outperforms the next best-\nperforming method by over 8.3% on datasets including AI-\nTEX, ELPV, and Mastcam.\nIn summary, our contributions are three-fold: i) we pro-\npose a distribution prototype diffusion learning framework\nthat jointly learns multiple Gaussian prototypes and the as-\nsociated diffusion bridge, creating a compact and discrim-\ninative embedding space; ii) we develop dispersion feature\nlearning in hyperspherical space to enhance inter-sample\nseparation and improve generalization; iii) we achieve state-\nof-the-art performance in 9 public datasets, demonstrating\nthe efficacy of our approach.\n2. Related Work\nOpen-set Supervised Anomaly Detection. Open-set su-\npervised anomaly detection (OSAD) seeks to develop a ro-\nbust anomaly detection framework that generalizes from\na limited set of training anomalies to effectively iden-\ntify previously unseen anomalies within an open-set con-\ntext [1, 27, 34, 41, 46]. Leveraging the prior knowledge pro-\nvided by observed anomalies, contemporary OSAD meth-\nods significantly mitigate false positive errors, thereby en-\nhancing overall detection performance [9, 45]. Recently,\nDRA [9] learns disentangled representations of observed,\npseudo, and residual anomalies to boost the detection of\nboth seen and unseen anomalies. In contrast, AHL [45]\nsimulates diverse heterogeneous anomaly distributions and\nemploys collaborative differentiable learning, significantly\nimproving the model’s generalization capacity.\nSchr¨odinger Bridge. Schr¨odinger bridge (SB), widely\nrecognized as the entropy-regularized optimal transport\n(OT) problem, involves learning a stochastic process that\nevolves from an initial probability distribution to a terminal\ndistribution under the influence of a reference measure[5,\n7, 16, 18, 23, 24, 37]. I2-SB [25] and UNSB [14] learn\na nonlinear diffusion process between two given distribu-\ntions or represent the SB problem as a series of adversar-\nial learning problems to realize the image transformation\ntask. Recently, LightSB [10, 17] introduces a novel, fast,\nand simple SB solver, which achieves optimal matching in\npractice through the Gaussian mixture parameterization of\nthe adjusted schr¨odinger potential.\n3. Preliminaries\nWe focus on how to build the connection between two dis-\ntributions p0 and p1, where the distributions are defined as\nabsolutely continuous Borel probability distributions with\nfinite second-order moments. Building upon the founda-\ntion of entropy-regularized optimal transport (EOT) [5, 7,\n16, 18, 23, 24, 32, 37], we review the related properties\nof EOT and the schr¨odinger bridge (SB) problem with a\nWiener prior.\nEntropy-regularized optimal transport (EOT). Given\ntwo point sets Z0 and Z1, we seek for the optimal transport\ncost between any two points z0 ∈Z0 and z1 ∈Z1. This\ntask may be formulated as an EOT problem with a parame-\nter ϵ > 0, i.e., minimizing the following objective:\nmin\nπ∈Π(p0,p1){\nZ\nRD\nZ\nRD\n1\n2∥z0 −z1∥2π(z0, z1)dz0dz1\n+ϵKL(z ∥p0 × p1)},\n(1)\n\n\nwhere Π(p0, p1) denotes the set of transport plans, i.e., joint\nprobability distributions on RD × RD with marginals p0\nand p1, respectively, and KL denotes Kullback-Leibler di-\nvergence. The minimizer π∗of Eqn. (1) is guaranteed to\nexist, be unique, and absolutely continuous, and is referred\nas the EOT plan.\nSchr¨odinger Bridge (SB). We define Ωas the space\nof RD-valued functions over time t ∈[0, 1], representing\ntrajectories in RD that start at t = 0 and end at t = 1.\nWe denote the set of probability distributions over Ω, i.e.,\nstochastic processes, by P(Ω). The differential of the stan-\ndard Wiener process is represented by dWt. For a process\nT ∈P(Ω), we denote its joint distribution at t = 0, 1 by\nπT ∈P(RD ×RD). Similarly, we use T|z0,z1 to denote the\ndistribution of T for t ∈(0, 1), conditioned on T’s values\nz0 and z1 at t = 0 and t = 1, respectively.\nLet W ϵ\n∈P(Ω) represents a Wiener process with\nvolatility ϵ > 0, starting from p0 at t = 0. Its differential\nis governed by the stochastic differential equation (SDE):\ndW ϵ\nt = √ϵ dWt. The Schr¨odinger bridge problem with the\nWiener prior W ϵ between p0 and p1 is minimizing:\nmin\nT ∈F(p0,p1) KL(T ∥W ϵ),\n(2)\nwhere F(p0, p1) ⊂P(Ω) denotes the subset of stochastic\nprocesses that begin with distribution p0 at t = 0 and reach\np1 at t = 1. This problem has a unique solution, a SDE\ndiffusion process T ∗defined: dZt = g∗(Zt, t) dt + dW ϵ\nt .\nThe process T ∗is referred to as SB, and g∗: RD ×[0, 1] →\nRD is the optimal drift.\nCharacterization of solutions. The EOT plan π∗=\nπT ∗takes a specific form [18]:\nπ∗(z0, z1) = u∗(z0)exp(−∥z0 −z1∥2\n2ϵ\n)v∗(z1),\n(3)\nwhere u∗, v∗: R →R+ are measurable functions known as\nSchr¨odinger potentials. The optimal drift g∗is derived as:\ng∗(z, t) = ϵ▽zlog\nZ\nRD N(z′|z, (1 −t)ϵID)v∗(z′)dz′, (4)\n4. Method\n4.1. Problem Formulation\nLet Xtr = {(xi, yi)} denotes a weakly-supervised training\nset with only image-level labels, where xi denotes one RGB\nimage and yi ∈{0, 1} denotes whether xi is an anomaly\nsample (anomaly: yi = 1, normal: yi = 0).\nHereby,\nXtr is consist of a normal subset X n\ntr (|X n\ntr | = N) and an\nanomaly subset X a\ntr (|X a\ntr | = M), formally, Xtr .= X n\ntr ∪X a\ntr ,\nwhere generally N ≫M.\nGiven a testing set Xte, we\nneed to predict whether one sample x ∈Xte is anomaly\nor normal. In OSAD, the anomaly patterns of the testing\nset do less recurred in those encountered training set. In\nother word, the distribution of anomalies are obviously dis-\ncrepant, i.e., P(X a\nte) ̸= P(X a\ntr ). Hereby, we need to learn a\nrobust anomaly detection model ψ from the training set Xtr,\nso that ψ accurately infers anomaly scores for test samples.\nOur abstract idea is to learn latent distribution proto-\ntypes that not only encapsulate normal samples in a con-\ncise manner but also discriminate against anomaly sam-\nples. Given the abundance of observed normal samples, one\nnatural approach is to characterize the distribution P(X n\ntr ),\nwhere samples outside this distribution, x /∈P(X n\ntr ), would\nbe awarded higher probabilities as anomalies. Considering\nthe inherent diversity of normal samples, we endeavor to\nlearn multiple simple distributions (e.g., Gaussians) as pro-\ntotypes PMGP, named multi-Gaussian prototypes (MGP). To\nembed input data into the prototype space, we introduce a\ngenerative bridge model ψp for distribution transformation.\nThe more abstract formulation is given as follows:\nmin\nψp,PMGP,fDp(ψp(Fn\ntr), ψp(Fa\ntr), PMGP)+λDs(Fn\ntr, Fa\ntr), (5)\ns.t. , ψp : P(F)\nbridge\n−→PMGP,\n(6)\nf : X\nfeature\n−→F,\n(7)\nwhere PMGP denotes the distribution prototypes to be\nlearned, ψp is the flow function across probability distribu-\ntions, Dp signifies the discriminative function in the space\nof prototypes, f stands for the feature extraction process,\nand Ds acts as a regularizer to increase feature discrim-\ninability. In the above formulation, besides distribution pro-\ntotype learning (DPL), we also introduce dispersion feature\nlearning (DFL) executed within a hyperspherical embed-\nding space, i.e., Ds(Fn\ntr, Fa\ntr). The advantage of DFL is to\nprevent abrupt feature collapse in feature learning, thus pre-\nserving discriminative qualities. The overview of our frame-\nwork is abstractly depicted in Fig. 1. Detailed elaboration1\non these aspects will be provided in the subsequent sections.\n4.2. Distribution Prototype Learning\nDistribution prototype learning is operated in the feature\nspace. Hence, to extract intermediate image features, we\ncan leverage those classic networks as the backbone, such\nas ResNet-18. Formally, the feature extraction process, de-\nnoted as a function f : X →F, transforms an image\nx into the intermediate feature x = f(x)2∈Rd. Conse-\nquently, we designate the intermediate feature sets of nor-\nmal and anomaly samples as Fn\ntr = {xn\ni |i = 1, · · · , N} and\nFa\ntr = {xa\ni |i = 1, · · · , M}, respectively.\nConsidering the rarity of anomalies and the diversity of\nnormal samples, it is pertinent to capture the intricate dis-\ntributions of normal samples by mapping the distribution\n1The concrete algorithm is deferred to the supplementary material.\n2Here a flatten operation is used on convolution maps for vectorization.\n\n\nNormal Features\nAnomaly Features\n…\n…\nDispersion Feature Learning\nDistribution Prototype Learning\nℒDFL\nNormal Samples\nAnomaly Samples\nDistribution Prototypes\nTied\nSchrödinger Bridge \nTied\nLearnable \n𝒫𝒫MGP: {𝝁𝝁𝑘𝑘, 𝛔𝛔𝑘𝑘}𝑘𝑘=1\n𝐾𝐾\nMIL-based Anomaly Score \nComputation\nAnomaly Score\nScore Prediction\nDistribution Flow \nDistribution Flow \nDispersion Loss\nEncoder\nℱtr\na\nℱtr\nn\n𝜓𝜓𝑝𝑝\n𝜓𝜓𝑝𝑝\nℒDPL\nn\n𝑓𝑓\nℒDPL\na\nFigure 1. Our proposed DPDL framework. It comprises three distinct modules: Distribution Prototype Learning (DPL, Sec. 4.2), Disper-\nsion Feature Learning (DFL, Sec. 4.3), and anomaly score prediction (Sec. 4.4). DPL transforms the distribution of normal samples to a\nspace of learnable multiple Gaussian prototypes through building schr¨odinger bridge, meantime pushing anomaly distribution away from\nthese prototypes. DFL operates in a hyperspherical space, enlarging the distances of intermediate features of all samples in a hyperspher-\nical space to strengthen feature generalization for detecting anomalies. The score prediction module leverages a multi-instance-learning\nmethod to compute anomaly scores.\np0 = P(Fn\ntr) of intermediate features to a distinct and well-\ncharacterized distribution p1 like Gaussian. But due to vari-\nous normal samples, we opt for a multi-Gaussian prototypes\n(MGP) comprising multiple Gaussian distributions as pro-\ntotypes PMGP = {Pi .= N(µi, σi)|i = 1, · · · , C}. Given\na normal sample xn\ni , we expect to align it with the closest\nprototype with high likelihood. However, in the open-set\nsetting, it is challenge to transform unseen points x ∼p0\nto the target distribution p1. Drawing inspiration from the\ncapability of diffusion generation models in aligning dis-\nparate distributions, we frame the transition from source\ndomain distribution p0 to target domain distribution p1 as a\nSchr¨odinger bridge problem. As formulated in Eqn. (6), we\nneed learn an essential bridge flow ψp : P(Fn\ntr)\nbridge\n−→PMGP.\nAfter bridge transformation, the condition probability in op-\ntimal transport plan conforms to:\nπ(ψp(x)|x) ∝\nC\nX\nc=1\nαcN(ψp(x); µc, σc))\n|\n{z\n}\n.=ϕ1(ψp(x))\n,\n(8)\nwhere the parameters {αc, µc, σc}C\nc=1 (known as Gaussian\nmixed model (GMM), abstracted into the function ϕ1) as\nwell as the flow function ψp (in bridge) need to be learned.\nFor simplicity, below we adopt diagonal matrices for σc.\nDrawing inspiration from previous works [10, 17], we\nreframe distribution prototype learning in Eqn. (8) as the\nSchr¨odinger bridge. Given the specific form taken by the\nEOT plan described in Eqn. (3), we redefine these measur-\nable functions u, v : R →R+, termed Schr¨odinger poten-\ntials, as follows:\nu(xn\ni ) .= exp(∥xn\ni ∥2\n2ϵ\n)ϕ0(xn\ni ),\n(9)\nv(ψ(xn\ni )) .= exp(∥ψ(xn\ni )∥2\n2ϵ\n)ϕ1(ψ(xn\ni )),\n(10)\nwhere ϕ0 is defined within the source feature domain Ftr,\nand ϕ1 is defined in Eqn. (8). ϵ is set to 0.001 in our exper-\niments. Accordingly, Eqn. (3) can be converted to:\nπ(xn\ni , ψ(xn\ni ))=ϕ0(xn\ni ) exp(⟨xn\ni , ψ(xn\ni )⟩\nϵ\n)ϕ1(ψ(xn\ni )), (11)\nHence, the condition probability of the transport plan in\nEqn. (8) could be exactly defined as\nπ(ψ(xn\ni )|xn\ni ) .= η(xn\ni , ψ(xn\ni ))ϕ1(ψ(xn\ni )),\n(12)\nwhere the connection factor is denoted as η(xn\ni , ψ(xn\ni )) =\n1\nϖ(xn\ni) exp( ⟨xn\ni,ψ(xn\ni)⟩\nϵ\n)\nwith\nthe\nnormalization\nterm\nϖ(xn\ni ) =\nR\nexp(⟨xn\ni , ψ(xn\ni )⟩)ϕ1(ψ(xn\ni ))dψ(xn\ni ). Accord-\ning to Eqns. (8) and (12), we can further derive a more\ntractable form:\nπ(ψ(xn\ni )|xn\ni )=eη(xn\ni )\nC\nX\nc=1\neαc(xn\ni )N(ψ(xn\ni );eµc(xn\ni ),σc), (13)\nwhere the normalization factor is denoted as eη(xn\ni ) =\n1/ PC\nc=1 eαc(xn\ni ), the coefficients of multi-Gaussian are de-\nfined as eαc(xn\ni ) = αcexp( 1\n2(xn\ni )⊺σcxn\ni + 1\nϵ (eµc)⊺(xn\ni )), and\nthe mean vectors are calculated as eµc(xn\ni ) = µc + 1\nϵ σcxn\ni .\n\n\nWe proceed with deriving the bridge function ψp that\nrepresents a SDE process: dxt = g(xt, t)dt+√ϵdWt where\nthe shift function g is solved. According to Eqn. (4), we can\nobtain the shift function of diffusion process as follows:\ng(xn\ni , t) = ϵρMGP(xn\ni )∇xn\ni log(N(xn\ni |0, ϵ(1 −t))I),\n(14)\nwhere the coefficients ρMGP defined on multiple dis-\ntribution prototypes is calculated as:\nρMGP(xn\ni )\n=\nPC\nc=1(αcN(eµc(xn\ni )|0, σc)N(hc(xn\ni , t)|0, Σt\nc)), with an-\nother Gaussian of hc(xn\ni , t) =\n1\nϵ(1−t)xn\ni + σ−1\nc\neµc(xn\ni ) and\nΣt\nc =\nt\nϵ(1−t)I + σ−1\nc . The derivation about Eqns. (13) and\n(14) is deferred to the supplementary material.\nDistribution prototypes initialization.\nJointly learning\nprototypes {αc, µc, σc}C\nc=1 and the bridge transformation\nψp (also the shift g) is a challenging task as they are in-\nterdependent. For this, we leverage a vector quantization\nfunction to learn a codebook E of prototypes within a dis-\ncrete latent space from training data. Specifically, given an\ninput image feature xn\ni , we can assign xn\ni to the closest pro-\ntotype ek by minimizing the L2 distance between xn\ni and\neach of prototypes ec ∈E, as follows:\nmin\n{ec} Exn\ni∈Ftr[∥xn\ni−ec∗∥2\n2], s.t. , c∗=arg min\nc\n∥xn\ni−ec∥2, (15)\nwhere c∗denotes the index of the prototype closest to xn\ni .\nThe learned {ec}C\nc=1 are used to initialize the mean vectors\n{µc}C\nc=1, while the variances of all prototypes are set to the\nidentity matrix. We observe that this initialization strategy\naccelerates the training process.\nDistribution loss of normal and anomaly samples. As p0\nand p1 are accessible only via samples Ftr and prototypes\nPMGP, we optimize the empirical form in Eqn. (2) for nor-\nmal samples as follows:\nLn\nDPL = 1\nN\nN\nX\ni=1\nlog ϖθ(xn\ni ) −1\nC\nC\nX\nc=1\nlog ϕ1(µc),\n(16)\nIn contrast, for anomaly samples, we aim to push anomaly\ndistribution away from p1, i.e., negative loss, formally,\nLa\nDPL = 1\nC\nC\nX\nc=1\nlog ϕ1(µc) −1\nM\nM\nX\ni=1\nlog ϖθ(xa\ni ),\n(17)\n4.3. Dispersion Feature Learning\nA critical challenge in OSAD is detecting previously un-\nseen anomalies in open-set environments.\nDue to the\nlimited anomaly observations, existing methods often use\npseudo-anomaly generation strategies as a means of effec-\ntive data augmentation. Nonetheless, the effectiveness of\nthese methods heavily depends on the quality of the pseudo-\nanomaly feature embeddings.\nNotably, pseudo-anomaly\ndistributions often inherit biases from the in-distribution\ndata, which differ from the unknown anomaly distributions\nin out-of-distribution data. Current methods fail to address\nthe relationship between observed and unknown anomalies,\nparticularly for out-of-distribution generalization.\nTo improve out-of-distribution detection, it is essential\nto promote a larger inter-sample dispersion, as greater dis-\ntances among in-distribution samples facilitate their more\neffective separation from out-of-distribution samples.\nIn\nother words, if all inter-sample distances are close to zero,\ni.e., collapse to a single point, it becomes impossible to dif-\nferentiate between samples. Hence, promoting separability\nthrough a larger inter-sample dispersion is critical for accu-\nrately identifying samples that do not belong to known cate-\ngories. To do so, we map the features into a hyperspherical\nspace. Draws inspiration from the vMF distribution [30] in\ndirectional statistics, we compute spherical Gaussian distri-\nbutions for unit-norm features bxi = xi/∥xi∥2\n2. The proba-\nbility density function of a unit vector bxi ∈RD is defined\nin the hyperspherical space as follows:\npD(bxi; bxj, κ) = FD(κ) exp(κ⟨bxi, bxj⟩),\n(18)\nwhere κ ≥0 controls the concentration of the distribution\naround the mean direction bxj and FD(κ) is the normaliza-\ntion factor. A larger κ value increases concentration around\nthe mean, while in the extreme case κ = 0, sample points\nare uniformly distributed on the hypersphere. Therefore, we\ndesign a dispersion loss to optimize large angular distances\nbetween the features of all samples:\nLDFL= 1\nU\nU\nX\ni=1\nlog\n1\nU −1\nU\nX\ni,j=1\n1{i ̸= j} exp(κ⟨bxi, bxj⟩), (19)\nwhere U = N + M, and κ is set to 10 in our experiments.\n4.4. Anomaly Score Prediction\nBased on the above designs, we leverage the multiple-\ninstance-learning (MIL)-based method proposed in [34] to\neffectively learn anomaly scores. Similar to the work [9],\nwe design three modules M = {Ma, Mn and Mr} for esti-\nmating anomaly scores. Firstly, for the feature map xi3, we\ngenerate pixel-wise feature vectors V = {vi}H′×W ′\ni=1\nto rep-\nresent the feature of small patches of the image xi, where\n(H′, W ′) denotes the size of the feature map. These pixel-\nwise representations are then mapped by an anomaly clas-\nsifier Sa to estimate pixel-level anomaly scores. To capture\nthose points with the most salient anomalies, we compute\nthe top-K most anomaly pixel points and define the loss\nfunction as:\nLMa(xi, yi) = Lbinary( 1\nK\nX\nTopK{Sa(vi; θa)}, yi), (20)\n3Here xi refers to convolution maps without flattening (as used above).\n\n\nwhere Lbinary refers to a binary classification loss function,\nand TopK selects the highest K anomaly scores among all\nthe vectors. Secondly, we use Mn to learn the normal fea-\ntures:\nLMn(xi, yi) = Lbinary(Sn(\n1\nH′ × W ′\nH′×W ′\nX\ni=1\nvi; θn), yi),\n(21)\nwhere Sn : V →R is a fully connected binary anomaly\nclassifier. Finally, we define Mr to compute the residual\nanomaly scores between fine-grained visual semantics and\nabstract prototypes:\nLMr = Lbinary(Sr((ψp(xi) −µc∗)/σc∗; θr), yi),\n(22)\nwhere c∗denotes the index of the most probable prototypes,\ni.e., c∗= arg maxc N(ψp(xi); µc, σc), and Sr utilize the\nsame method to obtain anomaly score as Sa.\nTraining. During the training phase, the SB and three\nprediction modules are jointly trained. To this end, we em-\nploy an objective function that encompasses three compo-\nnents as follows:\nL = LMa + LMn + LMr\n|\n{z\n}\nMIL-based learning\n+ Ln\nDPL + La\nDPL\n|\n{z\n}\nSB transform\n+ λLDFL\n| {z }\ndispersion\n. (23)\nwhere the coefficient λ modulates the relative importance\nof dispersion loss, and the learnable parameters include\n{αc, µc, σc}, θψp, and {θa, θn, θr}.\nInference. During the test phase, we find the most sim-\nilar class prototype through SB, and subsequently compute\nthe anomaly score by adding the scores from both Sa and\nSr, while subtracting the normal score obtained from Sn for\nthe given test image.\nIn summary, we reduce all the above processes into an\nalgorithm given in the supplementary material for clarity.\n5. Experiment\n5.1. Dataset and Evaluation Metric\nDataset To validate the effectiveness of DPDL, compre-\nhensive experiments are conducted on nine real-world AD\ndatasets, including six industrial defect detection datasets\n(MVTec AD [3], Optical [40], SDD [?\n], AITEX [38],\nELPV [8], Mastcam [13]) and three medical image datasets\n(Hyper-Kvasir [4], Brain-MRI [36], HeadCT [36]). We fol-\nlow the previous OSAD baselines [9, 45] to adopt two pro-\ntocols for sampling, including general setting and hard set-\nting. The general setting assumes that anomaly examples\nare randomly sampled from the anomaly class, while the\nhard setting samples from a single class to assess general-\nization to new or unseen anomaly classes.\nEvaluation Metric We utilize the widely adopted Area Un-\nder ROC Curve (AUC) as a metric to evaluate the perfor-\nmance across all methods and settings. All reported AUCs\nare averaged results over five independent runs.\n5.2. Baselines\nWe compare DPDL against six related state-of-the-art\nOSAD baselines, including SAOE [19, 31, 39], MLEP [27],\nFLOS [35], DevNet [34], DRA [9], and AHL [45]. MLEP,\nDevNet, DRA, and AHL are specifically designed for\nOSAD. SAOE is a supervised detector enhanced with syn-\nthetic anomalies and anomaly exposure, whereas FLOS is\nan imbalanced classifier leveraging focal loss.\n5.3. Implementation Details\nThe input image size is 448×448×3. We set K in the top-\nK MIL to 10% of the number of all scores per score map.\nAdamW optimizer [29] is used for the parameter optimiza-\ntion using an initial learning rate 2×10−4 with a weight de-\ncay of 1×10−5. DPDL is trained on one NVIDIA GeForce\nRTX 4090 GPU, which are trained using 50 epochs, with 20\niterations per epoch. Following previous protocol [9, 45],\nwe evaluate performance with anomaly sample numbers of\nM = 10 and M = 1, and for robust detection of unseen\nanomalies, we use CutMix [43] to create pseudo-anomaly\nsamples as augmented data for known anomalies. The pro-\ntotype quantity C is set to 32 as default. Our code will be\navailable at our site4.\n5.4. Results under General Setting\nTab. 1 highlights DPDL’s strong performance. In the chal-\nlenging scenario of single-anomaly detection, DPDL im-\nproves the performance of AHL [45] by more than 8.3%\non the AITEX, ELPV, and Mastcam datasets. Furthermore,\nit achieves significant improvements across six additional\ndatasets, which suggest the effective utilization of few-shot\nanomaly examples in DPDL, while mitigating overfitting to\nthe seen anomalies. When shifting to ten anomaly examples\nsettings, DPDL continues to maintain a significant lead with\nover 5.4% improvement on those datasets. Given the rich\nand diverse set of normal samples in these datasets, DPDL\nleverages the DPL component to encapsulate these samples\nwithin a compact, discriminative distribution space, while\neffectively pushing anomalous samples outside this space,\nthereby enabling accurate anomaly detection. In the setting\nwith ten abnormal samples, although existing methods have\nreached performance saturation on the MVTecAD, Optical,\nand SDD datasets, DPDL still has a certain lead, demon-\nstrating the strong ability of DPL and DFL in learning tight\nboundaries of normal sample distributions and generalizing\nto previously unseen anomaly domains. Furthermore, when\nevaluated on the medical datasets BrainMRI and HeadCT,\nDPDL demonstrates competitive performance despite these\ndatasets being notably small in scale and containing only a\nsingle class. This highlights the algorithm’s ability to de-\nliver robust results even in data-scarce conditions.\n4https://github.com/fuyunwang/DPDL\n\n\nTable 1. AUC performance (mean ± std) across nine real-world AD datasets is reported under the general setting. red highlights the best\nresults, and blue indicates sub-optimal outcomes. All baseline SOTA results are sourced from the original papers [9, 45].\nDataset\nDevNet\nFLOS\nSAOE\nMLEP\nDRA\nAHL\nDPDL (Ours)\nTen Training Anomaly Examples\nMVTec AD\n0.945±0.004\n0.939±0.007\n0.926±0.010\n0.907±0.005\n0.959±0.003\n0.970±0.002\n0.977±0.002\nOptical\n0.782±0.065\n0.720±0.055\n0.941±0.013\n0.740±0.039\n0.965±0.006\n0.976±0.004\n0.983±0.005\nSDD\n0.988±0.006\n0.967±0.018\n0.955±0.020\n0.983±0.013\n0.991±0.005\n0.991±0.001\n0.996±0.001\nAITEX\n0.887±0.013\n0.841±0.049\n0.874±0.024\n0.867±0.037\n0.893±0.017\n0.925±0.013\n0.975±0.007\nELPV\n0.846±0.022\n0.818±0.032\n0.793±0.047\n0.794±0.047\n0.845±0.013\n0.850±0.004\n0.937±0.003\nMastcam\n0.790±0.021\n0.703±0.029\n0.810±0.029\n0.798±0.026\n0.848±0.008\n0.855±0.005\n0.934±0.010\nHyper-Kvasir\n0.829±0.018\n0.773±0.029\n0.666±0.050\n0.600±0.069\n0.834±0.004\n0.880±0.003\n0.939±0.005\nBrainMRI\n0.958±0.012\n0.955±0.011\n0.900±0.041\n0.959±0.011\n0.970±0.003\n0.977±0.001\n0.969±0.005\nHeadCT\n0.982±0.009\n0.971±0.004\n0.935±0.021\n0.972±0.014\n0.972±0.002\n0.999±0.003\n0.981±0.003\nOne Training Anomaly Example\nMVTec AD\n0.780±0.020\n0.755±0.136\n0.834±0.007\n0.744±0.019\n0.883±0.008\n0.901±0.003\n0.927±0.002\nOptical\n0.523±0.003\n0.518±0.003\n0.815±0.014\n0.516±0.009\n0.888±0.012\n0.888±0.007\n0.915±0.002\nSDD\n0.881±0.009\n0.840±0.043\n0.781±0.009\n0.811±0.045\n0.859±0.014\n0.909±0.001\n0.917±0.003\nAITEX\n0.598±0.070\n0.538±0.073\n0.675±0.094\n0.564±0.055\n0.692±0.124\n0.734±0.008\n0.838±0.008\nELPV\n0.514±0.076\n0.457±0.056\n0.635±0.092\n0.578±0.062\n0.675±0.024\n0.828±0.005\n0.897±0.002\nMastcam\n0.595±0.016\n0.542±0.017\n0.662±0.018\n0.625±0.045\n0.692±0.058\n0.743±0.003\n0.838±0.011\nHyper-Kvasir\n0.653±0.037\n0.668±0.004\n0.498±0.100\n0.445±0.040\n0.690±0.017\n0.768±0.015\n0.821±0.007\nBrainMRI\n0.694±0.004\n0.693±0.036\n0.531±0.060\n0.632±0.017\n0.744±0.004\n0.866±0.004\n0.893±0.004\nHeadCT\n0.742±0.076\n0.698±0.092\n0.597±0.022\n0.758±0.038\n0.796±0.105\n0.825±0.014\n0.865±0.005\nTable 2. AUC results (mean ± std) under the hard setting. The best and second-best results are highlighted in red and blue, respectively.\nCarpet and Metal nut are subsets of MVTec AD. The datasets used are consistent with those in [9, 45], where those datasets only containing\none anomaly class are excluded to adapt for the hard setting. For detailed class-level results, please refer to the supplementary materia.\nDataset\nDevNet\nFLOS\nSAOE\nMLEP\nDRA\nAHL\nDPDL (Ours)\nTen Training Anomaly Examples\nCarpet (mean)\n0.847±0.017\n0.761±0.012\n0.762±0.073\n0.751±0.023\n0.935±0.013\n0.949±0.002\n0.956±0.004\nMetal nut (mean)\n0.965±0.011\n0.922±0.014\n0.855±0.016\n0.878±0.058\n0.945±0.017\n0.972±0.002\n0.978±0.002\nAITEX (mean)\n0.683±0.032\n0.635±0.043\n0.724±0.032\n0.626±0.041\n0.733±0.009\n0.747±0.002\n0.798±0.005\nELPV (mean)\n0.702±0.023\n0.642±0.032\n0.683±0.047\n0.745±0.020\n0.766±0.029\n0.788±0.003\n0.818±0.003\nMastcam (mean)\n0.588±0.011\n0.616±0.021\n0.697±0.014\n0.588±0.016\n0.695±0.004\n0.721±0.003\n0.778±0.007\nHyper-Kvasir (mean)\n0.822±0.019\n0.786±0.021\n0.698±0.021\n0.571±0.014\n0.844±0.009\n0.854±0.004\n0.864±0.002\nOne Training Anomaly Example\nCarpet (mean)\n0.767±0.018\n0.678±0.040\n0.753±0.055\n0.679±0.029\n0.901±0.006\n0.932±0.003\n0.941±0.006\nMetal nut (mean)\n0.855±0.016\n0.855±0.024\n0.816±0.029\n0.825±0.023\n0.932±0.017\n0.939±0.004\n0.944±0.003\nAITEX (mean)\n0.646±0.034\n0.624±0.024\n0.674±0.034\n0.466±0.030\n0.684±0.033\n0.707±0.007\n0.753±0.005\nELPV (mean)\n0.648±0.057\n0.691±0.008\n0.614±0.048\n0.566±0.111\n0.703±0.022\n0.740±0.003\n0.762±0.003\nMastcam (mean)\n0.511±0.013\n0.524±0.013\n0.689±0.037\n0.541±0.007\n0.667±0.012\n0.673±0.010\n0.733±0.004\nHyper-Kvasir (mean)\n0.595±0.023\n0.571±0.004\n0.406±0.018\n0.480±0.044\n0.700±0.009\n0.706±0.007\n0.715±0.004\n5.5. Results under the Hard Setting\nTab. 2 summarizes the performance comparison under the\nhard setting. It is evident that DPDL achieves the highest\nAUC scores in both the single-anomaly and ten-anomaly\nsample settings. Specifically, compared to the closest com-\npeting method, AHL [45], DPDL achieves an improve-\nment in AUC scores ranging from 0.6% to 7.9% in the ten\nanomaly examples settings and from 0.5% to 8.9% in the\none anomaly example settings, respectively. The observed\nimprovement can be attributed to the strong generalization\nability of DPDL in detecting unseen anomaly classes, even\nwhen the model is trained on only a single anomaly class.\n5.6. Ablation Study\nThe ablation study in Fig. 2 highlights the critical roles of\nthe DPL and DFL components in improving DPDL’s open-\nset anomaly detection. We denote the variants that remove\nonly DPL or DFL as ‘DPDL w/o DPL’ and ‘DPDL w/o\nDFL’, respectively. Compared to the full DPDL model, re-\nmoving neither DPL nor DFL leads to a significant AUC de-\ncline, illustrating their critical utility. Specifically, ‘DPDL\n\n\nMVTec AD\nOptical\nSDD\nAITEX\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nAUC\nDPDL\nDPDL w/o DPL\nDPDL w/o DFL\nELPV\nMastcamHyper-KvasirBrainMRI HeadCT\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nDPDL\nDPDL w/o DPL\nDPDL w/o DFL\nCarpet\nMetal_nut\nAITEX\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nDPDL\nDPDL w/o DPL\nDPDL w/o DFL\nELPV\nMastcam\nHyper-Kvasir\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nDPDL\nDPDL w/o DPL\nDPDL w/o DFL\nTen Training Anomaly Examples Under General Settings\nTen Training Anomaly Examples Under Hard Settings\nFigure 2. Ablation study for SB and DFL under the general settings and hard settings.\n8\n16\n32\n64\nC\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00\n1.02\nAUC\nMVTec AD\nOptical\nSDD\nAITEX\nELPV\nMastcam\nHyper-Kvasir\nBrainMRI\nHeadCT\n0.001\n0.01\n0.1\n1\n10\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00\n1.02\nMVTec AD\nOptical\nSDD\nAITEX\nELPV\nMastcam\nHyper-Kvasir\nBrainMRI\nHeadCT\n0.1\n1\n10\n100\n1000\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00\n1.02\nMVTec AD\nOptical\nSDD\nAITEX\nELPV\nMastcam\nHyper-Kvasir\nBrainMRI\nHeadCT\n0.0001\n0.001\n0.01\n0.1\n1\n10\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00\n1.02\nMVTec AD\nOptical\nSDD\nAITEX\nELPV\nMastcam\nHyper-Kvasir\nBrainMRI\nHeadCT\nFigure 3. Parameter sensitivity analysis for C, ϵ, κ and λ.\nTable 3. An ablation study for Mn, Ma and Mr.\nMn\nMa\nMr\nAITEX\nELPV\nMastcam\nTen Training Anomaly Examples Under General Settings\n0.928 ± 0.019 0.914 ± 0.021\n0.899 ± 0.036\n0.939 ± 0.023 0.919 ± 0.011\n0.908 ± 0.017\n0.963 ± 0.008 0.930 ± 0.013\n0.924 ± 0.019\n0.975 ± 0.007\n0.937 ± 0.003\n0.934 ± 0.010\nTen Training Anomaly Examples Under Hard Settings\n0.746 ± 0.025 0.797 ± 0.026\n0.723 ± 0.017\n0.758 ± 0.035 0.802 ± 0.024\n0.736 ± 0.028\n0.781 ± 0.014 0.811 ± 0.018\n0.762 ± 0.021\n0.798 ± 0.005\n0.818 ± 0.003\n0.778 ± 0.007\nw/o DPL’ exhibits the most significant performance drop\non industrial anomaly datasets, reflecting DPL’s prominent\nrole in learning precise and tight distribution boundaries\nfor normal samples. By transforming normal samples into\nGaussian distribution prototype space and pushing abnor-\nmal samples away, DPL enhances the recognition ability of\nanomaly samples. Meanwhile, ablation studies on ‘DPDL\nw/o DFL’ further highlight the critical role of the DFL com-\nponent. By performing discreteness feature learning in hy-\nperspherical space, DFL enhances the generalization ability\nto out-of-distribution anomalies.\nAdditionally, the ablation experiments on Mn, Ma, and\nMr across three datasets in Tab. 3 reveal their varying con-\ntributions. DPDL shows the most significant performance\ndrop when Mr is removed, which illustrates that Mr plays\nthe most critical role in detecting anomalies. Furthermore,\nthe removal of Ma and Mn lead to a noticeable performance\ndrop in DPDL, which underscores their essential roles.\n5.7. Parameter Sensitivity Analysis\nFig. 3 illustrates the results of the four hyperparameters un-\nder the general settings across nine datasets. Overall, the\nperformance remains stable within a certain range of hyper-\nparameter variations, demonstrating the DPDL’s robustness.\nPrototype quantity C.\nWe begin by investigating the\ncritical impact of the number of initialized prototypes\nC on distributed prototype learning in DPDL. We select\n{8, 16, 32, 64} as the values for the hyperparameters. As\nC increases, DPDL’s performance improves steadily, but\nexcessively large values of C hinder the model’s effective-\nness. This phenomenon is particularly pronounced on AI-\nTEX, Mastcam, MVTecAD, and Hyper-Kvasir, which con-\ntain more categories. One possible explanation is that a pro-\ntotype space with too few prototypes loses discriminative\ninformation, while an excessively large number of proto-\ntypes reduces the compactness of the space.\nDPL trajectory ϵ in Eqns. (9), (10), (11) and (14).\nIt\ncan be observed that, particularly on the AITEX, Mastcam,\nand MVTecAD datasets with a larger number of categories,\nDPDL exhibits a relatively stable performance decline as\nϵ increases. As a crucial parameter in SB, ϵ governs the\ntrajectory state. Since smaller values produce straighter tra-\njectories and larger values increase fluctuation, smaller ϵ fa-\ncilitates sampling more robust abstract prototypes from the\nrelatively dispersed conditional distribution.\nDFL tightness κ. According to Fig. 3, increasing κ gener-\nally enhances model performance, but values above κ = 10\nintroduce negative effects in certain scenarios. A possible\nreason is that excessive sample dispersion makes it more\n\n\nchallenging to tighten the normal distribution boundary.\nLoss parameter λ. We conduct a sensitivity analysis on\nthe loss parameters λ. It can be observe that setting λ =\n0.01 achieves optimal performance on seven larger-scale\ndatasets, while λ = 1 yields the best results on two datasets\nwith limited data. As λ increases, the performance declines,\npotentially due to gradient conflicts among the dispersion\nloss, SB transform loss and the main task loss.\n6. Conclusion\nWe propose Distribution Prototype Diffusion Learning\n(DPDL) for OSAD. DPDL leverages schr¨odinger bridge to\nmap the normal distribution to a prototype space, simultane-\nously repelling anomalies to facilitate precise anomaly de-\ntection. We propose a dispersion feature learning way in\nhyperspherical space, which benefits the detection of out-\nof-distribution anomalies.\nExperimental results illustrate\nDPDL’s robustness in diverse anomaly detection scenarios.\nReferences\n[1] Andra\nAcsintoae,\nAndrei\nFlorescu,\nMariana-Iuliana\nGeorgescu, Tudor Mare, Paul Sumedrea, Radu Tudor\nIonescu, Fahad Shahbaz Khan, and Mubarak Shah.\nUb-\nnormal:\nNew benchmark for supervised open-set video\nanomaly detection.\nIn Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npages 20143–20153, 2022. 1, 2\n[2] Aimira Baitieva, David Hurych, Victor Besnier, and Olivier\nBernard. Supervised anomaly detection for complex indus-\ntrial images. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 17754–\n17762, 2024. 1\n[3] Paul Bergmann, Michael Fauser, David Sattlegger, and\nCarsten Steger.\nMvtec ad–a comprehensive real-world\ndataset for unsupervised anomaly detection. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 9592–9600, 2019. 6, 1\n[4] Hanna Borgli, Vajira Thambawita, Pia H Smedsrud, Steven\nHicks, Debesh Jha, Sigrun L Eskeland, Kristin Ranheim\nRandel, Konstantin Pogorelov, Mathias Lux, Duc Tien Dang\nNguyen, et al. Hyperkvasir, a comprehensive multi-class im-\nage and video dataset for gastrointestinal endoscopy. Scien-\ntific data, 7(1):283, 2020. 6, 1\n[5] Yongxin Chen, Tryphon T Georgiou, and Michele Pavon.\nOn the relation between optimal transport and schr¨odinger\nbridges: A stochastic control viewpoint. Journal of Opti-\nmization Theory and Applications, 169:671–691, 2016. 2\n[6] Songmin Dai, Yifan Wu, Xiaoqiang Li, and Xiangyang\nXue.\nGenerating and reweighting dense contrastive pat-\nterns for unsupervised anomaly detection. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, pages 1454–\n1462, 2024. 1\n[7] Valentin De Bortoli, James Thornton, Jeremy Heng, and Ar-\nnaud Doucet. Diffusion schr¨odinger bridge with applications\nto score-based generative modeling. Advances in Neural In-\nformation Processing Systems, 34:17695–17709, 2021. 2\n[8] Sergiu Deitsch, Vincent Christlein, Stephan Berger, Claudia\nBuerhop-Lutz, Andreas Maier, Florian Gallwitz, and Chris-\ntian Riess. Automatic classification of defective photovoltaic\nmodule cells in electroluminescence images. Solar Energy,\n185:455–468, 2019. 6, 1\n[9] Choubo Ding, Guansong Pang, and Chunhua Shen. Catching\nboth gray and black swans: Open-set supervised anomaly\ndetection. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 7388–7398,\n2022. 1, 2, 5, 6, 7\n[10] Nikita Gushchin, Sergei Kholkin, Evgeny Burnaev, and\nAlexander Korotin. Light and optimal schr¨odinger bridge\nmatching.\nIn Forty-first International Conference on Ma-\nchine Learning, 2024. 2, 4\n[11] Liren He, Zhengkai Jiang, Jinlong Peng, Liang Liu, Qian-\ngang Du, Xiaobin Hu, Wenbing Zhu, Mingmin Chi, Yabiao\nWang, and Chengjie Wang. Learning unified reference rep-\nresentation for unsupervised multi-class anomaly detection.\narXiv preprint arXiv:2403.11561, 2024. 1\n[12] Teng Hu, Jiangning Zhang, Ran Yi, Yuzhen Du, Xu Chen,\nLiang Liu, Yabiao Wang, and Chengjie Wang. Anomalyd-\niffusion: Few-shot anomaly image generation with diffusion\nmodel. In Proceedings of the AAAI Conference on Artificial\nIntelligence, pages 8526–8534, 2024. 1\n[13] Hannah R Kerner, Kiri L Wagstaff, Brian D Bue, Danika F\nWellington, Samantha Jacob, Paul Horton, James F Bell,\nChiman Kwan, and Heni Ben Amor. Comparison of novelty\ndetection methods for multispectral images in rover-based\nplanetary exploration missions. Data Mining and Knowledge\nDiscovery, 34:1642–1675, 2020. 6, 1\n[14] Beomsu Kim,\nGihyun Kwon,\nKwanyoung Kim,\nand\nJong Chul Ye. Unpaired image-to-image translation via neu-\nral schr\\” odinger bridge. arXiv preprint arXiv:2305.15086,\n2023. 2\n[15] Daehyun Kim, Sungyong Baik, and Tae Hyun Kim. San-\nflow: Semantic-aware normalizing flow for anomaly detec-\ntion. Advances in Neural Information Processing Systems,\n36:75434–75454, 2023. 1\n[16] Hyunsu Kim, Jongmin Yoon, and Juho Lee. Fast ensem-\nbling with diffusion schr\\” odinger bridge. arXiv preprint\narXiv:2404.15814, 2024. 2\n[17] Alexander Korotin, Nikita Gushchin, and Evgeny Bur-\nnaev.\nLight schr\\” odinger bridge.\narXiv preprint\narXiv:2310.01174, 2023. 2, 4\n[18] Christian L´eonard. A survey of the schr\\” odinger problem\nand some of its connections with optimal transport. arXiv\npreprint arXiv:1308.0215, 2013. 2, 3\n[19] Chun-Liang Li, Kihyuk Sohn, Jinsung Yoon, and Tomas\nPfister. Cutpaste: Self-supervised learning for anomaly de-\ntection and localization. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npages 9664–9674, 2021. 1, 6\n[20] Hanxi Li, Jingqi Wu, Hao Chen, Mingwen Wang, and Chun-\nhua Shen.\nEfficient anomaly detection with budget anno-\ntation using semi-supervised residual transformer.\narXiv\npreprint arXiv:2306.03492, 2023. 1\n\n\n[21] Xiaofan Li, Zhizhong Zhang, Xin Tan, Chengwei Chen,\nYanyun Qu, Yuan Xie, and Lizhuang Ma. Promptad: Learn-\ning prompts with only normal samples for few-shot anomaly\ndetection. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 16838–\n16848, 2024. 1\n[22] Jingyi Liao, Xun Xu, Manh Cuong Nguyen, Adam Goodge,\nand Chuan Sheng Foo. Coft-ad: Contrastive fine-tuning for\nfew-shot anomaly detection. IEEE Transactions on Image\nProcessing, 2024. 1\n[23] Guan-Horng Liu, Tianrong Chen, Oswin So, and Evangelos\nTheodorou. Deep generalized schr¨odinger bridge. Advances\nin Neural Information Processing Systems, 35:9374–9388,\n2022. 2\n[24] Guan-Horng Liu, Yaron Lipman, Maximilian Nickel, Brian\nKarrer, Evangelos A Theodorou, and Ricky TQ Chen. Gen-\neralized schr\\” odinger bridge matching.\narXiv preprint\narXiv:2310.02233, 2023. 2\n[25] Guan-Horng Liu, Arash Vahdat, De-An Huang, Evange-\nlos A Theodorou, Weili Nie, and Anima Anandkumar. i2-\nsb: Image-to-image schr\\” odinger bridge. arXiv preprint\narXiv:2302.05872, 2023. 2\n[26] Jiaqi Liu, Kai Wu, Qiang Nie, Ying Chen, Bin-Bin\nGao, Yong Liu, Jinbao Wang, Chengjie Wang, and Feng\nZheng.\nUnsupervised continual anomaly detection with\ncontrastively-learned prompt. In Proceedings of the AAAI\nConference on Artificial Intelligence, pages 3639–3647,\n2024. 1\n[27] Wen Liu, Weixin Luo, Zhengxin Li, Peilin Zhao, Shenghua\nGao, et al. Margin learning embedded prediction for video\nanomaly detection with a few anomalies. In IJCAI, pages\n023–3, 2019. 1, 2, 6\n[28] Xinyue Liu,\nJianyuan Wang,\nBiao Leng,\nand Shuo\nZhang.\nDual-modeling decouple distillation for unsuper-\nvised anomaly detection. arXiv preprint arXiv:2408.03888,\n2024. 1\n[29] I Loshchilov. Decoupled weight decay regularization. arXiv\npreprint arXiv:1711.05101, 2017. 6\n[30] Kanti V Mardia and Peter E Jupp. Directional statistics. John\nWiley & Sons, 2009. 5\n[31] Amir Markovitz, Gilad Sharir, Itamar Friedman, Lihi Zelnik-\nManor, and Shai Avidan. Graph embedded pose clustering\nfor anomaly detection.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 10539–10547, 2020. 6\n[32] Maxence Noble, Valentin De Bortoli, Arnaud Doucet, and\nAlain Durmus. Tree-based diffusion schr¨odinger bridge with\napplications to wasserstein barycenters. Advances in Neural\nInformation Processing Systems, 36, 2024. 2\n[33] Guansong Pang, Chunhua Shen, and Anton Van Den Hen-\ngel. Deep anomaly detection with deviation networks. In\nProceedings of the 25th ACM SIGKDD international confer-\nence on knowledge discovery & data mining, pages 353–362,\n2019. 1\n[34] Guansong Pang,\nChoubo Ding,\nChunhua Shen,\nand\nAnton van den Hengel.\nExplainable deep few-shot\nanomaly detection with deviation networks. arXiv preprint\narXiv:2108.00462, 2021. 2, 5, 6\n[35] T-YLPG Ross and GKHP Doll´ar. Focal loss for dense ob-\nject detection.\nIn proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 2980–2988,\n2017. 6\n[36] Mohammadreza\nSalehi,\nNiousha\nSadjadi,\nSoroosh\nBaselizadeh, Mohammad H Rohban, and Hamid R Ra-\nbiee.\nMultiresolution knowledge distillation for anomaly\ndetection.\nIn Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition, pages\n14902–14912, 2021. 6, 1\n[37] Yuyang Shi, Valentin De Bortoli, Andrew Campbell, and Ar-\nnaud Doucet. Diffusion schr¨odinger bridge matching. Ad-\nvances in Neural Information Processing Systems, 36, 2024.\n2\n[38] Javier Silvestre-Blanes, Teresa Albero-Albero, Ignacio Mi-\nralles, Rub´en P´erez-Llorens, and Jorge Moreno. A public\nfabric database for defect detection methods and results. Au-\ntex Research Journal, 19(4):363–374, 2019. 6, 1\n[39] Jihoon Tack, Sangwoo Mo, Jongheon Jeong, and Jinwoo\nShin. Csi: Novelty detection via contrastive learning on dis-\ntributionally shifted instances. Advances in neural informa-\ntion processing systems, 33:11839–11852, 2020. 6\n[40] Matthias Wieler and Tobias Hahn. Weakly supervised learn-\ning for industrial optical inspection. In DAGM symposium\nin, page 11, 2007. 6, 1\n[41] Xincheng Yao, Ruoqi Li, Jing Zhang, Jun Sun, and\nChongyang Zhang.\nExplicit boundary guided semi-push-\npull contrastive learning for supervised anomaly detection.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 24490–24499, 2023.\n1, 2\n[42] Xincheng Yao, Ruoqi Li, Zefeng Qian, Lu Wang, and\nChongyang Zhang. Hierarchical gaussian mixture normal-\nizing flow modeling for unified anomaly detection. arXiv\npreprint arXiv:2403.13349, 2024. 1\n[43] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-\nlarization strategy to train strong classifiers with localizable\nfeatures. In Proceedings of the IEEE/CVF international con-\nference on computer vision, pages 6023–6032, 2019. 6\n[44] Qihang Zhou, Guansong Pang, Yu Tian, Shibo He, and\nJiming Chen. Anomalyclip: Object-agnostic prompt learn-\ning for zero-shot anomaly detection.\narXiv preprint\narXiv:2310.18961, 2023. 1\n[45] Jiawen Zhu, Choubo Ding, Yu Tian, and Guansong Pang.\nAnomaly heterogeneity learning for open-set supervised\nanomaly detection. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n17616–17626, 2024. 1, 2, 6, 7\n[46] Yuansheng Zhu, Wentao Bao, and Qi Yu. Towards open set\nvideo anomaly detection. In European Conference on Com-\nputer Vision, pages 395–412. Springer, 2022. 1, 2\n\n\nDistribution Prototype Diffusion Learning for Open-set Supervised\nAnomaly Detection\nSupplementary Material\n7. Dataset Statistics\nExtensive experiments are conducted on nine real-world\nanomaly detection (AD) datasets. Tab. 4 provides key statis-\ntics for all datasets used in this study. We follow the exact\nsame settings as in previous open-set supervised anomaly\ndetection (OSAD) studies.\nSpecifically, for the MVTec\nAD dataset, we adhere to the original split, dividing the\nnormal samples into training and test sets. For the other\neight datasets, normal samples are randomly partitioned\ninto training and test sets at a 3:1 ratio.\nTable 4. The statistical information for nine real-world anomaly\ndetection (AD) datasets, with the first 15 rows detailing the subsets\nof the MVTec AD dataset.\nDataset\nOriginal Training\nOriginal Test\n|C|\nType\nNormal\nNormal Anomaly\nCarpet\n5\nTextture\n280\n28\n89\nGrid\n5\nTextture\n264\n21\n57\nLeather\n5\nTextture\n245\n32\n92\nTile\n5\nTextture\n230\n33\n83\nWood\n5\nTextture\n247\n19\n60\nBottle\n3\nObject\n209\n20\n63\nCapsule\n5\nObject\n219\n23\n109\nPill\n7\nObject\n267\n26\n141\nTransistor\n4\nObject\n213\n60\n40\nZipper\n7\nObject\n240\n32\n119\nCable\n8\nObject\n224\n58\n92\nHazelnut\n4\nObject\n391\n40\n70\nMetal nut\n4\nObject\n220\n22\n93\nScrew\n5\nObject\n320\n41\n119\nToothbrush\n1\nObject\n60\n12\n30\nMVTecAD\n73\n-\n3629\n467\n1258\nOptical\n1\nObject\n10500\n3500\n2100\nSDD\n1\nTextture\n594\n286\n54\nAITEX\n12 Textture\n1692\n564\n183\nELPV\n2\nTextture\n1131\n377\n715\nMastcam\n11\nObject\n9302\n426\n451\nHyper-Kvasir\n4\nMedical\n2021\n674\n757\nBrainMRI\n1\nMedical\n73\n25\n155\nHeadCT\n1\nMedical\n75\n25\n100\n• MVTec AD [3] is a widely-used benchmark for defect de-\ntection, comprising 15 distinct categories, each of which\nincludes one or several subcategories. The dataset con-\ntains a total of 73 fine-grained anomaly classes at either\nthe texture or object level.\n• Optical [40] is a synthetic dataset designed for industrial\noptical inspection and defect detection. The artificially\ngenerated data mimics real-world tasks.\n• SDD [? ] is a defect product image detection dataset\nwith pixel-level defect annotations. The original images,\nwhich have a resolution of 500 × 1250, are vertically di-\nvided into three segments. Each segment is then anno-\ntated at the pixel level.\n• AITEX [38] is a fabric defect detection dataset that in-\ncludes 12 defect categories with pixel-level annotations.\nThe original images, which have a resolution of 4096 ×\n256, are cropped into multiple 256 × 256 patches. Each\npatch is then re-annotated at the pixel level.\n• ELPV [8] is a dataset for defect detection in electrolumi-\nnescence (EL) images of solar cells. It includes two types\nof defects, corresponding to different types of solar cells:\nmonocrystalline and polycrystalline.\n• Mastcam [13] is a novelty detection dataset con-\nstructed from geological images captured by the mul-\ntispectral imaging system installed on the Mars rover.\nThe dataset includes typical images and images from\n11 novel geological classes.\nEach image comprises\nboth shorter-wavelength (color) channels and longer-\nwavelength (grayscale) channels, with this study focusing\non the shorter-wavelength channels.\n• Hyper-Kvasir [4] is a large-scale, open-access gastroin-\ntestinal dataset collected during real endoscopy and\ncolonoscopy procedures.\nIt comprises four main cate-\ngories and 23 subcategories of endoscopic and colono-\nscopic images. This work focuses on endoscopic images,\nwhere anatomical landmark categories are considered as\nnormal samples and pathological categories are treated as\nabnormal samples.\n• BrainMRI [36] is a brain tumor detection dataset obtained\nthrough magnetic resonance imaging (MRI).\n• HeadCT [36] is a dataset for detecting intracranial hemor-\nrhage obtained through head computed tomography (CT)\nscans.\n8. Full Results under General Setting\nTab. 5 presents a comprehensive comparison of the pro-\nposed DPDL method with state-of-the-art (SOTA) ap-\nproaches under general settings.\nIt reports performance\nmetrics for each category within the MVTec AD dataset.\nOverall, the DPDL model consistently outperforms base-\nline methods across all application scenarios in both ten-\nshot and one-shot settings, achieving the best performance\nin terms of Area Under the Curve (AUC).\n\n\nTable 5. AUC performance (mean ± std) across nine real-world AD datasets is reported under the general setting. red highlights the best\nresults, and blue indicates sub-optimal outcomes. All baseline SOTA results are sourced from the original papers [9, 45].\nDataset\nOne Training Anomaly Example\nTen Training Anomaly Examples\nDevNet\nFLOS\nSAOE\nMLEP\nDRA\nAHL\nDPDL (Ours)\nDevNet\nFLOS\nSAOE\nMLEP\nDRA\nAHL\nDPDL (Ours)\nCarpet\n0.746±0.076\n0.755±0.026\n0.766±0.098\n0.701±0.091\n0.859±0.023\n0.877±0.004 0.914±0.006\n0.867±0.040\n0.780±0.009\n0.755±0.136\n0.781±0.049\n0.940±0.027 0.953±0.001\n0.988±0.002\nGrid\n0.891±0.040\n0.871±0.076\n0.921±0.032\n0.839±0.028\n0.972±0.011\n0.975±0.005 0.999±0.001\n0.967±0.021\n0.966±0.005\n0.952±0.011\n0.980±0.009\n0.987±0.009 0.992±0.002\n0.999±0.001\nLeather\n0.873±0.026\n0.791±0.057\n0.996±0.007\n0.781±0.020\n0.989±0.005\n0.988±0.001 0.996±0.001\n0.999±0.001\n0.993±0.004 1.000±0.000\n0.813±0.158\n1.000±0.000 1.000±0.000\n1.000±0.000\nTile\n0.752±0.038\n0.787±0.038\n0.935±0.034\n0.927±0.036\n0.965±0.015\n0.968±0.001 0.994±0.002\n0.987±0.005\n0.952±0.010\n0.944±0.013\n0.988±0.009\n0.994±0.006 1.000±0.000\n0.999±0.001\nWood\n0.900±0.068\n0.927±0.065\n0.948±0.009\n0.660±0.142\n0.985±0.011\n0.987±0.003 0.998±0.002\n0.999±0.001 1.000±0.000\n0.976±0.031\n0.999±0.002 0.998±0.001 0.998±0.000\n0.998±0.001\nBottle\n0.976±0.006\n0.975±0.023\n0.989±0.019\n0.927±0.090\n1.000±0.000\n1.000±0.000 1.000±0.000\n0.993±0.008\n0.995±0.002\n0.998±0.003\n0.981±0.004 1.000±0.000 1.000±0.000\n1.000±0.000\nCapsule\n0.564±0.032\n0.666±0.020\n0.611±0.109\n0.558±0.075\n0.631±0.056\n0.665±0.030 0.757±0.017\n0.865±0.057\n0.902±0.017\n0.850±0.054\n0.818±0.063 0.935±0.022\n0.930±0.001\n0.976±0.004\nPill\n0.769±0.017\n0.745±0.064\n0.652±0.078\n0.656±0.061\n0.832±0.034\n0.840±0.003 0.842±0.002\n0.866±0.038 0.929±0.012\n0.872±0.049\n0.845±0.048\n0.904±0.024\n0.918±0.001\n0.923±0.001\nTransistor\n0.722±0.032\n0.709±0.041\n0.680±0.182\n0.695±0.124\n0.668±0.068\n0.796±0.003 0.748±0.002\n0.924±0.027\n0.862±0.037\n0.860±0.053 0.927±0.043\n0.915±0.025\n0.926±0.009\n0.928±0.001\nZipper\n0.922±0.018\n0.885±0.033\n0.970±0.033\n0.865±0.086\n0.984±0.016\n0.986±0.000 0.989±0.001\n0.990±0.009\n0.990±0.008\n0.995±0.004\n0.965±0.002 1.000±0.000 1.000±0.000\n1.000±0.000\nCable\n0.783±0.058\n0.790±0.039\n0.819±0.060\n0.688±0.017\n0.876±0.012\n0.858±0.011 0.935±0.008\n0.892±0.020\n0.890±0.063\n0.862±0.022\n0.857±0.062\n0.909±0.011 0.921±0.001\n0.929±0.000\nHazelnut\n0.979±0.010\n0.976±0.021\n0.961±0.042\n0.704±0.090\n0.977±0.030\n0.984±0.004 0.997±0.002\n1.000±0.000 1.000±0.000 1.000±0.000 1.000±0.000 1.000±0.000 1.000±0.000\n1.000±0.000\nMetal nut\n0.876±0.007\n0.930±0.022\n0.922±0.033\n0.878±0.038\n0.948±0.046\n0.952±0.003 0.948±0.007\n0.991±0.006\n0.984±0.004\n0.976±0.013\n0.974±0.009 0.997±0.002 0.998±0.000\n0.996±0.001\nScrew\n0.399±0.187\n0.337±0.091\n0.653±0.074\n0.675±0.294\n0.903±0.064\n0.927±0.009 0.977±0.004\n0.970±0.015\n0.940±0.017\n0.975±0.023\n0.899±0.039\n0.977±0.009 0.985±0.002\n0.995±0.001\nToothbrush\n0.753±0.027\n0.731±0.028\n0.686±0.110\n0.617±0.058\n0.650±0.029\n0.794±0.016 0.807±0.001\n0.860±0.066\n0.900±0.008\n0.865±0.062\n0.783±0.048\n0.826±0.021 0.921±0.007\n0.929±0.000\nMVTec AD\n0.780±0.020\n0.755±0.136\n0.834±0.007\n0.744±0.019\n0.883±0.008\n0.901±0.003 0.927±0.002\n0.945±0.004\n0.939±0.007\n0.926±0.010\n0.907±0.005\n0.959±0.003 0.970±0.002\n0.977±0.002\nOptical\n0.523±0.003\n0.518±0.003\n0.815±0.014\n0.516±0.009\n0.888±0.012\n0.888±0.007 0.915±0.002\n0.782±0.065\n0.720±0.055\n0.941±0.013\n0.740±0.039\n0.965±0.006 0.976±0.004\n0.983±0.005\nSDD\n0.881±0.009\n0.840±0.043\n0.781±0.009\n0.811±0.045\n0.859±0.014\n0.909±0.001 0.917±0.003\n0.988±0.006\n0.967±0.018\n0.955±0.020\n0.983±0.013\n0.991±0.005 0.991±0.001\n0.996±0.001\nAITEX\n0.598±0.070\n0.538±0.073\n0.675±0.094\n0.564±0.055\n0.692±0.124\n0.734±0.008 0.838±0.008\n0.887±0.013\n0.841±0.049\n0.874±0.024\n0.867±0.037\n0.893±0.017 0.925±0.013\n0.975±0.007\nELPV\n0.514±0.076\n0.457±0.056\n0.635±0.092\n0.578±0.062\n0.675±0.024\n0.828±0.005 0.897±0.002\n0.846±0.022\n0.818±0.032\n0.793±0.047\n0.794±0.047\n0.845±0.013 0.850±0.004\n0.937±0.003\nMastcam\n0.595±0.016\n0.542±0.017\n0.662±0.018\n0.625±0.045\n0.692±0.058\n0.743±0.003 0.838±0.011\n0.790±0.021\n0.703±0.029\n0.810±0.029\n0.798±0.026\n0.848±0.008 0.855±0.005\n0.934±0.010\nHyper-Kvasir\n0.653±0.037\n0.668±0.004\n0.498±0.100\n0.445±0.040\n0.690±0.017\n0.768±0.015 0.821±0.007\n0.829±0.018\n0.773±0.029\n0.666±0.050\n0.600±0.069\n0.834±0.004 0.880±0.003\n0.939±0.005\nBrainMRI\n0.694±0.004\n0.693±0.036\n0.531±0.060\n0.632±0.017\n0.744±0.004\n0.866±0.004 0.893±0.004\n0.958±0.012\n0.955±0.011\n0.900±0.041\n0.959±0.011 0.970±0.003 0.977±0.001\n0.969±0.005\nHeadCT\n0.742±0.076\n0.698±0.092\n0.597±0.022\n0.758±0.038\n0.796±0.105\n0.825±0.014 0.865±0.005\n0.982±0.009\n0.971±0.004\n0.935±0.021\n0.972±0.014\n0.972±0.002 0.999±0.003\n0.981±0.003\nTable 6. Detailed class-level AUC results (mean ± std) under the hard setting. The best and second-best results are highlighted in red and\nblue, respectively. Carpet and Metal nut are subsets of MVTec AD.\nDataset\nOne Training Anomaly Example\nTen Training Anomaly Examples\nDevNet\nFLOS\nSAOE\nMLEP\nDRA\nAHL\nDPDL (Ours)\nDevNet\nFLOS\nSAOE\nMLEP\nDRA\nAHL\nDPDL (Ours)\nCarpet\nColor\n0.716±0.085 0.467±0.278 0.763±0.100 0.547±0.056 0.879±0.021 0.894±0.004 0.909±0.001 0.767±0.015 0.760±0.005 0.467±0.067 0.698±0.025 0.886±0.042 0.929±0.007 0.933±0.002\nCut\n0.666±0.035 0.685±0.007 0.664±0.165 0.658±0.056 0.902±0.033 0.934±0.003 0.941±0.003 0.819±0.037 0.688±0.059 0.793±0.175 0.653±0.120 0.922±0.038 0.943±0.002 0.951±0.004\nHole\n0.721±0.067 0.594±0.142 0.772±0.071 0.653±0.065 0.901±0.033 0.935±0.014 0.945±0.009 0.814±0.038 0.733±0.014 0.831±0.125 0.674±0.076 0.947±0.016 0.960±0.003 0.964±0.003\nMetal\n0.819±0.032 0.701±0.028 0.780±0.172 0.706±0.047 0.871±0.037 0.931±0.007 0.940±0.001 0.863±0.022 0.678±0.083 0.883±0.043 0.764±0.061 0.933±0.022 0.921±0.003 0.938±0.005\nThread\n0.912±0.044 0.941±0.005 0.787±0.204 0.831±0.117 0.950±0.029 0.966±0.005 0.970±0.002 0.972±0.009 0.946±0.005 0.831±0.297 0.967±0.006 0.989±0.004 0.991±0.001 0.993±0.000\nMean\n0.767±0.018 0.678±0.040 0.753±0.055 0.679±0.029 0.901±0.006 0.932±0.003 0.941±0.006 0.847±0.017 0.761±0.012 0.762±0.073 0.751±0.023 0.935±0.013 0.949±0.002 0.956±0.004\nMetal nut\nBent\n0.797±0.048 0.851±0.046 0.864±0.032 0.743±0.013 0.952±0.020 0.954±0.003 0.958±0.001 0.904±0.022 0.827±0.075 0.901±0.023 0.956±0.013 0.990±0.003 0.989±0.000 0.991±0.002\nColor\n0.909±0.023 0.821±0.059 0.857±0.037 0.835±0.075 0.946±0.023 0.933±0.008 0.938±0.003 0.978±0.016 0.978±0.008 0.879±0.018 0.945±0.039 0.967±0.011 0.958±0.001 0.969±0.005\nFlip\n0.764±0.014 0.799±0.058 0.751±0.090 0.813±0.031 0.921±0.029 0.931±0.002 0.940±0.004 0.987±0.004 0.942±0.009 0.795±0.062 0.805±0.057 0.913±0.021 0.937±0.003 0.955±0.003\nScratch\n0.952±0.052 0.947±0.027 0.792±0.075 0.907±0.085 0.909±0.023 0.934±0.005 0.936±0.002 0.991±0.017 0.943±0.002 0.845±0.041 0.805±0.153 0.911±0.034 0.999±0.000 0.992±0.002\nMean\n0.855±0.016 0.855±0.024 0.816±0.029 0.825±0.023 0.932±0.017 0.939±0.004 0.944±0.003 0.965±0.011 0.922±0.014 0.855±0.016 0.878±0.058 0.945±0.017 0.972±0.002 0.978±0.002\nAITEX\nBroken end\n0.712±0.069 0.645±0.030 0.778±0.068 0.441±0.111 0.708±0.094 0.704±0.005 0.761±0.010 0.658±0.111 0.585±0.037 0.712±0.068 0.732±0.065 0.693±0.099 0.735±0.010 0.796±0.001\nBroken pick\n0.552±0.003 0.598±0.023 0.644±0.039 0.476±0.070 0.731±0.072 0.727±0.003 0.760±0.014 0.585±0.028 0.548±0.054 0.629±0.012 0.555±0.027 0.760±0.037 0.683±0.002 0.784±0.011\nCut selvage\n0.689±0.016 0.694±0.036 0.681±0.077 0.434±0.149 0.739±0.101 0.753±0.007 0.765±0.007 0.709±0.039 0.745±0.035 0.770±0.014 0.682±0.025 0.777±0.036 0.781±0.006 0.796±0.005\nFuzzyball\n0.617±0.075 0.525±0.043 0.650±0.064 0.525±0.157 0.538±0.092 0.647±0.007 0.715±0.013 0.734±0.039 0.550±0.082 0.842±0.026 0.677±0.223 0.701±0.093 0.775±0.024 0.808±0.003\nNep\n0.722±0.023 0.734±0.038 0.710±0.044 0.517±0.059 0.717±0.052 0.703±0.005 0.757±0.005 0.810±0.042 0.746±0.060 0.771±0.032 0.740±0.052 0.750±0.038 0.792±0.007 0.811±0.005\nWeft crack\n0.586±0.134 0.546±0.114 0.582±0.108 0.400±0.029 0.669±0.045 0.706±0.009 0.758±0.006 0.599±0.137 0.636±0.051 0.618±0.172 0.370±0.037 0.717±0.072 0.713±0.003 0.790±0.004\nMean\n0.646±0.034 0.624±0.024 0.674±0.034 0.466±0.030 0.684±0.033 0.707±0.007 0.753±0.005 0.683±0.032 0.635±0.043 0.724±0.032 0.656±0.041 0.733±0.009 0.747±0.002 0.798±0.004\nELPV\nMono\n0.634±0.087 0.717±0.025 0.563±0.102 0.649±0.027 0.735±0.031 0.774±0.013 0.785±0.002 0.599±0.040 0.629±0.072 0.569±0.035 0.756±0.045 0.731±0.021 0.745±0.004 0.793±0.003\nPoly\n0.662±0.050 0.665±0.021 0.665±0.173 0.483±0.247 0.671±0.051 0.705±0.006 0.738±0.008 0.804±0.022 0.662±0.042 0.796±0.084 0.734±0.078 0.800±0.064 0.831±0.011 0.843±0.004\nMean\n0.648±0.057 0.691±0.008 0.614±0.048 0.566±0.111 0.703±0.022 0.740±0.003 0.762±0.003 0.702±0.023 0.646±0.032 0.683±0.047 0.745±0.020 0.766±0.029 0.788±0.003 0.818±0.003\nMastcam\nBedrock\n0.495±0.028 0.499±0.056 0.636±0.072 0.532±0.036 0.668±0.012 0.679±0.012 0.732±0.003 0.550±0.053 0.499±0.098 0.636±0.068 0.512±0.062 0.658±0.021 0.673±0.006 0.757±0.004\nBroken-rock\n0.533±0.020 0.569±0.025 0.699±0.058 0.544±0.088 0.645±0.053 0.661±0.009 0.738±0.004 0.547±0.018 0.608±0.085 0.712±0.052 0.651±0.063 0.649±0.047 0.722±0.004 0.783±0.002\nDrill-hole\n0.555±0.037 0.539±0.077 0.697±0.074 0.636±0.066 0.657±0.070 0.654±0.004 0.738±0.011 0.583±0.022 0.601±0.009 0.682±0.042 0.660±0.002 0.725±0.005 0.760±0.003 0.797±0.004\nDrt\n0.529±0.046 0.591±0.042 0.735±0.020 0.624±0.042 0.713±0.053 0.724±0.006 0.745±0.005 0.621±0.043 0.652±0.024 0.761±0.062 0.616±0.048 0.760±0.033 0.772±0.004 0.818±0.002\nDump-pile\n0.521±0.020 0.508±0.021 0.682±0.022 0.545±0.127 0.767±0.043 0.756±0.011 0.764±0.003 0.705±0.011 0.700±0.070 0.750±0.037 0.696±0.047 0.748±0.066 0.802±0.005 0.830±0.005\nFloat\n0.502±0.020 0.551±0.030 0.711±0.041 0.530±0.075 0.670±0.065 0.702±0.005 0.739±0.007 0.615±0.052 0.736±0.041 0.718±0.064 0.671±0.032 0.744±0.073 0.765±0.002 0.816±0.012\nMeteorite\n0.467±0.049 0.462±0.077 0.669±0.037 0.476±0.014 0.637±0.015 0.616±0.013 0.704±0.004 0.554±0.021 0.568±0.053 0.647±0.030 0.473±0.047 0.716±0.004 0.691±0.001 0.785±0.017\nScuff\n0.472±0.031 0.508±0.070 0.679±0.048 0.492±0.037 0.549±0.027 0.581±0.020 0.714±0.002 0.528±0.034 0.575±0.042 0.676±0.019 0.504±0.052 0.636±0.086 0.656±0.009 0.777±0.003\nVeins\n0.527±0.023 0.493±0.052 0.688±0.069 0.489±0.028 0.699±0.045 0.687±0.017 0.789±0.003 0.589±0.072 0.608±0.044 0.686±0.053 0.510±0.090 0.620±0.036 0.650±0.003 0.766±0.008\nMean\n0.511±0.013 0.524±0.013 0.689±0.037 0.541±0.007 0.667±0.012 0.673±0.010 0.733±0.004 0.588±0.011 0.616±0.021 0.697±0.014 0.588±0.016 0.695±0.004 0.721±0.003 0.778±0.007\nHyper-Kvasir\nBarretts\n0.672±0.014 0.703±0.040 0.382±0.117 0.438±0.111 0.772±0.019 0.792±0.007 0.793±0.000 0.834±0.012 0.764±0.066 0.698±0.037 0.540±0.014 0.824±0.006 0.829±0.002 0.832±0.004\nBarretts-short-seg 0.604±0.048 0.538±0.033 0.367±0.050 0.532±0.075 0.674±0.018 0.651±0.006 0.658±0.003 0.799±0.036 0.810±0.034 0.661±0.034 0.480±0.107 0.835±0.021 0.895±0.003 0.906±0.002\nEsophagitis-a\n0.569±0.051 0.536±0.040 0.518±0.063 0.491±0.084 0.778±0.020 0.760±0.006 0.758±0.001 0.844±0.014 0.815±0.022 0.820±0.034 0.646±0.036 0.881±0.035 0.878±0.021 0.878±0.003\nEsophagitis-b-d\n0.536±0.033 0.505±0.039 0.358±0.039 0.457±0.086 0.577±0.025 0.622±0.014 0.652±0.002 0.810±0.015 0.754±0.073 0.611±0.017 0.621±0.042 0.837±0.009 0.815±0.010 0.841±0.002\nMean\n0.595±0.023 0.571±0.004 0.406±0.018 0.480±0.044 0.700±0.009 0.706±0.007 0.715±0.004 0.822±0.019 0.786±0.021 0.698±0.021 0.571±0.014 0.844±0.009 0.854±0.004 0.864±0.002\n9. Detailed Class-level AUC Results under\nHard Setting\nTo evaluate the performance of the DPDL framework in\ndetecting emerging anomaly classes, we conducted exper-\niments under challenging settings and provided detailed\nresults on six multi-subset datasets, including per-class\nanomaly performance, as shown in Tab. 6.\nOverall, the\nDPDL model achieved the highest AUC scores across both\nM = 1 and M = 10 settings.\n\n\n10. The Algorithm of DPDL\nAlgorithm 1 Distribution Prototype Diffusion Learning\n1: Input: Input X = {(xi, yi)}, C, ϵ, κ\n2: for epoch = 1 to n do\n3:\nExtract features F\nfeature\n←−X\n4:\nDistribution of normal samples transform PMGP\nbridge\n←−\nP(F)\n5:\nDistribution Prototype Learning LDPL = Ln\nDPL +\nLa\nDPL\n6:\nDispersion Feature Learning LDFL\n7:\nSample xi ∼X, ec∗∼PMGP\n8:\nCalculate scores Sa ←Ma, Sn ←Mn, Sr ←Mr\n9: end for\n10: Output : Anomaly score S ←Sr + Sa −Sn\n\n\n11. Derivation of Eqns. (13) and (14)\nWe use Eqns. (8) and (12) to derive Eqn. (13) as follows:\nπ(ψ(xn\ni )|xn\ni ) =\n1\nϖ(xn\ni ) exp(⟨xn\ni , ψ(xn\ni )⟩\nϵ\n)\nC\nX\nc=1\nαcN(ψ(xn\ni ); µc, σc)\n=\n1\nϖ(xn\ni )\nC\nX\nc=1\nαc(2π)−D/2|σc|−1/2 exp(⟨xn\ni , ψ(xn\ni )⟩\nϵ\n) exp(−1\n2(ψ(xn\ni )))⊤σ−1\nc (ψ(xn\ni ) −µc))\n=\n1\nϖ(xn\ni )\nC\nX\nc=1\nαc(2π)−D/2|σc|−1/2 exp( 1\n2ϵ(2xn\ni\n⊤ψ(xn\ni ) −ψ(xn\ni )⊤ϵσ−1\nc ψ(xn\ni )⊤+ 2µ⊤\nc ϵσ−1\nc ψ(xn\ni ) −µ⊤\nc ϵσ−1\nc µc))\n=\n1\nϖ(xn\ni )\nC\nX\nc=1\nαc(2π)−D/2|σc|−1/2 exp( 1\n2ϵ(−ψ(xn\ni )⊤ϵσ−1\nc ψ(xn\ni )⊤+ 2 (1\nϵ σcxn\ni + µc)⊤\n|\n{z\n}\neµc(xn\ni)\nϵσ−1\nc ψ(xn\ni ) −µ⊤\nc ϵσ−1\nc µc))\n=\n1\nϖ(xn\ni )\nC\nX\nc=1\nαc(2π)−D/2|σc|−1/2 exp(−1\n2ϵ(ψ(xn\ni ) −eµc(xn\ni )⊤ϵσ−1\nc (ψ(xn\ni ) −eµc(xn\ni ))))\nexp( 1\n2ϵ(−µ⊤\nc ϵσ−1\nc µc + eµ⊤\nc (xn\ni )ϵσ−1\nc\neµ⊤\nc (xn\ni ))\n=\n1\nϖ(xn\ni )\nC\nX\nc=1\nαc exp(µ⊤\nc ϵσ−1\nc µc + (eµ⊤\nc ϵ−1σc + eµc)⊤(xn\ni )ϵσ−1\nc (eµ⊤\nc ϵ−1σc + eµc)(xn\ni )\n2ϵ\n)N(ψ(xn\ni ); eµc(xn\ni ), σc)\n=\n1\nϖ(xn\ni )\nC\nX\nc=1\nαc exp( 1\n2ϵ2 (xn\ni )⊤σcxn\ni + 1\nϵ (eµc)⊤(xn\ni ))\n|\n{z\n}\neαc(xn\ni)\nN(ψ(xn\ni ); eµc(xn\ni ), σc)\n= eη(xn\ni )\nC\nX\nc=1\neαc(xn\ni )N(ψ(xn\ni ); eµc(xn\ni ), σc)\nwhere eη(xn\ni ) =\n1\nϖ(xn\ni) =\n1\nPC\nc=1 eαc(xn\ni).\n\n\nAccording to Eqn. (4), we derive Eqn. (14) as follows:\ng(xn\ni , t) = ϵ▽xn\nilog\nZ\nRD N(ψ(xn\ni )|xn\ni , (1 −t)ϵI) exp(∥ψ(xn\ni )∥2\n2ϵ\n)ϕ1(ψ(xn\ni ))dψ(xn\ni )\n= ϵ▽xn\nilog\nZ\nRD N(ψ(xn\ni )|xn\ni , (1 −t)ϵI) exp(∥ψ(xn\ni )∥2\n2ϵ\n)\nC\nX\nc=1\nαcN(ψ(xn\ni ); eµc, σc)dψ(xn\ni )\n= ϵ▽xn\nilog((2π)−D\n2 |(1 −t)ϵI|−1\n2\nC\nX\nc=1\n{αc|σc|−1\n2\nZ\nRD exp(−(ψ(xn\ni ) −xn\ni )⊤(ψ(xn\ni ) −xn\ni )\n2ϵ(1 −t)\n−(ψ(xn\ni ) −eµc)ϵσ−1\nc (ψ(xn\ni ) −eµc)\n2ϵ\n+ ψ(xn\ni )⊤ψ(xn\ni )\n2ϵ\n)})\n= ϵ▽xn\nilog(exp(−xn\ni\n⊤xn\ni\n2ϵ(1 −t))\nC\nX\nc=1\n{αc|σc|−1\n2 exp(−eµ⊤\nc ϵσ−1 eµc\n2ϵ\n)})\nZ\nRD exp(−1\n2[ψ(xn\ni )⊤(\nt\nϵ(1 −t)I + σ−1\nc )\n|\n{z\n}\nΣtc\nψ(xn\ni )] + [\n1\nϵ(1 −t)xn\ni + σ−1\nc\neµc]⊤\n|\n{z\n}\nhc(xn\ni,t)\nψ(xn\ni )dψ(xn\ni ))\n= ϵ▽xn\nilog((2π)−D\n2 exp(−xn\ni\n⊤xn\ni\n2ϵ(1 −t))\nC\nX\nc=1\n{αc(2π)−D\n2 |σc|−1\n2 exp(−eµ⊤\nc ϵσ−1\nc\neµc\n2ϵ\n)\n|\n{z\n}\nN(xn\ni|0,ϵ(1−t))I\nC\nX\nc=1\n(αcN(eµc(xn\ni )|0, σc)N(hc(xn\ni , t)|0, Σt\nc))\n|\n{z\n}\nρMGP(xn\ni)\n})\n= ϵρMGP(xn\ni )∇xn\ni log(N(xn\ni |0, ϵ(1 −t))I)\n\n\n"}
{"text": "Exercises on the Kepler ellipses through a\nfixed point in space, after Otto Laporte\nGert Heckman\nRadboud University Nijmegen\nDedicated to Tom Koornwinder on the occasion\nof his 80th birthday\nAbstract\nThis article has a twofold purpose. On the one hand I would like\nto draw attention to some nice exercises on the Kepler laws, due to\nOtto Laporte from 1970. Our discussion here has a more geometric\nflavour than the original analytic approach of Laporte.\nOn the other hand it serves as an addendum to a paper of mine\nfrom 1998 on the quantum integrability of the Kovalevsky top. Later\nI learned that this integrability result had been obtained already long\nbefore by Laporte in 1933.\n1\nIntroduction\nIn the first decade of this century Maris van Haandel and I taught for several\nyears a master class for high school students on the Kepler laws of planetary\nmotion. The proof that the orbits of the planets are ellipses is usually given by\nclever calculus tricks, which might leave the innocent student with a feeling\nof black magic, although opinions can differ. For example, Herbert Goldstein\ndescribes this proof as the “simplest way to integrate the equation for the\norbit”, see Section 3.7 of his excellent text book on classical mechanics [2].\nIn the preparation of our master class we found a proof, that was more\ngeometric in nature, and based on the focus-focus characterization of ellipses\n[4]. After the standard initial discussion in Section 2 of the conservation laws\nof angular momentum and total energy, and their consequences for the Kepler\n1\narXiv:2502.21222v1  [math.SG]  28 Feb 2025\n\n\nproblem, our proof will be recalled in the Section 3. An elegant alternative\ngeometric proof based on the focus-directrix characterization of ellipses was\ngiven by Alexander Givental [1]. Several other proofs, like the original one of\nIsaac Newton from 1687 and the one by Richard Feynman from 1964, were\ndiscussed in modern mathematical language in [4].\nRecently I became aware of a paper by Otto Laporte on some geometric\nproperties of the Kepler ellipses through a fixed point in space [11]. His results\nwere obtained while teaching classical mechanics during numerous years in\norder to provide interesting exercises for students learning the mathematics\nof the Kepler laws. His analytic results will be conveniently derived in a\ngeometric way in Section 4.\nThe final Section 5 serves as an addendum to an old paper of mine on\nthe quantization of the Kovalevsky top [5].\nI would like to thank Rainer Kaenders and the anonymous referee for\nuseful comments.\n2\nThe familiar conservation laws\nLet r be the radius vector of a point in R3 and let the scalar r denotes its\nlength. If r moves in time t then ˙r denotes its velocity and ¨r its acceleration.\nAs usual the dot always stands for the derivative with respect to time. The\nKepler problem studies the solutions of Newton’s equation of motion\nµ¨r = F\nfor an inverse square force field F = −kr/r3 defined on R3 minus the origin.\nThe vector r describes the relative motion of a particle with mass m around\nanother particle with mass M. The parameter µ = mM/(m + M) is called\nthe reduced mass and k = GmM the coupling constant, with G Newton’s\nuniversal gravitational constant.\nThe second law of Kepler that the motion is planar and that the radius\nvector traces out equal areas in inequal times is easy to prove. Moreover it\nholds for a general central force field F, that is a force field of the form\nF(r) = f(r)r/r\nwith f a scalar function on R3 minus the origin. Writing p = µ˙r for the\nmomentum vector it follows from the Leibniz product rule that the angular\n2\n\n\nmomentum vector L = r × p is conserved, which in case L ̸= 0 implies that\nthe motion takes place in the plane perpendicular to L. Since the area of\nthe surface traced out by the radius vector r in a time interval t0 < t < t1 is\nequal to\n1\n2\nZ t1\nt0\n|r × ˙r| dt = L(t1 −t0)/(2µ)\nwe conclude that the radius vector r in a central force field sweeps out equal\nareas in equal times.\nIf the central force field F is in addition spherically symmetric, that is\nF(r) = f(r)r/r\nwith f a scalar function on R+, then the potential function V is defined by\nV (r) = −\nZ\nf(r)dr\nand satisfies d{V (r)}/dt = −f(r)(r · ˙r)/r by the chain rule. In turn this\nimplies that the total energy\nH = p2/(2µ) + V (r)\nis conserved for solutions of Newton’s equation of motion. For the Newtonian\nforce field f(r) = −k/r2 the potential function becomes V (r) = −k/r.\n3\nA geometric focus-focus proof\nIn this section an ellipse will be the geometric locus of points in a plane for\nwhich the sum of the distances to two given points is constant. The two\ngiven points are called the foci, and the sum of the distances is denoted 2a\nand called the major axis. The distance between the given foci is denoted 2c,\nand 2b > 0, defined by a2 = b2 + c2, is called the minor axis. The quotient\n0 ≤e = c/a ≤1 is called the eccentricity of the ellipse. If e = 0 then the\nellipse becomes a circle, while if e = 1 then the ellipse degenerates to a line\nsegment.\nLet us continue the discussion at the end of the previous section, and\nlet us assume throughout this section that both L ̸= 0 (excluding collinear\nmotion) and H < 0 are fixed. Consider the following figure of the plane\n3\n\n\nperpendicular to L. The circle C with center 0 and radius −k/H > 0 is the\nboundary of a disc where motion with fixed energy H < 0 can take place.\nIndeed, we have\nH = p2/(2µ) −k/r ≥−k/r\nand so r ≤−k/H with equality if and only if p = 0. The solutions t 7→r\nof the Kepler problem starting from rest at points of C fall straight onto the\norigin 0. For this reason C is called the fall circle [16].\nLet s = −kr/(rH) be the projection of r from the center 0 on this circle\nC. The line L through r with direction vector p is the tangent line to the\norbit E at position r with momentum p. Let t be the orthogonal reflection of\nthe point s in the tangent line L. As time varies, the position vector r moves\nalong the orbit E and also p = µ˙r and L move along with it, and likewise\nthe point s moves along the fall circle C. It is a good question to investigate\nhow the point t moves.\nb\nb\nb\nb\nb\nb\n0\nt\np\nr\ns\nn\nC\nE\nL\nN\nTheorem 3.1. The point t is equal to K/(µH) with\nK = p × L −kµr/r\nthe so called Lenz vector. The Lenz vector K and therefore also the vector t\nare conserved quantities for the Kepler problem.\n4\n\n\nProof. The line N spanned by n = p×L is perpendicular to L. The point t is\nobtained from s = −kr/(rH) by subtracting twice the orthogonal projection\nof s −r on the line N, and therefore\nt = s −2((s −r) · n)n/n2.\nUsing u · (v × w) = (u × v) · w for all vectors u, v, w in R3 we get\n2(r −s) · n = 2(H + k/r)r · (p × L)/H = p2L2/(µH)\nand since n2 = p2L2 we conclude that\nt = −kr/(rH) + n/(µH) = K/(µH)\nwith K = p × L −kµr/r the Lenz vector. The second claim that ˙K = 0 is\nderived by a straightforward computation using the Leibniz product rule for\ndifferentiation, and is left to the reader as an exercise.\nCorollary 3.2. The orbit E is an ellipse with foci 0 and t, and major axis\nequal to 2a = −k/H.\nProof. Since orthogonal reflections preserve lengths we have\n|t −r| + |r −0| = |s −r| + |r −0| = |s −0| = −k/H.\nHence E is an ellipse with foci 0 and t, and with major axis 2a = −k/H.\nThis geometric proof of the law of ellipses is taken from [4]. The conserved\nvector t = K/µH is a priori well motivated both in geometric and physical\nterms. In most text books on classical mechanics, like the one by Herbert\nGoldstein [2], or in the original article by Wilhelm Lenz [12] the vector K is\njust written down out of the blue and its motivation comes only a posteriori\nfrom the conservation law ˙K = 0 and as a vector pointing in the direction\nopposite to the focus t of the elliptical orbit E.\nThe vector K has been (re)discovered many times before, going back\nto Hermann and Laplace and others [4]. In the literature it is commonly\ncalled the Runge–Lenz vector, or also just the Lenz vector. Pauli introduced\na quantized version of the Lenz vector to give an elegant derivation of the\nBalmer formulae for the hydrogen spectrum [14], [15]. Pauli did this work in\nthe fall of 1925 at Hamburg, where he was assistent with Lenz.\n5\n\n\nBy definition we find e = 2c/(2a) = −K/(µH) : −k/H = K/(kµ) for the\neccentricity of E. The square length of the Lenz vector is equal to\nK · K = (p × L) · (p × L) −2(p × L) · (kµr/r) + k2µ2 = 2µHL2 + k2µ2\nby straightforward inspection. If 2c is the distance between the two foci of\nthe elliptical orbit E then\n4c2 = t · t = (2µHL2 + k2µ2)/(µ2H2)\nand together with 4a2 = 4b2 + 4c2 = k2/H2 we arrive at 4b2 = −2L2/(µH).\nThe area of the region bounded inside E is πab, and therefore\nπab = LT/2µ\nwith T the period of the orbit. Hence we obtain\na3\nT 2 =\naL2\n4π2b2µ2 = −2ab2µH\n4π2b2µ2\n=\nk\n4π2µ = G(m + M)\n4π2\nusing k = GmM and µ = mM/(m + M). Since the mass m of any planet\nis negligible compared to the mass M of the sun we conclude that the ratio\na3/T 2 is the same for all planets, which is how Kepler formulated his harmonic\nlaw. This ends our discussion of the three Kepler laws: the ellipse law, the\narea law and the harmonic law.\n4\nAll Kepler ellipses through a fixed point\nLet us continue with the notation of the previous section, that is let us fix\nan energy H = p2/2µ −k/q < 0 and let C be the falling circle with center\nat the origin 0 and radius −k/H. Otto Laporte asked himself the question\nwhat can be said about the one parameter family F of all Kepler ellipses E\nhaving that same fixed energy H < 0 and passing through a fixed point r\nin space [11]. Our geometric approach for the Kepler problem answers these\nquestions rather easily.\nFor example, what is the locus T of the foci t as these Kepler ellipses\nthrough the fixed point r vary? The geometry gives a quick answer, because\n|t −r| = |s −r| = s −r = −k/H −r = 2a −r\n6\n\n\nand so t traverses a circle with center r and radius 2a−r. The ellipse in this\none parameter family with smallest eccentricity e = 1 + 2Hr/k is the one\nwith r at its perihelion and r −s at its aphelion, while the one with largest\neccentricity e = 1 is the fall from standstill at s reaching 0 in finite time\nT/2 with infinite velocity. Indeed, all Kepler ellipses with the same energy\nH have the same major axes 2a = −k/H, and hence also the same period\nT by the harmonic law. In particular all motions starting at r at the same\ntime return at r simultaneously.\nb\nb\nb\nb\nb\nb\n0\nt\nr\ns\nt\nt\nC\nE\nE\nE\nE\nT\nB\nAnother question that Laporte posed is to describe the locus B of points\nthat bounds the region swept out by all Kepler ellipses through the fixed\npoint r. If q is a point on such an ellipse E with focus t then\nq + |q −r| ≤q + |q −t| + |t −r| + r −r = −k/H −k/H −r = 4a −r\nby the triangle inequality, and equality holds if t lies on the line segment\nfrom q to r. Hence the region swept out by these Kepler ellipses through r\nwith energy H < 0 is bounded by an ellipse with foci 0 and r and major axis\n4a −r.\n7\n\n\nHis last question deals with the directrices of E with respect to the origin,\nas E varies in the family F of Kepler ellipses through the fixed point r. The\ndirectrix D of such an E with respect to the origin is given by d + K⊥with\nd = L2K/K2 and K⊥the orthogonal complement of K. Indeed the distance\nfrom r to this directrix D is equal to\n(d −r) · K/K = (L2 −r · K)/K = kµr/K = r/e\nwith e = K/(kµ) the eccentricity of E, as should. The degenerate ellipse E\nthrough r with maximal eccentricity e = 1 has directrix equal to r⊥while\nthe ellipse E through r with minimal eccentricity e = 1 + 2Hr/k = (a −r)/a\nhas directrix equal to\n(1 + a/(a −r))r + r⊥\nat least if r ̸= a.\nLet us assume for the rest of this section that 0 < r < a , which in\nturn implies that the complement of the region swept out by this family\nof directrices is bounded. Let E denote the curve bounding that bounded\ncomplement. A natural Ansatz would be that E is an ellipse with foci r and\nu = ar/(a −r) and with long axis equal to (2a −r)r/(a −r).\nb\nb\nb\nb\nb\nb\nb\nb\nb\n0\nt\nr\ns\nw\nv\nu\nd\nC\nC\nE\nE\nL\nD\n8\n\n\nTheorem 4.1. The orthogonal reflection of the vector u = ar/(a −r) in the\ndirectrix D = d + K⊥of the Kepler ellipse E through r is equal to\nv = ar/(a −r) −rt/(a −r),\nwhich in turn implies that v −r = r(r −t)/(a −r). In particular we get\n|v−r| = (2a−r)r/(a−r) and so v moves along a circle C with center r and\nradius (2a−r)r/(a−r) as E moves in the family F of Kepler ellipses through\nr. Hence this family of directrices of E is the family of tangents to an ellipse\nE with foci r and u = ar/(a −r), with long axis equal to (2a −r)r/(a −r)\nand with eccentricity [r2/(a −r)] : [(2a −r)r/(a −r)] = r/(2a −r).\nProof. The orthogonal reflection v of u = ar/(a−r) with mirror the directrix\nD = d + K⊥is given by the formula\nv = u −2((u −d) · K)K/K2,\nand the desired rewriting goes as follows. Since\nu · K = ar · K/(a −r) = a(L2 −kµr)/(a −r), d · K = L2\nwe get\n2((u −d) · K) K = 2r(L2 −akµ) K/(a −r) = rK2 t/(a −r) .\nHere we have used\n2a = −k/H, K = µHt, K2 = 2µHL2 + k2µ2 .\nThis proves that v = ar/(a −r) −rt/(a −r) and hence we conclude that\n|v −r| = r(2a −r)/(a −r). The rest of the theorem follows just like the\nargument of the previous section.\nRemark 4.2. The ellipse E has eccentricity r/(2a −r) and so its directrix\nD with respect to the focus r is equal to −(2a−r)r/r+r⊥. This suggests that\nin case r = a the dual curve E becomes a parabola, and in case a < r < 2a\nthe dual curve E becomes a hyperbola. We leave it to the interested reader\nto show that the above geometric argument can be adapted to include these\ncases as well.\n9\n\n\n5\nFinal remarks\nIn the fall of 1995 I spent a month at the Mittag Leffler Institute in Stock-\nholm. In the impressive library I was brousing through the correspondences\nof G¨osta Mittag Leffler with Sophie Kowalevski about her discovery of the\nfamous integrable top, and later went down to the basement of the Institute\nto get myself a reprint of her Acta paper from 1889 [9]. Motivated by my\nprevious work with Eric Opdam on hypergeometric functions associated with\nroot systems (which was partly motivated by understanding how the inte-\ngrals of motion for the classical Calogero–Moser system could be lifted to its\nquantization) I checked by trial and error that her classical integral of motion\ncould be lifted to a conserved quantity for the corresponding quantum top,\nand wrote a short paper with the algebraic details of the proof [5].\nIn 2005 I got a friendly letter of the Russian physicist Igor Komarov, ex-\nplaining that both the quantum integrability of the Kowalevski top had been\ndone long before in 1933 by Otto Laporte [10], and also that my approach by\ndoing the calculations in the universal enveloping algebra of the Euclidean\nmotion group of R3 had been anticipated by him in 1981 [7] with several\nrelated results in the following years [8]. I should have written back then a\nshort addendum to my paper explaining my ignorance of this earlier work\nby Laporte and Komarov, but postponed this idea with the plan of getting\nback to the quantum Kowalevski top and see if some better understanding\nof the corresponding spectral problem could be obtained.\nIt did not work out that way as I failed in this attempt, and later I\nforgot about it, until I read a few years ago the autobiography “Der Teil\nund das Ganze” of Werner Heisenberg. In Chapter 3 Heisenberg tells about\nhis contacts with Wolfgang Pauli and Otto Laporte, which revitalized my\ninterest in the person of Laporte. All three were graduate students of Arnold\nSommerfeld in M¨unchen with graduation years 1921 (P), 1923 (H) and 1924\n(L). Subsequently Laporte went as a postdoc to the National Bureau of\nStandards in Maryland. In 1926 he joined the physics faculty at Ann Arbor\nin Michigan as colleague of Sam Goudsmit and George Uhlenbeck, and stayed\nthere for the rest of his life. The paper on the Kepler ellipses through a fixed\npoint in space of 1970 was one of his last, written after many years of teaching\nclassical mechanics. By shining some extra light now on this Kepler paper\nof Laporte I hope to have made up for the omission in my old work of 1998.\n10\n\n\nReferences\n[1] Alexander Givental, Kepler’s Laws and Conic Sections, Arnold Math.\nJournal 2 (2016), 139-148.\n[2] Herbert Goldstein, Classical Mechanics, Addison-Wesley, Second Edi-\ntion, 1980.\n[3] David L. Goodstein and Judith R. Goodstein, Feynman’s Lost Lecture:\nThe Motion of Planets Around the Sun, Norton and Company, New\nYork, 1996.\n[4] Maris van Haandel and Gert Heckman, Teaching the Kepler Laws for\nFreshmen, The Mathematical Intelligencer 31:2 (2009), 40-44.\n[5] Gert Heckman, Quantum Integrability of the Kovalevsky Top, Indaga-\ntiones Mathematicae 9:3 (1998), 359-365.\n[6] Werner Heisenberg, Der Teil und das Ganze, Piper Verlag, Berlin, 1969.\n[7] Igor V. Komarov, Kovalevskaya basis for the hydrogen atom, Theor.\nMath. Phys. 47 (1981), 76-72.\n[8] Igor V. Komarov, Remarks on Kowalevski’s top, J. Phys. A 34:11\n(2001), 2111-2120.\n[9] Sophie Kowalevski, Sur le Probl`eme de la Rotation d’ un Corps Solide\nautour d’un Point Fixe, Acta Math. 12:1 (1889), 177–232,\n[10] Otto Laporte, Note on Kowalewski’s Top in Quantum Mechanics, Phys-\nical Review 43 (1933), 548-551.\n[11] Otto Laporte, On Kepler Ellipses Starting from a Point in Space, Amer-\nican Journal of Physics 38:7 (1970), 837-840.\n[12] Wilhelm Lenz, ¨Uber den Bewegungsverlauf und Quantenzust¨ande der\ngest¨orten Keplerbewegung, Zeitschrift f¨ur Physik 24 (1924), 197-207.\n[13] Isaac Newton, The Principia, Mathematical Principles of Natural Phi-\nlosophy, New Translation by I. Bernard Cohen and Anne Whitman,\nUniversity of California Press, Berkeley, 1999.\n11\n\n\n[14] W. Pauli, ¨Uber das Wasserstoffspektrum vom Standpunkt der neuen\nQuantenmechanik, Zeitschrift f¨ur Physik 36 (1926), 336-363.\n[15] B.L. van der Waerden, Sources of Quantum Mechanics, Dover Publica-\ntions, New York, 1968.\n[16] Oswald Thomas, Astronomie – Tatsachen und Probleme, Das Bergland-\nBuch, Salzburg, 1949.\nGert Heckman, Radboud University Nijmegen: g.heckman@math.ru.nl\n12\n\n\n"}
{"text": "Astronomy & Astrophysics manuscript no. main\n©ESO 2025\nMarch 3, 2025\nReconstructing orbits of galaxies in extreme regions (ROGER). IV.\nUnveiling galaxy evolution patterns in OmegaWINGS clusters.\nHernán Muriel1, 2, 3\n, David Pérez-Millán4, 5 , Martín de los Rios6, 7, 8 , Andrea Biviano4, 9 , Valeria Coenda1, 2 ,\nHéctor J. Martínez1, 2 , Andrés N. Ruiz1, 2 , Benedetta Vulcani10 , and Selene Levis1\n1 Instituto de Astronomía Teórica y Experimental (IATE), CONICET - UNC, Laprida 854, X5000BGR, Córdoba, Argentina\n2 Observatorio Astronómico, Universidad Nacional de Córdoba, Laprida 854, X5000BGR, Córdoba, Argentina\n3 INAF-Osservatorio Astronomico di Trieste, via G.B. Tiepolo, 11 - I-34143 Trieste, Italy\n4 Instituto de Radioastronomia y Astrofisica, UNAM, Campus Morelia, Michoacán CP 58089, Mexico\n5 INAF–Osservatorio di Astrofisica e Scienza dello Spazio di Bologna, Via Gobetti 93/3, 40129 Bologna, Italy\n6 Departamento de Física Teórica, Universidad Autónoma de Madrid, 28049 Madrid, Spain\n7 Instituto de Física Teórica, IFT-UAM/CSIC, C/ Nicolás Cabrera 13-15, Universidad Autónoma de Madrid, Cantoblanco, Madrid\n28049, Spain\n8 SISSA - International School for Advanced Studies, Via Bonomea 265, 34136 Trieste, Italy\n9 IFPU Institute for Fundamental Physics of the Universe, via Beirut, 2 - I-34014 Trieste, Italy\n10 INAF- Osservatorio astronomico di Padova, Vicolo Osservatorio 5, I-35122 Padova, Italy\nReceived XXXX; accepted XXXX\nABSTRACT\nContext. Clusters of galaxies have proven to be efficient systems in modifying various properties of galaxies, such as star formation\nor morphology. However, projection effects impose serious challenges in determining how, when, and to what extent galaxies are\naffected by the cluster environment.\nAims. Using innovative techniques to classify galaxies based on their history within the cluster, we aim to determine how galaxies of\ndifferent classes are affected by the cluster environment.\nMethods. We applied the ROGER code to select trajectories of galaxies in the phase space for 35 galaxy clusters from the\nOmegaWINGS survey. A new algorithm was applied to minimize contamination effects.\nResults. We found that both morphological transformation and the quenching of star formation begin shortly after galaxies enter the\ncluster. Even though over the last 2 −3 Gyr, galaxies entering clusters have undergone significant transformations in both their star\nformation and morphology these transformation processes are not complete, that is, they are not completely quenched and are not\nearly type yet. Backsplash galaxies and recent infallers show a higher fraction of jellyfish galaxies compared to older cluster members,\nsuggesting that the timescale of this phenomenon is typically less than 3 Gyr.\nKey words. galaxies: general – galaxies: stellar content – galaxies: evolution – galaxies: clusters: general\n1. Introduction\nGalaxy clusters represent the most densely populated virialized\nenvironments. They are defined by a strong gravitational\npotential, hot ionized intracluster gas, and may host up to\nthousands of galaxies. It is well established that various\ngalaxy properties change systematically depending on their\nenvironment: morphology (e.g., Dressler 1980; Whitmore\net al. 1993; Domínguez et al. 2001; Bamford et al. 2009;\nPaulino-Afonso et al. 2019; Vulcani et al. 2023), color (e.g.,\nBlanton et al. 2005; Martínez & Muriel 2006; Weinmann\net al. 2006; Martínez et al. 2008), luminosity (e.g., Adami\net al. 1998; Coenda et al. 2006), the fraction of star-forming\ngalaxies (e.g., Hashimoto et al. 1998; Mateus & Sodré 2004;\nBlanton & Moustakas 2009; Schaefer et al. 2017; Coenda\net al. 2019; Vulcani et al. 2010; Paccagnella et al. 2016;\nPérez-Millán et al. 2023) and the gas content (e.g., Giovanelli\n& Haynes 1985; Cortese et al. 2011; Brown et al. 2017).\nIn cluster environments, galaxies generally evolve to become\nredder, to have earlier-type morphologies, and exhibit older\nstellar populations. Additionally, cluster galaxies show reduced\ngas content, lower levels of star formation, and weaker emission\nlines in their spectra than galaxies in less dense environments\nlike in the field.\nSeveral mechanisms are responsible for influencing galaxies\nwithin clusters, particularly by depleting gas and subsequently\nhalting star formation. One key process is ram-pressure stripping\n(RPS, e.g., Gunn & Gott 1972; Abadi, Moore, & Bower\n1999; Book & Benson 2010; Vijayaraghavan & Ricker 2015;\nSteinhauser, Schindler, & Springel 2016), which removes cold\nand warm gas from galaxies due to the pressure exerted by\nthe intracluster hot gas. Another mechanism that can deplete\nthe gas supply is tidal stripping caused by the gravitational\npotential of the cluster (e.g., Gnedin 2003; Villalobos et al. 2014;\nLópez-Gutiérrez et al. 2022).\nIn intermediate-density regions, such as cluster outskirts and\ngroups, galaxy-galaxy interactions, also known as harassment,\nplay a more significant role (e.g., Spitzer & Baade 1951;\nMoore et al. 1998). This process can lead to both gas loss\nand morphological changes. Tidal stripping from these galaxy\nencounters can strip stars or truncate the stellar disc of galaxies,\ntransforming them into spheroid-dominated systems (e.g., Smith\net al. 2015). Morphological evolution is thought to be largely\nArticle number, page 1 of 9\narXiv:2502.20446v1  [astro-ph.GA]  27 Feb 2025\n\n\nA&A proofs: manuscript no. main\ndriven by mergers (e.g., Toomre 1977; Barnes 1992; Martin et al.\n2018), which are common in galaxy groups but much rarer in\nclusters due to the high relative velocities of galaxies. Major\nmergers tend to form spheroidal systems (Navarro & White\n1994), while gas-rich minor mergers may lead to the formation\nof massive disks (Jackson et al. 2022).\nAlthough the environment is important in shaping the\nproperties of galaxies, internal processes that depend on stellar\nmass have proven to be very efficient in shutting down the star\nformation. Some of these processes are: supernova-driven winds\n(e.g., Bower et al. 2012; Stringer et al. 2012), halo heating\n(Marasco et al. 2012), feedback from massive stars (e.g., Dalla\nVecchia & Schaye 2008; Hopkins et al. 2012), and active galactic\nnuclei (AGN) feedback (e.g.,Nandra et al. 2007; Hasinger 2008;\nSilverman et al. 2008; Cimatti et al. 2013). These mechanisms\nare collectively known as “mass quenching”.\nIt is well known that the observed properties of galaxies in\nclusters depend on the projected distance to the cluster center,\nwith morphology being one of the first examples (Whitmore\n& Gilmore 1991). However, the projected distance is too\nimprecise to determine both the fall time and the type of orbit.\nTypically, the orbits of substructures falling into a cluster-size\nhalos are highly radial and eccentric (Gill et al. 2005). As a\nconsequence, at a given radius within a cluster, there can be\nboth newly infalling objects and others that have been orbiting\nfor several gigayears. However, when considering both the\ncluster-centric distance and velocity with respect to the mean\nvelocity of the cluster galaxies (i.e., the phase-space diagram),\nrecent infallers generally exhibit higher velocities compared to\nlong-term galaxies. This results in different regions of the phase\nspace being more populated by objects with similar orbits and\ninfall times. Moreover, radial and eccentric orbits may result in\nthe galaxy having a trajectory that crosses the virial radius after\npassing through the cluster’s pericenter, which in phase space\ntends to place it at large projected radii and low velocities. These\ngalaxies are known as backsplash (Balogh et al. 2000; Mamon\net al. 2004).\nRhee et al. (2017) correlate the position in the projected\nphase space with the time since infall (the time since a galaxy\ncrossed for the first time the virial radius) and derive regions of\nconstant mean infall time for galaxies in the different regions\nof the phase space. These authors divide the phase space into\ndifferent zones that they associate with different infall times.\nSimilar analyses have been conducted by Pasquali et al. (2019).\nDe los Rios et al. (2021) and Coenda et al. (2022) proposed\na novel method to separate galaxies in the phase space, taking\ninto account the type of orbit. The technique employs machine\nlearning algorithms to assign probabilities of having a specific\ntype of orbit or the time spent within the cluster. To implement\nthis algorithm, these authors developed the code ROGER\n(Reconstructing Orbits of Galaxies in Extreme Regions), which\nis publicly available 1 and will be described in section 3.\nROGER\nhas\nbeen\nrecently\napplied\nto\na\nsample\nof\nX-ray-emitting clusters by Martínez et al. (2023). They\nobserve that significant morphological evolution occurs only\nin virialized galaxies within clusters. However, they also\nnote that blue galaxies recently entering the cluster may\nhave already experienced morphological changes. They explore\nwhether quenching timescales are generally shorter than those\nrequired for morphological changes. Their findings indicate\n1 There\nare\ntwo\nimplementations\navailable:\nin\nR\nhttps://github.com/Martindelosrios/ROGER\nand\nin\npython\nhttps://github.com/Martindelosrios/pyROGER.\nthat quenching happens more rapidly across all predicted\ndynamical categories. Moreover, while quenching is noticeably\naccelerated as soon as galaxies enter clusters, morphological\ntransformations take longer times, requiring prolonged exposure\nto the cluster’s physical mechanisms. In contrast, Sampaio et al.\n(2024) analyzed a sample of clusters in the SDSS and found\nthat morphological transition precedes complete star formation\nquenching.\nMartínez et al. (2023) also analyzed the galaxy populations\nby color for each of the classes provided by ROGER that are:\ncluster galaxies, backsplash galaxies, recent infallers, infallers,\nand interlopers. For cluster and backsplash galaxies, they limited\nthe analysis to red galaxies due to the contamination produced\nby projection effects (see Coenda et al. 2022 for a detailed\ndiscussion). Martínez et al. (2023) found that blue recent\ninfallers tend to be smaller than both infalling galaxies and\ninterlopers, which are similar in size. Based on these results, they\nsuggested that a single pass through the cluster environment can\nalter the galaxy’s morphology, and also shrink the size of blue\ngalaxies. Marasco et al. (2023) analyze a sample of galaxies\nwith MUSE data in groups and clusters to explore how the\naging of stellar populations can lead to a morphological change\nin galaxies where star formation has been rapidly quenched.\nThey found that the morphological transformation is completed\nafter 1.5–3.5 Gyr, occurring faster for more efficient quenching\nscenarios.\nGalaxies that halt star formation in less than 1.5 Gyr exhibit\ndistinctive spectral features, such as the absence of emission\nlines and Hδ absorption, and are referred to as post-starburst\ngalaxies\n(PSB,\nDressler\n&\nGunn\n1982).\nAnalyzing\nthe\ncharacteristics of their stellar populations and their distribution\nwithin clusters can provide important insights into the physical\nmechanisms behind the suppression of star formation. Analyzing\nWINGS and OmegaWINGS cluster galaxies in phase space,\nPaccagnella et al. (2017) found that PSBs comprise a mix of\ngalaxies with diverse accretion histories. PSBs with the most\npronounced Hδ feature seem to have been recently accreted. In\nterms of stellar masses, magnitudes, colors, and morphologies,\nthey found that PSBs fall between passive and emission-line\ngalaxies, characteristic of a population transitioning from\nstar formation activity to a passive state. Analyzing different\nenvironments, Paccagnella et al. (2019) found that the fraction\nof PSBs increases with halo mass, suggesting that mechanisms\ncommon in densely populated regions, such as ram-pressure\nstripping, play a significant role in the formation of many PSB\ngalaxies in these environments (see also Socolovsky et al. 2019;\nVulcani et al. 2020; Wilkinson et al. 2021; Werle et al. 2022).\nRam-pressure stripping can effectively remove gas from\ngalaxies during their initial entry into clusters, with galaxies on\nradial orbits experiencing more severe gas loss (e.g., Jaffé et al.\n2015; Yoon et al. 2017; Jaffé et al. 2018). Additionally, radio\nobservations reveal disrupted and stripped gas while the galaxy’s\nstellar core remains largely intact, a phenomenon consistent with\nRPS. In some cases, star formation has been detected in the\nstripped tails (e.g., Gavazzi et al. 1995; Poggianti et al. 2019;\nLee et al. 2022; Roberts et al. 2022; Gullieuszik et al. 2023). Due\nto the appearance caused by this effect, these galaxies are known\nas jellyfish galaxies. Jaffé et al. (2018) studied the phase-space\ndistribution of cluster galaxies and found that jellyfish galaxies\ntend to have higher peculiar velocities (Biviano et al. 2024)\nand more radially elongated orbits compared to the overall\ncluster population. They also found that those jellyfish with the\nlongest gas tails are moving at high speeds, and close to cluster\ncores, where ram pressure is stronger. They concluded that\nArticle number, page 2 of 9\n\n\nH. Muriel: Unveiling galaxy evolution patterns\nmany jellyfish galaxies likely formed through rapid, outside-in\nram-pressure stripping during their first infall into the cluster.\nSalinas et al. (2024) estimated the duration of optical tails of\njellyfish galaxies, from their initial appearance to their eventual\ndisappearance. They found that galaxy tails first emerge around\n∼1.16 R200 and vanish approximately 0.6 Gyr after reaching the\npericenter.\nIn this work, we investigate the properties of galaxies in\nclusters depending on their orbit type using the predictions of\nthe ROGER code. We are particularly interested in comparing\ngalaxies that have been within the cluster from more than 2\nGyr ago and have completed several orbits (hereafter “cluster\ngalaxies”), with those that have made a single passage through\nthe cluster center and are now located outside the cluster virial\nregion (hereafter “backsplash galaxies”). This comparison aims\nto understand the initial impact that the cluster environment\nexerts on galaxies during their first encounter. To achieve this\ngoal, we analyze the morphology and age of the populations,\nas well as the fractions of passive galaxies, recently quenched\ngalaxies, PSB, and jellyfish galaxies.\nIn this paper, we use a subsample of OmegaWINGS clusters\n(OW), which are described in section 2. The ROGER code\nis detailed in section 3, while in subsection 3.2, we present\nthe technique used to correct for misclassification effects. The\nproperties of the galaxies are analyzed in section 4, where\nmorphology, the fraction of quenched galaxies, age, and the\nfractions of PSB and jellyfish galaxies are compared by orbit.\nIn section 5, we summarize our findings and present our\nconclusions. Throughout this paper we assume a concordance\nΛCDM cosmology with ΩM = 0.3, ΩΛ = 0.7, and H0 =\n70 km s−1 Mpc\n−1.\n2. Data\n2.1. The cluster sample\nThe galaxy clusters used in this study constitute a subset derived\nfrom the OW spectroscopic survey (Gullieuszik et al. 2015;\nMoretti et al. 2017). This survey serves as an extension of\nthe WIde-field Nearby Galaxy Cluster Survey (WINGS, Fasano\net al. 2006; Varela et al. 2009; Moretti et al. 2014), which is\na multi-wavelength survey aimed to cover the outskirts of 76\nmassive clusters (of which, 46 were spectroscopically observed),\nspanning a redshift range of 0.04 ≤z ≤0.07, selected from the\nROSAT All-Sky Survey (Ebeling et al. 1996).\nOW expands the spatial coverage of the WINGS survey for\n46 of these clusters, acquiring imaging data in the B, and V\nbands across an area of approximately 1 deg2. Moreover, it\nencompasses a broad range of velocity dispersion (σ ∼500 −\n1, 300 km s−1), X-ray luminosity (LX ∼0.2 −5 × 1044 erg s−1),\nand R200 (radius where the mean interior density is 200 times\nthe critical density of the Universe) of about 1−3 Mpc. The R200\nvalues for the cluster sample are taken from Biviano et al. (2017).\nThe target selection process for the spectroscopic observations\nmirrored that of the photometric survey (Cava et al. 2009;\nMoretti et al. 2017). These selections were based on galaxies\nwith a total magnitude brighter than V = 20, excluding those\nsignificantly above the color-magnitude sequence with B −V >\n1.20. Additionally, galaxies brighter than MV = −17.4 (the\nabsolute magnitude limit of a galaxy with V = 20 at the redshift\nof the most distant cluster in the sample, reaching a magnitude\ncompleteness of 80%), were chosen (see Paccagnella et al.\n2017). Spectroscopic follow-up for OW included observations\nusing the VST fibre spectrograph. OW spectra cover a range of\napproximately 3800 to 9000 Å, with a resolution of 3.5 −6 Å.\nIn this work, we have restricted the sample to clusters that have\nat least 50% completeness in spectroscopy, resulting in a total of\n35 clusters.\n2.2. The spectrophotometric code\nAlthough the sample of clusters and galaxies selected in\nthis work is the same one used in Salerno et al. (2020),\nthe spectroscopic properties of galaxies were recalculated\nfollowing Fritz et al. (2017). The stellar population properties\nare determined using the spectrophotometric code SINOPSIS2\n(SImulatiNg OPtical Spectra wIth Stellar population models).\nSINOPSIS utilizes the theoretical spectra of simple stellar\npopulations (SSP) across 12 different ages, ranging from 106 yr\nto the age of the Universe at the redshift of the galaxy (tu);\nand four metallicity values: sub-solar (Z = 0.004), solar (Z =\n0.017), and super-solar (Z = 0.03, 0.04). The SSP models are\nbased on work by Charlot & Bruzual (in prep.), using a Chabrier\n(2003) IMF with masses ranging from 0.1 to 100 M⊙. After\nobtaining the best fit, an age binning is applied, reducing the\nfinal resolution to four age bins.\nThe stellar age bins are defined according to stellar\npopulation features (Fritz et al. 2007), thus:\n⋆Stellar age bin 1 (0 - 19.95 Myr): Characterized by emission\nlines and the strongest ultraviolet emission.\n⋆Stellar age bin 2 (19.95 - 571.5 Myr): Hydrogen lines\nfrom the Balmer series reach their maximum intensity in\nabsorption, while the Cak,h UV lines still have low (almost\nundetectable) equivalent widths.\n⋆Stellar age bin 3 (0.5715 - 5.754 Gyr): Balmer absorption\nlines decrease in intensity, while the k calcium line reaches\nits maximum level in absorption.\n⋆Stellar age bin 4 (5.754 Gyr - tu): SSPs in this age bin\nare reddest, and the main spectral characteristics show an\nasymptotic behavior. The 4000 Å break (D4000) attains the\nhighest values.\nThe resulting star formation rates (SFRs) for these four bins\nform the final star formation history (SFH). Total stellar mass\nis obtained by rescaling the aperture mass using the aperture and\ntotal magnitudes in the V-band. In this way, we are assuming that\nthe color gradient between the aperture and the whole galaxy\nis negligible, as done by other authors (e.g., Kauffmann et al.\n2003) Detailed descriptions of the code can be found in Fritz\net al. (2007), Fritz et al. (2017), and Pérez-Millán et al. (2023).\nTo address incompleteness corrections (both geometrical and\nmagnitude-based) in the spectroscopic catalogs, the ratio of\nspectra yielding a redshift to the total number of galaxies in the\nparent photometric catalog was utilized, computed as a function\nof V magnitude and radial distance from the brightest cluster\ngalaxy. All calculations were conducted while weighting each\ngalaxy to account for both types of incompleteness. The ultimate\nspectroscopic sample comprises 14,801 galaxies within 35 out of\n46 OW clusters.\nIn this study, we analyze galaxy properties based on\nmorphological types, using the morphologies estimated by\nFasano et al. (2012) and Vulcani et al. (2023). These\nauthors employ the MORPHOT code, a non-parametric and\nfully empirical method for automatically estimating galaxy\nmorphology. MORPHOT utilizes 21 morphological diagnostics\nthat can be directly computed from galaxy images and provides\n2 https://jacopofritz.wixsite.com/webpage/sinopsis\nArticle number, page 3 of 9\n\n\nA&A proofs: manuscript no. main\ntwo distinct classifications: one derived from a maximum\nlikelihood semi-analytical method and the other from a neural\nnetwork approach. The final morphological estimator combines\nboth techniques and has been tested on an additional sample\nof visually classified WINGS galaxies (see also, Vulcani et al.\n2011a).\nThe MORPHOT code assigns each galaxy a TYFIN type\nranging from −6 to 11. We classify them into two groups:\nearly-types, which include galaxies with TYFIN ≤0 such as\ncD, elliptical, and S0 galaxies; and late-types, with TYFIN > 0,\ncorresponding to early spirals, late spirals, and irregular galaxies.\n3. ROGER classification\n3.1. Orbital classes\nGiven the position of a galaxy in the Projected Phase Space\nDiagram (PPSD), specifically, its projected distance to the\ncluster center in terms of R200, and its line-of-sight velocity\nrelative to the cluster in units of σ, ROGER3 was trained\nby De los Rios et al. (2021) to compute the probabilities for\na galaxy to belong to different classes using three Machine\nLearning techniques: K-Nearest Neighbors (KNN), Support\nVector Machines, and Random Forest. It is worth to mention\nthat these machine learning methods were trained using galaxies\nfrom the MultiDark Planck 2 simulation (Klypin et al. 2016),\nwhere the real classes and properties of each galaxy are known.\nROGER was trained using a sample of 34 massive clusters. This\nsample is more massive than that of OmegaWINGS (medians\nof log(M200) are 15.04 and 14.67, respectively) and have a\nlarger velocity dispersion (median values of 1255 and 744 km/s,\nrespectively). However, ROGER uses ∆vlos/σ, which exhibits\nsimilar behaviors in both samples. This similarity is directly\nrelated to the self-similarity of halos, so the application of\nROGER to a less massive sample does not introduce significant\nbiases. ROGER computes, for each galaxy, its probability to\nbelong to each of the following five orbital classes, as defined\nby De los Rios et al. (2021):\n(1) Cluster galaxies (CL): Galaxies designated as satellites\nof the cluster, having maintained this status for over 2 Gyr.\nThe majority reside within R200, with a few exceptions (∼4%),\ntemporarily beyond R200 in their orbital trajectories.\n(2) Backsplash galaxies (BS): Galaxies that have traversed\nR200 precisely twice, once on their entry into the cluster and\nagain on their exit. Located outside R200, these galaxies have\nundergone a single transit through the cluster core. They are\nlikely to eventually become CL in the future.\n(3) Recent infallers (RIN): These are galaxies located within\nR200, having crossed it just once on their entry and no earlier\nthan 2 Gyr ago. These galaxies are currently undergoing the\nenvironmental effects of the cluster for the first time. Some may\ntransition into backsplash galaxies (BS) in the future.\n(4) Infallers (IN): These galaxies have spent their entire\nlifetimes outside R200 and are currently in the process of falling\ninto the cluster, indicated by their negative radial velocities\nrelative to the cluster.\n(5) Interlopers (ITL): These galaxies have likewise remained\noutside R200 for the entirety of their lifetimes; however, they are\nnot approaching the cluster and lack any physical association\nwith it. Their presence in the PPSD is solely attributable to\nprojection effects.\n3 In this work we used the python implementation of ROGER that is\navailable in: https://github.com/Martindelosrios/pyROGER\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nR/R200\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nvlos/\nCL\nBS\nRIN\nITL\nFig. 1: Distribution of galaxies in the phase space according\nto the classification produced by ROGER. The types of orbits\nconsidered are: cluster galaxies (CL, red); backsplash galaxies\n(BS, orange); recent infallers (RIN, green) and interlopers (ITL,\ngrey).\nLike any selection method based on the phase space, the\nselection of the orbital classes by ROGER is affected by potential\ndeviations from the state of relaxation. Since ROGER is trained\nwith galaxies in dark matter halos selected from the MultiDark\ncosmological simulations, it exhibits the same biases as an\nobserved sample. In the present analysis, we will consider\ncategories CL, BS, RIN, and ITL. The IN class was not included\nin our analysis, as it does not represent a statistically significant\nsample. Although ∼12% of the total sample has the highest\nprobability corresponding to the IN class, this percentage is\nreduced to less than 2% (91 galaxies across the entire stellar\nmass range) when applying the Coenda et al. (2022) criteria4,\nsuggesting that the sample of galaxies classified as IN based on\nthe highest probability could be heavily contaminated by other\nclasses. On the other hand, galaxies classified as ITL should not\nnecessarily be interpreted as field galaxies, as due to hierarchical\nclustering, a significant fraction of these galaxies could belong\nto groups or filaments around the clusters.\nBased on the findings of De los Rios et al. (2021), we employ\nthe trained KNN technique to determine class probabilities for\nour galaxies. ROGER was applied to our data and each galaxy\nwas assigned an orbit type based on its highest probability of\nbelonging to a given orbit and weighted by the probability to\nbelong to this category. The resulting number of galaxies in each\npredicted class is provided in Table 1. In Fig. 1 we display the\nPPSD positions of galaxies that meet our classification criteria,\nrepresented by different colors.\n3.2. Correcting the effects of misclassification\nThe determination of orbits in the phase space is strongly\nlimited by issues of contamination and incompleteness, we\n4 In order to reduce contamination among classes in the PPSD\ndiagram, Coenda et al. (2022) tested various threshold values and\nselected those that produce pure samples (high precision) for each class\nwithout sacrificing statistical power (high sensitivity). They proposed\nthe optimal threshold values for each class.\nArticle number, page 4 of 9\n\n\nH. Muriel: Unveiling galaxy evolution patterns\nTable 1: Total number of galaxies and by morphological type for\neach of the types of orbits considered.\nProperty\nCL\nBS\nRIN\nITL\nAll\n2080 (669)\n997 (393)\n928 (550)\n530 (477)\nET\n1424 (484)\n524 (198)\n587 (339)\n222 (187)\nLT\n524 (147)\n314 (132)\n288 (180)\n238 (230)\nNotes. The numbers in parentheses correspond to the weighted values\n(see text for more details).\nrefer the reader to Coenda et al. (2022) for a thorough\ndiscussion on this matter. To mitigate these problems, Martínez\net al. (2024) proposed a method to improve the accuracy of\ndetermining the distribution of galaxy properties by statistically\ncorrecting for misclassification errors in the PPSD. The method\ninvolves the use of an estimation the confusion matrix, which\ncontains the information of how each class is contaminated\nfrom misclassified galaxies that actually belong to the other\nclasses. By inverting this matrix, the authors are able to\nobtain better determinations of the intrinsic distributions of\ngalaxy properties, such as color. The method was tested using\nsimulated galaxy clusters, showing significant improvements in\nthe accuracy of classifying galaxies, particularly for those in\ncluster and backsplash categories. The corrected distributions\neliminate most of the contamination from misclassified galaxies,\noffering a more reliable analysis of galaxy properties in various\nenvironments. The authors emphasize that their method allows\nfor a more precise interpretation of observational data, reducing\nbiases caused by misclassifications.\nThe method of Martínez et al. (2024) allows us to compute\nfor each galaxy in our samples an individual weight depending\non its class i, mass M, and rest-frame B −V color. We proceed\nas follows:\n1. We split galaxies into four bins in log(M/M⊙): [9.485, 9.8),\n[9.8, 10.2), [10.2, 10.6), and [10.6, 11.5].\n2. For each class i and mass bin j we compute the observed\ncolor distribution using 11 equal size bins in B −V color,\nranging from −0.3 to 1.7: fobs(i, j, k), where k denotes the\nbin in color.\n3. Following Martínez et al. (2024), we obtain, for each\nclass and mass range, the reconstructed color distribution\nfrec(i, j, k). We use the confusion matrix estimated by the\nauthors.\n4. The weight we assign to a galaxy of class i, stellar mass\nin the j−th bin, and color in the k−th bin is the quotient:\nfrec(i, j, k)/ fobs(i, j, k).\nThe derived weights are ≤1 and have a median of approximately\n0.6.\n4. Results: Comparing physical properties of CL,\nBS, RIN, and ITL galaxies\n4.1. Morphology\nAs discussed in the introduction, a single passage through the\ninner regions of the cluster can not only alter the star formation\nbut also modifies the morphology of a galaxy. Depending on\nthe ROGER class, the morphological type has been estimated\n5 According to Pérez-Millán et al. 2023, the sample is complete in\nstellar mass for log(M/M⊙) ≥9.48.\nFig. 2: Fractions of early (upper panel) and late (bottom panel)\ntype galaxies as a function of the stellar mass for CL, BS, RIN,\nand ITL galaxies.\nfor 80 to 90% of the galaxies. Fig. 2 shows fractions of early\nand late type galaxies (ET and LT, respectively) as a function\nof stellar mass for CL, BS, RIN, and ITL classes. In this and\nthe following figures, errors were obtained using the bootstrap\nresampling technique. The fractions of early- and late-type\nare not mirror-like for two reasons: in addition to the fact\nthat not all galaxies in the sample have been morphologically\nclassified, each galaxy has an assigned weight depending on its\ncharacteristics. The first thing we observe is that, regardless of\nthe type of orbit, the ET fractions systematically increase (and\nLT fractions decrease) with stellar mass. Similar results were\nfound by Vulcani et al. (2011b) and Vulcani et al. (2011a). If\nwe compare the fractions at fixed stellar mass bin, we observe\nthat the highest fractions of ET correspond to cluster members\nand the lowest values to ITL. BS and RIN have intermediate\nvalues between the two previous ones, with RIN having higher\nfractions of ET than BS. For the LT fractions, we again find\nthat CL and ITL are the extreme cases, with CL having the\nlowest LT fractions, which, depending on the stellar mass,\nranges between 0.2 and 0.3. While BS and RIN also exhibit\nintermediate fractions, in this case, they are indistinguishable\nfrom each other.\n4.2. Star formation quenching\nThe left column of Fig. 3 shows the fractions of CL, BS, RIN and\nITL passive galaxies in four bins of stellar mass (we assume that\na galaxy is passive if its SFR ≤0.0001 M⊙yr−1). In agreement\nwith what is observed for ET galaxy fractions (see Fig. 2), the\nhighest (lowest) fraction of passive galaxies corresponds to CL\n(ITL), while BS and RIN have intermediate values. RIN have\na higher fraction of passive galaxies than BS across almost the\nentire stellar mass range. As expected, in all cases, an increase\nArticle number, page 5 of 9\n\n\nA&A proofs: manuscript no. main\nFig. 3: Left panel: Fractions of CL, BS, RIN, and ITL\npassive galaxies as a function of stellar mass. Top panel:\nAll morphological types; central and bottom panels: Early\nand late types, respectively, for CL and BS galaxies. Right\npanel: Fraction of recently strongly quenched galaxies. Types of\ngalaxies are presented as shown in the left panel.\nin the fraction of passive galaxies is observed as stellar mass\nincreases.\nFor CL and BS galaxies, we have also analyzed the fractions\nof passive galaxies as a function of morphological types. Since\nCL and BS are mostly ET, they show similar behavior to the\noverall sample (see central left panel of Fig. 3), although with\nsystematically higher values, while LT galaxies (lower panel)\nshow a weaker dependence on stellar mass. This could be\nindicating that low mass LT galaxies are being more affected\nby the environment, compensating for the lower quenching\nthey experience by the mass-quenching compared to more\nmassive ones. These two opposing phenomena tend to flatten the\ndependence of star formation quenching on mass.\nWith the aim of assessing how intense the recent quenching\nof star formation has been, we have calculated the fraction of\ngalaxies that have experienced strong star formation quenching\nover the last ∼0.5 Gyr. This fraction, which we refer to as\nRSQ (galaxies that have been Recently Strongly Quenched), is\ncalculated as the number of galaxies per class that have been\ncompletely quenched, or have reduced their star formation by\nhalf during the mentioned period, divided by the total number of\ngalaxies in the same class that were star-forming at the beginning\nof the interval. In practice, the calculation involves comparing\nthe SFR between the age bins 0 −19.95 Myr and 19.95 −571.5\nMyr (bin 2, SFR2), which are the first two age bins computed\nby the SINOPSIS code (see Table 1 in Pérez-Millán et al. 2023).\nAccording to Ruiz et al. (2023), backsplash galaxies at z = 0\nhave been outside R200 for 1.25 Gyr on average, meaning that\nthe period over which we calculate the fraction is less than half\nthe time the galaxies have been in the backsplash phase. In other\nFig. 4: Luminosity-weighted Age as a function of the stellar\nmass. Types of galaxies are presented as shown in Fig. 3.\nwords, most of the BS galaxies should already be outside R200\nby the age of bin 2, while most of the RIN galaxies should\nhave already entered the cluster in the same period. For this\ncalculation, we consider star-forming galaxies to be those with\nSFR2 ≥0.001M⊙yr−1; however, the general conclusions do not\ndepend on the choice of this threshold. The RSQ fraction have\nbeen computed as a function of stellar mass and are shown in the\nright panel of Fig. 3.\nThe behavior of the RSQ fractions is very similar to that\nof the fraction of galaxies with SFR = 0; in other words, the\ngalaxies that have experienced the strongest quenching are the\ncluster galaxies. It is important to note that the calculation of\nthe RSQ fraction is based on the sample of galaxies that were\nstar-forming in age bin 2, which, in the case of CL galaxies,\nare the fewest. Of the small number of CL galaxies that were\nstar-forming in bin 2, approximately 80% have been strongly\nquenched, whereas this percentage is 50% for ITL. RIN galaxies,\ndespite their short time in the cluster, show a significant degree\nof quenching, although clearly less than that of the CL galaxies.\nThe BS galaxies, although they were mostly already outside R200\nin age bin 2, have continued their quenching process, something\nnoted by Ruiz et al. (2023).\n4.3. Age\nGalaxies with different dynamical histories are expected to show\ndifferences in luminosity-weighted age. The top panel of Fig. 4\nshows that CL are on average always the oldest at all masses.\nWhen comparing the ages of BS, RIN, and ITL galaxies, no\nstatistically significant difference is observed between these\npopulations, which could be related to the fact that BS and\nRIN galaxies have spent most of their lives outside the cluster.\nAlthough these populations have shown clear differences in the\ndegree of star formation quenching, this has not been sufficient\nto alter the average age of the samples.\nThe middle and lower panels of Fig. 4 compare the ages\nof CL and BS galaxies for ET and LT galaxies, respectively,\nshowing that the differences observed for the total samples are\nmainly due to late-type galaxies.\nArticle number, page 6 of 9\n\n\nH. Muriel: Unveiling galaxy evolution patterns\nFig. 5: Fractions of post-starburst galaxies within the predicted\nclasses CL, BS, RIN, and ITL.\n4.4. Post-starburst galaxies\nAny process of star formation quenching can be studied by\nanalyzing the population of galaxies that, although no longer\nforming stars, have done so recently. This type of galaxy is\nknown as post-starburst galaxies, and they can be identified\nthrough spectral analysis. Fritz et al. (2014) identified in the\nWINGS survey k+a/a+k spectra that exhibit a combination of\nfeatures characteristic of both K and A-type stars, with strong\nHδ absorption lines and an absence of emission lines. These\nare typical of post-starburst/post-starforming galaxies where star\nformation was abruptly halted at some point within the last 0.5–1\nGyr.\nUsing the criteria established by Fritz et al. (2014) to\nclassify PSB galaxies, Paccagnella et al. (2017) and Paccagnella\net al. (2019) identified PSB galaxies in OmegaWINGS cluster\nenvironments. Fig. 5 shows the PSB fractions for CL, BS,\nRIN, and ITL galaxies. Although the fractions are very small\nin all cases, it can be seen that cluster galaxies have the\nhighest fraction of PSB, while ITL show the lowest fraction.\nWith intermediate values, BS and RIN galaxies do not show\nstatistically significant differences between each other. The fact\nthat CL galaxies have the highest fraction of PSB, aligns with\nwhat was observed in the RSQ fractions6, which indicated that\nclusters have remained efficient in quenching the star formation.\n4.5. Jellyfish galaxies\nThe quenching of star formation is directly related to the loss of\ngas in galaxies. One of the most extreme examples of gas loss\nare the so-called Jellyfish galaxies, where, due to the action of\nthe intracluster medium gas, galaxies can lose high percentages\nof their gas. This phenomenon can be further intensified by tidal\nforces, either from the cluster potential or through interactions\nwith other galaxies.\nThe pioneering work of Gunn & Gott (1972) showed that\nboth the velocity of galaxies and the density of the intracluster\nmedium are fundamental parameters in gas removal. The\npresence of Jellyfish galaxies will also depend on various factors,\nsuch as the time spent in the cluster, how close to the cluster\ncenter a galaxy passes, the duration of the Jellyfish phenomenon,\nthe amount of gas available to be removed and the inclination of\nthe galaxy relative to the velocity with respect to the intracluster\n6 Note that the PSB fraction refers to all galaxies, while the RSQ\nfraction refers only to star-forming galaxies in age bin 2.\nFig. 6: Fraction of jellyfish galaxies within the predicted classes\nCL, BS, and RIN. b) As in panel a) for JClass 3, 4, and 5.\nmedium (e.g., Kronberger et al. 2008). Since the different orbit\nclasses provided by ROGER occupy different regions in phase\nspace, the fraction of Jellyfish galaxies is also expected to\ndepend on the orbit type.\nPoggianti et al. (2016) have conducted the first systematic\nsearch for galaxies that are being stripped of their gas at\nlow redshift, selecting galaxies with different degrees of\nmorphological evidence for gas stripping. They have visually\ninspected images and identified 344 candidates in 71 galaxy\nclusters of the OW+WINGS sample. The authors tentatively\ncategorized candidates into five groups based on the visual signs\nof stripping observed in the optical bands (JClass), ranging\nfrom the most extreme cases (JClass =\n5) to progressively\nless intense ones, with JClass = 1 being the least pronounced.\nConsequently, the JClass =\n5 and 4 categories include the\nmost reliable candidates and feature the most notable examples\nof classic jellyfish galaxies. JClass = 3 candidates are likely\nundergoing stripping and/or experiencing ram pressure events,\nwhereas JClasses =\n2 and 1 are tentative cases for which\nthe current imaging does not allow for definitive conclusions.\n(Vulcani et al. 2022) identified an additional subsample of ram\npressure stripping candidates that were missed by the visual\ninspection performed by (Poggianti et al. 2016).\nIn the left panel of Fig. 6, the fractions of galaxies classified\nas jellyfish in any of the classes are shown. ITL galaxies were\nexcluded from this analysis because, being objects external to the\ncluster, a significant fraction of the ITL-jellyfish is expected to\nbe linked to galaxy interactions rather than effects caused by the\nintra-cluster gas. As expected, the fractions are extremely low,\nbut a clear difference by class is observed, with CL showing the\nlowest fraction and RIN the highest, while BS have intermediate\nvalues. The right panel of Fig. 6 shows the fraction of jellyfish\ngalaxies for JClass values 3, 4, and 5, where we can see that\nCL galaxies continue to display the lowest fraction, while BS\nand RIN are indistinguishable, though with greater uncertainties\ncompared to those presented in the left panel.\n5. Summary and conclusions\nWe have studied different properties of cluster galaxies using the\nROGER code, which allows the selection of the most probable\norbits of galaxies in the projected phase space. For the first time\nin this type of studies, a technique was applied to minimize the\nconsequences of misclassification caused by projection effects\nArticle number, page 7 of 9\n\n\nA&A proofs: manuscript no. main\n(Martínez et al. 2024), which introduce serious biases in galaxy\nsamples and, consequently, in the conclusions drawn from them\n(see Coenda et al. 2022). The types of orbits analyzed are: cluster\ngalaxies (CL), backsplash galaxies (BS), recent infallers (RIN),\nand interlopers (ITL). We focus on comparing cluster galaxies\nthat likely orbited the cluster more than once, with backsplash\ngalaxies that have made a single passage and are now outside the\ncluster. The main goal is to understand the initial impact of the\ncluster environment on galaxies after their first incursion within\nthe virial radius.\nFor this analysis, a subsample of 35 OW clusters was\nused, and the following galaxy properties were considered:\nmorphology, fraction of quenched galaxies, age, fraction of PSB\ngalaxies, and fraction of jellyfish galaxies. In all cases, the\nanalyses were performed as a function of the galaxies’ stellar\nmass. Our main conclusions are:\n1. Regardless of class, ET fractions increase and LT fractions\ndecrease with stellar mass in agreement with previous results\nbased on the same data (Vulcani et al. 2011b,a; Pérez-Millán\net al. 2023). Separating galaxies by class, higher ET fractions\nare found in cluster members, while ITL show the lowest\nvalues. BS and RIN have intermediate values, with RIN\nshowing the highest ET fraction. For LT fractions, CL has\nthe lowest values, ranging from 0.2 to 0.3 depending on\nstellar mass. BS and RIN show similar, indistinguishable\nLT fractions. The fact that the morphological type fractions\nof BS and RIN are clearly different from those of ITL\nsuggests that morphological transformation begins shortly\nafter galaxies enter the cluster, although it is clearly not\nyet complete, as clusters exhibit significantly higher ET\nfractions than those of BS and RIN. These results are\nconsistent with those reported by Martínez et al. 2023;\nVulcani et al. 2011b,a.\n2. BS and RIN galaxies exhibit significantly higher fractions\nof quenched galaxies compared to ITL, although lower\nthan CL, indicating that the quenching of star formation\nproceeds rapidly once galaxies enter the cluster. Based on\nthe fraction of RSQ (Recently Strongly Quenched) galaxies,\nwe found that galaxies that have experienced the strongest\nquenching during the last 0.5 Gyr are the cluster galaxies.\nApproximately 80% of the CL galaxies have been strongly\nquenched during this period of time, whereas this percentage\nis 50% for the ITL galaxies. It is important to clarify that\nthe fractions of RSQ galaxies are calculated from galaxies\nthat were still forming stars at z ∼0.04, which, in the\ncase of clusters, represent a smaller fraction compared\nto other orbital types. What our results indicate is that\nthe few cluster galaxies still forming stars 0.5 Gyr ago,\nwere rapidly quenched. This phenomenon is consistent with\nthe slow-then-rapid quenching scenario (e.g., Wetzel et al.\n2013; Maier et al. 2019), in which slow quenching, or\n“strangulation”, begins when a galaxy crosses the R200 radius\nand gas inflow is halted. During this phase, galaxies continue\nforming stars but show elevated metallicities as quenching\nbegins. As they move toward the cluster’s denser core, a\nrapid phase follows in which star formation is abruptly\nhalted due to increasing ram pressure, which can also strip\naway cold gas, even in massive galaxies. RIN galaxies,\ndespite their short time in the cluster, also show a significant\ndegree of quenching. BS galaxies, although they were mostly\nalready outside R200 in age bin 2, have continued their\nquenching process, something noted by Ruiz et al. 2023.\n3. The mean ages of BS, RIN, and ITL galaxies are statistically\nindistinguishable and clearly younger than cluster members.\nAlthough these populations show clear differences in the\ndegree of star formation quenching, this fact has not been\nsufficient to alter the average age of the samples. When\nanalyzed by morphological type, the differences between BS\nand CL are mainly due to late-type galaxies.\n4. In agreement with Paccagnella et al. (2017), cluster galaxies\nhave the highest fraction of PSB, while ITLs exhibit the\nlowest values. BS and RIN galaxies show intermediate\nvalues with no statistically significant differences between\nthem. Although the fractions of PSB and RSQ galaxies have\nbeen calculated with respect to different sub-samples7, the\nfact that CL class (composed of galaxies that have been in\nthe cluster for more than 2 Gyr) shows the highest fractions\nof both PSB and RSQ suggests that some galaxies require a\nlonger time to be completely quenched. On the other hand,\nalthough BS and RIN galaxies exhibit significant fractions\nof quenched galaxies, there is still a considerable number of\ngalaxies available to be quenched during successive orbits\nwithin the cluster in order to become PSB.\n5. The observed fractions of jellyfish galaxies clearly depend\non the type of orbit, with CL showing the lowest fraction\nand RIN the highest, while BS have intermediate values.\nConsidering that the jellyfish phase lasts between 1 and\n2 Gyr (e.g., Jaffé et al. 2018), it is expected that RIN,\nwhich entered the cluster less than 2 Gyr ago, are the most\naffected. On the other hand, BS galaxies, which according\nto Ruiz et al. (2023) spend on average 1.8 Gyr in a cluster,\nwould have already experienced the jellyfish phenomenon,\nand for some of them, the phenomenon might no longer be\nobservable. Finally, CL galaxies, which have been inside the\ncluster for more than 2 Gyr, are not expected to be observed\nin their jellyfish phase. These results are in agreement with\nYun et al. (2019) who found that, typically, jellyfish galaxies\nare late infallers (< 2.5−3 Gyr ago, at z = 0) in the TNG100\nsimulations. Similarly, Rohr et al. (2023), using the TNG50\ncosmological simulation, found that the peak RPS period\nbegins within approximately 1 Gyr of infall and lasts for ∼2\nGyr.\nOur results suggest that, although over the past 2 −\n3 Gyr galaxies entering clusters have undergone significant\ntransformations in both their star formation and morphology, it\nwill still be necessary for them to orbit within the cluster for a\nlonger time to complete the transformation processes.\nGalaxies located outside R200 that have already made their\nfirst passage through the cluster center continue undergoing\ntransformations, particularly the quenching of the star formation.\nConsidering that an average backsplash galaxy at z\n=\n0,\naccording to Ruiz et al. (2023), spends ∼1.8 Gyr in its diving\nphase and ∼1.2 Gyr outside R200, and that the fraction of passive\nBS galaxies is clearly lower than that in clusters, we conclude\nthat the quenching process of a galaxy entering the cluster can\nlast more than 3 Gyr. However, the fact that RIN, which on\naverage should have spent less time in the cluster than BS, and\nwhich show a higher fraction of passive galaxies than BS, we\ncannot rule out the possibility that, despite the decontamination\nprocesses applied in this work, some degree of contamination\nbetween different orbital types still persists. In particular, BS\nmay be experiencing some level of contamination from infalling\ngalaxies and RIN from CL. An alternative explanation for\nthe observed differences between BS and RIN arises when\n7 The PSB fractions are calculated for all galaxies in a given class,\nwhile the RSQ fractions are based on those that were star forming 0.5\nGyr ago.\nArticle number, page 8 of 9\n\n\nH. Muriel: Unveiling galaxy evolution patterns\nconsidering the times at which those two classes first dived into\nclusters. BS galaxies crossed the clusters’ core when the clusters\nwere less massive. At a lookback time of ∼3 Gyr, clusters can\nbe up to some ∼20% less massive than at z = 0 (see for instance\nvan der Burg et al. 2015). Therefore, these galaxies were affected\nby the environment of typically lesser massive clusters than the\none acting upon RIN at the present, and thus less affected by\nthe environment during their passage through the clusters inner\nregions compared to RIN that are still moving inside the clusters.\nFurthermore, while BS galaxies were crossing the clusters, those\nthat would later become RIN galaxies were being pre-processed\nby the environment in the outskirts of the clusters, adding up to\nthe mass quenching experienced by all galaxies (note that BS\nand RIN galaxies are approximately the same age). This model\nis consistent with results recently found by Levis et al. (in prep.).\nThese authors analyze a sample of groups of galaxies in the TNG\nsimulations and find that the timing of a galaxy’s entry into the\ngroup significantly influences its evolution, with later-arriving\nRIN galaxies experiencing stronger effects from the intragroup\nmedium than earlier-arriving BS galaxies.\nAcknowledgements. The authors thank the referee for her/his comments\nand suggestions that helped us improve the clarity of the paper. This\npaper has been partially supported with grants from Consejo Nacional de\nInvestigaciones Científicas y Técnicas (PIP 11220210100064CO), Argentina,\nthe Agencia Nacional de Promoción Científica y Tecnológica (PICT 2020-3690,\nPICT-2021-I-A-00700),\nArgentina,\nSecretaría\nde\nCiencia\ny\nTecnología,\nUniversidad Nacional de Córdoba, Argentina. MdlR is supported by the Next\nGeneration EU program, in the context of the National Recovery and Resilience\nPlan, Investment PE1 – Project FAIR “Future Artificial Intelligence Research”\nand acknowledges financial support from the Comunidad Autónoma de Madrid\nthrough the grant SI2/PBG/2020-00005. D.P.M acknowledges financial support\nfrom the UNAM-DGAPA-PAPIIT IN111620 grant, Mexico, and from a\nCONAHCyT scholarship.\nReferences\nAbadi, M. G., Moore, B., & Bower, R. G. 1999, MNRAS, 308, 947\nAdami, C., Biviano, A., & Mazure, A. 1998, A&A, 331, 439\nBalogh, M. L., Navarro, J. F., & Morris, S. L. 2000, ApJ, 540, 113\nBamford, S. P., Nichol, R. C., Baldry, I. K., et al. 2009, MNRAS, 393, 1324\nBarnes, J. E. 1992, ApJ, 393, 484\nBiviano, A., Moretti, A., Paccagnella, A., et al. 2017, A&A, 607, A81\nBiviano, A., Poggianti, B. M., Jaffé, Y., et al. 2024, ApJ, 965, 117\nBlanton, M. R., Eisenstein, D., Hogg, D. W., Schlegel, D. J., & Brinkmann, J.\n2005, ApJ, 629, 143\nBlanton, M. R. & Moustakas, J. 2009, ARA&A, 47, 159\nBook, L. G. & Benson, A. J. 2010, ApJ, 716, 810\nBower, R. G., Benson, A. J., & Crain, R. A. 2012, MNRAS, 422, 2816\nBrown, T., Catinella, B., Cortese, L., et al. 2017, MNRAS, 466, 1275\nCava, A., Bettoni, D., Poggianti, B. M., et al. 2009, A&A, 495, 707\nChabrier, G. 2003, PASP, 115, 763\nCimatti, A., Brusa, M., Talia, M., et al. 2013, ApJL, 779, L13\nCoenda, V., de los Rios, M., Muriel, H., et al. 2022, MNRAS, 510, 1934\nCoenda, V., Mast, D., Martínez, H. J., Muriel, H., & Merchán, M. E. 2019, A&A,\n621, A98\nCoenda, V., Muriel, H., Donzelli, C. J., et al. 2006, AJ, 131, 1989\nCortese, L., Catinella, B., Boissier, S., Boselli, A., & Heinis, S. 2011, MNRAS,\n415, 1797\nDalla Vecchia, C. & Schaye, J. 2008, MNRAS, 387, 1431\nDe los Rios, M., Martínez, H. J., Coenda, V., et al. 2021, MNRAS, 500, 1784\nDomínguez, M., Muriel, H., & Lambas, D. G. 2001, AJ, 121, 1266\nDressler, A. 1980, ApJ, 236, 351\nDressler, A. & Gunn, J. E. 1982, ApJ, 263, 533\nEbeling, H., Voges, W., Bohringer, H., et al. 1996, MNRAS, 283, 1103\nFasano, G., Marmo, C., Varela, J., et al. 2006, A&A, 445, 805\nFasano, G., Vanzella, E., Dressler, A., et al. 2012, MNRAS, 420, 926\nFritz, J., Moretti, A., Gullieuszik, M., et al. 2017, ApJ, 848, 132\nFritz, J., Poggianti, B. M., Bettoni, D., et al. 2007, A&A, 470, 137\nFritz, J., Poggianti, B. M., Cava, A., et al. 2014, A&A, 566, A32\nGavazzi, G., Contursi, A., Carrasco, L., et al. 1995, A&A, 304, 325\nGill, S. P. D., Knebe, A., & Gibson, B. K. 2005, MNRAS, 356, 1327\nGiovanelli, R. & Haynes, M. P. 1985, ApJ, 292, 404\nGnedin, O. Y. 2003, ApJ, 582, 141\nGullieuszik, M., Giunchi, E., Poggianti, B. M., et al. 2023, ApJ, 945, 54\nGullieuszik, M., Poggianti, B., Fasano, G., et al. 2015, A&A, 581, A41\nGunn, J. E. & Gott, J. R. I. 1972, ApJ, 176, 1\nHashimoto, Y., Oemler, Augustus, J., Lin, H., & Tucker, D. L. 1998, ApJ, 499,\n589\nHasinger, G. 2008, A&A, 490, 905\nHopkins, P. F., Quataert, E., & Murray, N. 2012, MNRAS, 421, 3522\nJackson, R. A., Kaviraj, S., Martin, G., et al. 2022, MNRAS, 511, 607\nJaffé, Y. L., Poggianti, B. M., Moretti, A., et al. 2018, MNRAS, 476, 4753\nJaffé, Y. L., Smith, R., Candlish, G. N., et al. 2015, MNRAS, 448, 1715\nKauffmann, G., Heckman, T. M., White, S. D. M., et al. 2003, MNRAS, 341, 54\nKlypin, A., Yepes, G., Gottlöber, S., Prada, F., & Heß, S. 2016, MNRAS, 457,\n4340\nKronberger, T., Kapferer, W., Ferrari, C., Unterguggenberger, S., & Schindler,\nS. 2008, A&A, 481, 337\nLee, J. H., Lee, M. G., Mun, J. Y., Cho, B. S., & Kang, J. 2022, ApJL, 931, L22\nLópez-Gutiérrez, M. M., Bravo-Alfaro, H., van Gorkom, J. H., Caretta, C. A., &\net al. 2022, MNRAS, 517, 1218\nMaier, C., Ziegler, B. L., Haines, C. P., & Smith, G. P. 2019, A&A, 621, A131\nMamon, G. A., Sanchis, T., Salvador-Solé, E., & Solanes, J. M. 2004, A&A, 414,\n445\nMarasco, A., Fraternali, F., & Binney, J. J. 2012, MNRAS, 419, 1107\nMarasco, A., Poggianti, B. M., Fritz, J., et al. 2023, MNRAS, 525, 5359\nMartin, G., Kaviraj, S., Devriendt, J. E. G., Dubois, Y., & Pichon, C. 2018,\nMNRAS, 480, 2266\nMartínez, H. J., Coenda, V., & Muriel, H. 2008, MNRAS, 391, 585\nMartínez, H. J., Coenda, V., Muriel, H., de los Rios, M., & Ruiz, A. N. 2023,\nMNRAS, 519, 4360\nMartínez, H. J., de los Rios, M., Coenda, V., et al. 2024, A&A, in press.\nMartínez, H. J. & Muriel, H. 2006, MNRAS, 370, 1003\nMateus, A. & Sodré, L. 2004, MNRAS, 349, 1251\nMoore, B., Lake, G., & Katz, N. 1998, ApJ, 495, 139\nMoretti, A., Gullieuszik, M., Poggianti, B., et al. 2017, A&A, 599, A81\nMoretti, A., Poggianti, B. M., Fasano, G., et al. 2014, A&A, 564, A138\nNandra, K., Georgakakis, A., Willmer, C. N. A., et al. 2007, ApJL, 660, L11\nNavarro, J. F. & White, S. D. M. 1994, MNRAS, 267, 401\nPaccagnella, A., Vulcani, B., Poggianti, B. M., et al. 2017, ApJ, 838, 148\nPaccagnella, A., Vulcani, B., Poggianti, B. M., et al. 2016, ApJL, 816, L25\nPaccagnella, A., Vulcani, B., Poggianti, B. M., et al. 2019, MNRAS, 482, 881\nPasquali, A., Smith, R., Gallazzi, A., et al. 2019, MNRAS, 484, 1702\nPaulino-Afonso, A., Sobral, D., Darvish, B., et al. 2019, A&A, 630, A57\nPérez-Millán, D., Fritz, J., González-Lópezlira, R. A., et al. 2023, MNRAS, 521,\n1292\nPoggianti, B. M., Fasano, G., Omizzolo, A., et al. 2016, AJ, 151, 78\nPoggianti, B. M., Gullieuszik, M., Tonnesen, S., et al. 2019, MNRAS, 482, 4466\nRhee, J., Smith, R., Choi, H., et al. 2017, ApJ, 843, 128\nRoberts, I. D., Lang, M., Trotsenko, D., et al. 2022, ApJ, 941, 77\nRohr, E., Pillepich, A., Nelson, D., et al. 2023, MNRAS, 524, 3502\nRuiz, A. N., Martínez, H. J., Coenda, V., et al. 2023, MNRAS, 525, 3048\nSalerno, J. M., Martínez, H. J., Muriel, H., et al. 2020, MNRAS, 493, 4950\nSalinas, V., Jaffé, Y. L., Smith, R., et al. 2024, MNRAS, 533, 341\nSampaio, V. M., de Carvalho, R. R., Aragón-Salamanca, A., et al. 2024,\nMNRAS, 532, 982\nSchaefer, A. L., Croom, S. M., Allen, J. T., et al. 2017, MNRAS, 464, 121\nSilverman, J. D., Mainieri, V., Lehmer, B. D., et al. 2008, ApJ, 675, 1025\nSmith, R., Sánchez-Janssen, R., Beasley, M. A., et al. 2015, MNRAS, 454, 2502\nSocolovsky, M., Maltby, D. T., Hatch, N. A., et al. 2019, MNRAS, 482, 1640\nSpitzer, Lyman, J. & Baade, W. 1951, ApJ, 113, 413\nSteinhauser, D., Schindler, S., & Springel, V. 2016, A&A, 591, A51\nStringer, M. J., Bower, R. G., Cole, S., Frenk, C. S., & Theuns, T. 2012, MNRAS,\n423, 1596\nToomre, A. 1977, in Evolution of Galaxies and Stellar Populations, ed. B. M.\nTinsley & D. C. Larson, Richard B. Gehret, 401\nvan der Burg, R. F. J., Hoekstra, H., Muzzin, A., et al. 2015, A&A, 577, A19\nVarela, J., D’Onofrio, M., Marmo, C., et al. 2009, A&A, 497, 667\nVijayaraghavan, R. & Ricker, P. M. 2015, MNRAS, 449, 2312\nVillalobos, Á., De Lucia, G., & Murante, G. 2014, MNRAS, 444, 313\nVulcani, B., Fritz, J., Poggianti, B. M., et al. 2020, ApJ, 892, 146\nVulcani, B., Poggianti, B. M., Aragón-Salamanca, A., et al. 2011a, MNRAS,\n412, 246\nVulcani, B., Poggianti, B. M., Dressler, A., et al. 2011b, MNRAS, 413, 921\nVulcani, B., Poggianti, B. M., Finn, R. A., et al. 2010, ApJL, 710, L1\nVulcani, B., Poggianti, B. M., Gullieuszik, M., et al. 2023, ApJ, 949, 73\nVulcani, B., Poggianti, B. M., Smith, R., et al. 2022, ApJ, 927, 91\nWeinmann, S. M., van den Bosch, F. C., Yang, X., & Mo, H. J. 2006, MNRAS,\n366, 2\nWerle, A., Poggianti, B., Moretti, A., et al. 2022, ApJ, 930, 43\nWetzel, A. R., Tinker, J. L., Conroy, C., & van den Bosch, F. C. 2013, MNRAS,\n432, 336\nWhitmore, B. C. & Gilmore, D. M. 1991, ApJ, 367, 64\nWhitmore, B. C., Gilmore, D. M., & Jones, C. 1993, ApJ, 407, 489\nWilkinson, A., Almaini, O., Wild, V., et al. 2021, MNRAS, 504, 4533\nYoon, H., Chung, A., Smith, R., & Jaffé, Y. L. 2017, ApJ, 838, 81\nYun, K., Pillepich, A., Zinger, E., et al. 2019, MNRAS, 483, 1042\nArticle number, page 9 of 9\n\n\n"}
{"text": "EDM: Equirectangular Projection-Oriented Dense Kernelized Feature Matching\nDongki Jung 1,2\nJaehoon Choi 2\nYonghan Lee 2\nSomi Jeong 1\nTaejae Lee 1\nDinesh Manocha 2 Suyong Yeon 1\n1NAVER LABS\n2University of Maryland\njdk9405@umd.edu\nInput\n(c) EDM (ours)\nWarp\n(b) Prev. SotA with Cubemap Proj.\nWarp\nMultiple \nPredictions\n(a) Prev. SotA\nWarp\nFigure 1. (a) Previous state-of-the-art [15] struggles to achieve accurate dense matching in equirectangular projection (ERP) images due to\ninherent distortions. (b) The ERP image can be transformed into a cubemap image, which consists of six perspective images. However, this\napproach demands multiple independent iterations of inference for each pair of perspective images, increasing computational complexity\nand losing the global information in the ERP image. (c) Our proposed method, EDM, leverages the spherical camera model, rendering it\nrobust against distortions. Warp refers to results obtained by multiplying the warped image with the predicted certainty map, demonstrating\nthat our method yields more accurate dense matches.\nAbstract\nWe introduce the first learning-based dense matching algo-\nrithm, termed Equirectangular Projection-Oriented Dense\nKernelized Feature Matching (EDM), specifically designed\nfor omnidirectional images.\nEquirectangular projection\n(ERP) images, with their large fields of view, are partic-\nularly suited for dense matching techniques that aim to\nestablish comprehensive correspondences across images.\nHowever, ERP images are subject to significant distortions,\nwhich we address by leveraging the spherical camera model\nand geodesic flow refinement in the dense matching method.\nTo further mitigate these distortions, we propose spherical\npositional embeddings based on 3D Cartesian coordinates\nof the feature grid. Additionally, our method incorporates\nbidirectional transformations between spherical and Carte-\nsian coordinate systems during refinement, utilizing a unit\nsphere to improve matching performance. We demonstrate\nthat our proposed method achieves notable performance en-\nhancements, with improvements of +26.72 and +42.62 in\nAUC@5° on the Matterport3D and Stanford2D3D datasets.\nProject Page: https://jdk9405.github.io/EDM\n1. Introduction\nOmnidirectional images, also known as 360° images, pro-\nvide significant advantages owing to their expansive fields\nof view, offering more contextual information and versatil-\nity [12, 21, 38, 63, 67]. These spherical images enable a\ncomprehensive representation of environments, facilitating\na deeper understanding of spatial information. Their utility\nextends to aiding robot navigation [40, 61] and autonomous\nvehicle driving [43] by minimizing blind spots. 360° im-\nages also can be utilized in a diverse range of applications,\nfrom creating immersive AR/VR experiences to practical\nuses in interior design [1], tourism [48], and real estate pho-\ntography [5]. Integrating omnidirectional images into vir-\ntual house tours allows customers to experience an immer-\nsive view, enabling them to fully engage themselves in the\nservice. Moreover, the adoption of omnidirectional images\ncontributes to more efficient data collection. By replacing\nthe need for multiple perspective images, omnidirectional\nimages can reduce both the cost and time associated with\ndata scanning. The large field of view provided by 360° im-\nages has also demonstrated superiority over narrower views\nin 3D motion estimation [18, 27, 42].\nFeature matching plays a critical role in numerous 3D\ncomputer vision tasks, including mapping and localiza-\ntion.\nTraditionally, Structure from Motion (SfM) [49]\n1\narXiv:2502.20685v1  [cs.CV]  28 Feb 2025\n\n\nleverages feature matching to estimate relative poses. Re-\ncent advancements have introduced semi-dense or dense\napproaches for feature matching such as LoFTR [55]\nand DKM [15], which demonstrate superior performance\nin repetitive or textureless environments compared to\nkeypoint-based methods [13, 28, 36, 46, 47]. These meth-\nods have been mainly developed for perspective 2D im-\nages and videos, but encounter challenges when applied to\nomnidirectional images. For example, to adapt matching\nmethods for spherical images, two prevalent approaches for\nsphere-to-plane projections are the equirectangular projec-\ntion (ERP) and the cubemap projection [63]. ERP images\nexhibit significant distortions, particularly near the pole re-\ngions, which hinder the effective application of perspective\nmethods. On the other hand, the cubemap format, consist-\ning of six perspective images, can be processed indepen-\ndently without such distortions.\nHowever, this approach\ninvolves the costly computation of multiple inferences for\neach pair of perspective images, resulting in the loss of\nglobal information from a single spherical image and di-\nminishing feature matching capabilities due to the reduced\nfield of view in each perspective image. These challenges\nare shown in Fig. 1 (a) and (b).\nMain Results\nIn this paper,\nwe propose EDM, a\ndistortion-aware dense feature matching method for om-\nnidirectional images, addressing challenges that existing\ndetector-free approaches [15, 16, 55] struggle to overcome.\nTo the best of our knowledge, EDM is the first learning-\nbased method designed for dense matching and relative\npose estimation between two omnidirectional images. As\nseen in Fig. 1, our method defines feature matching in 3D\ncoordinates, specifically addressing the challenges posed\nby distortions of ERP images. We accomplish this based\non the integration of two novel steps: a Spherical Spatial\nAlignment Module (SSAM) and specific enhancements in\nGeodesic Flow Refinement. The SSAM leverages spher-\nical positional embeddings for ERP images and incorpo-\nrates a decoder to generate the global matches. Further-\nmore, the Geodesic Flow Refinement step employs coor-\ndinate transformation to refine the residuals of correspon-\ndences. Compared to both recent sparse and dense feature\nmatching methods [15, 16, 19, 69], our approach results in\nsignificant performance improvement of +26.72 and +42.62\nAUC@5° in relative pose estimation for spherical images\non the Matterport3D [5] and Stanford2D3D [2] datasets.\nAdditionally, we evaluate our method qualitatively on the\nEgoNeRF [7] and OmniPhotos [4] datasets, demonstrating\nrobust performance across diverse environments. The main\ncontributions of this paper are summarized as follows:\n• We introduce a novel approach for estimating dense\nmatching across ERP images using geodesic flow on a\nunit sphere.\n• We propose a Spherical Spatial Alignment Module that\nutilizes Gaussian Process regression and spherical posi-\ntional embeddings to establish 3D correspondences be-\ntween omnidirectional images.\nIn addition, we use\nGeodesic Flow Refinement by enabling conversions be-\ntween coordinates to refine the displacement on the sur-\nface of the sphere.\n• With azimuth rotation for data augmentation, we achieve\nstate-of-the-art performance in dense matching and rela-\ntive pose estimation between two omnidirectional images.\n2. Related Work\nOmnidirectional Images\nThe popularity of consumer-\nlevel 360° cameras has led to increased interest in spherical\nimages, which offer comprehensive coverage of the field\nof view from a single vantage point.\nThese images are\noften represented using equirectangular projection (ERP)\n[63], facilitating their utilization in various computer vision\ntasks. Recent advancements in computer vision have lever-\naged ERP images for diverse tasks such as object detection\n[11, 53], semantic segmentation [24, 66], depth estimation\n[25, 32, 33, 45, 50, 60, 65], omnidirectional Simultaneous\nLocalization and Mapping [62], scene understanding [54],\nand neural rendering [8, 26, 29, 37].\nDespite the utility of ERP images, their unique geom-\netry presents several challenges in visual representation.\nAs ERP images are obtained through projecting a sphere\nonto a plane, a single spherical image can be expressed\nby multiple distinct ERP images. Additionally, ensuring\nperfect alignment of their left and right extremities is es-\nsential.\nWhile some research methods have introduced\nrotation-equivariant convolutions [9, 17] to address these is-\nsues, their implementation often demands increased compu-\ntational resources. To mitigate this constraint, we propose\nan azimuth rotation approach for data augmentation, under\nthe assumption that maintaining the downward orientation\nof scanned omnidirectional images parallel to gravity offers\nbenefits [3].\nFeature Matching\nLocal feature matching has relied\non detector-based methods, encompassing both traditional\nhand-crafted techniques [36, 46] and learning-based ap-\nproaches [13, 28, 34, 44, 59]. These methods typically in-\nvolve detecting keypoints, computing descriptor distances\nbetween paired keypoints, and performing matching via\nmutual nearest neighbor search. SuperGlue [47] introduces\na learning-based paradigm, optimizing visual descriptors\nusing an attentional graph neural network and an optimal\nmatching layer. However, detector-based methods face lim-\nitations in terms of accurately detecting keypoints, particu-\nlarly in repetitive or indiscriminative regions. In contrast,\ndetector-free or dense methods [15, 16, 39, 55, 57, 58] offer\n2\n\n\na solution to the keypoint detection issue, providing dense\nfeature matches at the pixel level.\nWhile the aforementioned methods are tailored for per-\nspective images, they often fail to address the unique chal-\nlenges of spherical cameras. SPHORB [69], an extension\nof ORB [46], mitigates distortion in ERP images using a\ngeodesic grid and local planar approximation [14]. Simi-\nlarly, learning-based matching methods such as SphereGlue\n[19, 20] and PanoPoint [68] adapt keypoint matching tech-\nniques for spherical imagery. CoVisPose [23, 41] explores\nlayout features for estimating camera poses over large base-\nlines yet remains constrained by detected feature infor-\nmation.\nTherefore, we propose a novel dense matching\nmethod that extracts all matches without keypoint detection\nin spherical images.\n3. Preliminaries\n3.1. Spherical and Cartesian Coordinate\nERP\nSphere\n(𝑆!, 𝑆\", 𝑆#)\n𝑥\n𝑦\n𝑧\n𝜃\nϕ\n𝒖= (𝜃, 𝜙)\nFigure 2. Coordinate system.\n\n\n\n\n\nSx = sin(θ) cos(ϕ)\nSy = sin(ϕ)\nSz = cos(θ) cos(ϕ)\n\n\n\n\n\n\n\nθ = arctan(Sx\nSz )\nϕ = arcsin(Sy\n|S|)\n(1)\nAlthough ERP images are displayed in 2D space, they actu-\nally represent a collection of flattened rays normalized to\na unit scale within a spherical camera model. Thus, we\ncan express the coordinate conversion equation u = π(S)\nbetween the spherical coordinates u = (θ, ϕ) and the 3D\nCartesian coordinates S = (Sx, Sy, Sz) as shown in Fig.\n2. Each value of θ ∈[−π, π] and ϕ ∈[−π\n2 , π\n2 ] indicates\nthe longitude and latitude. We utilize this coordinate trans-\nformation π(·) in Section 4.1 and Section 4.2 to handle the\nspherical camera model effectively.\n3.2. Dense Kernelized Feature Matching\nDense matching is the task of finding dense correspondence\nand estimating 3D geometry from two images (IA, IB). Re-\ncently, DKM [15] introduced a kernelized global matcher\nand warp refinement, formulating this problem as finding\na mapping f →u where u are 2D spatial coordinates.\nFirst, DKM extracts multi-scale features using a ResNet50\nencoder [22],\n{f l\nA}L\nl=1 = Encoder(IA),\n{f l\nB}L\nl=1 = Encoder(IB),\n(2)\nwhere the strides are defined as elements of the set l ∈\n{20, ..., 2L−1}. Coarse features are associated with stride\n{32, 16}, and fine features correspond to {8, 4, 2, 1}.\nAt the coarse level, it consists of a kernelized regres-\nsion to estimate the posterior mean µA|B using a Gaus-\nsian Process (GP) formulation. GP regression generates a\nprobabilistic distribution using the feature information con-\nditioned on frame B to estimate coarse global matches. The\nnormalized 2D feature grid f grid\nB\n∈Rh×w×2, where h and w\ndenote the resolution of the feature grid, is embedded into\nχB with an additional cosine embedding [51] to induce mul-\ntimodality in GP. The embedded coordinates are processed\nby an exponential cosine similarity kernel K to calculate\nµA|B,\nµA|B = KAB(KBB + σ2\nnI)−1χcoarse\nB\n,\n(3)\n\n\n\n\n\n\n\nKmn = exp\n \nτ\n \nfm · fn\np\n(fm · fm)(fn · fn) + ε\n−1\n!!\n,\nχcoarse\nB\n= cos(Wf grid\nB\n+ b),\n(4)\nwhere τ = 5, ϵ = 10−6, and the standard deviation of\nthe measurement noise σn = 0.1 in the experiments. W\nand b are the weights and biases of a 1 × 1 convolution\nlayer. Then, CNN embedding decoder [64] yields the ini-\ntial global matches ˆucoarse\nA→B and confidence of matches ˆc coarse\nA→B\nfrom the concatenation of the reshaped estimated posterior\nmean µgrid\nA|B and the coarse features,\n(ˆucoarse\nA→B, ˆc coarse\nA→B) = Decoder(µgrid\nA|B ⊕f coarse\nA\n).\n(5)\nAt the fine level, the warp refiners estimate the residual dis-\nplacement using the previous matches and feature informa-\ntion. The process is described as follows,\n\u0000△ˆul+1\nA→B, △ˆc l+1\nA→B\n\u0001\n= Refinerl+1\u0010\nf l+1\nA\n⊕f l+1\nB→A⊕Corrl+1\nΩk ⊕ˆul+1\nA→B−ul+1\nA\n\u0011\n,\n(6)\n\n\n\n\n\n\n\n\n\nf l+1\nB→A = fB⟨ˆul+1\nA→B⟩,\nf l+1\nB→A, Ωk = fB⟨Ωk, (ˆul+1\nA→B)⟩,\nCorrl+1\nΩk =\nX\nchannel\nf l+1\nA\nf l+1\nB→A, Ωk,\n(7)\nwhere Ωk(u) = u + p (∥p∥∞≤k) is the patch sized\nk, ⟨·⟩means the bilinear interpolation function, Corrl+1\nΩk\nrepresents local correlation between the features, and ul+1\nA\nindicates the grid in f l+1\nA . Finally, it recursively updates the\nmatching points and confidence by adding the residuals to\n3\n\n\nInput\nWarp\nEncoder\nGaussian \nProcess\nDecoder\nRefiner\nGeodesic Flow \nRefinement\nSpherical Spatial \nAlignment Module\nMulti-scale \nFeature Extraction\nPositional\nEmbedding\nSpherical\nCorrespondence\n&\nCertainty\nOutput\n𝑨\n𝑩\n𝑩→𝑨\n𝑨→𝑩\n𝝅\nCartesian\nTo\nSpherical\nSpherical\nTo\nCartesian\n𝝅!𝟏\nFigure 3.\nOverview of our approach. It consists of three steps:\nMulti-scale Feature Extraction, Spherical Spatial Alignment Mod-\nule (Sec. 4.1), and Geodesic Flow Refinement (Sec. 4.2).\nthe previous information and upsampling until reaching the\nsame resolution as the input images,\nˆul\nA→B = ˆul+1\nA→B + △ˆul+1\nA→B,\nˆc l\nA→B = ˆc l+1\nA→B + △ˆc l+1\nA→B.\n(8)\n4. Our Proposed Method\nThe overall process is illustrated in Fig. 3. Following the\napproach outlined in Section 3.2, we first utilize ERP im-\nages IA and IB as input and extract multi-scale features fA\nand fB. Different from [15], we reformulate the problem as\nfinding a mapping f →S using 3D Cartesian coordinates.\nWe introduce the Spherical Spatial Alignment Module, a\nglobal matcher utilizing a spherical camera system to com-\npensate for distortions caused by sphere-to-plane projection\nin ERP images. We then formalize the geodesic flow on\na unit sphere and establish projections between equirectan-\ngular and spherical spaces to refine matches. In addition,\nto enhance the robust accuracy of our method, we leverage\nrandomized azimuth rotation during the training process.\n4.1. Spherical Spatial Alignment Module\nOur Spherical Spatial Alignment Module (SSAM) conducts\nglobal matching at a coarse level through Gaussian Process\n(GP) regression, depicted in Fig. 4. GP predicts the poste-\nrior mean µA|B from the embeddings as in Eq. 3. Due to\nthe pronounced distortions in the polar regions of ERP im-\nages, spherical positional embedding/encoding is frequently\nemployed to mitigate this challenge [6, 30, 31]. Here, we\nexplicitly apply positional embeddings with 3D Cartesian\ncoordinates, derived from the 2D spherical feature grid and\nthe inverse transformation function π−1(·),\nχcoarse\nB\n= cos(Wπ−1(f grid\nB ) + b).\n(9)\nOur proposed positional embedding facilitates the utiliza-\ntion of embedded coordinates χcoarse\nB\nto promote distortion\nawareness within the ERP images. Additionally, this em-\nbedding ensures structural consistency along the boundaries\n𝜒!\n1x1 Conv\nSpherical Positional Embedding\n𝑓\"\n𝑓!\n⊕\n1x1 Conv\n⊗\n⊗\nembedding\nHW x HW\nC x H x W\nHW x 3\n𝜇\"|!\n𝐆𝐏( ⋅)\n𝑲𝑨𝑩\n𝑲𝑩𝑩\nGrid 𝑺𝑩\nHW x C’\n1 x H x W\nshared \nweights\nHW x HW\nC x H x W\nHW x C’\n3 x H x W\nFeatures\nPositional \nEmbedding\nPosterior \nMean\nDecoder\n(Eq. 9)\nCertainty *𝒄𝑨→𝑩\n𝐜𝐨𝐚𝐫𝐬𝐞\nGlobal Match ,𝑺𝑨→𝑩\n𝐜𝐨𝐚𝐫𝐬𝐞\nFigure 4. Our Spherical Spatial Alignment Module. We present\nSpherical Positional Embedding (red dotted box). The embed-\nding decoder generates the global matches ˆScoarse\nA→B. Here, the gray\ncurved lines represent the geodesic flow between SA and SB. ⊕\ndenotes concatenation, ⊗means reshape and matrix multiplica-\ntion. We provide the matrix dimensions of intermediate features\nfor reference.\n⊕\n⟨⋅⟩\n$𝒖!→#\n$%&\n△$𝒖!→#\n$%&\n$𝒖!→#\n$\n'𝑺!→#\n$\n𝝅\n𝝅\"𝟏\n'𝑺!→#\n$%&\n$𝒖!→#\n$%& −𝒖!\n$%&\n△̂𝑐!→#\n$%&\nK x H x W\n2 x H x W\n𝒇𝑨\n𝒍\n𝒇𝑩\n𝒍\n𝑪𝒐𝒓𝒓𝛀𝒌\n𝒍\n𝒇𝑩→𝑨\n𝒍\nC x H x W\n3 x H x W\n1x1 Conv\nC’ x H x W\n2 x H x W\n1 x H x W\n2 x H x W\n3 x H x W\nBilinear \nInterpolation\nFeatures\n3D Cartesian To  2D Spherical\n2D Spherical To 3D Cartesian\n(Eq. 11)\n(Eq. 12)\nRefiner𝒍+1\nFigure 5. Our proposed Geodesic Flow Refinement. Refining the\ndisplacement along curved lines on the spherical surface presents\nsignificant challenges. To address this, we project the displace-\nment into the ERP space for refinement (Cartesian to spherical)\nand subsequently unproject it back onto the spherical surface for\nfurther refinement (spherical to Cartesian).\nof ERP images by leveraging relative spatial information\nwithin the 3D Cartesian grid. The outputs of the subse-\nquent embedding decoder provide the initial global matches\nˆScoarse\nA→B on the unit sphere and the ERP certainty map ˆccoarse\nA→B,\n\u0010\nˆScoarse\nA→B, ˆccoarse\nA→B\n\u0011\n= Decoder(µA|B ⊕f coarse\nA\n).\n(10)\n4.2. Geodesic Flow Refinement\nIn our SSAM approach, as the geodesic flow must reside\non the unit sphere, directly defining warp refinement on\nthe surface of the sphere makes it impossible to update the\nresiduals linearly. Thus, we circumvent this problem by en-\nabling a conversion between the 3D Cartesian coordinates\nand the 2D equirectangular space, as illustrated in Fig. 5,\nˆul+1\nA→B = π(ˆSl+1\nA→B).\n(11)\nAfter following all the processes outlined in Eq. 6 for re-\nfinement, we update the residuals as described in Eq. 8.\n4\n\n\n𝑾𝒐𝒓𝒍𝒅\n𝑻\n𝑻𝒂𝒖𝒈\n𝜽𝒂𝒖𝒈\n𝐼𝐴←𝐼𝐴⟨𝜋𝑇𝐴\n𝑎𝑢𝑔𝜋−1 𝐼𝐴\n𝑔𝑟𝑖𝑑\n⟩\n𝐷𝐴←𝐷𝐴⟨𝜋𝑇𝐴\n𝑎𝑢𝑔𝜋−1 𝐷𝐴\n𝑔𝑟𝑖𝑑\n⟩\n𝑇𝐴←𝑇𝐴𝑇𝐴\n𝑎𝑢𝑔\nFigure 6. Maintaining consistent geometry, ERP can produce mul-\ntiple visual representations based on θaug.\nAs this refinement stage iterates repeatedly, the predicted\nˆul\nA→B is back-projected into 3D Cartesian coordinates,\nˆSl\nA→B = π−1(ˆul\nA→B).\n(12)\n4.3. Augmentation\nA single omnidirectional image can be transformed into\nmultiple distinct ERP images, as shown in Fig. 6. This\ntransformation is feasible by capturing the full spectrum of\nrays and ensuring a seamless representation in the spherical\ninput image, which facilitates the generation of diverse ERP\nimages while maintaining consistent geometric properties\nin the world space. Consequently, we define a horizontal\nrotation matrix T aug\nA\nwith a randomly selected azimuth an-\ngle θaug\nA ∈[0, 2π] during training. Based on T aug\nA , we rotate\nand redefine the ERP image IA, the depth map DA, and the\npose TA. Notably, this transformation adjusts TA and DA\ntogether, ensuring consistent geometry in the world space.\nThe same process is applied to the counterpart frame B.\n4.4. Loss\nUtilizing dense ground truth depth maps and aligned camera\nposes, we can derive ERP depth DA→B and matches SA→B\nduring the warping process from frame A to B within the\nspherical coordinate system. We adopt the certainty estima-\ntion method proposed by Edstedt et al. [15], which involves\nfinding consistent matches using relative depth consistency\nbetween frames A and B,\ncA→B =\n\f\f\f\f\nDA→B −DB\nDB\n\f\f\f\f < α,\n(13)\nwhere α is 0.05. The binary mask cA→B represents the\nground truth certainty map. Diverging from the approach\noutlined in Edstedt et al. [15], our method constrains the\npredicted matches ˆSl\nA→B, composed of 3D Cartesian co-\nordinates, to reside on the surface of the unit sphere. This\nimplies that the predicted matches can be interpreted as the\nray directions of the spherical camera. Instead of defining\nthe loss function based on the Euclidean distance between\nthe predicted matches ˆSl\nA→B and the ground truth matches\nSl\nA→B, we use the angular difference between the ray di-\nrections. Consequently, this approach ensures that ˆSl\nA→B\nis optimized along the surface of the unit sphere. We define\nour regression loss Ll\nr using cosine similarity to measure the\nangular difference. For the certainty loss Ll\nc, we employ the\nbinary cross-entropy function, as utilized in Edstedt et al.\n[15],\nLl\nr =\nX\ngrid\ncl\nA→B ⊙(1 −∥Sl\nA→B · ˆSl\nA→B∥\n∥Sl\nA→B∥∥ˆSl\nA→B∥\n),\n(14)\nLl\nc =\nX\ngrid\ncl\nA→Blogˆcl\nA→B + (1 −cl\nA→B)log(1 −ˆcl\nA→B).\n(15)\nThe total loss function comprises a weighted sum of the re-\ngression loss and the certainty loss, as detailed in Edstedt\net al. [15], Melekhov et al. [39], Tan et al. [56], Zhou et al.\n[70], with λ set at 0.01,\nLtotal =\nL\nX\nl=1\nLl\nr + λLl\nc.\n(16)\n5. Experiments\n5.1. Experiments Settings\nMatterport3D Dataset\nTraining our method requires\nERP input images, ground truth depth maps, and aligned\nposes. The Matterport3D dataset [5] encompasses 90 in-\ndoor scenes represented by 10,800 panoramas reconstructed\nas textured meshes. However, the dataset lacks pose and\ndepth information for skybox images, which are essential\nfor creating ERP images. Previous works have addressed\nthis limitation by rendering both images and depth maps\nfrom the textured mesh [71] or by employing 360° SfM\nto estimate poses [45]. In our approach, we generate the\nposes for skybox images directly from the originally pro-\nposed camera poses in Matterport3D. Through experimen-\ntation, we found that treating the 12th camera pose, out of\nthe 18 viewpoints (comprising 6 rotations and 3 tilt angles)\nin each panorama, identically to the second skybox image\ndid not result in any issues. We define the remaining poses\nfor the skybox images by rotating 90° in each direction from\nthe second pose. We adhere to the official benchmark split,\nutilizing 61 scenes for training, 11 for validation, and 18\nfor testing. For two-view pose estimation, it is necessary\nto create pairs of overlapped images. We achieve this by\ntransforming ERP depth maps between frames within the\nspherical coordinate system. Pixels where the depth differ-\nence is below a specified threshold, e.g. 0.1, are classified as\ninliers. Subsequently, we compare the ratio of these inliers\nto the total number of pixels. We organize both the train-\ning and testing datasets based on the overlap ratio of image\npairs and the benchmark split. Specifically, images with the\noverlap ratio exceeding 30% are distributed into respective\ntraining and testing splits. As a result, the training set con-\ntains 44,700 pairs, while the test set comprises 4,575 pairs.\n5\n\n\nWe resize the resolution of ERP images and depth maps to\n640 × 320.\nStanford2D3D Dataset\nStanford2D3D [2] consists of\ndata scanned from six large-scale indoor spaces collected\nfrom three distinct buildings. This dataset contains a rela-\ntively small number of 1,413 panorama images and, there-\nfore, is utilized exclusively for testing purposes. We assess\nthe overlap ratio between frames and include them in the\ntest split if their ratio exceeds 50%. A total of 3,460 pairs\nare incorporated into the test set. During testing, we resize\nthe resolution to 640 × 320.\nEgoNeRF and OmniPhotos Dataset\nEgoNeRF [7] intro-\nduces 11 synthetic scenes created with Blender [10] and\n11 real scenes captured with a RICOH THETA V cam-\nera. OmniPhotos [4] provides a dataset captured with an\nInsta360 ONE X camera. Both datasets contain egocentric\nscenes captured with a casually rotating camera stick. Con-\nsequently, their rotation axes, pole regions, or camera height\nchange, resulting in different distortions compared to Mat-\nterport3D or Stanford2D3D. We present additional qualita-\ntive results from these datasets to validate our method.\nImplementation Details\nWe employ the AdamW [35]\noptimizer with a weight-decay factor of 10−2, a learning\nrate of 5·10−6 for multiscale feature extractor, and 10−4 for\nthe SSAM and the Geodesic Flow Refiner. EDM is trained\nfor 300,000 steps with a batch size of 4 in a single RTX 3090\nGPU, which takes approximately two days to complete.\nDuring evaluation, the balanced sampling approach using\nkernel density estimation [15] tends to establish correspon-\ndences primarily in concentrated areas with high probability\ndistributions, making it unsuitable for omnidirectional im-\nages. Thus, we randomly sample up to 5,000 matches after\ncertainty filtering with a threshold of 0.8 to ensure corre-\nspondences cover the entire area.\n5.2. Experimental Results\nWe compare our proposed method EDM with four differ-\nent methods: 1) SPHORB [69] is a hand-crafted keypoint-\nbased feature matching algorithm. 2) SphereGlue [19] is a\nlearning-based keypoint matching method. Both SPHORB\n[69] and SphereGlue [19] are specifically designed for\nspherical images.\n3) DKM [15] and 4) RoMa [16] are\nstate-of-the-art dense matching algorithms for perspective\nimages. To estimate the essential matrix and the relative\npose for spherical cameras, Solarte et al. [52] proposed a\nnormalization strategy and non-linear optimization within\nthe classic 8-point algorithm. We adopt this for two-view\npose estimation in all quantitative comparisons.\nTable 1 shows the quantitative results of the pose esti-\nmation in Matterport3D. Despite SPHORB and SphereGlue\nMethod\nImage\nFeature\nAUC\n@5°\n@10°\n@20°\nSPHORB [69]\nERP\nsparse\n0.38\n1.41\n3.99\nSphereGlue [19]\nERP\nsparse\n11.29\n19.95\n31.10\nDKM [15]\npersepctive\ndense\n18.43\n28.50\n38.44\nRoMa [16]\nperspective\ndense\n12.45\n22.37\n34.24\nEDM (ours)\nERP\ndense\n45.15\n60.99\n73.60\nTable 1. Quantitative comparison on Matterport3D with recent\nalgorithms. EDM improves AUC@5° by 26.72.\nMethod\nImage\nFeature\nAUC\n@5°\n@10°\n@20°\nSPHORB [69]\nERP\nsparse\n0.14\n1.01\n4.08\nSphereGlue [19]\nERP\nsparse\n11.25\n22.41\n36.57\nDKM [15]\nperspective\ndense\n12.46\n22.18\n34.13\nRoMa [16]\nperspective\ndense\n11.48\n22.52\n37.07\nEDM (ours)\nERP\ndense\n55.08\n71.65\n82.72\nTable 2. Quantitative comparison on Stanford2D3D with recent\nalgorithms. EDM improve AUC@5° by 42.62.\nbeing designed for the ERP images, the presence of tex-\ntureless or repetitive regions, which are common in indoor\nenvironments of Matterport3D, leads to performance degra-\ndation in the keypoint-based methods. SPHORB fails to es-\ntimate the essential matrix correctly due to the limited num-\nber of matching points. EDM demonstrates significantly\nhigher performance than all the other methods.\nFigure 7 illustrates the qualitative results in Matter-\nport3D. The previous methods designed for perspective im-\nages, such as DKM and RoMa, exhibit good matching abil-\nity but encounter challenges when confronted with the dis-\ntortions of ERP. While SphereGlue and SPHORB perform\nwell in discriminative regions, their performance deterio-\nrates as the overlap ratio decreases, resulting in numerous\nfalse positive matches. In contrast, EDM can estimate dense\ncorrespondences regardless of occlusion and textureless ar-\neas.\nDue to the similarity in results between DKM and\nRoMa, we have only included the former to maintain a con-\ncise visualization. Experimental results in Fig.8 depict the\nrelationship between image overlap ratio and AUC@20°\nperformance. As expected, a decrease in the overlap ra-\ntio leads to severe performance degradation in the previous\nworks. On the other hand, our proposed method demon-\nstrates robustness in more challenging scenes, maintaining\nsimilar performance levels until the overlap decreases to\n60%, compared to other methods.\nFor a fair comparison, we use another benchmark\ndataset, Stanford2D3D. We validate EDM using a model\ntrained on Matterport3D without additional training on\nStanford2D3D. In Table 2, EDM outperforms the previous\n6\n\n\n(a) Keypoint-based\n(b) DKM\n(c) EDM (ours)\n(a) Keypoint-based\n(b) DKM\n(c) EDM (ours)\n90% ⎼\n80% ⎼90%\n70% ⎼80%\n60% ⎼70%\n50% ⎼60%\n40% ⎼50%\n30% ⎼40%\n60% ⎼70%\nFigure 7. Qualitative results on Matterport3D. (a) The blue lines represent the results of matching points from SPHORB [69]; the green\nlines correspond to SphereGlue [19]. Both (b) DKM [15] and (c) EDM depict the outcomes of multiplying the warped image with the\ncertainty map. EDM can estimate dense and accurate matches even in the presence of distortions and severe occlusions. The numbers\nbeside the images represent the overlap ratio, reflecting the difficulty of matching. Smaller numbers indicate more challenging scenes.\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n30 ⎼40% 40 ⎼50% 50 ⎼60% 60 ⎼70% 70 ⎼80% 80 ⎼90%\n90% ⎼\nSphereGlue\nDKM\nEDM (ours)\nOverlap Ratio\nAUC @ 20°\nFigure 8. Performance with respect to the overlap ratio. This high-\nlights the robustness of EDM in scenarios with varying levels of\noverlap, particularly in challenging conditions where the overlap\nratio is limited.\nworks by a significant margin, especially in scenes with se-\nvere occlusion. The certainty map demonstrates EDM’s ro-\nbustness, particularly in handling occluded scenes. Addi-\ntionally, although the panorama images in Stanford2D3D\nKeypoint-based\nEDM (ours)\nKeypoint-based\nEDM (ours)\nFigure 9. Qualitative results on Stanford2D3D. The blue and green\nlines correspond to SPHORB and SphereGlue.\ncontain missing regions in the upper and lower parts of\nthe sphere, the proposed spherical positional embedding en-\nables the network to predict matching correspondences ac-\ncurately, as shown in Fig. 9.\n5.3. Additional Qualitative Results\nTo demonstrate the robust performance of our method\nacross diverse environments, we qualitatively validate EDM\nusing additional datasets such as EgoNeRF and OmniPho-\ntos. As it is primarily trained on indoor environments [5]\nwhere the camera is oriented parallel to gravity, severely\nslanted image pairs of rotational scenes or outdoor envi-\n7\n\n\nImages\nWarp\nImage\nWarp\nFigure 10. Qualitative results on EgoNeRF [7] and OmniPhotos\n[4]. Despite being primarily trained on indoor scenes, EDM effec-\ntively estimates dense matching on these datasets, demonstrating\nits generalization capability across diverse environments.\nronments may cause EDM to fail in accurately estimating\ncorrespondences. However, despite these differences in set-\ntings, EDM demonstrates the ability to conduct dense fea-\nture matching robustly, as shown in Fig. 10.\nFurthermore, we demonstrate the applicability of our\nmethod to various omnidirectional downstream tasks. As\nshown in Fig. 11, our approach successfully performs trian-\ngulation from pairs of omnidirectional images. By leverag-\ning EDM’s capability to predict dense correspondences, the\ntriangulated points yield a dense 3D reconstruction. For a\nmore comprehensive discussion, please refer to the supple-\nmentary materials.\n5.4. Ablation Study\nDKM’s dependence on the pinhole camera model makes it\ninherently unsuitable for learning with ERP images. To en-\nsure the fair comparison, we modified the warping process\nin the loss function of DKM to support spherical cameras,\nresulting in DKM∗. As shown in Table 3, this demonstrates\nthe structural effectiveness of our proposed bidirectional co-\nordinate transformation. The proposed positional embed-\ndings result in improvements based on the coordinate sys-\ntem of the spherical camera model. We observe that utiliz-\ning a 3D grid input of Cartesian coordinates yields better\nperformance than 2D spherical ones. Additionally, in our\nmethod, positional embedding with a linear layer slightly\noutperforms spherical positional encoding with sinusoidal\n[31]. Table 3 also confirms the advantage of our rotational\naugmentation. Through this augmentation technique, we\ncan effectively address the challenge of a limited number\nof datasets for omnidirectional images in dense matching\ntasks.\n6. Conclusion, Limitations, and Future Work\nIn this paper, we present, for the first time, a novel dense\nfeature matching method tailored for omnidirectional im-\nMethod\nPositional\nBidirectional\nRotational\nAUC\nEmbedding\nTransformation\nAugmentation\n@5°\n@10°\n@20°\nDKM∗\n2D linear\n-\n-\n19.83\n33.06\n46.24\nOurs\n2D linear\n✓\n-\n29.67\n45.90\n60.82\nOurs\n2D linear\n✓\n✓\n35.03\n51.14\n65.07\nOurs\n3D linear\n✓\n-\n34.64\n50.82\n65.16\nOurs\n3D linear\n✓\n✓\n45.15\n60.99\n73.60\nOurs\n3D sinusoidal\n✓\n✓\n42.39\n58.27\n70.98\nTable 3. Ablation study for the proposed method. DKM∗indi-\ncates the DKM model trained on Matterport3D with a modified\nloss function for ERP images. Compared to DKM∗, our method\nenhances performance through the proposed spherical positional\nembedding in SSAM, bidirectional transformation via Geodesic\nFlow Refinement, and rotational augmentation.\nFigure 11.\nTriangulation results on Matterport3D and Stan-\nford2D3D. These point clouds are generated through spherical tri-\nangulation using the estimated poses between two omnidirectional\nimages. Our method can reconstruct dense point clouds in texture-\nless regions, which are particularly challenging in indoor environ-\nments.\nages. Leveraging the foundational principles of DKM, we\nintegrate the inherent characteristics of the spherical camera\nmodel into our dense matching process using geodesic flow\nfields. This integration instills distortion awareness within\nthe network, thereby enhancing its performance specifically\nfor ERP images. However, it is important to note that our\nmethod is predominantly trained on indoor datasets where\nthe camera is vertically oriented, rendering it somewhat vul-\nnerable to extreme rotations or outdoor environments. To\naddress this limitation, future endeavors will focus on di-\nversifying the training data and data augmentation to en-\ncompass a wider range of environments, fortifying the ro-\nbustness of our network. Furthermore, we aim to extend\nour method into downstream tasks, particularly for visual\nlocalization and mapping applications for omnidirectional\nimages.\n8\n\n\nReferences\n[1] Friska Amalia and Ahmad Fitriyansah. Case study of 360\nimage viewer software utilization in interior design presenta-\ntion to improve product immersion. In ICCED. IEEE, 2023.\n1\n[2] Iro Armeni, Sasha Sax, Amir R Zamir, and Silvio Savarese.\nJoint 2d-3d-semantic data for indoor scene understanding.\narXiv preprint arXiv:1702.01105, 2017. 2, 6\n[3] Matheus A Bergmann, Paulo GL Pinto, Thiago LT da Sil-\nveira, and Cl´audio R Jung.\nGravity alignment for single\npanorama depth inference. In SIBGRAPI. IEEE, 2021. 2\n[4] Tobias Bertel, Mingze Yuan, Reuben Lindroos, and Christian\nRichardt. Omniphotos: casual 360 vr photography. ACM\nTransactions on Graphics (TOG), 39(6):1–12, 2020. 2, 6, 8\n[5] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej\nHalber, Matthias Niessner, Manolis Savva, Shuran Song,\nAndy Zeng, and Yinda Zhang.\nMatterport3d: Learning\nfrom rgb-d data in indoor environments.\narXiv preprint\narXiv:1709.06158, 2017. 1, 2, 5, 7\n[6] Zhaoxi Chen, Guangcong Wang, and Ziwei Liu. Text2light:\nZero-shot text-driven hdr panorama generation. ACM Trans-\nactions on Graphics (TOG), 41(6):1–16, 2022. 4\n[7] Changwoon Choi, Sang Min Kim, and Young Min Kim. Bal-\nanced spherical grid for egocentric view synthesis. In CVPR,\n2023. 2, 6, 8\n[8] Dongyoung Choi, Hyeonjoong Jang, and Min H Kim. Om-\nnilocalrf: Omnidirectional local radiance fields from dy-\nnamic videos. arXiv preprint arXiv:2404.00676, 2024. 2\n[9] Taco S Cohen, Mario Geiger, Jonas K¨ohler, and Max\nWelling. Spherical cnns. arXiv preprint arXiv:1801.10130,\n2018. 2\n[10] Blender Online Community. Blender - a 3D modelling and\nrendering package. Blender Foundation, Stichting Blender\nFoundation, Amsterdam, 2018. 6\n[11] Benjamin Coors, Alexandru Paul Condurache, and Andreas\nGeiger. Spherenet: Learning spherical representations for\ndetection and classification in omnidirectional images.\nIn\nECCV, 2018. 2\n[12] Thiago LT da Silveira, Paulo GL Pinto, Jeffri Murrugarra-\nLlerena, and Cl´audio R Jung. 3d scene geometry estimation\nfrom 360 imagery: A survey. ACM Computing Surveys, 55\n(4):1–39, 2022. 1\n[13] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-\nnovich. Superpoint: Self-supervised interest point detection\nand description. In CVPR Workshops, 2018. 2\n[14] Marc Eder, Mykhailo Shvets, John Lim, and Jan-Michael\nFrahm. Tangent images for mitigating spherical distortion.\nIn CVPR, 2020. 3\n[15] Johan Edstedt, Ioannis Athanasiadis, M˚arten Wadenb¨ack,\nand Michael Felsberg.\nDKM: Dense kernelized feature\nmatching for geometry estimation. In CVPR, 2023. 1, 2,\n3, 4, 5, 6, 7\n[16] Johan\nEdstedt,\nQiyu\nSun,\nGeorg\nB¨okman,\nM˚arten\nWadenb¨ack, and Michael Felsberg. Roma: Revisiting ro-\nbust losses for dense feature matching.\narXiv preprint\narXiv:2305.15404, 2023. 2, 6\n[17] Carlos Esteves, Christine Allen-Blanchette, Ameesh Maka-\ndia, and Kostas Daniilidis. Learning so (3) equivariant rep-\nresentations with spherical cnns. In ECCV, 2018. 2\n[18] Cornelia Ferm¨uller and Yiannis Aloimonos. Geometry of\neye design: Biology and technology. In Multi-Image Anal-\nysis: 10th International Workshop on Theoretical Founda-\ntions of Computer Vision Dagstuhl Castle, Germany, March\n12–17, 2000 Revised Papers, pages 22–38. Springer, 2001. 1\n[19] Christiano Gava, Vishal Mukunda, Tewodros Habtegebrial,\nFederico Raue, Sebastian Palacio, and Andreas Dengel.\nSphereglue: Learning keypoint matching on high resolution\nspherical images. In CVPR Workshops, 2023. 2, 3, 6, 7\n[20] Christiano Gava, Yunmin Cho, Federico Raue, Sebastian\nPalacio, Alain Pagani, and Andreas Dengel. Spherecraft: A\ndataset for spherical keypoint detection, matching and cam-\nera pose estimation. In WACV, 2024. 3\n[21] Julia Guerrero-Viu, Clara Fernandez-Labrador, C´edric De-\nmonceaux, and Jose J Guerrero. What’s in my room? object\nrecognition on indoor panoramic images. In ICRA. IEEE,\n2020. 1\n[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition.\nIn CVPR,\n2016. 3\n[23] Will Hutchcroft, Yuguang Li, Ivaylo Boyadzhiev, Zhiqiang\nWan, Haiyan Wang, and Sing Bing Kang. Covispose: Co-\nvisibility pose transformer for wide-baseline relative pose\nestimation in 360° indoor panoramas. In ECCV. Springer,\n2022. 3\n[24] Chiyu Jiang, Jingwei Huang, Karthik Kashinath, Philip Mar-\ncus, Matthias Niessner, et al. Spherical cnns on unstructured\ngrids. arXiv preprint arXiv:1901.02039, 2019. 2\n[25] Hualie Jiang, Zhe Sheng, Siyu Zhu, Zilong Dong, and Rui\nHuang. Unifuse: Unidirectional fusion for 360 panorama\ndepth estimation. IEEE Robotics and Automation Letters, 6\n(2):1519–1526, 2021. 2\n[26] Hakyeong Kim, Andreas Meuleman, Hyeonjoong Jang,\nJames Tompkin, and Min H Kim.\nOmnisdf: Scene re-\nconstruction using omnidirectional signed distance functions\nand adaptive binoctrees. arXiv preprint arXiv:2404.00678,\n2024. 2\n[27] Jong Weon Lee, Suya You, and Ulrich Neumann.\nLarge\nmotion estimation for omnidirectional vision. In Proceed-\nings IEEE Workshop on Omnidirectional Vision (Cat. No.\nPR00704), pages 161–168. IEEE, 2000. 1\n[28] Kunhong Li, Longguang Wang, Li Liu, Qing Ran, Kai Xu,\nand Yulan Guo. Decoupling makes weakly supervised local\nfeature better. In CVPR, 2022. 2\n[29] Longwei Li, Huajian Huang, Sai-Kit Yeung, and Hui Cheng.\nOmnigs: Omnidirectional gaussian splatting for fast radiance\nfield reconstruction using omnidirectional images.\narXiv\npreprint arXiv:2404.03202, 2024. 2\n[30] Meng Li, Senbo Wang, Weihao Yuan, Weichao Shen, Zhe\nSheng, and Zilong Dong. S2Net: Accurate panorama depth\nestimation on spherical surface. IEEE Robotics and Automa-\ntion Letters, 8(2):1053–1060, 2023. 4\n[31] Xiang Li, Haoyuan Cao, Shijie Zhao, Junlin Li, Li Zhang,\nand Bhiksha Raj. Panoramic video salient object detection\nwith ambisonic audio guidance. In AAAI, 2023. 4, 8\n9\n\n\n[32] Yuyan Li, Zhixin Yan, Ye Duan, and Liu Ren. Panodepth:\nA two-stage approach for monocular omnidirectional depth\nestimation. In 3DV. IEEE, 2021. 2\n[33] Yuyan Li, Yuliang Guo, Zhixin Yan, Xinyu Huang, Ye Duan,\nand Liu Ren. Omnifusion: 360 monocular depth estimation\nvia geometry-aware fusion. In CVPR, 2022. 2\n[34] Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao,\nand Xiaowei Zhou. Gift: Learning transformation-invariant\ndense visual descriptors via group cnns. Advances in Neural\nInformation Processing Systems, 32, 2019. 2\n[35] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 6\n[36] David G Lowe.\nDistinctive image features from scale-\ninvariant keypoints. International journal of computer vi-\nsion, 60:91–110, 2004. 2\n[37] Yikun Ma, Dandan Zhan, and Zhi Jin. Fastscene: Text-driven\nfast 3d indoor scene generation via panoramic gaussian splat-\nting. arXiv preprint arXiv:2405.05768, 2024. 2\n[38] Kevin Matzen, Michael F Cohen, Bryce Evans, Johannes\nKopf, and Richard Szeliski. Low-cost 360 stereo photog-\nraphy and video capture. ACM Transactions on Graphics\n(TOG), 36(4):1–12, 2017. 1\n[39] Iaroslav Melekhov, Aleksei Tiulpin, Torsten Sattler, Marc\nPollefeys, Esa Rahtu, and Juho Kannala. Dgc-net: Dense\ngeometric correspondence network. In WACV. IEEE, 2019.\n2, 5\n[40] Emanuele Menegatti, Takeshi Maeda, and Hiroshi Ishiguro.\nImage-based memory for robot navigation using properties\nof omnidirectional images. Robotics and Autonomous Sys-\ntems, 47(4):251–267, 2004. 1\n[41] Negar\nNejatishahidin,\nWill\nHutchcroft,\nManjunath\nNarayana, Ivaylo Boyadzhiev, Yuguang Li, Naji Khos-\nravan, Jana Koˇseck´a, and Sing Bing Kang.\nGraph-covis:\nGnn-based multi-view panorama global pose estimation. In\nCVPR, 2023. 3\n[42] Randal C Nelson and John Aloimonos. Finding motion pa-\nrameters from spherical motion fields (or the advantages of\nhaving eyes in the back of your head). Biological cybernet-\nics, 58(4):261–273, 1988. 1\n[43] Gaurav Pandey, James R McBride, and Ryan M Eustice.\nFord campus vision and lidar data set.\nThe International\nJournal of Robotics Research, 30(13):1543–1552, 2011. 1\n[44] Jerome Revaud, Cesar De Souza, Martin Humenberger, and\nPhilippe Weinzaepfel. R2d2: Reliable and repeatable detec-\ntor and descriptor. Advances in neural information process-\ning systems, 32, 2019. 2\n[45] Manuel Rey-Area, Mingze Yuan, and Christian Richardt.\n360monodepth: High-resolution 360deg monocular depth\nestimation. In CVPR, 2022. 2, 5\n[46] Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary\nBradski. Orb: An efficient alternative to sift or surf. In ICCV.\nIeee, 2011. 2, 3\n[47] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz,\nand Andrew Rabinovich.\nSuperglue:\nLearning feature\nmatching with graph neural networks. In CVPR, pages 4938–\n4947, 2020. 2\n[48] Olivier Saurer, Friedrich Fraundorfer, and Marc Pollefeys.\nOmnitour: Semi-automatic generation of interactive virtual\ntours from omnidirectional video. In 3DPVT, 2010. 1\n[49] Johannes L Schonberger and Jan-Michael Frahm. Structure-\nfrom-motion revisited. In CVPR, 2016. 1\n[50] Zhijie Shen, Chunyu Lin, Kang Liao, Lang Nie, Zishuo\nZheng, and Yao Zhao. Panoformer: Panorama transformer\nfor indoor 360° depth estimation. In ECCV. Springer, 2022.\n2\n[51] Herman P Snippe and Jan J Koenderink.\nDiscrimination\nthresholds for channel-coded systems. Biological cybernet-\nics, 66(6):543–551, 1992. 3\n[52] Bolivar Solarte, Chin-Hsuan Wu, Kuan-Wei Lu, Yi-Hsuan\nTsai, Wei-Chen Chiu, and Min Sun. Robust 360-8pa: Re-\ndesigning the normalized 8-point algorithm for 360-fov im-\nages. In ICRA. IEEE, 2021. 6\n[53] Yu-Chuan Su and Kristen Grauman. Learning spherical con-\nvolution for fast features from 360 imagery.\nAdvances in\nneural information processing systems, 30, 2017. 2\n[54] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Hohonet:\n360 indoor holistic understanding with latent horizontal fea-\ntures. In CVPR, 2021. 2\n[55] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and\nXiaowei Zhou. Loftr: Detector-free local feature matching\nwith transformers. In CVPR, 2021. 2\n[56] Dongli Tan, Jiang-Jiang Liu, Xingyu Chen, Chao Chen,\nRuixin Zhang, Yunhang Shen, Shouhong Ding, and Ron-\ngrong Ji.\nEco-tr:\nEfficient correspondences finding via\ncoarse-to-fine refinement. In ECCV. Springer, 2022. 5\n[57] Prune Truong, Martin Danelljan, and Radu Timofte. Glu-\nnet: Global-local universal network for dense flow and cor-\nrespondences. In CVPR, 2020. 2\n[58] Prune Truong, Martin Danelljan, Luc Van Gool, and Radu\nTimofte. Learning accurate dense correspondences and when\nto trust them. In CVPR, 2021. 2\n[59] Michał Tyszkiewicz, Pascal Fua, and Eduard Trulls. Disk:\nLearning local features with policy gradient. Advances in\nNeural Information Processing Systems, 33:14254–14265,\n2020. 2\n[60] Fu-En Wang, Yu-Hsuan Yeh, Min Sun, Wei-Chen Chiu, and\nYi-Hsuan Tsai. Bifuse: Monocular 360 depth estimation via\nbi-projection fusion. In CVPR, 2020. 2\n[61] Niall Winters, Jos´e Gaspar, Gerard Lacey, and Jos´e Santos-\nVictor. Omni-directional vision for robot navigation. In Pro-\nceedings IEEE Workshop on Omnidirectional Vision (Cat.\nNo. PR00704), pages 21–28. IEEE, 2000. 1\n[62] Changhee Won, Hochang Seok, Zhaopeng Cui, Marc Polle-\nfeys, and Jongwoo Lim. Omnislam: Omnidirectional local-\nization and dense mapping for wide-baseline multi-camera\nsystems. In ICRA. IEEE, 2020. 2\n[63] Mai Xu, Chen Li, Shanyi Zhang, and Patrick Le Callet.\nState-of-the-art in 360 video/image processing: Perception,\nassessment and compression. IEEE Journal of Selected Top-\nics in Signal Processing, 14(1):5–26, 2020. 1, 2\n[64] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao,\nGang Yu, and Nong Sang. Learning a discriminative feature\nnetwork for semantic segmentation. In CVPR, 2018. 3\n10\n\n\n[65] Ilwi Yun, Hyuk-Jae Lee, and Chae Eun Rhee. Improving 360\nmonocular depth estimation via non-local dense prediction\ntransformer and joint supervised and self-supervised learn-\ning. In AAAI, 2022. 2\n[66] Chao Zhang, Stephan Liwicki, William Smith, and Roberto\nCipolla. Orientation-aware semantic segmentation on icosa-\nhedron spheres. In ICCV, 2019. 2\n[67] Fanglue Zhang, Junhong Zhao, Yun Zhang, and Stefanie\nZollmann. A survey on 360° images and videos in mixed\nreality: Algorithms and applications. Journal of Computer\nScience and Technology, 38(3):473–491, 2023. 1\n[68] Hengzhi Zhang, Hong Yi, Haijing Jia, Wei Wang, and\nMakoto Odamaki. Panopoint: Self-supervised feature points\ndetection and description for 360deg panorama. In CVPR\nWorkshops, 2023. 3\n[69] Qiang Zhao, Wei Feng, Liang Wan, and Jiawan Zhang.\nSphorb: A fast and robust binary feature on the sphere. In-\nternational journal of computer vision, 113:143–159, 2015.\n2, 3, 6, 7\n[70] Qunjie Zhou,\nTorsten Sattler,\nand Laura Leal-Taixe.\nPatch2pix: Epipolar-guided pixel-level correspondences. In\nCVPR, 2021. 5\n[71] Nikolaos Zioulis, Antonis Karakottas, Dimitrios Zarpalas,\nand Petros Daras. Omnidepth: Dense depth estimation for\nindoors spherical panoramas. In ECCV, 2018. 5\n11\n\n\n"}
{"text": "JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\n1\nCleanMel: Mel-Spectrogram Enhancement for\nImproving Both Speech Quality and ASR\nNian Shao, Rui Zhou, Pengyu Wang, Xian Li, Ying Fang, Yujie Yang and Xiaofei Li\nAbstract—In this work, we propose CleanMel, a single-channel\nMel-spectrogram denoising and dereverberation network for\nimproving both speech quality and automatic speech recogni-\ntion (ASR) performance. The proposed network takes as input\nthe noisy and reverberant microphone recording and predicts\nthe corresponding clean Mel-spectrogram. The enhanced Mel-\nspectrogram can be either transformed to speech waveform\nwith a neural vocoder or directly used for ASR. The proposed\nnetwork is composed of interleaved cross-band and narrow-\nband processing in the Mel-frequency domain, for learning the\nfull-band spectral pattern and the narrow-band properties of\nsignals, respectively. Compared to linear-frequency domain or\ntime-domain speech enhancement, the key advantage of Mel-\nspectrogram enhancement is that Mel-frequency presents speech\nin a more compact way and thus is easier to learn, which will\nbenefit both speech quality and ASR. Experimental results on\nfour English and one Chinese datasets demonstrate a significant\nimprovement in both speech quality and ASR performance\nachieved by the proposed model. Code and audio examples of\nour model are available online 1.\nIndex Terms—Speech enhancement, Mel-frequency, speech\ndenoising, speech dereverberation, automatic speech recognition\nI. INTRODUCTION\nT\nHIS work studies single-channel speech enhancement\nusing deep neural networks (DNNs), to improve both\nspeech quality and automatic speech recognition (ASR) perfor-\nmance. A large class of speech enhancement methods employ\nDNNs to map from noisy and reverberant speech to corre-\nsponding clean speech, conducted either in time domain [1],\n[2] or in time-frequency domain [3]–[5]. These methods can\nefficiently suppress noise, but not necessarily improve ASR\nperformance due to the speech artifacts/distortions caused\nby speech enhancement networks [6]. In [7], it was found\nthat time-domain enhancement is more ASR-friendly than\nfrequency-domain enhancement. A time domain progressive\nlearning method is proposed in [8], which also shows the\nsuperiority of time domain speech enhancement, and the\nprogressive learning mechanism is very effective for robust\nASR by mitigating the over suppression of speech. In [9], [10],\nASR performance is largely improved by decoupling frontend\nNian Shao, Pengyu Wang, Ying Fang and Yujie Yang are with Zhejiang\nUniversity, and also with Westlake University, Hangzhou, China (e-mail:\n{shaonian, wangpengyu, fangying, yangyujie}@westlake.edu.cn).\nRui Zhou, Xian Li and Xiaofei Li are with the School of Engineering,\nWestlake University, and also with the Institute of Advanced Technology,\nWestlake Institute for Advanced Study, Hangzhou, China (e-mail: {zhourui,\nlixian, lixiaofei}@westlake.edu.cn).\nNian Shao and Rui Zhou equally contributed to this work. Corresponding:\nlixiaofei@westlake.edu.cn\n1https://audio.westlake.edu.cn/Research/CleanMel.html\nenhancement and backend recognition. In [10], it is shown that\nan advanced time-frequency domain network, i.e. CrossNet,\ncan even outperform the time domain ARN network.\nSpeech enhancement in Mel-frequency domain, or similarly\nin rectangular bandwidth (ERB) domain, has been developed\nunder various contexts in the literature. Mel-frequency and\nERB bands model human speech perception of spectral en-\nvelope and signal periodicity, within which speech enhance-\nment is more perceptually and computationally efficient than\nwithin linear-frequency domain or time domain. In [11]–\n[13], spectral envelope enhancement is performed within the\nERB bands, and then applying pitch filtering [11], [12] or\ndeep filtering [13] to recover the enhanced speech. Sub-band\nnetworks [3], [14] and full-band/sub-band fusion networks,\ni.e. FullSubNet [15], [16], have been recently proposed and\nachieved outstanding performance. However, separately pro-\ncessing sub-bands in the linear-frequency domain leads to a\nlarge computational complexity. To reduce the number of sub-\nbands and thus the computational complexity, Fast FullSubNet\n[17] and the work of [18] proposed to perform sub-band\nprocessing in the Mel-frequency domain, and then transform\nback to linear frequency with a joint post-processing network.\nIn [19], [20], speech enhancement is directly conducted in\nthe Mel-frequency domain and then a separate neural vocoder\nis used to recover speech waveform. These methods improve\nspeech enhancement capability by alleviating the burden of\nenhancing full-band speech details and also by leveraging\nthe powerful full-band speech recovery capacity of advanced\nneural vocoder, as a result, achieve higher speech quality\ncompared to their linear-frequency counterparts.\nIn this work, we propose CleanMel, a single-channel Mel-\nspectrogram enhancement network for improving both speech\nquality and ASR performance. Different from the previous\nworks [11]–[13], [17] that performing speech enhancement\nin the ERB or Mel domain, and then applying a joint pitch\nfiltering or deep filtering to obtain the enhanced speech, this\nwork decouples the Mel-spectrogram enhancement and post-\nprocessing parts by targeting the enhancement network with\nclean Mel-spectrogram. The enhanced Mel-spectrogram can\nbe directly used for ASR, or transformed back to wave-\nform with a separate neural vocoder as is done in [19].\nCompared to linear-frequency spectrogram or time-domain\nwaveform, Mel-frequency presents speech in a more compact\nand less-detailed way (but still perceptually efficient) and\nhas a lower feature dimension (number of frequencies) from\nthe perspective of machine learning, which would result in\nlower the prediction error. This is beneficial for both speech\nquality improvement and ASR: (i) Neural vocoders have been\narXiv:2502.20040v1  [eess.AS]  27 Feb 2025\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\n2\nextensively studied in the field of Text-to-Speech, and are\ncapable of efficiently transforming Mel-spectrogram back to\ntime-domain waveform. Therefore, the low-error property of\nMel-spectrogram enhancement can be hopefully maintained by\nthe neural vocoders and higher speech quality can be achieved.\n(ii) From the perspective of ASR, there seems no need to\nfirst recover the less-accurate full-band speech details and then\ncompress to Mel-frequency. A direct and more-accurate Mel-\nspectrogram estimation would be preferred.\nWe adapt the network architecture of our previous proposed\n(online) SpatialNet [21], [22] with some modifications to better\naccommodate Mel-spectrogram enhancement. SpatialNet is\ncomposed of interleaved cross-band and narrow-band blocks\noriginally proposed for processing multichannel STFT fre-\nquencies. The narrow-band block processes STFT frequencies\nindependently to learn the spatial information presented in\nnarrow-band (one frequency), such as the convolutive sig-\nnal propagation and the spatial correlation of noise. And\nthe cross-band block was designed for learning the across-\nfrequency dependencies of narrow-band information. As for\nsingle-channel Mel-spectrogram enhancement in this work, the\nnarrow-band block processes Mel frequencies independently to\nalso learn the (single-channel) convolutive signal propagation\nof target speech, which is crucial not only for conducting\ndereverberation of target speech but also for discriminating\nbetween target speech and interfering signals. The cross-band\nblock is now reinforced and utilized to learn the full-band\nspectral pattern in the Mel-frequency domain.\nIn addition, we have studied several critical issues when\ndecoupling the Mel-spectrogram enhancement front-end and\nASR/Vocoder back-ends. i) We have systematically studied\nand compared different learning targets that can be used for\nMel-spectrogram enhancement, including logMel mapping,\nMel ratio masking and the clipping issue of logMel; ii) We\nhave developed a data normalization scheme to align the signal\nlevels of cascaded front-end and back-end models; iii) We have\ndeveloped an online neural vocoder to enable online speech\nenhancement.\nExperiments are conducted on five public datasets (four\nEnglish and one Chinese) for speech denoising and derever-\nberation individually or jointly. Importantly, we adopt a more\nrealistic evaluation setup: from multiple data sources of clean\nspeech, real-measured room impulse responses (RIRs) and\nnoise signals, we collected and organized a relatively large-\nscale training set, based on which we train the network for\nonce and directly test it on all the five test sets. Experiments\nshow that the proposed model achieves the state-of-the-art\n(SOTA) speech enhancement performance in term of speech\nperceptual quality. Moreover, on top of various pre-trained\nand advanced ASR models, the proposed model prominently\nimproves the ASR performance on all datasets. These results\ndemonstrate that our trained models have the potential to be\ndirectly employed to real applications.\nII. PROBLEM FORMULATION\nThe noisy and reverberant single-channel speech signals can\nbe represented in the time domain as\ny(n) = s(n) ∗a(n) + e(n)\n(1)\nwhere n stands for the discrete time index. s(n) and e(n)\nrepresents the clean source speech and ambient noise, respec-\ntively. a(n) denotes RIR and ∗the convolution operation.\nIn this work, only static speaker is considered, thence the\nRIR is time-invariant. RIR is composed of the direct-path\npropagation, early reflections and late reverberation.\nWe conduct joint speech denoising and dereverberation in\nthis work, which amounts to estimate the (Mel-spectrogram\nof) desired direct-path speech x(n) = s(n) ∗adp(n) from\nmicrophone recording y(n), where adp(n) denotes the direct-\npath part in RIR. The training target of the proposed network\nand the training signals of neural vocoders will all be derived\nwith x(n).\nThe proposed method is performed in the time-frequency\ndomain. By applying STFT to Eq. (1), based on the convolu-\ntive transfer function approximation [23], we can obtain:\nY (f, t) ≈S(f, t) ∗A(f, t) + E(f, t)\n(2)\nwhere f ∈{0, ..., F −1} and t ∈{1, ..., T} denote the\nindices of frequency and time frame, respectively. Y (f, t),\nS(f, t) and E(f, t) are the STFT of respective signals, and\nX(f, t) is the STFT of direct-path speech. A(f, t) is the\nconvolutive transfer function associated to a(n). Convolution\n∗is conducted along time. In the STFT domain, the time\ndomain convolution s(n)∗a(n) is decomposed as (frequency-\nindependently) narrow-band convolutions S(f, t) ∗A(f, t).\nSpeech dereverberation in this work highly relies on learning\nthis narrow-band convolution. For nosie reduction, one im-\nportant way for discriminating between speech and stationary\nnoise is to test the signal stationarity, which can be modeled\nin narrow-band as well.\nIII. MEL-SPECTROGRAM ENHANCEMENT\nIn this work, we propose to enhance the Mel-spectrograms,\nwhich then can be directly fed into an ASR model, or\ntransformed to waveforms with a neural vocoder.\nA. Learning Target: Clean Mel-spectrogram\nThe power-based or magnitude-based Mel-spectrogram of\nthe target speech X(f, t), denoted as Xmel(fmel, t), can be ob-\ntained by weighted summing the squared magnitude |X(f, t)|2\nor magnitude |X(f, t)| over frequencies with the triangle\nweight functions of Mel filterbanks, where fmel ∈{1, ..., Fmel}\ndenotes the index of Mel-frequency. Our preliminary exper-\niments showed that using power-based or magnitude-based\nMel-spectrograms achieve similar enhancement performance,\nthence we can choose either of them according to the Mel\nsetup of ASR or neural vocoder backend models. In this\nwork, we use the power-based Mel-spectrogram according to\nthe setup of commonly-used ASR models. Then, the logMel-\nspectrogram, namely the logarithm of Xmel(fmel, t)\nXlogmel(fmel, t) = log(max{Xmel(fmel, t), ϵ})\n(3)\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\n3\ncan be taken as the input feature of ASR or neural Vocoder\nbackends. The base of logarithm (e or 10) should be consistent\nto the one of back-ends as well, while e is used in this work.\nThe Mel-spectrogram is clipped with a small value of ϵ to\navoid applying logarithm to close-zero values.\nNormally, ϵ is set to be very small, e.g. 1e-10, to maintain\ncomplete speech information. However, in our preliminary\nexperiments, we found that very small speech values are\nnot very informative for both ASR and neural vocoder, and\nthus can be clipped without harming performance. Moreover,\nsince those small values are highly contaminated by noise\nor reverberation, the prediction error of them could be very\nlarge. For these reasons, we set ϵ to a relatively large value,\ne.g. 1e-5 (when the maximum value of time domain signal\nis normalized. The signal normalization methods will be\npresented in Section III-D). Fig. 1 gives an example of our\ntarget logMel-spectrogram, in which about 40% TF bins are\nclipped.\nIn this work, we evaluate two different learning targets as\nfor Mel-spectrogram enhancement.\nLogMel mapping: The clean logMel-spectrogram can be\ndirectly predicted with the network. The training loss is set\nto the mean absolute error (MAE) loss between the predicted\nand the clean logMel-spectrogram, namely\nLMAE =\n1\nFmelT\nFmel\nX\nfmel=1\nT\nX\nt=1\n| ˆXlogmel(fmel, t) −Xlogmel(fmel, t)|,\n(4)\nwhere ˆXlogmel(fmel, t) is the network output.\nMel ratio mask: Ratio mask is one type of popular learning\ntargets for speech magnitude enhancement [24]. For each time-\nmel-frequency bin, the Mel ratio mask is defined as\nM(fmel, t) = min\n s\nXmel(fmel, t)\nYmel(fmel, t) , 1\n!\n,\n(5)\nwhere Ymel(fmel, t) denotes the power level of noisy Mel-\nspectrogram. The square root function transforms the power\ndomain to the magnitude domain. The min(·) function rectifies\nthe mask into the range of [0, 1]. The mean squared error\n(MSE) of the ratio mask is taken as the training loss, namely\nLMRM =\n1\nFmelT\nFmel\nX\nfmel=1\nT\nX\nt=1\n(M(fmel, t) −ˆ\nM(fmel, t))2,\n(6)\nwhere ˆ\nM(fmel, t) denotes the model prediction of M(fmel, t).\nThen, the enhanced logMel-spectrogram can be obtained as\nˆXlogmel(fmel, t) = log(max{ ˆ\nM(fmel, t)2Ymel(fmel, t), ϵ}). (7)\nB. The CleanMel Network\nFig. 2 shows the network architecture. The proposed net-\nwork takes as input (the real (R(·)) and imaginary (I(·)) parts\nof) the STFT of microphone recording, i.e. Y (f, t), denoted\nas y:\ny[f, t, :] = [R(Y (f, t)), I(Y (f, t))] ∈R2,\n(8)\n0\n20\n40\n60\n80\nMel Frequency\n(a) ϵ =1e-10\n0.0\n1.0\n2.0\n3.0\n4.0\nTime (s)\n0\n20\n40\n60\n80\nMel Frequency\n(b) ϵ =1e-5\nln(1e-10)\nln(1e-5)\n0\nln(1e-10)\nln(1e-5)\n0\nFig. 1. An example of logMel spectrogram with a clip value of ϵ =1e-10 or\nϵ =1e-5.\nwhere [:] represents to take values of one dimension of a tensor.\nThe network is composed of an input layer, interleaved cross-\nband and narrow-band blocks first in the linear-frequency do-\nmain and then in the Mel-frequency domain, a Mel-filterbank,\nand finally a Linear output layer. The input layer conducts\ntemporal convolution on y with a kernel size of 5, obtaining\nthe hidden representation with the dimensions of F × T × H.\nThen one cross-band block and one narrow-band block process\nthe hidden tensor in linear-frequency. Mel-filterbank (with\ntriangle weight functions) transforms the frequency dimension\nfrom F linear frequencies to Fmel Mel frequencies, which is\nrealized with a non-trainable matrix multiplication. Then L\ninterleaved cross-band and narrow-band blocks process the\ntensor in Mel-frequency. After the final narrow-band block, the\noutput Linear layer transforms H-dim to 1-dim as either the\nenhanced logMel-spectrogram or the Mel ratio mask. Note that\nSigmoid activation is applied for predicting Mel ratio mask.\n1) Narrow-band block: As shown in Eq. (2), the time\ndomain\nconvolution\ncan\nbe\ndecomposed\nas\nfrequency-\nindependently narrow-band convolutions, while the latter has\nmuch smaller complexity compared to the former in terms\nof the order of room filters. Therefore, modeling the narrow-\nband convolution would be much more efficient than mod-\neling the time domain convolution. The convolution model\nof target speech provides not only necessary information for\ndereverberation of the target speech but also discriminative\ninformation between the target speech and other interfering\nsources. In addition, in narrow-band, non-stationary speech\nand stationary noise can be well discriminated by testing the\nsignal stationarity. For these reasons, we propose the narrow-\nband network, which processes frequencies independently\nalong the time dimension, and all frequencies share the same\nnetwork.\nThe narrow-band convolution in Eq. (2) is defined between\nthe complex-valued STFT coefficients of source speech and\nroom filter. Thence we process the complex-valued STFT\ncoefficients of noisy signal (in a hidden space), instead of\nother features such as magnitude, to retain the convolution\ncorrelation. We first process in (the finer) linear-frequency\nwith one narrow-band block to fully exploit the convolution\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\n4\nc) Cross-band Block\n…\nF-GConv1d\nPReLU\nF-GConv1d\nPReLU\nF-Linear\nF-Linear\nLinear + SiLU\nLinear + SiLU\nLayer Norm\nLayer Norm\nLayer Norm\na) System overview\nb) (Online/Offline) Narrow-band Block\nLayer Norm\nForward\nMamba\nBackward\nMamba\navg.\nCross-band Block\nNarrow-band Block\nT-Conv1d\nLinear\nNoisy and reverberant \nSTFT Coefficients\n𝐹𝐹× (𝑇𝑇× 2)\n𝑇𝑇× (𝐹𝐹× 𝐻𝐻)\n𝐹𝐹× (𝑇𝑇× 𝐻𝐻)\n𝑇𝑇× 𝐻𝐻× (𝐹𝐹)\nVocoder\nASR model\nBack-ends\nTraining Loss\nEnhanced  \nLogMel\nCross-band Block\nMel-Filterbank\nNarrow-band Block\n𝑇𝑇× (𝐹𝐹mel × 𝐻𝐻)\n𝐹𝐹mel × (𝑇𝑇× 𝐻𝐻)\n𝐹𝐹mel × 𝑇𝑇× (𝐻𝐻)\n𝐹𝐹mel × 𝑇𝑇× 1\nrepeat 𝐿𝐿times\nFig. 2. Model architecture of the proposed speech enhancement system. a) System overview. The input dimension of neural blocks/layers are presented before\neach of them in the form ‘batch dimension x (dimension of one sample in batch)”. b) The online/offline narrow-band block. The solid lines stands for online\nprocessing. The solid plus dashed lines stands for offline processing. c) The cross-band block.\ninformation, then after Mel-filterbanks, we process in (the\ncoarser) Mel-frequency with more narrow-band blocks.\nThe narrow-band network is composed of Mamba blocks\n[25]. Mamba is a recently proposed network architecture based\non structured state space sequence models, which was shown\nvery efficient for learning both short-term and long-term de-\npendencies of sequential data. Besides short-term correlations\nof signals, there exist some long-term dependencies should be\nexploited for narrow-band speech enhancement. For example,\nthe convolution model is time invariant in a very long period\nof time for static speakers. Moreover, Mamba has a linear\ncomputational complexity w.r.t time, which is suitable for\nstreaming processing of long audio signals. Specifically, one\nnarrow-band block consists of a forward Mamba for online\nprocessing and an optional backward Mamba (averaging with\nforward output) for offline processing.\n2) Cross-band block: The cross-band block is used to learn\nfull-band/cross-band dependencies of signals. In the original\nSpatialNet [21], the cross-band block is designed for learning\nthe linear relationship of inter-channel features (e.g. inter-\nchannel phase different) across frequencies. In this work,\nsingle channel does not have such inter-channel information.\nInstead, the cross-band block can learn the full-band spectral\npattern of signals in the linear/Mel-frequency domain, which\nis also critical information for (especially single-channel)\nspeech enhancement. The cross-band block processes frames\nindependently along the frequency dimension, and all frames\nshare the same network.\nSpecifically, we adopt the original cross-band block as in\nSpatialNet, which is composed of cascaded frequency con-\nvolutional layer (F-GConv1d), across-frequency linear layers\n(F-Linear) and a second frequency convolution layer. The\nfrequency convolutional layers perform 1-D convolution along\nfrequency to learn correlations between adjacent frequencies.\nThe across-frequency linear layer processes all frequencies\ntogether to learn full-band dependencies. One cross-band block\nis first applied in the linear-frequency domain to learn de-\ntailed full-band correlations. To reduce the model complexity,\nthe hidden dimension H is compressed to a much smaller\ndimension, such as H/12, and then each hidden dimension is\nindependently processed by the across-frequency linear layer\nwith a complexity of F 2. After Mel-filterbank, the cross-band\nblocks learn full-band correlations across Mel frequencies,\nwhere the model complexity is largely reduced from F 2 to\nF 2\nmel. Correspondingly, the hidden dimension is remained as\nH (no longer compressed) to reinforce the capability of full-\nband learning. We set all the Mel-frequency cross-band blocks\nto share the same across-frequency linear layers.\nC. Back-ends\nAt the inference stage, CleanMel is followed by either an\nASR model or a neural vocoder. The ASR model and neural\nvocoder are separately trained with the CleanMel network.\n1) ASR: At inference, the enhanced logMel-spectrogram\nis directly fed to an already-trained ASR system, without\nperforming any fine-tuning or joint-training. Different ASR\nsystems may have different configurations in STFT settings,\nnumber of mel frequencies and base of logarithm. To seam-\nlessly integrate the enhanced logMel-spectrogram into one\nASR model, our CleanMel would adopt the same configu-\nrations as the ASR model. The training cost of CleanMel\nis not very high, so it can be easily re-trained for a new\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\n5\nASR system, especially for those large-scale ASR systems and\nalready-deployed ASR systems. In this work, we only conduct\noffline ASR combined with offline CleanMel.\n2) Neural vocoder: The vocoder we adopt in this work\nis Vocos [26], a recently proposed Generative Adversarial\nNetwork (GAN)-based neural vocoder. The generator of Vocos\npredicts the STFT coefficients of speech at frame level and\nthen generates waveform through inverse STFT. Vocos uses\nthe multiple-discriminators and multiple-losses proposed in\nHiFi-GAN [27], but it significantly improves the computa-\ntional efficiency compared to HiFi-GAN that directly generates\nwaveform at sample level. In this work, to unify the front-\nend and back-end processing, we have made several necessary\nmodifications to Vocos as follows:\n• The magnitude-based Mel-spectrogram of original Vocos\nis modified as power-based to be consistent with the\nfront-end and ASR models, where the two cases were\nshown to achieve similar performance in our preliminary\nexperiments.\n• The sampling rate of signals, the STFT configurations and\nthe number of Mel-frequencies of the Vocos are adjusted\naccording to the setup of front-end and ASR models.\n• The original Vocos is designed for offline processing,\nas it employs non-causal convolution layers. To enable\nonline processing, we modified Vocos to be causal by\nsubstituting each non-causal convolution layer with its\ncausal version. The online vocoder still performs quite\nwell. In addition, to reduce the computational complexity\nof online processing, the 75% STFT overlap of original\nVocos is reduced to be 50% overlap, which still achieves\ncomparable performance.\nThe offline and online Vocos are used to work with the offline\nand online CleanMel, respectively. Vocos models are trained\nwith our direct-path target speech x(n).\nD. Signal Normalization\nWhen separate front-end and back-end models are cascaded,\nsignal normalization should be performed not only to facilitate\nthe training of respective models but also to align the signal\nlevel of cascaded models. For offline processing, the normal-\nization method used in Vocos is also applied for CleanMel.\nSpecifically, a random gain is applied to time domain signal\nto ensure that the maximum level of the resulting signal\nlies within the range of -1 to -6 dBFS. This normalization\nmanner ensures the maximum level of sample values is close\nto and smaller than 1, thence the generated waveform can be\ndirectly played with full volume and without clipping effect.\nFor CleanMel, we apply this normalization to noisy signal\nand utilize the same gain of noisy signal to the corresponding\nclean target signal. In this way, the enhanced Mel-spectrogram\ncan be directly fed to Vocos. When applying this time-\ndomain normalization, the clip value ϵ for computing logMel-\nspectrogram is set to 1e-5 for both CleanMel and Vocos. As for\nASR, ASR models normally have a separate Mel-spectrogram\nnormalization operation, which will be applied to re-normalize\nthe enhanced Mel-spectrogram.\nFor online processing, the time domain normalization\nmethod is no longer applicable. Instead, an online STFT-\ndomain normalization is used for both CleanMel and Vocos.\nSpecifically, for CleanMel, the noisy and target speech are\nnormalized in the STFT domain as ˜Y (f, t) = Y (f, t)/µ(t)\nand\n˜X(f, t) = X(f, t)/µ(t), where µ(t) is a recursively\ncalculated meanvalue of STFT magnitude of Y (f, t), i.e.\nµ(t) = αµ(t −1) + (1 −α) 1\nF\nPF −1\nf=0 |Y (f, t)|. The smoothing\nweight is set to α = K−1\nK+1, by which the recursive smoothing\nis equivalent to using a K-long rectangle smoothing window.\nWhen training Vocos using target speech signal x(n), we still\nfirst apply the time domain normalization mentioned above,\nthen apply an extra online normalization. Specifically, µ(t)\nis computed with and also applied to X(f, t), and then the\ncorresponding logMel-spectrogram is computed as the input of\nVocos generator. Accordingly, the output of Vocos generator\n(before applying inverse STFT) would be an estimation of\nnormalized X(f, t). To go back to the signal level of time\ndomain normalization, the recursive normalization factor µ(t)\nis multiplied back to the estimation of normalized X(f, t)\nand then applying inverse STFT, after which Vocos losses\n(including Mel loss and discriminator losses) are computed.\nAt inference, online normalization is applied to the noisy\ninput, and the enhanced logMel-spectrogram is directly fed\ninto Vocos. The recursive normalization factor computed with\nthe noisy input is multiplied to the estimated STFT coefficients\nby Vocos and then applying inverse STFT to obtain the\nfinal waveform which is time-domain normalized and can be\ndirectly played. When applying this online normalization, the\nclip value ϵ for computing logMel-spectrogram is set to 1e-4\nfor CleanMel and the input of Vocos generator.\nIV. EXPERIMENTAL SETUPS\nIn the section, we present the experiment datasets, ex-\nperimental configurations, evalution metrics and comparison\nmethods.\nA. Dataset\n1) Speech enhancement training dataset: The proposed\nmodel is trained with synthetic noisy/clean speech pairs.\nReverberant speech signals are generated by convolving source\nspeech signals with RIRs, then added with noise signals. Clean\nspeech signals are generated by convolving source speech\nsignals with the direct-path part of RIRs.\nIn this work we conduct speech enhancement for both\nMandarin Chinese and English, and we will evaluate our\nmodel on five different datasets (as will be shown later). We\nattempt to train the model once and test it on all the datasets,\nwhich will reflect the general capability of the model under\nvarious situations. To do this, we collect data (in terms of\nsource speech, RIRs and noise) from multiple public datasets,\nand form a training set with sufficient speech quality and\nenvironment/device diversity.\nSource speech: Source speech signals are collected from 6\ndatasets, including AISHELL I [28], AISHELL II [29] and\nTHCHS30 [30] for Chinese, and EARS [31], VCTK [32]\nand DNS I challenge [33] for English. For each language,\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\n6\nTABLE I\nCLEAN SOURCE SPEECH USED IN THIS WORK.\nLanguage\nSpeech dataset\nDuration (hours) #Speakers\nChinese\nAISHELLI [28]\n25\n240\nAISHELL II [29]\n178\n987\nTHCHS30 [30]\n18\n49\nEnglish\nEARS [31]\n81\n92\nVCTK [32]\n32\n49\nDNS I challenge [33]\n94\n1,263\nTABLE II\nREAL-MEASURED RIRS USED IN THIS WORK.\nRIR dataset\n#RIRs\nT60 range (second)\nACE [35]\n84\n[0.33, 1.22]\nAIR [36]\n204\n[0.08, 4.50]\nARNI [37]\n1,000\n[0.51, 1.20]\ndEchorate [38]\n594\n[0.17, 0.75]\nMultiChannel [39]\n234\n0.16, 0.36, 0.61\nNaturalReverb [40]\n270\n[0.02, 2.00]\nREVERB [41]\n240\n[0.25, 0.70]\nRWCP [42]\n182\n[0.10, 0.72]\nabout 200 hours of high-quality speech data are selected from\nthe original datasets, based on the raw score of DNSMOS\nP.835 [34]. The selection thresholds are set to 3.6 and 3.5\nfor Chinese and English data, respectively. Except that the\nentire EARS training set is included, since EARS involves\nvarious emotional speech that cannot be well evaluated by the\nDNSMOS. The amount of data selected from each dataset is\nsummarized in Table I.\nRIR: We use real-measured RIRs from multiple public\ndatasets [35]–[42]. Table II shows the statistics of these RIR\ndatasets. For all (even multi-channel) datasets, all RIRs are\nused, except that we uniformly sampled 1,000 RIRs from the\nvery large original ARNI dataset.\nThe reverberation time, i.e. T −60, of RIRs mostly lie in\nthe range of (0, 1.5) seconds, except for a few rooms in AIR\n[36] and NaturalReverb [40]. The distribution of the T60s are\nshown in Fig. 3. Besides the wide distribution range of T60s,\nthese RIRs also have large diversity in terms of environments\nand measuring devices. For example, NaturalReverb [40] is\nrecorded in 271 spaces encountered by humans during daily\nlife, AIR [36] is measured with a dummy head for binaural\napplications, RWCP [42] use a Head-Torso as source speaker,\netc. When synthesizing reverberant speech, 80% source speech\nsamples are convolved with a randomly selected RIR, while\nthere is no RIR convolution for the rest 20% samples to\naccount for the near-field applications where reverberation is\nnegligible.\nNoise: Speech and noise are mixed with a random signal-to-\nnoise ratio (SNR) between -5 dB and 20 dB. We use the noise\nsignals from the DNS challenge [43] and the RealMAN dataset\n[44]. The DNS challenge dataset has about 181 hours of noise\nsampled from AudioSet [45] and Freesound. The RealMAN\ndataset has 106 hours of ambient noise recorded in 31 daily\nlife scenes, including various indoor, semi-outdoor, outdoor\nand transportation scenes.\n0.0\n0.15\n0.3\n0.45\n0.6\n0.75\n0.9\n1.05\n1.2\n1.35\n1.5\n4.5\nT60 (s)\n0\n5\n10\n15\n20\n25\nPercentage (%)\n...\nFig. 3. T60 distribution of RIRs used in this work.\nTABLE III\nMODEL CONFIGURATIONS OF THE PROPOSED MEL SPECTROGRAM\nENHANCEMENT NETWORKS.\nMode\nModel size\nDepth(L + 1) Hidden(H) #Param(M) FLOPs(G/s)\nOnline CleanMel-S\n16\n96\n2.7\n18.1\nOffline CleanMel-S\n8\n96\n2.5\n32.9\nCleanMel-L\n16\n144\n7.2\n127.8\n2) Speech enhancement evaluation datasets: The speech\nenhancement performance of the proposed model is evaluated\non five datasets. For Chinese, we use the public test set\n(static speaker) of the RealMAN dataset [44]. For English,\nthe evaluation is conducted on 4 different datasets: (1) the\nCHiME4 challenge ‘isolated1chtrack’ test set [46], (2) the one-\nchannel test set of REVERB [41], (3) the test set of DNS I\nchallenge [33], (4) the public EARS blind test dataset [31].\n3) ASR evaluation datasets: The ASR performance of the\nproposed model is evaluated on three datasets. For Chinese,\nRealMAN (static) [44] is evaluated, with the pre-trained\nWenetSpeech ASR model provided in ESPNet 2. For English,\nCHiME4 [46] and REVERB [41] are evaluated, with their\nrespective pre-trained ASR models obtained using ESPNet.\nNote that, the ASR models used in this work have the same\nSTFT and mel-frequency configurations, thence one CleanMel\nmodel can be used for all of them.\nB. Configurations\n1) Data configurations: The sampling rate of all data is\nset to 16 kHz. STFT is applied using Hanning window with\na length of 512 samples (32 ms) and a hop size of 128\nand 256 samples (8 and 16 ms) for the offline and online\nmodels, respectively. The offline model has a finer temporal\nresolution than the online model since it is used for ASR\nin this work and its temporal resolution is aligned with the\nASR models. However, we empirically found that, compared\nto the 16-ms hope size, the 8-ms hope size does not benefit\nmuch for the speech enhancement performance. The number\n2https://github.com/espnet/espnet/tree/master/egs2\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\n7\nof Mel frequencies is set to FMel = 80 for the frequency\nrange of 0-8 kHz. The same STFT implementation (ESPNet\nimplementation3) is used for the CleanMel networks, neural\nvocoders and ASR models, to avoid configuration mismatch.\nThe natural logarithm (base of e) is used.\n2) Network configurations: Follow [21], the kernel size of\nthe T-Conv1d in the input module and the F-GConv1d layers\nin the cross-band block are both set to 5. As shown in Fig. 2,\nin narrow-band block, forward-only and forward/backward\nMamba layers are set for online and offline processing,\nrespectively. We set up two model scales for the offline\nmodels, referred to as CleanMel-S and CleanMel-L. And the\nonline model scale is set approximately to CleanMel-S. The\nconfigurations are shown in Table IV-B. The depth L of online\nmodels are set to twice the one of corresponding offline models\nto have the similar model size. Due to the different setups\nof STFT hop size, the computational complexity, i.e. FLOPs,\nof offline models are roughly twice the one of corresponding\nonline models.\n3) Training and inference setups: For CleanMel, AdamW\noptimizer [47] with an initial learning rate of 10−3 is used\nfor training. The learning rate exponentially decays with\nlr ←0.001 × 0.99epoch. Gradient clipping is applied with a\ngradient norm threshold 10. The batch size are set to 32.\nTraining samples are synthesized in an on-the-fly manner, and\n100,000 samples are considered as one training epoch. The\nCleanMel-S and CleanMel-L models are trained by 100 and\n150 epochs, respectively. Afterward, we average the model\nweights of the last 10 epochs as the final model for inference.\nFor training the Vocos neural vocoder [26], we synthesized\n400,000 (direct-path) clean speech samples with both English\nand Chinese data used for CleanMel training. The training\nconfigurations keep unchanged as in its original work. For\nASR evaluation, as already mentioned, the pre-trained ASR\nmodels obtained using in ESPNet are used.\nC. Evaluation metrics\nSpeech enhancement performance is evaluated with Percep-\ntual Evaluation of Speech Quality (PESQ) [48], DNSMOS\nP.808 [49] and P.835 [34], where the background, signal and\noverall scores for P.835 are all reported. Word Error Rate\n(WER) and Character Error Rate (CER) are used to evaluate\nEnglish and Chinese ASR performances, respectively.\nD. Comparison models\nWe compare with five advanced speech enhancement mod-\nels, which were all claimed in their original papers to be\nable to conduct joint speech denoising and dereverberation,\nincluding (i) FullSubNet [15] is a LSTM-based full-band and\nsub-band fusion network originally proposed for online speech\ndenoising, and extended to speech dereverberation in [16].\nFor offline processing, we change the uni-directional LSTMs\nto be bi-directional. (ii) Demucs [50] is an online speech\nenhancement model that operates directly on waveforms with\na U-net. (iii) VoiceFixer [19] also performs Mel-spectrogram\n3https://github.com/espnet/espnet/blob/master/espnet2/layers/stft.py\nenhancement (using a ResUNet) and generates the waveform\nusing a neural vocoder, but is developed only for offline speech\nenhancement. (iv) StoRM [51] is a powerful diffusion-based\noffline speech enhancement system. (v) SpatialNet [21] and\noSpatialNet [22] perform speech enhancement in the STFT\nlinear-frequency domain, and offer the backbone network for\nthe proposed CleanMel model. In SpatialNet and oSpatialNet,\nself-attention (and temporal convolution) and Mamba are\nadopted for learning the narrow-band spatial information in\nan offline and an online manner, respectively. For offline\nprocessing, besides comparing with the original SpatialNet,\nwe also implemented a variant of it with bi-directional Mamba\nfor narrow-band blocks (same architecture with the proposed\nmethod), which is referred to as SpatialNet-Mamba, and\nserves as the linear-frequency baseline for the proposed Mel-\nfrequency model. Note that, SpatialNet and oSpatialNet were\noriginally proposed for multi-channel speech enhancement,\nand this work is the first one to fully evaluate and analyze the\ncapability of them for single-channel speech enhancement.\nTo conduct fair comparisons, we re-train the FullSubNet,\nStoRM and oSpatialNet/SpatialNet models using the same\ntraining data utilized for the proposed model. However, we\nfound that it is not easy to re-train the Demucs and Voicefixer\nmodels. In [50], different data augmentation strategies are\napplied when training Demucs on different datasets. Voice-\nfixer [19] was designed for 44.1 kHz signals. Re-training\nDemucs and VoiceFixer request careful data engineering or\nhyperparameter search. Therefore, we use the pre-trained\ncheckpoints of Demucs and VoiceFixer to perform speech\nenhancement on only English data. For Demucs, we use the\n‘dns64’ model provided by the authors 4. For VoiceFixer, we\nuse the default pre-trained model provided in the open-source\npackage 5. Following the VoiceFixer default inference setup,\ntest waveforms are first up-sampled to 44.1 kHz to perform\nspeech enhancement and then down-sampled back to 16 kHz\nfor evaluation.\nV. RESULTS AND ANALYSES\nIn this section, we present and analyze the speech enhance-\nment performance and ASR results, and compare the model\nsize and computational complexity as well.\nA. Speech enhancement results\nTable IV and V show the DNSMOS and PESQ scores,\nrespectively. We analyze the results in the following aspects:\n1) Comparing the training targets of logMel mapping and\nMel ratio mask: For online processing, mapping consistently\noutperforms mask in DNSMOS, mainly due to the higher\nresidual noise of mask, which can be reflected by the much\nlower BAK scores of mask on the very noisy CHiME4 and\nRealMAN test sets. When applying the predicted ratio mask\non the noisy spectrogram, if speech is highly contaminated\nby noise, there will exist certain residual noise even if the\npredicted error of mask is small. By contrast, mapping directly\n4https://github.com/facebookresearch/denoiser\n5https://github.com/haoheliu/voicefixer\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\n8\nTABLE IV\nDNSMOS SCORES ON REAL TEST SETS.\nMode\nEnhancement\nmethod\nCHiME4 real\nEARS\nREVERB real\nDNS w. reverb.\nRealMAN static\nP.835\nP.808\nP.835\nP.808\nP.835\nP.808\nP.835\nP.808\nP.835\nP.808\nOVL SIG BAK\nOVL SIG BAK\nOVL SIG BAK\nOVL SIG BAK\nOVL SIG BAK\nunproc.\n1.37 1.99 1.34 2.43 2.02 2.94 2.04 2.80 1.39 1.74 1.53 2.81 1.42 1.89 1.50 2.74 1.53 2.03 1.75 2.56\nOnline\nFullSubNet [15]\n2.41 3.05 3.08 3.22 2.73 3.12 3.55 3.52 2.72 3.13 3.60 3.61 2.41 2.98 3.15 3.36 2.49 2.97 3.33 3.12\nDemucs [50]\n2.84 3.12 3.90 3.26 2.94 3.21 3.94 3.38 2.96 3.25 3.94 3.50 2.50 2.82 3.66 3.17\n-\noSpatialNet [22]\n2.61 3.15 3.36 3.52 2.85 3.22 3.65 3.65 3.03 3.33 3.93 3.84 2.79 3.16 3.70 3.67 2.79 3.19 3.66 3.46\nCleanMel-S-map (ours) 2.95 3.26 3.86 3.77 2.89 3.24 3.69 3.72 3.17 3.45 4.02 3.94 3.08 3.36 3.97 3.84 2.90 3.24 3.81 3.52\nCleanMel-S-mask (ours) 2.74 3.26 3.43 3.59 2.83 3.19 3.64 3.70 3.07 3.40 3.91 3.80 2.95 3.30 3.81 3.75 2.73 3.18 3.53 3.36\nOffline\nFullSubNet [15]\n2.39 2.93 3.18 3.31 2.47 3.10 3.11 3.27 2.71 3.11 3.62 3.62 2.74 3.16 3.51 3.54 2.47 2.97 3.27 3.13\nVoiceFixer [19]\n2.95 3.25 3.89 3.65 2.94 3.25 3.83 3.55 2.88 3.26 3.61 3.62 3.11 3.40 3.97 3.84\n-\nStoRM [51]\n3.29 3.57 4.03 3.87 2.97 3.37 3.65 3.79 3.25 3.53 4.01 4.01 3.25 3.53 4.03 3.87 3.04 3.42 3.80 3.68\nSpatialNet [21]\n2.76 3.32 3.39 3.51 2.86 3.27 3.58 3.59 3.07 3.41 3.87 3.84 2.91 3.33 3.68 3.65 2.88 3.32 3.66 3.45\nSpatialNet-Mamba\n2.94 3.35 3.71 3.65 2.94 3.31 3.69 3.67 3.09 3.43 3.85 3.86 2.89 3.35 3.58 3.68 2.95 3.37 3.69 3.50\nCleanMel-S-map (ours) 3.33 3.58 4.11 3.96 3.02 3.36 3.73 3.79 3.39 3.62 4.13 4.08 3.33 3.58 4.10 4.03 3.25 3.55 4.03 3.82\nCleanMel-S-mask (ours) 3.26 3.56 4.01 3.81 2.97 3.34 3.68 3.71 3.31 3.58 4.08 4.01 3.25 3.53 4.05 3.97 3.15 3.49 3.93 3.71\nCleanMel-L-mask (ours) 3.30 3.59 4.04 3.87 2.99 3.35 3.70 3.74 3.31 3.58 4.08 4.02 3.25 3.53 4.05 4.00 3.15 3.49 3.93 3.71\nTABLE V\nPESQ SCORES ON TEST SETS WITH CLEAN REFERENCE SPEECH.\nMode\nEnhancement\nmethod\nCHiME4\nsimu.\nDNS\nw.o. reverb.\nREVERB\nsimu.\nRealMAN\nstatic\nunproc.\n1.20\n1.58\n1.50\n1.14\nOnline\nFullSubNet [15]\n1.80\n2.71\n2.39\n1.36\nDemucs [50]\n1.64\n2.63\n1.92\n-\noSpatialNet [22]\n1.81\n2.77\n2.56\n1.87\nCleanMel-S-map (ours)\n1.77\n2.73\n2.63\n1.79\nCleanMel-S-mask (ours)\n1.99\n2.82\n2.63\n1.75\nOffline\nFullSubNet [15]\n1.87\n2.82\n2.48\n1.61\nVoiceFixer [19]\n1.57\n1.92\n1.67\n-\nStoRM [51]\n1.76\n2.74\n2.52\n1.71\nSpatialNet [21]\n1.90\n2.82\n2.87\n1.93\nSpatialNet-Mamba\n1.90\n2.90\n3.06\n2.10\nCleanMel-S-map (ours)\n2.09\n2.95\n2.92\n2.00\nCleanMel-S-mask (ours)\n2.17\n2.97\n2.85\n1.95\nCleanMel-L-mask (ours)\n2.35\n3.07\n2.97\n2.01\npredicts the logMel-spectrogram of speech, which can avoid\nsuch residual noise. However, as shown in Table V, mask\nachieves higher PESQ scores than mapping on the highly noisy\nCHiME4 and DNS (w.o. reverb) sets. PESQ measures the\nperceptual similarity of enhanced speech and reference clean\nspeech. The higher PESQ scores indicates that mask performs\nbetter on retrieving the target speech through extracting the\ntarget speech from the noisy speech with a mask. Directly\nmapping may erroneously remove or boost those speech com-\nponents highly contaminated by noise. The enhanced logMel-\nspectrograms are transformed to waveforms with a neural\nvocoder, and the speech quality measured with DNSMOS is\n0\n20\n40\n60\n80\nMel Frequency\n(a) noisy\n0\n20\n40\n60\n80\nMel Frequency\n(b) clean\n0\n20\n40\n60\n80\nMel Frequency\n(c) mask\n0.0\n1.0\n2.0\n3.0\n4.0\nTime (s)\n0\n20\n40\n60\n80\nMel Frequency\n(d) mapping\nln(1e-5)\n0\nln(1e-5)\n0\nln(1e-5)\n0\nln(1e-5)\n0\nFig. 4. An example of Mel spectrogram enhancement with the targets of Mel\nratio mask and logMel mapping. The orange box marks the removed part of\nspeech by mapping and the red box marks the residual noise by mask.\nmore affected by the residual noise caused by mask than the\nmapping error of logMel-spectrogram. Fig 4 shows an example\nof the enhanced logMel-spectrogram by mask and mapping,\nwhich verifies our discussions.\nFor offline processing, the differences between mask and\nmapping discussed above still present, but get much smaller,\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\n9\nfor example the difference of BAK scores on the CHiME4\nand RealMAN test sets become much smaller. Compared to\nthe online network, the offline network achieves much smaller\nprediction errors for both mask and mapping, thence their\ndifferences also become smaller as they both target the same\nclean speech.\n2) Comparing with oSpatialNet and SpatialNet-Mamba:\noSpatialNet and SpatialNet-Mamba adopt the same network\narchitectures and serve as the linear-frequency baselines for\nthe proposed CleanMel networks. For both online and offline\nprocessing, the proposed Mel-spectrogram enhancement net-\nworks noticeably outperform their linear-frequency baselines\nin DNSMOS, but not necessarily in PESQ. This phenomenon\nis consistent with the findings in the VoiceFixer paper [19].\nThe GAN-based nerual vocoder is good at generating high-\nquality speech, but will possibly reduce the (PESQ) similarity\nof the enhance speech with the reference speech.\n3) Comparing with online baseline networks: Demucs [50]\nperforms well for denoising but not for dereverberation, as\nit achieves leading performance in DNSMOS P.835 on the\n(low-reverberation) CHiME4 and EARS sets, but not for\nother reverberant test sets. Although it was developed for\nmulti-channel speech enhancement in [22], oSpatialNet also\nperforms very well for single-channel speech enhancement\ncompared with other comparison models. oSpatialNet and\nthe proposed CleanMel network work better especially for\ndereverberation due to their narrow-band processing of room\nfilter convolution.\n4) Comparing with offline baseline networks: VoiceFixer\n[19] is an important baseline for the proposed CleanMel\nnetwork, as they both perform Mel-spectrogram enhancement.\nVoiceFixer adopts an advanced ResUNet. We can see that the\nproposed network noticeably outperforms VoiceFixer, which\nverifies the strong capability of the proposed cross-band and\nnarrow-band combination network. StoRM [51] is a diffusion-\nmodel-based generative network, which often achieves the\nSOTA performance for speech enhancement. The proposed\nnetwork consistently outperforms StoRM. Their performance\ngaps of DNSMOS scores are not large, but the gaps of PESQ\nscores are very large. This indicates that although generative\nmodel can generate high-quality speech, but its fidelity to\nthe source speech is hard to be ensured. SpatialNet [21],\nespecially its Mamba variant, i.e. SpatialNet-Mamba, as a\npure discriminative speech enhancement network, achieves\npromising performance.\nOverall, by combining a powerful discriminative Mel-\nspectrogram enhancement network and a GAN-based genera-\ntive neural vocoder, the proposed network achieves new SOTA\nspeech enhancement performance for both online and offline\nprocessing, and for both denoising and dereverberation. By\nscaling up the proposed network, the performance can be fur-\nther improved. Taking the mask scheme as an example, com-\npared to the small model, the large model noticeably improves\nthe PESQ scores (although not the DNSMOS scores). The\nperformance improvement of large model is clearly audible\nwhen listening to the enhanced speech demos.\nTABLE VI\nASR RESULTS, WER (%) FOR THE ENGLISH CHIME4 AND REVERB\nSETS, AND CER (%) FOR THE CHINESE REALMAN SET.\nEnhancement\nmethod\nCHiME4\nREVERB simu. REVERB real RealMAN\nstatic\nsimu. real\nnear\nfar\nnear\nfar\nunprocessed\n15.3 13.1\n3.7\n4.7\n6.0\n6.3\n20.1\nclean\n3.1\n-\n3.4\n3.4\n-\n-\n7.7\nMulti-channel*\n13.0 10.8\n3.5\n3.7\n3.6\n4.4\n-\n1ch WPE [52]\n-\n-\n3.7\n4.6\n5.5\n5.8\n-\nFullSubNet [15]\n19.4 17.3\n4.1\n5.6\n6.1\n6.0\n22.0\nVoiceFixer† [19]\n24.3 26.4\n5.9\n8.8\n8.9\n10.1\n-\nStoRM [51]\n21.9 18.2\n4.2\n5.5\n7.3\n7.3\n28.4\nSpatialNet [21]\n20.5 16.9\n4.2\n5.2\n5.7\n5.1\n18.5\nSpatialNet-Mamba 17.6 13.7\n3.9\n4.0\n4.3\n4.9\n16.5\nCleanMel-S-map\n12.5\n9.8\n3.7\n3.9\n4.2\n4.5\n16.7\nCleanMel-S-mask\n11.9\n9.3\n3.6\n3.7\n3.9\n4.3\n16.5\nCleanMel-L-mask\n9.6\n7.7\n3.5\n3.7\n3.5\n3.6\n14.4\n* 5-channel Beamformit and 8-channel WPE [52] + Beamformit serve\nas the multi-channel baseline methods for CHiME4 and REVERB,\nrespectively.\n†VoiceFixer is evaluated in the same way as the proposed model, namely\nthe enhanced Mel-spectrogram is directly fed to ASR models. To fit the\nSTFT setups of VoiceFixer, the ASR models used for VoiceFixer were re-\ntrained, which are different from the ones used for other methods. Please\nrefer to Table VII for more information about the re-trained ASR models.\nB. ASR results\nTable VI presents the ASR results. We can observe that the\nASR results are consistent with the common finding in the\nfiled [53] that single-channel speech enhancement networks\nusually do not help for ASR due to the speech artifacts\ncaused by the networks. VoiceFixer performs Mel-spectrogram\nenhancement using an advanced ResUNet. The poor ASR\nresults of VoiceFixer show that ResUNet brings severe artifacts\nto the enhanced Mel-spectrogram. StoRM does not work well\nfor ASR, which further verifies that the generated speech has\nlow fidelity to the source speech.\nOne exception is SpatialNet, especially SpatialNet-Mamba,\nwho show clear positive ASR effects on the REVERB-\nreal and RealMAN sets. We believe that the advantages of\nSpatialNet lie in its narrow-band processing mechanism: i)\nnarrow-band processing is especially efficient for modeling\nthe much simpler narrow-band room filter convolution; ii) as\ndiscussed in [14], narrow-band processing can avoid the so-\ncalled wide-band artifacts, such as the blurred and/or wrongly\ndeleted/inserted wide-band spectra, which will be very harmful\nfor ASR.\nThe proposed CleanMel networks further improve the ASR\nperformance on top of SpatialNet-Mamba. Directly enhancing\nthe Mel-frequency spectra is more optimal for ASR than\nfirst enhancing the detailed linear-frequency spectra and then\ncompressing to Mel-frequency. Compared to linear-frequency\nprocessing, the lower-dimensional Mel-frequency representa-\ntion is easier to learn. However, this is more valid for the cross-\nband processing part, but not for the narrow-band (frequency-\nwise or dimension-wise) processing part. That is possibly why\nthe proposed CleanMel networks are more advantageous on\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\n10\nTABLE VII\nASR RESULT COMPARISONS BETWEEN TAKING AS INPUT ENHANCED\nMEL-SPECTROGRAM AND VOCODER WAVEFORM. WER (%) FOR THE\nENGLISH CHIME4 AND REVERB SETS, AND CER (%) FOR THE\nCHINESE REALMAN SET.\nEnhancement\nmethod\nCHiME4\nREVERB simu. REVERB real RealMAN\nstatic\nsimu. real\nnear\nfar\nnear\nfar\nunprocessed\n15.6 13.5\n4.3\n5.6\n6.8\n7.4\n-\nVoiceFixer\n24.3 26.4\n5.9\n8.8\n8.9\n10.1\n-\nwaveform\n27.6 30.6\n8.1\n11.9\n10.9\n11.5\n-\nunprocessed\n15.3 13.1\n3.7\n4.7\n6.0\n6.3\n20.1\nCleanMel-S-mask 11.9\n9.3\n3.6\n3.7\n3.9\n4.3\n16.5\nwaveform\n13.4 10.1\n3.7\n4.2\n4.0\n4.4\n17.9\nCleanMel-L-mask\n9.6\n7.7\n3.5\n3.7\n3.5\n3.6\n14.4\nwaveform\n10.2\n8.4\n3.8\n3.9\n3.4\n3.7\n15.7\nthe denoising task of CHiME4 (more relying on cross-band\nprocessing) than the dereverberation task of REVERB (more\nrelying on narrow-band processing). For ASR, Mel ratio mask\nperforms better than logMel mapping, but the performance\ngaps are not large. This indicates that ASR is less affected by\nthe larger residual noise caused by mask than the prediction\nerror of logMel mapping. Scaling up is very effective for\nfurther improving the ASR performance, as shown by the\nresults of the large mask model.\nOverall, by adopting an advanced backbone network (i.e.\nSpatialNet-Mamba), developing an effective Mel-spectrogram\nenhancement framework, and scaling up the model size, the\nproposed CleanMel-L-mask network achieves substantial ASR\nimprovements relative to unprocessed speech.\nC. Influence of neural vocoder to ASR performance\nWe use a GAN-based neural vocoder to transform the\nenhanced Mel-spectrogram to waveform, which may modify\nthe original Mel-spectrogram and reduce the fidelity, although\na Mel-spectrogram loss is adopted in the vocoder to maintain\nthe Mel-spectrogram fidelity. Table VII compares the ASR\nresults between taking as input the enhanced Mel-spectrogram\nand the vocoder waveform. Note that the ASR models used\nfor VoiceFixer is re-trained accroding to its STFT and Mel-\nfrequency setups. We can see that the neural vocoders in-\ndeed reduce the Mel-spectrogram fidelity and thus the ASR\nperformance. However, the performance reductions are not\nsignificant, and the ASR results of vocoder waveform for\neach model closely insist on the results of its corresponding\nenhanced Mel-spectrogram. This means the vocoder waveform\ncould be also an alternative/suboptimal choice for ASR. If it is\nnot easy to re-train a CleanMel model according to the setups\nof a new ASR model, since vocoder waveform is irrelevant to\nthose setups, our pre-trained checkpoints can be used.\nD. Model size and computational complexity\nTable VIII shows the model size and computational com-\nplexity of the proposed model and comparison models. The\nTABLE VIII\nMODEL SIZE AND COMPUTATIONAL COMPLEXITY OF DIFFERENT MODELS.\nFOR CLEANMEL AND VOICEFIXER, THE #PARAM AND FLOPS OF\nMEL-SPECTROGRAM ENHANCEMENT NETWORK AND VOCODER ARE\nRESPECTIVELY GIVEN IN ADDITION, PRESENTING THE COMPUTATIONAL\nCOST FOR ASR (ONLY THE FORMER TERM) AND SPEECH ENHANCEMENT\n(THE ADDITION).\nMode\nModel\n#Param(M)\nFLOPs(G/s)\nOnline\nFullSubNet [15]\n5.6\n59.6\nDemucs [50]\n33.5\n15.4\noSpatialNet [22]\n1.7\n36.8\nCleanMel-S (ours)\n2.7 + 13.2\n18.1 + 1.7\nOffline\nFullSubNet [15]\n14.6\n157.9\nVoiceFixer [19]\n88.3 + 33.8\n20.9 + 103.0\nStoRM [51]\n55.1\n4600\nSpatialNet [21]\n1.6\n46.3\nSpatialNet-Mamba\n1.7\n34.9\nCleanMel-S (ours)\n2.5 + 13.2\n32.9 + 3.3\nCleanMel-L (ours)\n7.2 + 13.2\n127.8 + 3.3\nproposed model is composed of a Mel-spectrogram enhance-\nment network and an optional neural vocoder. The Mel-\nspectrogram enhancement network has a small model size\nand a large computational complexity, mainly due to the\nindependent computation of frequencies with shared narrow-\nband blocks. Compared to linear-frequency sub-band/narrow-\nband processing networks, including FullSubNet, oSpatialNet,\nSpatialNet(-Mamba), the proposed Mel-spectrogram enhance-\nment network has a smaller computational complexity, due\nto the much less frequencies to be processed, i.e. 80 Mel\nfrequencies versus 256 linear frequencies. As a diffusion\nmodel, the computation of StoRM is much more expensive\nthan other models.\nVI. CONCLUSION\nThis work proposed a single-channel Mel-spectrogram de-\nnoising and dereverberation network, named CleanMel. The\nlearning targets of logMel mapping and ratio mask have\nbeen compared, while the former suffers from less residual\nnoise and the latter preserves better the target speech. The\nadopted network architecture, i.e. interleaved cross-band and\nnarrow-band blocks, has been proven working well for single-\nchannel speech denoising and dereverberation in both the\nlinear-frequency domain and the proposed Mel-frequency do-\nmain. The high-quality enhanced Mel-spectrogram can be well\ntransformed to waveform with a neural vocoder and can also\nbe used for boosting the ASR performance. Mel-spectrogram\nplays a key role in the field of speech processing, so the\nproposed CleanMel model can be potentially used for many\nother tasks, such as self-supervised speech pre-training and\nhigh-quality speech synthesis.\nREFERENCES\n[1] Y. Luo and N. Mesgarani, “Conv-tasnet: Surpassing ideal time–\nfrequency magnitude masking for speech separation,” IEEE/ACM Trans-\nactions on Audio, Speech, and Language processing, vol. 27, no. 8, pp.\n1256–1266, 2019.\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\n11\n[2] A. D´efossez, G. Synnaeve, and Y. Adi, “Real time speech enhancement\nin the waveform domain,” in Interspeech 2020, 2020, pp. 3291–3295.\n[3] F. Xiong, W. Chen, P. Wang, X. Li, and J. Feng, “Spectro-temporal\nsubnet for real-time monaural speech denoising and dereverberation,” in\nInterspeech 2022, 2022, pp. 931–935.\n[4] Y. Hu, Y. Liu, S. Lv, M. Xing, S. Zhang, Y. Fu, J. Wu, B. Zhang, and\nL. Xie, “Dccrn: Deep complex convolution recurrent network for phase-\naware speech enhancement,” in Interspeech 2020, 2020, pp. 2472–2476.\n[5] A. Li, C. Zheng, L. Zhang, and X. Li, “Glance and gaze: A collaborative\nlearning framework for single-channel speech enhancement,” Applied\nAcoustics, vol. 187, p. 108499, 2022.\n[6] K. Iwamoto, T. Ochiai, M. Delcroix, R. Ikeshita, H. Sato, S. Araki, and\nS. Katagiri, “How bad are artifacts?: Analyzing the impact of speech\nenhancement errors on asr,” in Interspeech 2022, 2022, pp. 5418–5422.\n[7] K. Kinoshita, T. Ochiai, M. Delcroix, and T. Nakatani, “Improving noise\nrobust automatic speech recognition with single-channel time-domain\nenhancement network,” in ICASSP 2020 - 2020 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), 2020,\npp. 7009–7013.\n[8] Z. Nian, J. Du, Y. Ting Yeung, and R. Wang, “A time domain progressive\nlearning approach with snr constriction for single-channel speech en-\nhancement and recognition,” in ICASSP 2022 - 2022 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), 2022,\npp. 6277–6281.\n[9] Y. Yang, A. Pandey, and D. Wang, “Time-domain speech enhancement\nfor robust automatic speech recognition,” in Interspeech 2023, 2023, pp.\n4913–4917.\n[10] ——, “Towards decoupling frontend enhancement and backend recog-\nnition in monaural robust asr,” arXiv preprint arXiv:2403.06387, 2024.\n[11] J.-M. Valin, “A hybrid dsp/deep learning approach to real-time full-\nband speech enhancement,” in 2018 IEEE 20th International Workshop\non Multimedia Signal Processing (MMSP), 2018, pp. 1–5.\n[12] J.-M. Valin, U. Isik, N. Phansalkar, R. Giri, K. Helwani, and A. Kr-\nishnaswamy, “A perceptually-motivated approach for low-complexity,\nreal-time enhancement of fullband speech,” in Interspeech 2020, 2020,\npp. 2482–2486.\n[13] H. Schr¨oter, A. N. Escalante-B., T. Rosenkranz, and A. Maier, “Deep-\nfilternet: Perceptually motivated real-time speech enhancement,” in In-\nterspeech 2023, 2023, pp. 2008–2009.\n[14] X. Li and R. Horaud, “Narrow-band deep filtering for multichannel\nspeech enhancement,” arXiv preprint arXiv:1911.10791, 2019.\n[15] X. Hao, X. Su, R. Horaud, and X. Li, “Fullsubnet: A full-band and sub-\nband fusion model for real-time single-channel speech enhancement,”\nin ICASSP 2021 - 2021 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), 2021, pp. 6633–6637.\n[16] R. Zhou, W. Zhu, and X. Li, “Speech dereverberation with a reverbera-\ntion time shortening target,” in ICASSP 2023 - 2023 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), 2023,\npp. 1–5.\n[17] X. Hao and X. Li, “Fast fullsubnet: Accelerate full-band and sub-band\nfusion model for single-channel speech enhancement,” arXiv preprint\narXiv:2212.09019, 2022.\n[18] V. Kothapally, Y. Xu, M. Yu, S.-X. Zhang, and D. Yu, “Deep neural mel-\nsubband beamformer for in-car speech separation,” in ICASSP 2023 -\n2023 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), 2023, pp. 1–5.\n[19] H. Liu, X. Liu, Q. Kong, Q. Tian, Y. Zhao, D. Wang, C. Huang, and\nY. Wang, “Voicefixer: A unified framework for high-fidelity speech\nrestoration,” in Interspeech 2022, 2022, pp. 4232–4236.\n[20] Y. Tian, W. Liu, and T. Lee, “Diffusion-based mel-spectrogram enhance-\nment for personalized speech synthesis with found data,” in 2023 IEEE\nAutomatic Speech Recognition and Understanding Workshop (ASRU),\n2023, pp. 1–7.\n[21] C. Quan and X. Li, “Spatialnet: Extensively learning spatial information\nfor multichannel joint speech separation, denoising and dereverberation,”\nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\nvol. 32, pp. 1310–1323, 2024.\n[22] ——, “Multichannel long-term streaming neural speech enhancement for\nstatic and moving speakers,” IEEE Signal Processing Letters, vol. 31,\npp. 2295–2299, 2024.\n[23] X. Li, L. Girin, S. Gannot, and R. Horaud, “Multichannel speech\nseparation and enhancement using the convolutive transfer function,”\nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\nvol. 27, no. 3, pp. 645–659, 2019.\n[24] D. Wang and J. Chen, “Supervised speech separation based on deep\nlearning: An overview,” IEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing, vol. 26, no. 10, pp. 1702–1726, 2018.\n[25] A. Gu and T. Dao, “Mamba: Linear-time sequence modeling with\nselective state spaces,” in First Conference on Language Modeling, 2024.\n[26] H. Siuzdak, “Vocos: Closing the gap between time-domain and fourier-\nbased neural vocoders for high-quality audio synthesis,” in The Twelfth\nInternational Conference on Learning Representations, 2024.\n[27] J. Kong, J. Kim, and J. Bae, “Hifi-gan: generative adversarial networks\nfor efficient and high fidelity speech synthesis,” in Proceedings of\nthe 34th International Conference on Neural Information Processing\nSystems, 2020.\n[28] H. Bu, J. Du, X. Na, B. Wu, and H. Zheng, “Aishell-1: An open-source\nmandarin speech corpus and a speech recognition baseline,” in 2017\n20th conference of the oriental chapter of the international coordinating\ncommittee on speech databases and speech I/O systems and assessment\n(O-COCOSDA).\nIEEE, 2017, pp. 1–5.\n[29] J. Du, X. Na, X. Liu, and H. Bu, “AISHELL-2: Transforming Mandarin\nASR Research Into Industrial Scale,” ArXiv, Aug. 2018.\n[30] D. Wang and X. Zhang, “Thchs-30: A free chinese speech corpus,” arXiv\npreprint arXiv:1512.01882, 2015.\n[31] J. Richter, Y.-C. Wu, S. Krenn, S. Welker, B. Lay, S. Watanabe,\nA. Richard, and T. Gerkmann, “Ears: An anechoic fullband speech\ndataset benchmarked for speech enhancement and dereverberation,” in\nInterspeech 2024, 2024, pp. 4873–4877.\n[32] J. Yamagishi, C. Veaux, and K. MacDonald, “Cstr vctk corpus: English\nmulti-speaker corpus for cstr voice cloning toolkit (version 0.92),”\n2019.\n[Online].\nAvailable:\nhttps://api.semanticscholar.org/CorpusID:\n213060286\n[33] C. K. Reddy, V. Gopal, R. Cutler, E. Beyrami, R. Cheng, H. Dubey,\nS. Matusevych, R. Aichner, A. Aazami, S. Braun, P. Rana, S. Srinivasan,\nand J. Gehrke, “The interspeech 2020 deep noise suppression challenge:\nDatasets, subjective testing framework, and challenge results,” in Inter-\nspeech 2020, 2020, pp. 2492–2496.\n[34] C. K. Reddy, V. Gopal, and R. Cutler, “Dnsmos p. 835: A non-intrusive\nperceptual objective speech quality metric to evaluate noise suppressors,”\nin ICASSP 2022-2022 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP).\nIEEE, 2022, pp. 886–890.\n[35] J. Eaton, N. D. Gaubitch, A. H. Moore, and P. A. Naylor, “Estimation of\nroom acoustic parameters: The ace challenge,” IEEE/ACM Transactions\non Audio, Speech, and Language Processing, vol. 24, no. 10, pp. 1681–\n1693, 2016.\n[36] M. Jeub, M. Schafer, and P. Vary, “A binaural room impulse response\ndatabase for the evaluation of dereverberation algorithms,” in 2009 16th\nInternational Conference on Digital Signal Processing, 2009, pp. 1–5.\n[37] K. Prawda, S. J. Schlecht, and V. V¨alim¨aki, “Calibrating the sabine and\neyring formulas,” The Journal of the Acoustical Society of America, vol.\n152, no. 2, pp. 1158–1169, 2022.\n[38] D. D. Carlo, P. Tandeitnik, C. Foy, N. Bertin, A. Deleforge, and\nS. Gannot, “dechorate: a calibrated room impulse response dataset for\necho-aware signal processing,” EURASIP Journal on Audio, Speech, and\nMusic Processing, vol. 2021, pp. 1–15, 2021.\n[39] E. Hadad, F. Heese, P. Vary, and S. Gannot, “Multichannel audio\ndatabase in various acoustic environments,” in 2014 14th International\nWorkshop on Acoustic Signal Enhancement (IWAENC), 2014, pp. 313–\n317.\n[40] J. Traer and J. H. McDermott, “Statistics of natural reverberation enable\nperceptual separation of sound and space,” Proceedings of the National\nAcademy of Sciences, vol. 113, no. 48, pp. E7856–E7865, 2016.\n[41] K. Kinoshita, M. Delcroix, T. Yoshioka, T. Nakatani, E. Habets, R. Haeb-\nUmbach, V. Leutnant, A. Sehr, W. Kellermann, R. Maas, S. Gannot, and\nB. Raj, “The reverb challenge: A common evaluation framework for\ndereverberation and recognition of reverberant speech,” in 2013 IEEE\nWorkshop on Applications of Signal Processing to Audio and Acoustics,\n2013, pp. 1–4.\n[42] S. Nakamura, “Acoustic sound database collected for hands-free speech\nrecognition and sound scene understanding,” in International Workshop\non Hands-Free Speech Communication, 2001, pp. 43–46.\n[43] C. K. Reddy, H. Dubey, K. Koishida, A. Nair, V. Gopal, R. Cutler,\nS. Braun, H. Gamper, R. Aichner, and S. Srinivasan, “Interspeech 2021\ndeep noise suppression challenge,” in Interspeech 2021, 2021, pp. 2796–\n2800.\n[44] B. Yang, C. Quan, Y. Wang, P. Wang, Y. Yang, Y. Fang, N. Shao,\nH. Bu, X. Xu, and X. Li, “RealMAN: A real-recorded and annotated\nmicrophone array dataset for dynamic speech enhancement and localiza-\ntion,” in The Thirty-eight Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track, 2024.\n[45] J. F. Gemmeke, D. P. W. Ellis, D. Freedman, A. Jansen, W. Lawrence,\nR. C. Moore, M. Plakal, and M. Ritter, “Audio set: An ontology and\nhuman-labeled dataset for audio events,” in ICASSP 2017 - 2017 IEEE\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\n12\nInternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP), 2017, pp. 776–780.\n[46] E. Vincent, S. Watanabe, J. Barker, and R. Marxer, “The 4th chime\nspeech separation and recognition challenge,” URL: http://spandh. dcs.\nshef. ac. uk/chime challenge/(last accessed on 1 August, 2018), 2016.\n[47] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”\nin International Conference on Learning Representations, 2019.\n[48] A. W. Rix, J. G. Beerends, M. P. Hollier, and A. P. Hekstra, “Perceptual\nevaluation of speech quality (pesq)-a new method for speech quality\nassessment of telephone networks and codecs,” in ICASSP 2001 -\n2001 IEEE international conference on acoustics, speech, and signal\nprocessing (ICASSP), vol. 2.\nIEEE, 2001, pp. 749–752.\n[49] C. K. Reddy, V. Gopal, and R. Cutler, “Dnsmos: A non-intrusive\nperceptual objective speech quality metric to evaluate noise suppressors,”\nin ICASSP 2021-2021 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP).\nIEEE, 2021, pp. 6493–6497.\n[50] A. D´efossez, G. Synnaeve, and Y. Adi, “Real time speech enhancement\nin the waveform domain,” in Interspeech 2020, 2020, pp. 3291–3295.\n[51] J.-M. Lemercier, J. Richter, S. Welker, and T. Gerkmann, “Storm: A\ndiffusion-based stochastic regeneration model for speech enhancement\nand dereverberation,” IEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing, vol. 31, pp. 2724–2737, 2023.\n[52] T. Nakatani, T. Yoshioka, K. Kinoshita, M. Miyoshi, and B.-H. Juang,\n“Speech dereverberation based on variance-normalized delayed linear\nprediction,” IEEE Transactions on Audio, Speech, and Language Pro-\ncessing, vol. 18, no. 7, pp. 1717–1731, 2010.\n[53] K. Iwamoto, T. Ochiai, M. Delcroix, R. Ikeshita, H. Sato, S. Araki, and\nS. Katagiri, “How bad are artifacts?: Analyzing the impact of speech\nenhancement errors on asr,” in Interspeech 2022, 2022, pp. 5418–5422.\n\n\n"}
{"text": "Variations in Relevance Judgments\nand the Shelf Life of Test Collections\nAndrew Parry\nUniversity of Glasgow\nMaik Fröbe\nFriedrich-Schiller-\nUniversität Jena\nHarrisen Scells\nUniversity of Kassel\nFerdinand Schlatt\nFriedrich-Schiller-\nUniversität Jena\nGuglielmo Faggioli\nUniversity of Padua\nSaber Zerhoudi\nUniversität Passau\nSean MacAvaney\nUniversity of Glasgow\nEugene Yang\nJohns Hopkins University\nAbstract\nThe fundamental property of Cranfield-style evaluations, that sys-\ntem rankings are stable even when assessors disagree on individual\nrelevance decisions, was validated on traditional test collections.\nHowever, the paradigm shift towards neural retrieval models af-\nfected the characteristics of modern test collections, e.g., documents\nare short, judged with four grades of relevance, and information\nneeds have no descriptions or narratives. Under these changes, it is\nunclear whether assessor disagreement remains negligible for sys-\ntem comparisons. We investigate this aspect under the additional\ncondition that the few modern test collections are heavily re-used.\nGiven more possible query interpretations due to less formalized\ninformation needs, an “expiration date” for test collections might be\nneeded if top-effectiveness requires overfitting to a single interpre-\ntation of relevance. We run a reproducibility study and re-annotate\nthe relevance judgments of the 2019 TREC Deep Learning track.\nWe can reproduce prior work in the neural retrieval setting, show-\ning that assessor disagreement does not affect system rankings.\nHowever, we observe that some models substantially degrade with\nour new relevance judgments, and some have already reached the\neffectiveness of humans as rankers, providing evidence that test\ncollections can expire.\nCCS Concepts\n• Information systems →Evaluation of retrieval results.\nKeywords\nEvaluation; Test collections; Collection Reliability\nACM Reference Format:\nAndrew Parry, Maik Fröbe, Harrisen Scells, Ferdinand Schlatt, Guglielmo\nFaggioli, Saber Zerhoudi, Sean MacAvaney, and Eugene Yang. 2025. Varia-\ntions in Relevance Judgments and the Shelf Life of Test Collections. In Pro-\nceedings of the 48th International ACM SIGIR Conference on Research and De-\nvelopment in Information Retrieval (SIGIR ’25), July 13–18, 2025, Padua, Italy.\nACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/XXXXXXX.\nXXXXXXX\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nSIGIR ’25, Padua, Italy\n© 2025 ACM.\nACM ISBN 978-1-4503-XXXX-X/XX/XX\nhttps://doi.org/10.1145/XXXXXXX.XXXXXXX\nTable 1: Overview of annotator agreement studies that we\nreproduce in the TREC Deep Learning scenario. We contrast\ninformation needs (length of title, description, narrative),\ndocuments (domain and length), evaluations (measure and\nrelevance grades) and observed results of each study (we show\nthe agreement as overlap and the system correlations as 𝜏).\nStudy\nInformation Need\nDocs.\nEvaluation\nResults\nTitle Desc.\nNarr.\nDom.\nLen.\nMeas.\nRel.\nAgr. Corr.\nLesk, Salton [23]\n×\n100.0\n×\nMedical 103.7\nPrec.\n0–1\n0.30\n×\nBurgin [4]\n×\n30.4\n×\nIR\n15.7\nMAP\n0–1\n0.49\n×\nVoorhees [46]\n2.6\n20.4\n64.7\nNews\n550.5\nMAP\n0–1\n0.36 0.890\nOur Reproduction 8.3\n×\n×\nQA\n30.4\nnDCG\n0–3\n0.15 0.897\n1\nIntroduction\nTREC-style test collections enable the evaluation and, thereby, the\ndevelopment of retrieval systems. Making these collections robust\nand reusable is not trivial, partly because of the inherent subjec-\ntivity of relevance [45]. Assessing the reliability of relevance judg-\nments involves assessing inter-annotator agreement and is com-\nparably expensive [4, 23, 46]. Consequently, only a few studies\nhave investigated how the variability of relevance judgments af-\nfects the evaluation of retrieval systems. However, those findings\nwere highly important for the design of retrieval test collections as\nthey showed that even when relevance is subjective and therefore\nannotators disagree [38, 42], subsequent aggregated effectiveness\nevaluations [4, 23] and system rankings [46] are stable, ensuring\nthe reusability of test collections.\nContrasting older collections, topics in modern test collections\noften do not have descriptions or narratives, which is known to\naffect agreement [33], documents are often shorter, and relevance\nis assessed on a graded scale, which has not been investigated in\nre-annotation studies. Table 1 contrasts previous studies and the\nproperties of the corresponding test collections with the TREC Deep\nLearning 2019 [8] (DL’19) test collection we use in our reproducibil-\nity work. The changes in modern collections potentially increase\nassessment variability because an assessor can freely interpret the\nquery intent and context that may be missing from the document to\nassess its relevance on a fine-grained scale. In this reproducibility\nstudy, we investigate if the robustness of classical test collections\nstill holds for modern test collections under these new conditions.\nThese shifts in test collection design coincide with advances\nin retrieval methodologies; retrieval pipelines increasingly com-\nprise components where neural language models estimate rele-\nvance. These systems have continuously improved over multiple\narXiv:2502.20937v1  [cs.IR]  28 Feb 2025\n\n\nSIGIR ’25, July 13–18, 2024, Padua, Italy\nA. Parry et al.\niterations through improved training processes [31], through dis-\ntillation from large language models [32, 40], or by directly using\nlarge language models for ranking [43]. We hypothesize that iter-\native model improvements on static benchmarks may overfit to\nthe specific opinions expressed in static relevance judgements, in-\ntroducing systemic bias, which can be characterized as “bias by\nresearch design” in which the influence of previous works can bias\nnew findings implicitly towards a common benchmark [18]. The\nTREC DL’19 track remains a commonly used standard for bench-\nmarking systems, being cited and most likely evaluated hundreds\nof times (Cited 32 times at ECIR’24 and SIGIR’24 alone). Craswell\net al. [9] alluded to this point, warning against the iterative design\nprocesses employed in ad-hoc neural ranking.\nThus, we look to replicate the classic setting for evaluating the\nrobustness of collections proposed by Voorhees [46] for modern\ncollections, with the additional hypothesis that bias in the system\norderings may be introduced through model iteration instead of\nhuman disagreement. Since a single expert human assessor’s rele-\nvance grades on a given topic are subjective and “equally plausible”\nas another assessor’s, we can compare the effectiveness of systems\nusing multiple “equally correct” sets of relevance judgments. If\na system’s relative effectiveness degrades when evaluated using\nalternative judgments, it may indicate that models overfit the pref-\nerences expressed by the collection’s official judgments.\nIn this paper, we conduct a re-annotation of TREC DL’19 using\ntwo “secondary” annotators for every topic. Based on our new rel-\nevance judgments, we first provide a qualitative analysis of this\nprocess and validate the findings of previous work in a modern\nsetting. That is, we observe low agreement among assessors but sys-\ntem rankings remain stable. Secondly, treating human annotators\nas manual TREC runs, we find that one can no longer meaning-\nfully discriminate between the state-of-the-art and human judges\non this collection. A reasonable “upper bound” effectiveness is\naround nDCG@10 of 0.81 (due to the assessment ambiguity). We\nsuggest that reported results that are higher than this bound on\nDL’19 might already overfit to the relevance labels. Note that we\nare not aware of any existing system reporting results exceeding\nthis bound.1 However, the leading systems today are very close,\nwith RankZephyr [32] and RankGPT [43] reaching between 0.78–\n0.80 nDCG@10 with a sufficiently strong first stage. We additionally\nfind that closed-sourced large language models and systems dis-\ntilled from them can fluctuate greatly in relative effectiveness, often\ndegrading system rank on our new relevance judgments.\nOur reproducibility effort spawned from the ECIR 2024 Collab-\na-thon on Neural IR [24]. A subset of the IR community discussed\nthat the current generation of retrieval models may have reached\na practical upper bound on existing test collections, but we were\nunsure when to consider a test collection to be saturated or expired.\nOur study reproduced prior findings on the inherent subjectivity\nand variability of relevance judgments (especially in the absence\nof a well-defined narrative), and it established an empirical upper\nbound on TREC DL’19. Therefore, we encourage caution when\nattempting to maximize a given metric as we approach and exceed\nhuman bounds on relevance estimation. Exceeding human effec-\ntiveness, given the subjectivity of relevance, may not lead to an\n1We do not consider large ensemble systems, given their risks [2].\noptimal system but instead stagnation of progress in the field. We\npublish our code and relevance judgments for reproducibility and\nto encourage future studies.2\n2\nRelated Work\nThis section outlines evaluation frameworks in retrieval, previous\nefforts to assess collection reliability, and judgement stability.\nOffline Evaluation. The Cranfield experiments introduced the\nmodern offline test collection in ad-hoc ranking; experts curated\na set of queries, and texts were retrieved from a static collection\nand exhaustively annotated, allowing for offline relevance judg-\nments [6]. Completeness of relevance judgments, i.e., the exhaustive\nannotation of corpora, is a key factor in the reliability of an offline\ntest collection [48]; however, the scale of modern corpora makes\nsuch a task infeasible. As such, TREC-style collections are collated\nby pooling the runs of multiple diverse systems to create a set of\nquery–document pairs to be annotated under the assumption that\nunpooled documents are not relevant [47]. Assessors then label\nthese pairs and assign a grade of relevance. A single annotator is\noften assigned to each topic; this decision generally reduces costs\nand has been validated by several studies measuring agreement\nbetween annotators as an effect on the reliability of a collection.\nRelevance judgments are subjective and, therefore, can vary\namong annotators [25], with low inter-annotator agreement [37],\nwhich can indicate low collection reliability. As such, narratives\nand training are usually provided to annotators describing the\ninformation need underlying a query [15]. We consider that these\nhomogenized judgments may be a weak point for ongoing collection\nreliability as we learn from them in a semi-supervised fashion\nthrough effective systems acting as teachers.\nNotions of Relevance. The reliability of offline test collections is\ncontested due to the intrinsic subjectivity of relevance under differ-\nent human annotators [34, 45], annotation settings [33], relevance\ngrading [10] and, more recently, the process of collecting candidate\ndocuments via pooling [54]. As these concerns have been raised, one\nmethod to validate offline test collections is to re-annotate judge-\nments. Lesk and Salton [23] and Burgin [4] empirically validated\nthe collection reliability under annotator disagreement, observing\nthat system order is maintained under different annotators by mea-\nsuring variance in precision and recall. When web-scale corpora\nrendered exhaustive annotation impossible, depth pooling became\npopularized to provide a minimal set of documents that should\nbe annotated to discriminate system effectiveness. Concerns were\ntwo-fold: that systems that contributed to the pool would have an\nadvantage over new systems and that the reduced annotations may\ndisrupt the stability of system orderings. Voorhees [46] found that\nsystem order stability could be maintained under the re-annotation\nof test collection pools from TREC-4 [14] and -6 [50].\nCollection Retirement. Previous work has discussed the retire-\nment of a test collection in recent years, though not retired before\ndissemination the use of TREC 2021 Deep Learning test collec-\ntion [9] is discouraged by NIST due to the large increase in corpora\nsize leading to pooling bias; both NIST [49] and participants in the\n2https://github.com/Parry-Parry/sigir25-annotation\n\n\nVariations in Relevance Judgments and the Shelf Life of Test Collections\nSIGIR ’25, July 13–18, 2024, Padua, Italy\ntrack [20] had concerns regarding the ability to measure recall due\nto incomplete labeling. However, we are not aware of any work\nthat considers the effect of collection popularity and subsequent\nbias induced by fitting to relevance judgments over time; our focus\non precision is in line with the philosophy of neural systems and\nthe Deep Learning track.\nCraswell et al. [9] warns against using TREC Deep Learning test\ncollections for experimental iteration to reduce the chance of overfit-\nting to judgments. Though this guidance is generally followed with\nthe MSMARCO-dev set acting as a suitable alternative, the broad\nuse of existing strong systems validated on TREC Deep Learning\n2019 [17, 32, 52] may lead to overfitting via semi-supervision. We\ndraw a parallel with the work of Armstrong et al. [3], which warned\nthat progress might stagnate due to poor practice in evaluation.\nSimilarly, we note that the continued comparison to a dataset that\nguides our architecture and training pipeline may lead to stagnation\nin system development.\nWithin a modern ranker training pipeline, several components\ndepend on existing models for effectiveness. Hard negative min-\ning is a process in which, instead of random negative examples\nin a contrastive loss, samples are taken from the top most rele-\nvant documents to a query with the assumption that the majority\nwill be non-relevant [21]. It is common to employ one or several\nmodels’ pooled rankings to sample negatives [41, 53], with these\nmodels often chosen based on their in-domain effectiveness. More\nexplicitly, pipelines are frequently composed of multiple stages of\ndistillation [39, 51, 53], which is a semi-supervised process in which\na student model is optimized to approximate the output of a teacher\nmodel [16]. Again, these models will be chosen based on in-domain\neffectiveness and coupled with hard negatives.\n3\nEvaluating Reliability through Re-Annotation\nThe reliability of a test collection, given the subjectivity of relevance,\nis often validated by re-annotation [4, 23, 46]. Modern IR models\nare often optimized to mimic previously effective systems through\ndata-driven processes. We examine how these models, designed\nafter a collection’s release, perform on new judgments following\nthe process of Voorhees [46].\n3.1\nReproduction Source\nWe describe the original annotation process applied by Voorhees\n[46]. This work was carried out as a component of the curation of\nthe TREC-4 [14] and TREC-6 [50] corpora. As is standard in the cura-\ntion of pooled TREC corpora, one “primary\" annotator was assigned\nto each topic, subsequently judging each document in the pool. Two\nadditional annotators were assigned to each topic to validate the\neffect of re-annotation on depth-pooled corpora. A sub-sample of\njudgements was taken from the pool and re-annotated. In TREC-4,\na maximum of 200 relevant documents per topic were taken as a\nsecondary pool for re-annotation. Additionally, 200 non-relevant\ndocuments were sampled per topic. A binary judgement was as-\nsigned to each query–document pair, relevant or non-relevant. The\ntexts constituting the primary annotation pool, left unjudged in the\nsecondary pool, are included in all system evaluations, maintaining\nidentical pooling biases. The comparison of “secondary\" annota-\ntions was carried out to assess hypotheses that resurface frequently\nin IR literature. This work focuses on the subjectivity of relevance\nand how pooling affects this subjectivity. Thus, experiments were\nconducted over a depth-pooled corpus under construction at the\ntime. This process produces a subset of judgements where each\nquery–document pair was re-annotated twice.\nWhere possible, we follow the evaluation process of Voorhees\n[46], including analysis of the probability of two systems swapping\nin effectiveness under different annotators. We define an annota-\ntion 𝑎as a tuple {𝑞,𝑑,𝑟} composed of query 𝑞, document 𝑑, and\njudgement 𝑟∈[0, 1, 2, 3]. For a given annotator pair and a primary\nannotator, assigned 𝑛query-document pairs, we have three anno-\ntation sets 𝐴1,𝐴2,𝐴3 where 𝐴𝑖= {𝑎𝑗}𝑛\n𝑗=0. For 𝑚queries, as noted\nby Voorhees [46], there are 3𝑚possible permutations. Thus, these\npermutations allow for the greater exploration of judgements that\ncould have been produced from a given annotation process. From\nthe 3 annotation sets, a permutation 𝐴∗is sampled 𝑠times, and\nsystems are evaluated against each permutation. The probability\nof a swap between systems 𝑖and 𝑗can be evaluated under a given\nmeasure 𝑓given judgements 𝐴∗. For systems 𝑆, we produce a ma-\ntrix 𝐵∈Z|𝑆|×|𝑆| where 𝐵[𝑖, 𝑗] is the number of annotation samples\nwhich caused systems 𝑆𝑖to have higher effectiveness than 𝑆𝑗. The\nswap probability between these systems can then be computed as\nmin(𝐵[𝑖,𝑗],𝐵[𝑗,𝑖])\n𝑠\n. Due to the symmetric treatment of system com-\nparisons, this computation is bounded between 0 and 0.5.\n3.2\nRe-Annotation of a Modern Collection\nWhile the original study focused on binary judgments, our work\nadapts this methodology to graded relevance and neural systems\nunder a modern evaluation setting. Our reproduction is conducted\non the TREC Deep Learning 2019 track (DL’19) [8]. Specifically,\nwe use the passage collection, which is generally more popular.\nThe proposal of more granular measures that explicitly consider\nthe ordering of a ranking, such as discounted cumulative gain, as\nopposed to set-based measures, such as Precision@𝑘, generally re-\nquires graded relevance as opposed to the binary relevance grades of\nolder collections. DL’19 was annotated under this setting with four\ngrades of relevance: ‘perfectly relevant,’ ‘highly relevant,’ ‘related’,\nand ‘non-relevant.’ This distinction leads to some key differences\nin our process and analysis.\nWe treat the original judgements of DL’19 as those of the “pri-\nmary” annotator. In our study, two annotators from a pool of 8\nretrieval academics are assigned to each topic and act as “secondary”\nannotators aligning with the original study in that each topic is an-\nnotated by 2 new annotators; however, the original study does not\nspecify the number of total annotators. The size of the annotator\npool used in this work is similar to that of TREC collections [42],\nalbeit with a reduced total annotation budget due to cost and follow-\ning in the sampling setting of Voorhees [46]. We create a secondary\npool composed of all documents in the pool with a grade of 1\nor more and sample 100 documents per annotator from the non-\nrelevant documents of the primary pool. We reduce the number of\nnon-relevant judgments from the pool motivated by the hypoth-\nesis that agreement is far greater over what is non-relevant; for\nset-based measures, this choice leads to a balance between relevant\nand non-relevant texts and has minimal effect on nDCG.\nOur process for pool creation uses all judged documents at least\nrelated to a topic and assigns topics to annotators in a round-robin\n\n\nSIGIR ’25, July 13–18, 2024, Padua, Italy\nA. Parry et al.\nfashion to balance annotation load while maintaining two annota-\ntors per topic. A priority queue of annotator pairs ordered by their\ncurrent annotation load is produced and assigned the next topic\nwith the greatest number of judgements. The load on each annota-\ntor is then balanced by swapping large topics to annotators with\nminimal load in a second round-robin. We then assign non-relevant\ntexts to each annotator pair with a total mean annotation load of\n1125 pairs per annotator.\nAnnotator Instruction. Each annotator was provided with annota-\ntion guidelines, including the original relevance grade descriptions\nfrom DL’19. Annotators were made aware that instances had been\npreviously annotated for DL’19; however, they were not provided\nwith relevance grades or a grade distribution. Due to the specialist\nknowledge required to assess relevance in, for example, medical\ntopics, annotators were allowed to familiarise themselves with\ntopics at their discretion. To facilitate reproducibility and prevent\nexposure to discussion and instances of DL’19 relevance judgments,\nwhich may be indexed on common search engines, we instead en-\nforce the use of the ChatNoir search endpoint [30], which indexes\nClueWeb22 [28] and retrieves using BM25.\n3.3\nFurther Investigation\nBeyond our reproduction setting, we utilise re-annotated query-\ndocument pairs to investigate other aspects of collection reliability.\nJudgement Aggregation. Voorhees [46] apply several aggregation\noperations including permutation as described in Section 3.1 and\naggregate operators. We perform maximum, minimum, and mean\npooling over judgments. In the case of mean pooling, we do not\nperform further quantization; for example, three texts of grades 1,\n1.5, and 2 where the second text was annotated 1 and 2 by different\nannotators would be compared in that order.\nAbsent Narratives. The absence of narratives or larger descrip-\ntions for the TREC Deep Learning collections allows an additional\nfocus for our investigation: Annotators were prompted to note cases\nof ambiguity and, more generally, their difficulties in annotation.\nFurthermore, annotators were asked to create their narratives for\neach query so that agreement among annotators could be investi-\ngated with more granularity in the presence of a natural language\ndescription of their thought process during annotation. In doing so,\nwe measure query intent as a factor in agreement.\nTo ablate this factor, we fix narratives after the curation of judge-\nments to assess how narrative interpretation can affect agreement\nand downstream measures of effectiveness. We take three topics of\nlow, median, and high agreement measured via Fleiss 𝜅between the\nprimary and secondary annotators; half of the annotators reference\none narrative, and half reference the other. We can then further iso-\nlate sources of disagreement to query intent and intrinsic ambiguity\nof a query title both qualitatively and quantitatively.\nAnnotators as Oracles. User models underly many common IR\nmeasures such as discounted cumulative gain [19] and mean re-\nciprocal rank (@𝑘) [5]. In performing offline evaluations of search\nsystems using these measures, we aim to model effectiveness via\nuser behavior approximated by relevance judgments. In principle,\nan optimal system would approach a score of 1 for a normalized\nTable 2: Modern systems included in our analysis. Rightmost\ncolumns are the source of labels for semi-supervised learn-\ning; neural is any encoder-type model, and LLM is a Large\nLanguage Model. Absence of both indicates binary labels.\nDistillation\nModel\nType\nNeuralLLM\nColBERT [22]\nLate interaction bi-encoder\n×\n×\nSPLADE ED++ [11, 12] Learned sparse encoder\n✓\n×\nMonoT5 [27]\nSeq2Seq cross-encoder\n×\n×\nMonoELECTRA [31, 40]BERT-based cross-encoder\n✓\n✓\nSetEncoder [39]\nBERT-based list-wise cross-encoder\n✓\n✓\nRankZephyr [32]\nDecoder-only list-Wise cross-encoder\n×\n✓\nRankGPT [43]\nDecoder-only list-wise cross-encoder\n?\n?\nmetric (assuming 𝑘relevant documents exist). However, given the\nsubjective and variable nature of relevance under different query\nintents, such effectiveness, though possible, may not be indicative\nof a truly effective system. Human annotators outside the original\nannotation setting may represent strong search systems as they\nhave exhaustively annotated available documents to some cutoff 𝑘.\nAs such, we measure the performance of each annotator under\noriginal relevance judgments as an indicator of how non-pooled\nmodern systems are performing relative to a more realistic ora-\ncle. Similarly to idealized discounted cumulative gain, we sort by\nrelevance grade post-aggregation. Though graded relevance lacks\ngranularity in contrast to scalar relevance scores, through in-grade\npermutations (shuffling), we validate that variations in downstream\nmeasures of effectiveness are minimal (SD < 0.0001).\n4\nEvaluation\nIn this section, we outline our investigation and analyze findings\nwhen evaluating using secondary annotations.\n4.1\nExperiment Design\nFollowing the re-annotation process of Voorhees [46], we observe\nthe impact of re-annotation on the DL’19 test collection, using\npermutations to assess modern system bias.\nDataset. The MSMARCO passage corpus is a collection of around\n8.8 million segmented documents covering a broad range of ad-hoc\nsearch topics [26]; this collection and accompanying training topics\nwere mined and annotated from Bing query logs. We re-annotate the\nTREC Deep Learning 2019 test collection [8], composed of 43 topics.\nUnlike the development split released originally with MSMARCO,\nthe DL’19 test set uses densely annotated depth-pooled topics taken\nfrom the associated TREC track that year.\nPool Description. The DL’19 track primarily focused on the com-\nparison of neural systems in ad-hoc ranking. For equivalence to\nthe re-ranking task, the top 100 texts of all submitted systems from\nboth re-ranking and retrieval tasks were included in our annotation\npool. The top 10 texts from each of the 75 systems (44% Neural LM,\n27% Neural, 29% lexical/traditional) were added to our pool, and\nwe use HiCal [1] to collect an additional 100 texts per topic.\n\n\nVariations in Relevance Judgments and the Shelf Life of Test Collections\nSIGIR ’25, July 13–18, 2024, Padua, Italy\nModels. We compare several neural architectures and training\nsettings. In addition to models that contributed to the original anno-\ntation pool, we include systems that use other models for either data\naugmentation or semi-supervised training via distillation. Table 2\nillustrates the degree of teacher distillation used in each architec-\nture; ColBERT and SPLADE act as end-to-end retrieval or first-stage\nretrievers, whereas all others re-rank BM25 [35] or ColBERT as\nnoted. For monoT5, monoELECTRA, and SetEncoder, we include\nmultiple model sizes, base and large for BERT/ELECTRA variants\nand base and 3B for T5. For monoELECTRA, we do not use the\noriginal checkpoints released by Pradeep et al. [31]; we instead use\ndistilled checkpoints released by Schlatt et al. [40] as we are inter-\nested in systems that use modern processes. When using RankGPT,\nwe employ GPT-4, GPT-4 Turbo, and GPT-4o.\nMeasures. Voorhees [46] measure inter-annotator agreement in\na pair-wise setting using the overlap of judgements defined as\nthe number of common judgements divided by the total number\nof judgements. We additionally use Cohen’s 𝜅over both 4-grade\nand binarized relevance and Fleiss’ 𝜅, allowing for comparisons\nbetween 3 annotators (1 primary, 2 secondary) simultaneously. In\nthe TREC-4 and 6 test collections, macro-averaged precision was\nthe primary measure of effectiveness, and therefore, the previous\nstudy measured systems by MAP. The TREC DL tracks with four\ngrades of relevance instead primarily measure nDCG@10, and thus,\nour study is conducted over this metric. Aligned with Voorhees\n[46], we measure downstream system order correlation between\njudgement sets using measure Kendall’s 𝜏; additionally, we measure\nSpearman’s 𝜌and RBO.\nSystem Order Robustness. As we have multiple annotators per\nquery, we can observe pooled judgments across several hypothetical\nsets; we take all possible combinations of annotators and evaluate\nboth pooled and new models to observe the stability of system order\nunder new judgments. For a given judgment set, we produce a rank-\ning of systems by a target metric (i.e., nDCG@10) and compare this\nsystem order to the system ordering under the original judgments.\nWe aggregate rank Δ for all possible natural combinations over each\nsystem to observe how frequently a system degrades over other\nassessed systems. We measure significant differences in retrieval\neffectiveness via a paired t-test with Bonferroni correction for dif-\nferent annotator combinations. We measure significant changes\nin system rank by a Wilcoxon signed test comparing annotator\ncombinations with the system order by official TREC judgements.\n4.2\nPilot Study Findings\nWe conducted a pilot study over the top 10 pooled texts for 10 ran-\ndomly sampled topics. Table 3 summarizes the results. We observe\nthat the prevalence of grades 1 and 3 is inflated in these annotations\ncompared to the original annotations. In discussions with anno-\ntators, there was ambiguity about what separates a relevant text\nfrom a highly relevant text. Additionally, cultural context leads to\nambiguity in interpreting a query; for example, a query “dog day af-\nternoon meaning” was difficult to interpret without broader context\nas a film exists with that name. The query could be interpreted as\nthe meaning of the phrase or the film. From these ambiguities, we\nallowed annotators to use a controlled search engine to familiarise\nthemselves with the topic. Annotators not from the USA generally\nnoted the difficulty in USA-centric topics.\nTable 3: Comparing the ration of annotation grades between\nthe primary annotators and secondary annotators in a pilot\nstudy of 10 topics.\nGrade Ratio\nAnnotators\n0\n1\n2\n3\n|𝐴|\nPrimary\n0.487\n0.190\n0.308\n0.015\n1143\nSecondary\n0.560\n0.213\n0.108\n0.118\n600\nTable 4: Inter-Annotator agreement over combinations of\nannotators. Agreement is shown under binarised and 4-grade\nrelevance.\nFleiss’ 𝜅\nOverlap\nCohen’s 𝜅\nGroup\n4\n2\n4\n2\n4\n2\n|Q|\nPrimary+1\n0.22\n0.44\n0.12\n0.46\n0.12\n0.31\n14\n1\n—\n—\n0.42\n0.72\n0.19\n0.37\nPrimary+2\n0.28\n0.40\n0.17\n0.48\n0.17\n0.31\n12\n2\n—\n—\n0.47\n0.74\n0.22\n0.38\nPrimary+3\n0.17\n0.31\n0.11\n0.45\n0.11\n0.28\n8\n3\n—\n—\n0.63\n0.89\n0.27\n0.47\nPrimary+4\n0.28\n0.37\n0.19\n0.48\n0.19\n0.30\n9\n4\n—\n—\n0.43\n0.71\n0.19\n0.34\n4.3\nCore Assessment of Agreement\nQuantification of relevance is confounded by many factors with sev-\neral works reporting what is generally considered low agreement\namong annotators [4, 23, 46]. Voorhees [46] report overlap among\nannotators as a measure of agreement observing values around 0.4.\nIn Table 4, we present agreement measurements over both 4 grade\nand binarized relevance. We observe that under 4-grade relevance,\nall agreement values degrade relative to numbers stated in previous\nstudies. Fleiss’ 𝜅measured over primary and secondary annota-\ntors indicates near-random agreement but generally improves by\naround 10 points in all cases over binary judgments. Overlap values\nover binary judgments generally correspond to those observed by\nVoorhees [46], and over solely secondary annotators, we observe\nhigh overlap values, suggesting that while disagreement stems from\nthe subjectivity of relevance and the annotation setting, the process\nof annotating over 4 grades is generally more variable regardless\nof topic granularity. In the third group, we find the lowest values\nin both 4 grade and binary relevance, which is again reflected in\nlow overlap and Cohen’s 𝜅values, potentially suggesting diverging\nquery intents or that this allocation of queries was particularly\nambiguous. However, we see increases in overlap values when mea-\nsured solely over secondary annotators, again suggesting that the\nannotation setting under which judgments are collected contributes\nto the improved agreement, as noted by Cuadra and Katter [10] and\nRegazzi [34] in previous annotation studies. Values of 𝜅are low in\nall cases, suggesting that generally quantifying relevance over 4\n\n\nSIGIR ’25, July 13–18, 2024, Padua, Italy\nA. Parry et al.\nFigure 1: Transitions in relevance grades from primary to\nsecondary annotations.\ngrades is more challenging for annotators than the re-annotation\nsetting of Voorhees [46]. The lack of pre-defined narratives clarify-\ning intent means annotators could interpret a query as they please,\npotentially resulting in three diverging topic interpretations.\nIn Figure 1, observe a Sankey flow diagram between the origi-\nnal relevance grade assigned by primary annotators and the new\ngrades by secondary annotators. Like previous work, a secondary\nannotation setting often leads to the reduction of relevance grades,\ne.g., 1 to 0 or 2 to 1. Validating our decision to sample a subset of\nnon-relevant judgements, 13% of 800 non-relevant texts change\ngrade, with 77% being a change to a grade of 1; we analyse outlier\ncases (providing a grade of 2 or more) in Section 4.4. Measuring\nfrequencies agnostic of previous judgments, grades 1 and 2 are\nannotated similarly across annotators, while grades 0 and 3 were\nannotated with greater variance. This variance in the case of grade\n0 is generally consistent across annotators, and we provide a deeper\nanalysis of grade 3 due to varying notions of relevance below.\n4.4\nNarratives, Intent, and Relevance\nThe observations of grade transitions in our work and generally low\nagreement noted by prior work, even in a binary setting, may be\nattributed to several factors noted in Section 2. Though we cannot\nfully isolate factors in the subjectivity of relevance, we choose to\ninvestigate the effect of narratives on inter-annotator agreement,\ngiven that recent corpora often do not have fixed narratives [7, 8],\nunlike those used in prior re-annotation studies [14]. We select\ntopics by median, minimum and max Fleiss’ 𝜅values from the\nannotation pool and consider where disagreement occurs and how\nnarratives differ. Chosen examples illustrate different aspects of\nagreement, differing query intents, and a narrative’s specificity.\nThe first, taken from Group 1, is a technical query in the med-\nical domain with a Fleiss 𝜅of 0.06, indicating disagreement near\nrandom guessing. The query “types of dysarthria from cerebral\npalsy” was described by one annotator as a query from a sibling\nof someone with cerebral palsy looking to understand how their\nsibling would be affected by their condition in the future. The other\nannotator described a medical context in which the definition of all\nconditions is known, and therefore, solely the connection between\ndysarthria and cerebral palsy was required. The second annotator\nstated a requirement that relevance was determined by the types\nand specificity of provided types of dysarthria connected to cere-\nbral palsy, with all other documents being non-relevant. Though\nboth are valid information needs, they represent vastly different\nquery intents, with one annotator’s grade frequency matching the\noverall annotation pool and the other stricter narrative leading to\nnear-total non-relevance.\nThe second, taken from Group 4, is a more general query, “is cdg\nairport in main paris” with a Fleiss 𝜅of 0.08, again indicating low\nagreement. However, both annotators described a need for exact\ndistances from the airport to either the center of Paris or, more\ngenerally, the city of Paris. Both annotators required a succinct\ndescription of the distance or travel times to the airport from Paris\nto be considered perfectly relevant. To be highly relevant, both con-\nsidered any mention of either travel time or distance measurements.\nIntents then diverged for partially relevant documents, with one an-\nnotator allowing for the mention of costs and other aspects of travel\nto and from the airport. In contrast, the other annotator required\nthe airport’s position, at minimum, mentioning that it is situated\nwith respect to Paris. Non-relevance was denoted by any other\nmention of the airport or mentions of distances to destinations near\nParis. We find larger differences in lower relevance grades, with\none annotator considering the majority of pooled documents to be\nnon-relevant when there is no mention of travel details. In contrast,\nthe other considers them to be relevant because they mention that\nthe airport is situated near Paris in a generic manner. The specificity\nof a query generally leads to lower agreement in our observations.\nWhile queries in domains such as law or medicine have a lower\noverall agreement, we still find cases where generic queries have\nlow agreement due to the interpretations of information needs.\nWe analyze outlier cases of texts initially labeled non-relevant\nthat, when evaluated under an alternative query intent, were\ndeemed perfectly relevant. In the 9 cases of this, there are cases\nwhere a reference is made to the entity of focus, for example “gold-\nfish” in the query, “do goldfish grow” but only implicitly, for exam-\nple “it” or “they”. The query was satisfied by a given document if\nthe context is inferred by the surrounding spans of text or from\nknowing that the document has been retrieved by some system\n(this is inevitable under pooling), but the information is not self-\ncontained. In some cases, annotators explicitly stated that they\nconsidered a document relevant, but when the information is not\nself-contained, it cannot be annotated as relevant; others took this\napproach without note.\nOverall, many annotation disagreements can occur not only in\nthe high-level query intent of the narrative but also in descriptions\nof relevance. A diagnostic tool and query-specific grade specifica-\ntions could be helpful in future work.\n4.5\nNarrative and Intent Ambiguity\nGiven the lack of public narratives provided in the release of TREC\nDeep Learning 2019, it is infeasible to isolate query intent as a factor\nin reliability without first collecting narratives. In fixing narratives,\nwe produce an additional set of relevance judgements for queries of\nlow, median, and high agreement by Fleiss 𝜅, including the primary\nannotator. In Figure 2, We classify these queries as difficult, median\nand easy with respect to query intent determination and measure\n\n\nVariations in Relevance Judgments and the Shelf Life of Test Collections\nSIGIR ’25, July 13–18, 2024, Padua, Italy\nAnnotator 2 (1)\nAnnotator 3 (1)\nAnnotator 6 (1)\nAnnotator 8 (1)\nAnnotator 1 (2)\nAnnotator 4 (2)\nAnnotator 5 (2)\nAnnotator 7 (2)\nAnnotator 2 (1)\nAnnotator 3 (1)\nAnnotator 6 (1)\nAnnotator 8 (1)\nAnnotator 1 (2)\nAnnotator 4 (2)\nAnnotator 5 (2)\nAnnotator 7 (2)\n0.2\n0.3 0.0\n0.2 0.5 0.0\n0.1 0.0 0.1 0.1\n0.2 0.1 0.2 0.1 0.3\n0.0 0.1 -0.1 0.1 0.3 0.2\n0.2 0.3 0.1 0.4 0.1 0.1 0.1\nDifficult Query\nAnnotator 2 (1)\nAnnotator 3 (1)\nAnnotator 6 (1)\nAnnotator 8 (1)\nAnnotator 1 (2)\nAnnotator 4 (2)\nAnnotator 5 (2)\nAnnotator 7 (2)\nAnnotator 2 (1)\nAnnotator 3 (1)\nAnnotator 6 (1)\nAnnotator 8 (1)\nAnnotator 1 (2)\nAnnotator 4 (2)\nAnnotator 5 (2)\nAnnotator 7 (2)\n0.3\n0.3 0.2\n0.4 0.6 0.1\n0.4 0.5 0.3 0.4\n0.2 0.4 0.2 0.4 0.3\n0.4 0.5 0.4 0.4 0.5 0.3\n0.3 0.4 0.3 0.3 0.5 0.2 0.6\nMedian Query\nAnnotator 2 (1)\nAnnotator 3 (1)\nAnnotator 6 (1)\nAnnotator 8 (1)\nAnnotator 1 (2)\nAnnotator 4 (2)\nAnnotator 5 (2)\nAnnotator 7 (2)\nAnnotator 2 (1)\nAnnotator 3 (1)\nAnnotator 6 (1)\nAnnotator 8 (1)\nAnnotator 1 (2)\nAnnotator 4 (2)\nAnnotator 5 (2)\nAnnotator 7 (2)\n0.5\n0.3 0.1\n0.2 0.3 0.3\n0.1 0.2 0.5 0.3\n0.3 0.2 0.6 0.4 0.4\n0.3 0.4 0.5 0.4 0.6 0.7\n0.1 0.2 0.4 0.4 0.7 0.3 0.6\nEasy Query\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nCohen's \nFigure 2: Comparing 3 queries of varying difficulty by inter-annotator agreement under fixed narratives (2 narratives (1) and (2)).\nAnnotators were divided into two groups and were assigned identical document pools. Color indicates strength of agreement;\nredder indicates stronger agreement, bluer indicates weaker agreement.\nCohen’s 𝜅between the judgements of both narrative groupings. It\nis clear that a fixed narrative reduces ambiguity with Cohen’s 𝜅,\nimproving by 24% and 46% for each annotation group between the\ndifficult and median query; however, between the median and easy\nquery, a decrease of 8% and an increase of 19% is observed.\nIn several cases, high agreement across different narrative groups\noccurs; in each case, narratives are of similar detail, defining what\nintent the user had in searching as well as what should be labelled\nrelevant; we consider that one factor in relevance annotation be-\nyond random noise may therefore be interpretation of a narrative\nwhich in the case of a primary annotator is of minimal concern but\nin human-in-the-loop annotation with generative systems may lead\nto implications for control as ultimately we can only express our\nneed or a demonstration to the system in natural language. When\ncomputing global improvements in agreement, we measure a 79%\nincrease in agreement between the difficult and median query and\nan additional 1% increase comparing the median and easy query.\nFrom these observations, we propose that the greatest factor in\ndisagreement is simply the ambiguity of the natural language used\nto express a query regardless of the narrative applied. Whether or\nnot this is only inherent in pooled corpora would require further\ninvestigation as in a setting such as that of TREC DL’19, the ad-hoc\nrealisation of a query intent will be biased by the top-𝑘texts chosen\nby pooled systems.\n4.6\nCore Assessment of System Order\nThe core objective in re-annotation studies is to validate that down-\nstream evaluation is not compromised by human bias in subjective\nrelevance judgements. By measuring the stability of system order\nand the correlation of system order by target metrics, we can assess\ncollection reliability. Following Voorhees [46] and as described in\nSection 3.1, we sample permutations of relevance judgements and\nobserve the stability of pair-wise system comparisons in Figure 3.\nComparisons closest to the top right of the figure represent those\nwhere nDCG differences are large; however, they frequently swap\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n nDCG@10\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nProbability of Swap\nNeural\nLLM-Neural\nLLM\nFigure 3: Comparing the stability of judgements through\nnDCG@10 and system swap probabilities over 10000 judge-\nment permutations. We filter lexical comparisons to improve\nvisibility in the central mass. ‘LLM-Neural’ denotes that a\npair contains one LLM-based and one Neural-based ranker.\nin system order. A clear pareto-frontier is composed of compar-\nisons between LLM and neural systems under judgement permuta-\ntions. Lexical-neural comparisons generally lie in the main body\nof neural-neural and llm-llm comparisons, suggesting similar sta-\nbility as was noted by Voorhees [46] over TREC-6. However, more\ngenerally, we replicate the findings of previous works that despite\nlow annotator agreement, under re-annotation, system ordering is\nhighly correlated with the original DL’19 annotations even under\n4-grade relevance. In Table 5, observe aggregate correlation values\nusing both natural combinations of annotators and the in-sample\napproach described by Voorhees [46].\nAs the In-Sample approach includes primary judgements, a\nmarginally higher correlation compared to combinations of solely\n\n\nSIGIR ’25, July 13–18, 2024, Padua, Italy\nA. Parry et al.\nModels\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nnDCG@10\nModel Type\nLexical\nNeural\nLLM\nFigure 4: Variance in nDCG@10 over annotator combina-\ntions. Effectiveness values on new annotations are translu-\ncent. Color of points indicates model type.\nTable 5: System ranking correlation over different annotator\ngroups measuring Kendall’s𝜏, Spearmans’s 𝜌and rank-biased\noverlap. ‘Combination’ is the mean of natural permutations\nof the 8 annotators, ‘In-Sample’ is the mean of judgment per-\nmutations as described by Voorhees [46]. All 𝜏and 𝜌values\nrepresent significant correlation.\nType\n𝜏\n𝜌\nRBO\nCombination\n0.879\n0.972\n0.888\nIn-Sample\n0.897\n0.977\n0.902\nsecondary annotators is found. To investigate where system order\ndiverges, we analyse ranking changes at a system level. In Figure 4\nsee that generally, new annotations led to a reduction in absolute\nnDCG@10 values relative to the original annotations, effectiveness\nmeasures on original annotations order systems, and that system\norder is not fully stable but LLM-based or distilled systems ap-\npear to be outliers, as such we explicitly measure the changes in\nsystem order concentrated around these systems. In Figure 5, we\ncombine all possible permutations of annotators across annotation\npairs. We measure changes in system order under these combina-\ntions and aggregate by system. Many lexical systems remain stable\nin ordering (Δ ≈0). Older neural approaches in the original sys-\ntem pool display higher variance but can improve or degrade in\nrank without significant difference. When considering only modern\nsystems, we observe that smaller models trained with supervised\nand semi-supervised approaches consistently improve system rank.\nWithin this group, however, larger semi-supervised models are\nmore robust than their smaller counterparts under new judgments.\nList-wise encoders generally degrade in rank. These models use an\nTable 6: Comparing aggregate annotations as rankings with\nseveral ranking architectures. Judged Only indicates that\noriginal runs have been filtered to contain solely annotated\ndocuments, allowing for a similar setting to the annotator\nrankings. Significance is with respect to Minimum (A), Mean\n(B) or Maximum (C) aggregation via a paired t-test with Bon-\nferroni correction (𝑝< 0.05).\nModel\nnDCG@10\nP@10\nMRR@10\nR@100\nJudged & Unjudged\nBM25\n0.51±0.08ABC 0.41±0.09ABC 0.70±0.12BC 0.49±0.10ABC\nSplade ED++\n0.73±0.07BC 0.62±0.10BC 0.91±0.07\n0.60±0.09ABC\nColBERT»RankZephyr 0.75 ± 0.07B\n0.67±0.10\n0.84±0.09\n0.67±0.09ABC\nColBERT»RankGPT-4o 0.78±0.06\n0.71±0.09\n0.87±0.08\n0.67±0.09ABC\nJudged Only\nBM25\n0.51±0.08ABC 0.41±0.09ABC 0.70±0.12BC 0.65±0.09ABC\nSplade ED++\n0.73 ± 0.07B\n0.63±0.10BC 0.91±0.07\n0.69±0.09BC\nColBERT»RankZephyr 0.77±0.07\n0.70±0.09\n0.85±0.08\n0.67±0.09ABC\nColBERT»RankGPT-4o 0.80±0.05\n0.73±0.09\n0.89±0.08\n0.67±0.09ABC\nMinimum (A)\n0.76±0.07\n0.67±0.10\n0.84±0.09\n0.75±0.06\nMean (B)\n0.81±0.05\n0.71±0.10\n0.90±0.08\n0.86±0.06\nMaximum (C)\n0.79±0.05\n0.70±0.09\n0.86±0.08\n0.86±0.06\ninteraction token to facilitate inter-document attention, assisting\nin out-of-domain effectiveness [39]. Changes in effectiveness are\nmore pronounced and are significant in smaller distilled language\nmodels. This is notable as though we see continuing improvement\nin smaller models out-of-domain through distillation. One should\nbe cautious that in-domain effectiveness may be less robust than\nbenchmarks indicate, which for many applications may be more\nimportant than robust effectiveness across heterogeneous corpora.\nRuns that are either zero-shot or distilled from LLM re-rankers\nshow the highest variance in relative effectiveness even within a\nsub-group such as those based on GPT 4 endpoints. GPT-4o-based\nruns significantly degrade in system rank when re-ranking BM25\nbut are more stable under a neural first-stage ColBERT. This is most\nlikely caused by positional bias found in list-wise ranking towards\nalready precise first-stage rankings [29, 44]. This hypothesis is\nvalidated by the frequent rank improvement of RankZephyr (the\nleft-most model in Figure 4), which was trained explicitly to reduce\nthis bias [32]. Overall, we consider that though high-quality data\n(i.e., granular training data from strong models) is important in\nimproving the robustness of neural models, these findings would\nsuggest that smaller models in our current training strategies may\nnot have the capacity to robustly generalize even in-domain.\n4.7\nSystems versus Humans\nRecall our argument that a “perfect” score on a normalized metric\nmay not be a feasible or desirable goal in ranking evaluation; as\nsuch, we measure the effectiveness of human annotators aggre-\ngated in multiple forms outlined in Section 3.3. In Table 6, see that\nsystems can exceed or reach parity with secondary annotators, par-\nticularly with respect to nDCG@10 and MRR@10, with multiple\nsystems, both LLM and encoder-based exceeding annotators. When\nexcluding unjudged documents (which is known to overestimate\n\n\nVariations in Relevance Judgments and the Shelf Life of Test Collections\nSIGIR ’25, July 13–18, 2024, Padua, Italy\nBM25 >> RankZephyr\nMono T5 3B\nColBERT >> RankZephyr\nBM25 >> MonoELECTRA Large\nSrchvrs PS Run 2\nColBERT >> RankGPT4O Full\nMono T5 Base\nColBERT >> RankGPT4\nBM25 >> MonoELECTRA Base\nBM25 >> RankGPT4\nColBERT >> RankGPT4 Turbo\nBM25 >> SetEncoder Large\nTUW19 P3 RE\nBM25 Base AX P\nColBERT\nICT-CKNRM B50\nColBERT >> RankGPT4O\nBM25 Base RM3 P\nBM25 Base PRF P\nSrchvrs PS Run 1\nBM25 >> Sparse Cross-Encoder\nBM25 Tuned AX P\nTUW19 P2 RE\nRun ID 2\nRun ID 5\nSPLADE ED++\nTUW19 P3 F\nUNH BM25\nUNH ExDL BM25\nBM25 Tuned P\nColBERT >> MonoELECTRA Large\nRun ID 4\nBM25 >> SetEncoder Base\nBM25 Base P\nRun ID 3\nBM25 Tuned RM3 P\nColBERT >> MonoELECTRA Base\nICT-CKNRM B\nIDST BERT P3\nIDST BERT P2\nIDST BERT P1\nMS DUET Passage\nICT-BERT2\nBM25 Tuned PRF P\nTUW19 P2 F\nTUW19 P1 RE\nBM25 >> RankGPT4 Turbo\nIDST BERT PR1\nSrchvrs PS Run 3\nIDST BERT PR2\nP Exp BERT\nP Exp RM3 BERT\nTUW19 P1 F\nColBERT >> SetEncoder Large\nBM25 >> RankGPT4O\nTUA1-1\nColBERT >> SetEncoder Base\nTest 1\nP BERT\nBM25 >> RankGPT4O Full\nSystem\n15\n10\n5\n0\n5\n10\n15\n System Rank\nType\nLexical\nNeural\nLLM\nFigure 5: Aggregated system rank changes over all natural combinations of secondary versus primary annotations measured\nusing nDCG@10. Negative Δ indicates a system frequently improves in rank.\nthe retrieval effectiveness [13, 36]), annotator performance mea-\nsured by precision can be exceeded by neural systems across nDCG,\nP, and MRR; however, recall remains lower in all settings. Given\nour findings in Figure 4, an observation that systems are better\nthan humans should be considered cautiously. However, the robust-\nness of the LLM-based RankZephyr observed both by system order\nrobustness over new judgments and its parity with human anno-\ntators suggests that with sufficient capacity, a smaller system can\nbe distilled from larger systems while remaining robust. Observing\nsimilar effectiveness to humans and confidence intervals providing\nno way to discriminate systems, we consider that achieving signifi-\ncant improvements would suggest far exceeding humans, which\nmay not be possible in a way conducive to improving information\naccess and may solely serve to improve precision on specific topics\nand intents. The ability of distilled systems to reach parity with\nhuman effectiveness across multiple annotator combinations while\nobtaining largely reduced recall in a judged setting, even when\nre-ranking a neural first-stage retriever, is undesirable for several\ndownstream tasks. Although precision is approaching or overtaking\nthe effectiveness of an oracle, recall remains a challenge in densely\nannotated collections.\n5\nConclusion\nWe considered important studies on the impact of the annotator\nagreement on retrieval evaluations in light of modern neural evalu-\nation scenarios. In reproducing re-annotation processes and evalu-\nations by Voorhees [46] on a modern test collection, we validate\nthe stability system ordering under re-annotation under 4-grade\nrelevance and ambiguous query intent. We raised concerns that\nthe broad and frequent use of test collections to “tune” learned\nsystems implicitly through influences from published works may\nreduce the reliability of a collection over time. We identified that\nthe subjectivity of relevance through query intent may be a factor\nin determining “overfitting” in ranking tasks. As a query may have\nseveral interpretations, we re-annotate the popular TREC DL’19\ncollection. We found that system order varies when evaluating large\nlanguage models and systems distilled from them. Furthermore,\nwe observed that the current state-of-the-art can outperform com-\nbinations of human annotators on original relevance judgments,\nsuggesting we may have reached a realistic bound on precision for\nthis collection. We posit that further improvement on this collection\nmay not indicate that a system is better than another in a meaning-\nful way. Our process has limitations—most notably, re-annotation is\nincredibly costly. Future work will investigate how to draw similar\nconclusions about a collection’s expiration without the need for\nextensive human effort.\nAcknowledgments\nWe thank Sean MacAvaney and others involved in making the\nsessions of the Collab-a-thon at ECIR‘24 [24] an engaging space for\ncollaboration.\nReferences\n[1] Mustafa Abualsaud, Nimesh Ghelani, Haotian Zhang, Mark D Smucker,\nGordon V Cormack, and Maura R Grossman. 2018. A System for Efficient\nHigh-Recall Retrieval. In The 41st International ACM SIGIR Conference on\nResearch & Development in Information Retrieval. ACM, 1317–1320.\n[2] Xavier Amatriain and Justin Basilico. 2017. Netflix recommendations: Beyond\nthe 5 stars (part 1). https://netflixtechblog.com/netflix-recommendations-\nbeyond-the-5-stars-part-1-55838468f429\n\n\nSIGIR ’25, July 13–18, 2024, Padua, Italy\nA. Parry et al.\n[3] Timothy G. Armstrong, Alistair Moffat, William Webber, and Justin Zobel. 2009.\nImprovements that don’t add up: ad-hoc retrieval results since 1998. In\nProceedings of the 18th ACM Conference on Information and Knowledge\nManagement (Hong Kong, China) (CIKM ’09). Association for Computing\nMachinery, New York, NY, USA, 601–610.\nhttps://doi.org/10.1145/1645953.1646031\n[4] Robert Burgin. 1992. Variations in relevance judgments and the evaluation of\nretrieval performance. Inf. Process. Manage. 28, 5 (July 1992), 619–627.\nhttps://doi.org/10.1016/0306-4573(92)90031-T\n[5] Olivier Chapelle, Donald Metlzer, Ya Zhang, and Pierre Grinspan. 2009.\nExpected reciprocal rank for graded relevance. In CIKM ’09: Proceeding of the\n18th ACM conference on Information and knowledge management. New York, NY,\nUSA, 621–630. http://doi.acm.org/10.1145/1645953.1646033\n[6] C. Cleverdon, J. Mills, and M. Keen. 1966. Factors Determining the Performance of\nIndexing Systems. Volume I. Design. Part 2. Appendices. Technical Report\nPB169574. Association of Special Libraries and Information Bureau, Cranfield\n(England). https:\n//ntrl.ntis.gov/NTRL/dashboard/searchResults/titleDetail/PB169574.xhtml Num\nPages: 261.\n[7] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2020.\nOverview of the TREC 2020 Deep Learning Track. In Proceedings of the 29th Text\nREtrieval Conference, TREC 2020, Virtual Event, Gaithersburg, MD, USA,\nNovember 16-20, 2020 (NIST Special Publication, Vol. 1266), Ellen M. Voorhees and\nAngela Ellis (Eds.). National Institute of Standards and Technology (NIST).\n[8] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M.\nVoorhees. 2019. Overview of the TREC 2019 Deep Learning Track. In 28th\nInternational Text Retrieval Conference, TREC 2019, Gaithersburg, Maryland, USA\n(NIST Special Publication), Ellen M. Voorhees and Angela Ellis (Eds.). National\nInstitute of Standards and Technology (NIST).\n[9] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Fernando Campos, Ellen M.\nVoorhees, and Ian Soboroff. 2021. TREC Deep Learning Track: Reusable Test\nCollections in the Large Data Regime. Proceedings of the 44th International ACM\nSIGIR Conference on Research and Development in Information Retrieval (2021).\nhttps://api.semanticscholar.org/CorpusID:233296851\n[10] Carlos A. Cuadra and Robert V. Katter. 1967. Opening the Black Box of\nRelevance. Journal of Documentation 23, 4 (April 1967), 291–303.\nhttps://doi.org/10.1108/eb026436\n[11] Thibault Formal, Carlos Lassance, Benjamin Piwowarski, and Stéphane\nClinchant. 2022. From Distillation to Hard Negative Sampling: Making Sparse\nNeural IR Models More Effective. In Proceedings of the 45th International ACM\nSIGIR Conference on Research and Development in Information Retrieval (Madrid,\nSpain) (SIGIR ’22). Association for Computing Machinery, New York, NY, USA,\n2353–2359. https://doi.org/10.1145/3477495.3531857\n[12] Thibault Formal, Benjamin Piwowarski, and Stéphane Clinchant. 2021. SPLADE:\nSparse Lexical and Expansion Model for First Stage Ranking. Association for\nComputing Machinery, New York, NY, USA, 2288–2292.\nhttps://doi.org/10.1145/3404835.3463098\n[13] Maik Fröbe, Lukas Gienapp, Martin Potthast, and Matthias Hagen. 2023.\nBootstrapped nDCG Estimation in the Presence of Unjudged Documents. In\nAdvances in Information Retrieval - 45th European Conference on Information\nRetrieval, ECIR 2023, Dublin, Ireland, April 2-6, 2023, Proceedings, Part I (Lecture\nNotes in Computer Science, Vol. 13980), Jaap Kamps, Lorraine Goeuriot, Fabio\nCrestani, Maria Maistro, Hideo Joho, Brian Davis, Cathal Gurrin, Udo\nKruschwitz, and Annalina Caputo (Eds.). Springer, 313–329.\nhttps://doi.org/10.1007/978-3-031-28244-7_20\n[14] Donna Harman. 1995. Overview of the Fourth Text REtrieval Conference\n(TREC-4). In Proceedings of The Fourth Text REtrieval Conference, TREC 1995,\nGaithersburg, Maryland, USA, November 1-3, 1995 (NIST Special Publication,\nVol. 500-236), Donna K. Harman (Ed.). National Institute of Standards and\nTechnology (NIST). http://trec.nist.gov/pubs/trec4/overview.ps.gz\n[15] Donna Harman. 2012. TREC-Style Evaluations. In Information Retrieval Meets\nInformation Visualization - PROMISE Winter School 2012, Zinal, Switzerland,\nJanuary 23-27, 2012, Revised Tutorial Lectures (Lecture Notes in Computer Science,\nVol. 7757), Maristella Agosti, Nicola Ferro, Pamela Forner, Henning Müller, and\nGiuseppe Santucci (Eds.). Springer, 97–115.\nhttps://doi.org/10.1007/978-3-642-36415-0_7\n[16] Geoffrey Hinton. 2015. Distilling the Knowledge in a Neural Network. arXiv\npreprint arXiv:1503.02531 (2015).\n[17] Sebastian Hofstätter, Sophia Althammer, Michael Schröder, Mete Sertkan, and\nAllan Hanbury. 2020. Improving Efficient Neural Ranking Models with\nCross-Architecture Knowledge Distillation. CoRR abs/2010.02666 (2020).\narXiv:2010.02666 https://arxiv.org/abs/2010.02666\n[18] Dirk Hovy and Shrimai Prabhumoye. 2021. Five sources of bias in natural\nlanguage processing. Linguistics and Language Compass 15, 8 (aug 2021).\nhttps://doi.org/10.1111/lnc3.12432\n[19] Kalervo Järvelin and Jaana Kekäläinen. 2002. Cumulated gain-based evaluation\nof IR techniques. ACM Trans. Inf. Syst. 20, 4 (2002), 422–446.\nhttps://doi.org/10.1145/582415.582418\n[20] Jaap Kamps and David Rau. [n. d.]. University of Amsterdam at TREC 2021:\nDeep Learning Track. https://api.semanticscholar.org/CorpusID:266088115\n[21] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu,\nSergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for\nOpen-Domain Question Answering. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing, EMNLP 2020, Online,\nNovember 16-20, 2020, Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu\n(Eds.). Association for Computational Linguistics, 6769–6781.\nhttps://doi.org/10.18653/V1/2020.EMNLP-MAIN.550\n[22] Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective\nPassage Search via Contextualized Late Interaction over BERT. In Proceedings of\nthe 43rd International ACM SIGIR conference on research and development in\nInformation Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020, Jimmy\nHuang, Yi Chang, Xueqi Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen,\nand Yiqun Liu (Eds.). ACM, 39–48. https://doi.org/10.1145/3397271.3401075\n[23] M.E. Lesk and G. Salton. 1968. Relevance assessments and retrieval system\nevaluation. Information Storage and Retrieval 4, 4 (Dec. 1968), 343–359.\nhttps://doi.org/10.1016/0020-0271(68)90029-6\n[24] Sean MacAvaney, Adam Roegiest, Aldo Lipani, Andrew Parry, Björn Engelmann,\nChristin Katharina Kreutz, Chuan Meng, Erlend Frayling, Eugene Yang,\nFerdinand Schlatt, Guglielmo Faggioli, Harrisen Scells, Iana Atanassova, Jana\nFriese, Janek Bevendorff, Javier Sanz-Cruzado, Johanne Trippas, Kanaad Pathak,\nKaustubh D. Dhole, Leif Azzopardi, Maik Fröbe, Marc Bertin, Nishchal Prasad,\nSaber Zerhoudi, Shuai Wang, Shubham Chatterjee, Thomas Jänich, Udo\nKruschwitz, Xi Wang, and Zijun Long. 2024. Report on the Collab-a-Thon at\nECIR 2024. SIGIR Forum 58, 1 (2024), 1–11.\nhttps://doi.org/10.1145/3687273.3687287\n[25] Stefano Mizzaro. 1997. Relevance: The Whole History. J. Am. Soc. Inf. Sci. 48, 9\n(1997), 810–832.\n[26] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan\nMajumder, and Li Deng. 2016. MS MARCO: A Human Generated MAchine\nReading COmprehension Dataset. In Proceedings of the Workshop on Cognitive\nComputation: Integrating neural and symbolic approaches 2016 co-located with the\n30th Annual Conference on Neural Information Processing Systems (NIPS 2016),\nBarcelona, Spain, December 9, 2016 (CEUR Workshop Proceedings, Vol. 1773),\nTarek Richard Besold, Antoine Bordes, Artur S. d’Avila Garcez, and Greg Wayne\n(Eds.). CEUR-WS.org.\n[27] Rodrigo Nogueira, Zhiying Jiang, Ronak Pradeep, and Jimmy Lin. 2020.\nDocument Ranking with a Pretrained Sequence-to-Sequence Model. In Findings\nof the Association for Computational Linguistics: EMNLP 2020, Trevor Cohn,\nYulan He, and Yang Liu (Eds.). Association for Computational Linguistics,\nOnline, 708–718. https://doi.org/10.18653/v1/2020.findings-emnlp.63\n[28] Arnold Overwijk, Chenyan Xiong, and Jamie Callan. 2022. ClueWeb22: 10\nBillion Web Documents with Rich Information. In SIGIR ’22: The 45th\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval, Madrid, Spain, July 11 - 15, 2022, Enrique Amigó, Pablo Castells, Julio\nGonzalo, Ben Carterette, J. Shane Culpepper, and Gabriella Kazai (Eds.). ACM,\n3360–3362. https://doi.org/10.1145/3477495.3536321\n[29] Andrew Parry, Sean MacAvaney, and Debasis Ganguly. 2024. Top-Down\nPartitioning for Efficient List-Wise Ranking. arXiv:2405.14589 [cs.IR]\nhttps://arxiv.org/abs/2405.14589\n[30] Martin Potthast, Matthias Hagen, Benno Stein, Jan Graßegger, Maximilian\nMichel, Martin Tippmann, and Clement Welsch. 2012. ChatNoir: a search engine\nfor the ClueWeb09 corpus. In The 35th International ACM SIGIR conference on\nresearch and development in Information Retrieval, SIGIR ’12, Portland, OR, USA,\nAugust 12-16, 2012, William R. Hersh, Jamie Callan, Yoelle Maarek, and Mark\nSanderson (Eds.). ACM, 1004. https://doi.org/10.1145/2348283.2348429\n[31] Ronak Pradeep, Yuqi Liu, Xinyu Zhang, Yilin Li, Andrew Yates, and Jimmy Lin.\n2022. Squeezing Water from a Stone: A Bag of Tricks for Further Improving\nCross-Encoder Effectiveness for Reranking. In Advances in Information Retrieval:\n44th European Conference on IR Research, ECIR 2022, Stavanger, Norway, April\n10–14, 2022, Proceedings, Part I (Stavanger, Norway). Springer-Verlag, Berlin,\nHeidelberg, 655–670. https://doi.org/10.1007/978-3-030-99736-6_44\n[32] Ronak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin. 2023. RankZephyr:\nEffective and Robust Zero-Shot Listwise Reranking is a Breeze! CoRR\nabs/2312.02724 (2023). https://doi.org/10.48550/ARXIV.2312.02724\narXiv:2312.02724\n[33] Alan M. Rees and Douglas G. Schultz. 1967. A Field Experimental Approach to the\nStudy of Relevance Assessments in Relation to Document Searching. Final Report to\nthe National Science Foundation. Volume I. Technical Report. Clearinghouse for\nFederal Scientific and Technical Information, Springfield, Va.\n[34] John J. Regazzi. 1988. Performance measures for information retrieval\nsystems—an experimental approach. Journal of the American Society for\nInformation Science 39, 4 (1988), 235–251. https://doi.org/10.1002/(SICI)1097-\n4571(198807)39:4<235::AID-ASI3>3.0.CO;2-H\n[35] Stephen E. Robertson, Steve Walker, Micheline Hancock-Beaulieu, Mike Gatford,\nand A. Payne. 1995. Okapi at TREC-4. In Proceedings of The Fourth Text REtrieval\nConference, TREC 1995, Gaithersburg, Maryland, USA, November 1-3, 1995 (NIST\n\n\nVariations in Relevance Judgments and the Shelf Life of Test Collections\nSIGIR ’25, July 13–18, 2024, Padua, Italy\nSpecial Publication, Vol. 500-236), Donna K. Harman (Ed.). National Institute of\nStandards and Technology (NIST).\nhttp://trec.nist.gov/pubs/trec4/papers/city.ps.gz\n[36] Tetsuya Sakai. 2008. Comparing metrics across TREC and NTCIR: the robustness\nto system bias. In Proceedings of the 17th ACM Conference on Information and\nKnowledge Management, CIKM 2008, Napa Valley, California, USA, October 26-30,\n2008, James G. Shanahan, Sihem Amer-Yahia, Ioana Manolescu, Yi Zhang,\nDavid A. Evans, Aleksander Kolcz, Key-Sun Choi, and Abdur Chowdhury (Eds.).\nACM, 581–590. https://doi.org/10.1145/1458082.1458159\n[37] Tetsuya Sakai. 2019. How to Run an Evaluation Task - With a Primary Focus on\nAd Hoc Information Retrieval. In Information Retrieval Evaluation in a Changing\nWorld - Lessons Learned from 20 Years of CLEF, Nicola Ferro and Carol Peters\n(Eds.). The Information Retrieval Series, Vol. 41. Springer, 71–102.\nhttps://doi.org/10.1007/978-3-030-22948-1_3\n[38] Linda Schamber, Michael B. Eisenberg, and Michael S. Nilan. 1990. A\nRe-Examination of Relevance: Toward a Dynamic, Situational Definition∗.\nInformation Processing & Management 26, 6 (Jan. 1990), 755–776.\nhttps://doi.org/10.1016/0306-4573(90)90050-C\n[39] Ferdinand Schlatt, Maik Fröbe, Harrisen Scells, Shengyao Zhuang, Bevan\nKoopman, Guido Zuccon, Benno Stein, Martin Potthast, and Matthias Hagen.\n2024. Set-Encoder: Permutation-Invariant Inter-Passage Attention for Listwise\nPassage Re-Ranking with Cross-Encoders. arXiv preprint arXiv:2404.06912 (2024).\n[40] Ferdinand Schlatt, Maik Fröbe, Harrisen Scells, Shengyao Zhuang, Bevan\nKoopman, Guido Zuccon, Benno Stein, Martin Potthast, and Matthias Hagen.\n2024. A Systematic Investigation of Distilling Large Language Models into\nCross-Encoders for Passage Re-ranking. arXiv:2405.07920 [cs.IR]\nhttps://arxiv.org/abs/2405.07920\n[41] Tao Shen, Xiubo Geng, Chongyang Tao, Can Xu, Xiaolong Huang, Binxing Jiao,\nLinjun Yang, and Daxin Jiang. 2022. LexMAE: Lexicon-Bottlenecked Pretraining\nfor Large-Scale Retrieval. ArXiv abs/2208.14754 (2022).\nhttps://api.semanticscholar.org/CorpusID:251953412\n[42] Ian Soboroff. 2024. Don’t Use LLMs to Make Relevance Judgments.\narXiv:2409.15133 [cs.IR] https://arxiv.org/abs/2409.15133\n[43] Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin\nChen, Dawei Yin, and Zhaochun Ren. 2023. Is ChatGPT Good at Search?\nInvestigating Large Language Models as Re-Ranking Agents. In Proceedings of\nthe 2023 Conference on Empirical Methods in Natural Language Processing, Houda\nBouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational\nLinguistics, Singapore, 14918–14937.\nhttps://doi.org/10.18653/v1/2023.emnlp-main.923\n[44] Yubao Tang, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Wei Chen, and Xueqi\nCheng. 2024. Listwise Generative Retrieval Models via a Sequential Learning\nProcess. ACM Trans. Inf. Syst. 42, 5 (2024), 133:1–133:31.\nhttps://doi.org/10.1145/3653712\n[45] Mortimer Taube. 1965. A Note on the Pseudo-Mathematics of Relevance.\nAmerican Documentation 16, 2 (1965), 69–72.\nhttps://doi.org/10.1002/asi.5090160204\n[46] Ellen Voorhees. 2000. Variations in Relevance Judgments and the Measurement\nof Retrieval Effectiveness. 36 No. 5 (2000-01-01 2000).\n[47] Ellen M. Voorhees. 1998. Variations in relevance judgments and the\nmeasurement of retrieval effectiveness. In Proceedings of the 21st annual\ninternational ACM SIGIR conference on Research and development in information\nretrieval (SIGIR ’98). Association for Computing Machinery, New York, NY, USA,\n315–323. https://doi.org/10.1145/290941.291017\n[48] Ellen M. Voorhees. 2019. The Evolution of Cranfield. In Information Retrieval\nEvaluation in a Changing World - Lessons Learned from 20 Years of CLEF, Nicola\nFerro and Carol Peters (Eds.). The Information Retrieval Series, Vol. 41. Springer,\n45–69. https://doi.org/10.1007/978-3-030-22948-1_2\n[49] Ellen M. Voorhees, Nick Craswell, and Jimmy Lin. 2022. Too Many Relevants:\nWhither Cranfield Test Collections?. In Proceedings of the 45th International ACM\nSIGIR Conference on Research and Development in Information Retrieval (Madrid,\nSpain) (SIGIR ’22). Association for Computing Machinery, New York, NY, USA,\n2970–2980. https://doi.org/10.1145/3477495.3531728\n[50] Ellen M. Voorhees and Donna K. Harman. 1997. Overview of the Sixth Text\nREtrieval Conference (TREC-6). Inf. Process. Manag. 36 (1997), 3–35.\nhttps://api.semanticscholar.org/CorpusID:9854240\n[51] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang,\nRangan Majumder, and Furu Wei. 2022. SimLM: Pre-training with\nRepresentation Bottleneck for Dense Passage Retrieval. In Annual Meeting of the\nAssociation for Computational Linguistics.\nhttps://api.semanticscholar.org/CorpusID:250311114\n[52] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang,\nRangan Majumder, and Furu Wei. 2023. SimLM: Pre-training with\nRepresentation Bottleneck for Dense Passage Retrieval. In Proceedings of the 61st\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers). Association for Computational Linguistics, Toronto, Canada, 2244–2258.\nhttps://doi.org/10.18653/v1/2023.acl-long.125\n[53] Xing Wu, Guangyuan Ma, Meng Lin, Zijia Lin, Zhongyuan Wang, and Songlin\nHu. 2023. Contextual masked auto-encoder for dense passage retrieval. In\nProceedings of the AAAI Conference on Artificial Intelligence, Vol. 37. 4738–4746.\n[54] Justin Zobel. 1998. How reliable are the results of large-scale information\nretrieval experiments?. In Proceedings of the 21st Annual International ACM SIGIR\nConference on Research and Development in Information Retrieval (Melbourne,\nAustralia) (SIGIR ’98). Association for Computing Machinery, New York, NY,\nUSA, 307–314. https://doi.org/10.1145/290941.291014\n\n\n"}
{"text": "Improving Open-world Continual Learning under the\nConstraints of Scarce Labeled Data\nYujie Li1,2, Xiangkun Wang1, Xin Yang∗1, Marcello Bonsangue2, Junbo Zhang3, Tianrui Li4\n1School of Computer and Artificial Intelligence, Southwestern University of Finance and Economics, China\n2The Leiden Institute of Advanced Computer Science, Leiden University, Netherlands\n3JD Intelligent Cities Research, China\n4School of Computing and Artificial Intelligence, Southwest Jiaotong University, China\n{liyj1201, xiangkunwang18}@gmail.com, yangxin@swufe.edu.cn, m.m.bonsangue@liacs.leidenuniv.nl,\nmsjunbozhang@outlook.com, trli@swjtu.edu.cn\nTask 1\nBase Task\nClass 3\nTask n\nClass i\n. . .\n. . .\nClass 1\nClass 2\n    is [class 1],      is [class 2], . . .,\n and now I can tell       is [class i],\nbut I can't tell what is      .\n    is [class 1],      is [class 2], . . .,\n     is [class 2] and      is [class 3].\n    is [class 1],      is [class 2],\n    is [class 3], but I can't tell what is       . \n    is [class 1],      is [class 2],\n     is [class 1], and        is [class 2].\n. . .\n. . .\n(w/ large labeled data)\nFigure 1: The illustration of open-world continual learning with few-shot data (OFCL) framework. As new tasks (Task 1, ..., Task 𝑛,\netc.) are introduced with new classes (Class 3, ..., Class 𝑖, etc.) in few-shot labeled examples, unknown samples may appear during\ntesting and the learner struggles with severer forgetting and overfitting. Hence, the OFCL requires the agent to detect open samples in\ntesting and learn new categories from scarce labeled data over time.\nABSTRACT\nOpen-world continual learning (OWCL) adapts to sequential tasks\nwith open samples, learning knowledge incrementally while pre-\nventing forgetting. However, existing OWCL still requires a large\namount of labeled data for training, which is often impractical in real-\nworld applications. Given that new categories/entities typically come\nwith limited annotations and are in small quantities, a more realistic\nsituation is OWCL with scarce labeled data, i.e., few-shot training\nsamples. Hence, this paper investigates the problem of open-world\nfew-shot continual learning (OFCL), challenging in (i) learning un-\nbounded tasks without forgetting previous knowledge and avoiding\noverfitting, (ii) constructing compact decision boundaries for open\ndetection with limited labeled data, and (iii) transferring knowledge\nabout knowns and unknowns and even update the unknowns to\nknowns once the labels of open samples are learned. In response, we\npropose a novel OFCL framework that integrates three key compo-\nnents: (1) an instance-wise token augmentation (ITA) that represents\nand enriches sample representations with additional knowledge, (2)\na margin-based open boundary (MOB) that supports open detection\nwith new tasks emerge over time, and (3) an adaptive knowledge\n∗Corresponding author.\nspace (AKS) that endows unknowns with knowledge for the updat-\ning from unknowns to knowns. Finally, extensive experiments show\nthe proposed OFCL framework outperforms all baselines remarkably\nwith practical importance and reproducibility. The source code is\nreleased at https://github.com/liyj1201/OFCL.\nKEYWORDS\nOpen-world Continual Learning, Knowledge Transfer, Continual\nLearning, Scarce Labeled Data, Data Mining\n1\nINTRODUCTION\nRecently, open-world continual learning (OWCL) [15, 21] is an\nemerging machine learning paradigm where a model continuously\nlearns from a sequence of tasks/data in an open environment where\nunseen classes may emerge during tests over time. Unlike tradi-\ntional continual learning (CL) that operates in a closed and fixed set\nof classes (i.e., the closed-world assumption), OWCL requires the\nmodel to not only learn new knowledge and update knowledge incre-\nmentally but also to recognize and adapt to open, previously unseen\nclasses without forgetting [5]. Therefore, OWCL provides a more\ncomprehensive formulation of real-world scenarios by involving the\narXiv:2502.20974v1  [cs.LG]  28 Feb 2025\n\n\nLi and Wang, et al.\npresence of open classes during tests, making it more suitable for\nmanaging the complexities of open data content [19, 21].\nDespite the increasing attention towards OWCL research, existing\nstudies [14, 16] face a major limitation due to their reliance on large\namounts of training data for learning each new task. This requirement\nfor abundant training samples, however, often contradicts the nature\nof the open and dynamic data, where practical applications usually\nencounter new tasks (or classes) with scarce labeled data or few-shot\ntraining samples [2, 38]. For instance, on social media platforms,\nuser-uploaded images (e.g., user-generated photos or social media\nposts) often feature novel objects or scenes that lack corresponding\nlabeled data. Unfortunately, obtaining sufficient manual and ground-\ntruth labels for training each new task or class is often impractical.\nHence, current OWCL models must be extended to operate with\nscarce labeled examples.\nTherefore, motivated by the demands of real-world applications,\nwe investigate the problem of OWCL under the restrictions of scarce\nlabeled data, i.e., Open-world Few-shot Continual Learning (OFCL).\nAs depicted in Figure 1, the scarce labeled data exacerbates the issues\nof misclassifying open samples into seen classes and catastrophic\nforgetting (CF) due to insufficient knowledge representation. Ac-\ncordingly, OFCL faces the following two key challenges: (1) Open\nDetection: The few-shot training/labeled data in OWCL increases\nthe difficulty of establishing robust boundaries for open detection.\n(2) Knowledge Transfer: Given the scarce labeled data, the learner\nnot only needs to transfer knowledge by incrementally acquiring new\nknowledge without CF but also be able to update the knowledge of\nopen samples to known classes once the labels of open samples are\nlearned. Therefore, to address these challenges, we propose a novel\nframework, termed OFCL, designed to tackle open-world continual\nlearning with scarce labeled data.\nSpecifically, we first propose an instance-wise token augmenta-\ntion (ITA) aimed at acquiring additional ‘knowledge’ to mitigate the\ninadequate representation caused by scarce labeled data. Moreover,\nITA can facilitate knowledge transfer by matching learnable tokens\nto each sample embedding. Additionally, given the scarce labeled\ndata where certain exemplar points (hubs) appear among the nearest\nneighbors of many other points, test samples will be assigned to\nit regardless of their true label, resulting in low accuracy [10]. To\nmitigate this, inspired by the embedding representations on a hyper-\nsphere [30], we introduce a novel and compact margin-based open\nboundary (MOB) and an adaptive knowledge space (AKS) consisting\nof learnable hyperspheres, where each hypersphere is characterized\nby a class centroid and an associated radius. In particular, enables\nthe formulation of compact decision boundaries between known and\nunknown samples, thereby enhancing open detection. Simultane-\nously, the AKS encourages the model to learn incrementally from\nunknowns and classify previously encountered unknown samples\nin new tasks, effectively transforming unknowns into knowns over\ntime. Extensive experimental results demonstrate that the proposed\nnovel OFCL framework significantly surpasses all baseline methods,\nhighlighting its practical significance and reproducibility.\nIn summary, our contributions can be outlined as follows:\n• Driven by practical applications, this paper formulates the\nproblem of open-world continual learning with scarce la-\nbeled data and tries to address the challenges of open de-\ntection and knowledge transfer with limited ground-truth\ndata.\n• Technically, we introduce instance-wise token augmenta-\ntions (ITA) for enhancing the semantic information of em-\nbeddings and mitigating forgetting. Subsequently, we pro-\npose a margin-based open boundary (MOB) and an adaptive\nknowledge space (AKS) to incorporate knowledge learned\nfrom knowns and unknowns, fostering knowledge transfer,\naccumulation, and even updating the unknowns.\n• Extensive comparisons with competitive OWCL approaches,\nfew-shot incremental learning methods and open detec-\ntion baselines demonstrate the superior performance of our\nOFCL framework. Additional studies also show the effec-\ntiveness and robustness of the proposed OFCL framework.\n2\nRELATED WORK\n2.1\nOpen-world Continual Learning (OWCL)\nIn contrast to the closed-world assumption, open-world continual\nlearning aims to reject examples from unseen classes (not appearing\nin training) and incrementally learn the open/unseen classes [5, 14],\nsimilar to learning in the real world that is full of unknowns or novel\nobjects [20, 21]. The innovative framework SOLA [21] enables AI\nagents to learn and adapt by themselves via their interactions with\nhumans and the open environment. PointCLIP [45] combines CLIP\nand GPT-3 to enable zero-shot classification, segmentation, and\nopen-set detection. Zhao et al. [42] introduce an Open World Object\nDetection (OWOD) framework containing an auxiliary proposal ad-\nvisor and a Class-specific Expelling Classifier (CEC). [16] proposes\nto integrate out-of-distribution detection with continual learning for\nopen-world continual learning.\nHowever, as mentioned in section 1, more and more real-world\nscenarios pose a greater challenge due to the limitation of extremely\nscarce training/labeled samples. Consequently, addressing open-\nworld learning in scarce labeled data becomes not only highly prac-\ntical but also increasingly challenging.\nCL Baselines\nOpen-world Assumption\nFew-shot Data\nTOPIC [29]\n✗\n✓\nALICE [24]\n✗\n✓\nSoftNet [38]\n✗\n✓\nSOLA [21]\n✓\n✗\nCEC [42]\n✓\n✗\nPro-KT [19]\n✓\n✗\nFeSSSS [3]\n✓\n✓\nTable 1: Competitive CL Baselines (selected) under Open-World\nAssumption and Few-Shot Data Constraints.\n\n\nImproving Open-world Continual Learning under the Constraints of Scarce Labeled Data\n2.2\nFew-shot Learning\nFew-shot learning [24, 29, 43] aims to learn new tasks with limited\ntraining data while avoiding the loss of previously learned informa-\ntion. TOPIC [29] introduces the few-shot learning with continual\nlearning benchmark setup and proposes a neural gas structure to\nbalance between the past and the current tasks. SvF [41] employs\nfrequency-aware regularization and feature space composition for\na balanced approach. CEC [40] decoupled feature extraction and\nclassification through a pre-trained backbone and a non-parametric\nclass mean classifier. SoftNet [38] utilizes a partial-frozen strategy,\nselectively masking a sub-network from a pre-trained network to\nprevent forgetting.\nWhile current methods yield satisfactory results, they remain\nrooted in the closed-world assumption, lacking the capability to iden-\ntify and address open/unknown samples during test phases [18, 39].\nFurthermore, there is a lack of methods for constructing a compre-\nhensive knowledge space that facilitates knowledge accumulation,\ntransfer, and updating throughout the incremental learning process.\nHence, given the open-world assumption, existing methods suffer\neven more from overfitting and CF, often leading to misinterpretation\nor revision of incorrect knowledge.\nMore recently (as shown in Table 1), [3, 8] tried to rethink ex-\nisting few-shot incremental learning under the open-set hypothesis.\nFeSSSS [2, 3] extended a variable-few-shot learning model to the\nopen world, detecting unknown samples by a simple Softmax thresh-\nolding approach (MSP [12]). Hyper-RPL [8] utilized the hyperbolic\nreciprocal point learning into a distillation-based framework to alle-\nviate the overfitting and forgetting. However, although both works\nmade attempts, they merely combined existing few-shot incremental\nlearning with simple open-set recognition methods, overlooking the\nchallenges posed by scarce labeled data in OWCL and facing signif-\nicant limitations in constructing compact open-set boundaries and\nadapting to dynamic environments.\n3\nMETHODOLOGY\nIn this section, we begin by formulating the OFCL problem. Then,\nwe detail the instance-wise token augmentation (ITA) for knowledge\ntransfer and present margin-based open boundary (MOB). Lastly, we\nintroduce an adaptive knowledge base (AKB) designed to represent\nknowledge from unknowns and to facilitate updating unknowns into\nknowns in a hypersphere-based embedding space.\nRemarks. Different from existing works, this paper addresses the\nlimitations of OWCL by focusing on a more practical and challeng-\ning few-shot learning scenario, introducing an innovative concept\nof OFCL. By integrating previous prompt-based learning strategies,\nour model can effectively adapt to limited labeled samples by ITA.\nMoreover, MOB leverages hyperspherical embeddings to construct\nmargin-based boundaries adaptively, ensuring compact and robust\nrepresentations for open detection. AKS, on the other hand, pio-\nneers an adaptive way to knowledge accumulation, transfer and re-\nplay, facilitating efficient knowledge utilization for both known and\nunknown samples, overcoming the limitations in previous OWCL\nmethods (e.g., logits-based thresholds in Pro-KT [19]).\n3.1\nProblem Statement\nOur investigated problem, Open-world few-shot continual learning\n(OFCL), aims to incrementally learn within an open-world assump-\ntion where unseen or open samples may appear in the test phase, and\ngeneralize well on new tasks with scarce labeled training data. It is\nimportant to note that we assume only the base task to have ample\ntraining samples (as shown in Figure 1), while each subsequent new\ntask (𝑡> 0) only involves scarce labeled training samples.\nProblem Definition. Given tasks {1, ...,𝑡, ...}, each training set\ncan be organized as 𝑁-way 𝐾-shot format 𝐷𝑡\n𝑡𝑟= {(𝑥𝑡,𝐶) | 𝑥𝑡∈\n𝑋𝑡\n𝑡𝑟(𝐶),𝐶∈𝐶𝐿𝑆𝑡\n𝑡𝑟}, where 𝑋𝑡\n𝑡𝑟(𝐶) is a set of 𝐾-samples of the class\n𝐶, 𝐶𝐿𝑆𝑡\n𝑡𝑟is the set of 𝑁classes in the current task 𝑡. Similarly, the\ntest set of task 𝑡can be defined as 𝐷𝑡\n𝑡𝑒= {(𝑥𝑡,𝐶) | 𝑥𝑡∈𝑋𝑡\n𝑡𝑒(𝐶),𝐶∈\n𝐶𝐿𝑆𝑡\n𝑡𝑒}. Here, due to the open-world assumption, we do not assume\nany ‘relations’ between the classes in the test set and the training set.\nSpecifically, some test samples may be unknown/open, i.e. belong-\ning to the set (𝐶𝐿𝑆𝑡\n𝑡𝑒−𝐶𝐿𝑆𝑡\n𝑡𝑟). In testing, we evaluate on all the test\nsamples 𝐷1\n𝑡𝑒∪... ∪𝐷𝑡\n𝑡𝑒continually. Hence, the purpose of OFCL is\nto develop a unified classifier 𝑓𝜃with parameter 𝜃that (1) detects un-\nknown/open samples, (2) classifies known samples correctly, and (3)\nsupports the transforming of unknowns into knowns incrementally.\n3.2\nInstance-wise Token Augmentation (ITA)\nInspired by the strong few-shot learning ability of prompt tuning [25,\n35, 36] methods, we propose a knowledge augmentation paradigm\nfor OFCL, referred to as Instance-wise Token Augmentation (ITA).\nNotably, ITA can facilitate the representation, accumulation, and\ntransfer of knowledge across various tasks to improve the knowledge\ntransfer.\nGiven a task 𝑡with its training set 𝐷𝑡\n𝑡𝑟, the learner first initializes\na batch of random tokens p𝑡as:\np𝑡= (𝑝𝑡\n1, ..., 𝑝𝑡\n𝑖, ..., 𝑝𝑡\n𝑙), with 𝑝𝑡\n𝑖∈R𝐿𝑝×𝐷𝑒.\n(1)\nwhere 𝑙is the number of additional tokens, 𝐿𝑃is the token length\nand 𝐷𝑒is the embedding dimension. During the training of task\n𝑡, we maintain a token frequency set 𝜈𝑡= (𝜈1, ...,𝜈𝑘, ...), where 𝜈𝑘\nrepresents the frequency of each 𝑝𝑡\n𝑘is selected to be appended into\nsample embeddings.\nHence, the essence of ITA lies in identifying and matching impor-\ntant tokens for sample augmentations. Here, we design an instance-\nwise mechanism to select useful additional tokens by looking up the\ntop-𝐾keys:\n(k∗, p∗) = argmin\n𝐾\n𝑙∑︁\n𝑖=1\n𝑠𝑖𝑚(h𝑡,𝑘𝑡\n𝑖) · 𝜈𝑖, with h𝑡= Q(𝑥𝑡),\n(2)\nwhere 𝑘𝑡\n𝑖is the key associated with token 𝑝𝑡\n𝑖, the 𝑠𝑖𝑚(·, ·) is a cosine\nsimilarity function and the Q is a query function encoding an input\n𝑥𝑡to the same dimension as the keys. Next, the input embedding h𝑡\nis augmented by the above subset p∗⊂pt:\nh′𝑡= h𝑡⊕p∗,\n(3)\nwhere ⊕denotes a dimension-wise concatenation.\nSubsequently, the improved loss function can be formulated as:\nL𝑎𝑢𝑔= L(𝑓𝜃(𝑓𝑝𝑟(h′𝑡), y𝑡) + 𝜆·\n𝑙∑︁\n𝑖=1\n𝑠𝑖𝑚(h𝑡,𝑘𝑡\n𝑖),\n(4)\n\n\nLi and Wang, et al.\n...\n...\nTask t\nITA (Sec. 3.2)\nCNN\n...\n...\n...\nBase Task\nTask 1\n Transformer\n...\n...\n...\nRepresentation\nAugmented Token\nQuery Token\n...\nQuery\nLoss\nCE\nLoss\nFew-shot labeled training samples\nmargin\nPrototype-based Loss\nMOB (Sec. 3.3)\nMoB\nLoss\nR\nOurs\nR\nless positive samples, more negative samples:\n...\nR\nR\nHyperspheres Pool\nR\n...\n...\n...\nAugmented\nAKS (Sec. 3.4)\n...\n...\nOpen Internet test samples\n...\nunknown\nknown\nOutput\nFigure 2: The overall OFCL framework consists of three key components: (1) ITA (Yellow): enhancing the samples by matching them\nwith appropriate additional tokens; (2) MOB (Purple): constructing compact decision boundaries of knowns by margin-based loss\nfunction for open detection; (3) AKS (Red): incorporating knowledge learned from both knowns and unknowns, and facilitating the\ntransition from unknowns to knowns.\nwhere h𝑡is the set of training samples projected embeddings, y𝑡are\nthe training labels, 𝜆is a trade-off parameter, 𝑓𝑝𝑟is the pre-trained\nbackbone. The first addend is the classification loss, and the second\none is a surrogate loss to pull selected keys closer to corresponding\nquery features.\nDifferent from previous prompt-based CL methods, our proposed\nITA stores the tokens learned from each task into a unified token\nbank (K, P), treating them as knowledge to avoid forgetting. During\ntesting, the model selects appropriate tokens from the token bank\n(K, P) for each test sample. Hence, the proposed ITA not only en-\nables the model to leverage prior knowledge but also mitigates the\nissue of overfitting with scarce labeled samples.\n3.3\nMargin-based Open Boundary (MOB)\nBy leveraging the augmented training samples, we facilitate the\nformation of more precise and compact decision boundaries for open\nsample detection. In contrast to existing approaches, our strategy\ngoes beyond simply merging current continual learning techniques\nwith open-set recognition.\nThus, we introduce the MOB in a hypersphere space to construct\ncompact boundaries from known samples or training classes, thereby\nnaturally improving the open detection for OFCL. Hyperspheres out-\nperform Euclidean distance in open-set recognition by reducing the\nhubness problem [30], where certain points disproportionately dom-\ninate nearest-neighbor lists, leading to misclassifications. In OFCL,\nwhere new, unseen classes may appear, the ability of hyperspherical\nembeddings to adapt is critical. These embeddings can continuously\nevolve as new data points are added, unlike Euclidean embeddings\nwhich can become skewed by hubs. Hyperspherical embeddings\nmaintain robustness in dynamic, low-sample environments, improv-\ning their suitability for open-set recognition.\nGiven a class 𝐶∈𝐶𝐿𝑆𝑡\n𝑡𝑟, we divide all samples into a positive set\nx+ = {𝑥𝑡|(𝑥𝑡,𝐶) ∈𝐷𝑡\n𝑡𝑟} containing all sample of the class 𝐶at task\n𝑡, and a negative set x−= {𝑥𝑡|(𝑥𝑡,𝐶′) ∈𝐷𝑡\n𝑡𝑟,𝐶′ ≠𝐶} containing\nall other samples of the same task. Our intuition here comes from\ncontrastive learning, where training samples belonging to 𝐶serve\nas positive samples, while all other training samples are treated as\nnegative instances (in Figure 2). Hence, the MOB loss function\nL𝑚𝑎𝑟𝑔𝑖𝑛for constructing decision boundaries of knowns can be\nformulated as:\nL𝑚𝑎𝑟𝑔𝑖𝑛= 1\n𝑁·\n∑︁\n𝐶∈𝐶𝐿𝑆𝑡𝑟\n(𝜆· 𝑟2\n𝐶\n+ 1\n𝛼· 𝑙𝑜𝑔[1 +\n∑︁\n𝑥∈x+\n𝑒𝛼·(𝑑𝑖𝑠(c𝐶,𝑓𝑝𝑟(𝑥))−𝑟𝐶)]\n+ 1\n𝛽· 𝑙𝑜𝑔[1 +\n∑︁\n𝑥∈x−\n𝑒−𝛽·(𝑑𝑖𝑠(c𝐶,𝑓𝑝𝑟(𝑥))−(𝑟𝐶+𝑚))]),\n(5)\nwhere 𝑁is the number of classes in the training set and 𝑚is the\nmargin, 𝑟𝐶denotes the hypersphere radius of class 𝐶around the\ncentroid c𝐶that we discuss below, 𝑑𝑖𝑠(·, ·) is a distance function,\n𝜆is a constant that balances the trade-off between the regulation\nof radius and margin loss item, and 𝛼and 𝛽are scaling factors for\npositive and negative sets, respectively. Note that the pre-trained\nbackbone function 𝑓𝑝𝑟here takes as input a sample instead of the\nembedding of an input as in Equation 4.\n\n\nImproving Open-world Continual Learning under the Constraints of Scarce Labeled Data\nSpecifically, we employ three key components in the MOB:\n(1) known classes centroids: learning a centroid as the center of\na hypersphere for each known class; (2) margin and radius: enforc-\ning a margin between different classes while automatically learning\na radius for each class; (3) open boundary: identifying unknowns re-\nlying on the establishment of compact boundaries for known classes.\nKnown Classes Centroids. Hyperspherical embeddings distrib-\nute data more uniformly, avoiding central clustering, and methods\nlike noHub [30] preserve class structure while maintaining unifor-\nmity. This balance allows hyperspheres to continuously adapt in\ndynamic, low-sample environments, improving accuracy in open-set\nscenarios. Hence, we define an embedding space full of hyperspheres\nwhere points cluster around a single centroid representation for each\nclass.\nGiven a class 𝐶from task 𝑡, its centroid c𝐶is:\nc𝐶=\n1\n|x+|\n∑︁\n𝑥𝑡∈x+\n𝑓𝑝𝑟(𝑥𝑡).\n(6)\nNotable, during the training process of {1, ...,𝑡, ...}, the model\nstores all learned centroids incrementally. Each centroid is treated\nas the center of a hypersphere, and thus next, an appropriate radius\nneeds to be determined for establishing compact boundaries between\nknowns and unknowns.\nMargin and Radius. Given a class 𝐶with few-shot training\nsamples, the number of negative samples |x−| is typically much\nlarger than the number of positive samples |x+|. To preserve distinct\ndistributions for different classes, we constrain the distance between\nthe class centroid c𝐶and every positive sample 𝑥∈x+ to be less\nthan a learnable radius:\n𝑑𝑖𝑠(c𝐶, 𝑓𝑝𝑟(𝑥)) < 𝑟𝐶,\n(7)\nwhere the radius 𝑟𝐶is optimized by Equation 5. Accordingly, for\nevery negative sample 𝑥∈x−of class 𝐶:\n𝑑𝑖𝑠(c𝐶, 𝑓𝑝𝑟(𝑥)) > 𝑟𝐶+ 𝑚,\n(8)\nwhere 𝑚is the margin.\nHence, the hypersphere of class 𝐶is constructed by optimizing\nthe radius 𝑟𝐶as:\n𝑟𝐶= 𝑞𝜎({𝑑𝑖𝑠(c𝐶, 𝑓𝑝𝑟(𝑥)) −𝑚| 𝑥∈x−}),\n(9)\nwhere 𝑞𝜎(·) is a quantile function and 𝜎serves as the constraint\ngoverning deviations. Each hypersphere learned from task 𝑡is for-\nmulated as:\n(𝐶𝐿𝑆𝑡\n𝑡𝑟, C𝑡, R𝑡) = {(𝐶𝑡\n1, c𝐶𝑡\n1,𝑟𝐶𝑡\n1 ), ..., (𝐶𝑡\n𝑁, c𝐶𝑡\n𝑁,𝑟𝐶𝑡\n𝑁)},\nwhere 𝑁is the number of classes in 𝐶𝐿𝑆𝑡\n𝑡𝑟.\nDuring the training process of {1, ...,𝑡, ...}, the model stores all\nlearned hyperspheres together:\n(𝐶𝐿𝑆𝑡𝑟, C, R) = {(𝐶𝐿𝑆1\n𝑡𝑟, C1, R1), ..., (𝐶𝐿𝑆𝑡\n𝑡𝑟, C𝑡, R𝑡), ...}.\nOpen Detection. In testing phrases, given a test sample 𝑥from\nan arbitrary task, we initially project 𝑥into all learned hyperspheres.\nSubsequently, to determine whether the test sample 𝑥is unknown,\nwe check the inclusion of 𝑥in all hyperspheres: If there is a class 𝐶\nsuch that the sample is contained within the hypersphere of 𝐶then 𝑥\nis classified within 𝐶, otherwise, 𝑥is identified as unknown.\nThen, the open detection procedure at task 𝑡is:\nStep 1: Identify the centroid c∗\n𝐶closest to 𝑥;\nStep 2: Calculate the distance between the selected centroid c∗\n𝐶\nand the test sample 𝑥;\nStep 3: If 𝑑∗≤𝑟∗, then 𝑥belongs to class 𝐶otherwise it is\nclassified as unknown.\nIn MOB, we treat the hypersphere centers as learnable parameters,\ndynamically updating them based on deep feature representations\nduring continual learning. Our method shares similarities with deep\ndistance metric classifiers that employ margin-based loss functions.\nMoreover, the prototypes are set to hypersphere centers, which are\nadaptively updated throughout the learning process, i.e., in the adap-\ntive knowledge space (AKS).\n3.4\nAdaptive Knowledge Space (AKS)\nConsidering (𝐶𝐿𝑆𝑡𝑟, C, R) as the knowledge learned from knowns,\nwe present an Adaptive Knowledge Space (AKS), designed not\nonly to store and transfer knowledge about unknowns acquired from\npreviously learned tasks, but also to facilitate updating unknown\nknowledge into known knowledge during OFCL.\nUnknown Knowledge Representation. After identifying unknowns,\nwe then employ clustering on the identified unknown samples as\nfollows: Given a detected unknown sample 𝑥let Γ𝜖(𝑥) represent\nthe 𝜖-neighborhood of 𝑥, 𝑀𝑖𝑛𝑃𝑡𝑠denotes the minimum number of\nobjects in Γ𝜖(𝑥), and 𝜌(𝑥) = |Γ𝜖(𝑥)| be its density value. We define\nthe boolean function 𝑇(𝑥) = 1 if 𝜌(𝑥𝑗) ≥𝑀𝑖𝑛𝑃𝑡𝑠and 𝑇(𝑥) = 0\notherwise. If 𝑇(𝑥) = 1, the unknown sample 𝑥is clustered with\ndensity-reachable points, and the entire space is partitioned into 𝑀\ngroups {𝐺1,𝐺2, ...,𝐺𝑀}. Otherwise, 𝑥is treated as a noise. Each\ngroup is then assigned a group centroid, which is determined using\nEquation 6. The corresponding radius is obtained by Equation 9.\nSince there are no ground-truth labels for unknowns, we generate\na set of pseudo-labels and assign them to each group. Similar to\nhyperspheres learned from known samples, we have :\n(𝐶𝐿𝑆𝑡\n𝑜𝑝𝑒𝑛, C𝑡, R𝑡) = {(𝐶𝑡\n1, c𝐶𝑡\n1,𝑟𝐶𝑡\n1 ), ..., (𝐶𝑡\n𝑀, c𝐶𝑡\n𝑀\n,𝑟𝐶𝑡\n𝑀\n)}.\nUpdating Unknowns to Knowns. At task 𝑡the integrated adaptive\nknowledge space consists of all the learned hyperspheres during\ntraining (𝐶𝐿𝑆𝑡𝑟, C, R) together with all the new (𝐶𝐿𝑆𝑡𝑜𝑝𝑒𝑛, C𝑡, R𝑡)\nassociated with the unknowns at all task until 𝑡. Because of this\nincremental growth, if previously encountered unknowns reappear,\nthey are classified with corresponding pseudo-labels. However, if a\nnew hypersphere learned during a subsequent task’s training overlaps\nwith an existing hypersphere with a pseudo-label, then we integrate\nthe corresponding pseudo-category into the newly trained categories,\nthus facilitating a transition from unknowns to knowns.\n3.5\nOverall Objective\nAt any task 𝑡, OFCL first obtains 𝐿-additional tokens via L𝑎𝑢𝑔and\nstores all the newly learned additional tokens together with those\npreviously learned. Subsequently, each sample is augmented by\nadditional knowledge, and an adaptive knowledge space contain-\ning knowledge learned from knowns and unknowns is constructed.\nHence, the overall objective is:\nmin\n(C𝑡,R𝑡),𝜃,(k𝑡,p𝑡)𝛾· L𝑚𝑎𝑟𝑔𝑖𝑛+ (1 −𝛾) · L𝑎𝑢𝑔,\n(10)\nwhere the 𝛾is a balance between L𝑚𝑎𝑟𝑔𝑖𝑛and L𝑎𝑔𝑢.\n\n\nLi and Wang, et al.\nAlgorithm 1 : Training Process of the OFCL framework\nInput: the training set 𝐷𝑡\n𝑡𝑟= {(X𝑡, Y𝑡)}, the set of training classes\n𝐶𝐿𝑆𝑡\n𝑡𝑟, the token bank (K, P) = {(k𝑡, p𝑡)}𝑇\n𝑡=1, a trainable classifier\n𝑓𝜃, a backbone 𝑓𝑝𝑟and the hyperparameter balance 𝛾.\nInitialize: (K, P), 𝑓𝜃.\n1: for t = 1, 2, ..., 𝑇do\n2:\nfor each epoch do\n3:\nfor (x𝑡, y𝑡) in each training set 𝐷𝑡𝑟\n𝑡with class 𝐶𝐿𝑆𝑡\n𝑡𝑟do\n4:\nMap x𝑛to h𝑡;\n5:\nGet the similarity of ℎ𝑡and each key of (k𝑡, p𝑡);\n6:\nSelect augmentation tokens with (k∗, p∗) Equation 2;\n7:\nEnhance h𝑡to h′𝑡with p∗via Equation 3;\n8:\nCalculate the augmented feature by 𝑓𝑝𝑟(h′𝑡);\n9:\nCalculate c𝐶for each class 𝐶in 𝐶𝐿𝑆𝑡\n𝑡𝑟via Equation 6;\n10:\nObtain positive and negative set of class 𝐶: x+, x−;\n11:\nObtain margin and radius via Equation 9\n12:\nCalculate the overall loss via Equation 10;\n13:\nend for\n14:\nUpdate 𝑓𝜃, (K, P), the radius 𝑟𝐶and margin 𝑚by backprop-\nagation;\n15:\nend for\n16:\nUpdate the overall AKS with 1) Hyperspheres learned from\n𝐷𝑡\n𝑡𝑟: (𝐶𝐿𝑆𝑡\n𝑡𝑟, C𝑡, R𝑡) and 2) the Hyperspheres learned from\nunknowns: (𝐶𝐿𝑆𝑡𝑜𝑝𝑒𝑛, C𝑡, R𝑡).\n17: end for\nTo better illustrate the proposed OFCL framework, we detail the\ntraining process in Algorithm 1. First, the token bank and classifier\nare initialized. Then, for each time step 𝑡, multiple training epochs\nare conducted, where each sample (x𝑡, y𝑡) undergoes feature map-\nping, similarity computation with stored keys, augmentation via\ntoken selection, and enhancement through the backbone network.\nClass-wise centroids, positive and negative sets, margins, and radii\nare computed to derive the overall loss, which is minimized via\nbackpropagation. Finally, the AKS is updated with hyperspheres rep-\nresenting both known and unknown knowledge, enabling continual\nadaptation to evolving data distributions.\nAccordingly, after learning task t, the model follows four steps\nduring inference:\nStep 1: Integrate new task knowledge (i.e., knowledge from la-\nbeled samples) into the current knowledge space, updating unknowns\nto knowns once the labels for open samples are learned.\nStep 2: During testing, detect open samples and ensure they are\nnot misclassified as known categories.\nStep 3: For samples identified as knowns, classify them correctly\nwhile avoiding forgetting of previous tasks.\nStep 4: Update the knowledge space adaptively by incorporating\nthe new knowledge from the open sample into the current knowledge\nspace.\nIn summary, our OFCL framework not only enhances the rep-\nresentation of known classes but also improves decision boundary\ncompactness for open detection. This integration demonstrates our\ninnovative development of existing techniques to address the unique\nchallenges of OFCL.\n4\nEXPERIMENTS AND RESULTS\n4.1\nExperimental Setup\nDatasets. We follow the dataset configuration in TOPIC [29]: (1)\nThe CUB200 dataset [32] encompasses 11,788 images depicting\n200 distinct bird species. We partition the 200 classes into 100\nbase classes for task 0 and 100 incremental classes for 10 tasks.\nEach task consists of 10 classes, with each class having 5 training\nsamples (i.e., a 10-way 5-shot setup). (2) The MiniImageNet dataset\n[31] comprises 100 classes with a total of 60,000 RGB images. We\npartition the 100 classes into 60 base classes for an initial base task\nand 40 incremental classes for 8 tasks. Each task consists of 5 classes,\neach with 5 training samples (i.e., a 5-way 5-shot configuration).\nImplementation Details. (1) We randomly shuffled the order of\ntasks in experiments. All results are the averages obtained from\nthree random shuffles. (2) Due to the current absence of available\ncomparative baselines for OFCL, we divided our experiments into\ntwo parts for clear and fair comparisons: unknown detection and\nknown classifications. (3) All baselines are trained by the Adam\noptimizer with a batch size of 25 and a learning rate of 0.03.\nBaselines. To ensure comprehensive and fair comparisons, we\nconsider two parts in the evaluation:\nUnknown Detection: we compare our OFCL against 8 com-\npetitive open detection baselines: MSP [12], KL [11], SSD [27],\nMaxLogit [4], Energy [22], ViM [33], KNN-based OOD [28], NNGuide\n[23].\nKnown Classification: we evaluate OFCL against 15 benchmark\ncontinual learning methods with scarce labeled data: iCaRL [26],\nTOPIC [29], CEC [40], Meta-FSCIL [7], ALICE [24], FeSSSS [2],\nPro-KT [19], LIMIT [44], NC-FSCIL [37], MCNet [13], SoftNet\n[38], CoSR [34], M-FSCIL [17], FSIL-GAN [1], and CPE-CLIP [9].\nMetrics. For unknown detection, we use the average area under\nthe ROC across all learned 𝑁-tasks 𝐴𝑈𝐶𝑁and the average false\npositive rate across 𝑁-tasks 𝐹𝑃𝑅𝑁as metrics [6]. For known clas-\nsification, we apply averaged accuracy 𝐴𝐶𝐶𝑁as the metric, where\n𝐴𝐶𝐶𝑁is calculated over 𝑁new tasks. To measure forgetting, we\ncalculate the performance dropping rate PD = 𝐴𝐶𝐶0 −𝐴𝐶𝐶𝑁, where\n𝐴𝐶𝐶0 is the classification accuracy in the base task [19].\nResults on Unknown Detection. Table 2 presents the results of\nunknown detection. Notably, the baselines exhibit poor performance\nunder the OFCL setting, underscoring their difficulty in adapting to\nreal-world scenarios.\nIn contrast, OFCL consistently outperforms all compared methods\nacross various configurations, as indicated by its superior 𝐴𝑈𝐶𝑁and\n𝐹𝑃𝑅𝑁values. This consistent advantage underscores the robustness\nand adaptability of OFCL in handling diverse learning conditions.\nThe ability of OFCL to delineate precise decision boundaries and\nestablish an adaptive knowledge space further highlights the effec-\ntiveness of our proposed approach in addressing complex knowledge\nboundaries. By dynamically integrating open-set detection with con-\ntinual learning, OFCL effectively mitigates the challenges posed\nby evolving knowledge distributions. Overall, OFCL demonstrates\nsignificant improvements demonstrated by OFCL in open detection\nperformance within real-world scenarios characterized by knowl-\nedge boundaries.\n\n\nImproving Open-world Continual Learning under the Constraints of Scarce Labeled Data\nDataset\nMethods\n𝐴𝑈𝐶𝑁↑\n𝐹𝑃𝑅𝑁↓\nCUB200\n(10-way 5-shot)\nMSP\n61.34\n92.95\nKL\n60.81\n93.10\nSSD\n37.70\n99.52\nMaxLogits\n61.31\n92.43\nEnergy\n60.81\n93.10\nViM\n54.16\n97.91\nKNN-based OOD\n37.20\n99.16\nNNGuide\n50.26\n96.97\nOFCL\n69.72\n69.96\nMiniImageNet\n(5-way 5-shot)\nMSP\n49.78\n94.91\nKL\n46.61\n95.10\nSSD\n50.64\n95.18\nMaxLogits\n46.78\n94.65\nEnergy\n46.61\n95.10\nViM\n59.56\n89.87\nKNN-based OOD\n52.88\n93.78\nNNGuide\n52.47\n90.95\nOFCL\n76.20\n71.30\nTable 2: Results(%) regarding unknown detection. We report the\nresults over 10 tasks for CUB200 (10-way 5-shot) and 8 tasks for\nMiniImageNet (5-way 5-shot).\nFigure 3: Visualizations of Known Classification.\nResults on Known Classification. In this experiment, we evaluate\nthe model’s performance on known classification against various\ncompetitive benchmarks under a same setting. Table 3 and Table 4\npresent the accuracy scores achieved by different methods on the\nCUB200 and MiniImageNet datasets, respectively.\nDespite the challenges of the OFCL problem, the proposed frame-\nwork consistently outperforms other baselines, demonstrating lower\ndrop rates and higher final accuracy across all classes. These results\nhighlight the effectiveness of instance-wise token augmentations in\nmitigating catastrophic forgetting and overfitting. Furthermore, as\nshown in Figure 3, the growing performance advantage of OFCL\nover time suggests superior adaptability in long-term learning sce-\nnarios compared to existing baselines.\nNotable, on the MiniImageNet dataset, CPE-CLIP achieves a PD\nscore of 7.46, outperforming ours. CPE-CLIP enhances CLIP’s pre-\ntraining mechanism, improving performance across various cross-\nmodal tasks (e.g., image retrieval, text generation). Its pre-training\ndataset overlaps with MiniImageNet, which may contribute to its\ncompetitive PD score. Additionally, in the base task, our model\nachieves an 𝐴𝐶𝐶𝑁of 96, compared to 90 for CPE-CLIP, further\ncontributing to the higher PD score. Hence, these results further\nunderscore the robustness of our OFCL in CL scenarios.\n4.2\nAblation Studies\nWe conduct ablation studies, analyzing the impact of three key com-\nponents: 1) the additional tokens selection mechanism in ITA, 2)\nthe whole instance-wise token augmentation, and 3) the whole AKS.\nThe average final accuracy 𝐴𝐶𝐶𝑁represents the mean accuracy over\nthe past 𝑁tasks, and results are presented in Table 5.\nFirst, removing the additional token selection mechanism in ITA\nleads to a noticeable decline in both known classification and un-\nknown detection performance. This highlights its critical role in\nmitigating overfitting at knowledge boundaries and enhancing open-\nset detection. Second, eliminating ITA results in the most significant\ndegradation in known classification, underscoring its effectiveness\nin reducing overfitting and addressing CF. Finally, removing the en-\ntire AKS module completely disables open-set detection. As shown\nin Table 5, this also degrades knowns classification performance,\nindicating that the softmax mechanism alone is insufficient for dis-\ntinguishing between known classes.\nMoreover, our method is highly decoupled from specific back-\nbone architectures, as it operates at the input and output levels,\nensuring broad adaptability. To validate this flexibility, we con-\nducted an ablation study using ViT, a widely adopted large-scale\nimage classification model, as the backbone with fine-tuning. On\nthe CUB200 dataset, OFCL achieves a significantly higher known\nclassification accuracy (63.80%) compared to ViT with fine-tuning\nalone (21.19%). This substantial performance gap demonstrates that\nstrong representational capacity alone is insufficient, highlighting the\neffectiveness of our framework in facilitating structured knowledge\ntransfer and open-set adaptation.\n4.3\nAdditional Results\nTime and Space Complexity. As new tasks continually emerge,\nthe complexity of AKS scales linearly with O(𝑛). For the CUB200,\nafter training all tasks, the final AKS only occupies 0.048 MB. As\nfor the MiniImageNet, the final AKS requires 0.024 MB. The lookup\ntime for instance-wise tokens matching is 0.001 seconds, and the\ndecision time is less than 0.01 seconds.\n1\n5\n10\n20\n25\nAdditional token size\n1\n5\n10\n20\n25\nToken length\n56.93\n67.89\n68.32\n69.39\n67.01\n64.66\n67.52\n67.84\n68.45\n68.26\n67.9\n69.76\n68.08\n67.47\n65.36\n67.32\n68.97\n63.3\n62.67\n61.29\n67.5\n67.68\n66.35\n60.66\n60.62\n(a)\n5\n10\n15\n20\n25\n30\nTotal # of additional tokens for each task\n55\n60\n65\n70\n75\n80\nACCN\nAUCN\nFPRN\n(b)\nFigure 4: (a): 𝐴𝐶𝐶𝑁w.r.t token length 𝐿𝑃and additional token\nsize 𝐾, given 𝑙= 25. (b): 𝐴𝐶𝐶𝑁w.r.t. 𝑙(i.e., the total number\nof additional tokens of each task) with 𝐿𝑃= 5 and 𝐾= 5 (take\nCUB200 for illustration).\nParameter Sensitivity Analysis. Consider the crucial hyperpa-\nrameters, namely (1) the margin 𝑚and the constraint governing\ndeviations 𝜎(in Equation 9), and (2) the token length 𝐿𝑃, additional\ntoken size 𝐾, and the amount of additional tokens per task 𝑙.\n\n\nLi and Wang, et al.\nMethods\n𝐴𝐶𝐶𝑁in each task (%) ↑\nPD↓\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\niCaRL\n68.68\n52.65\n48.61\n44.16\n36.62\n29.52\n27.83\n26.26\n24.01\n23.89\n21.16\n47.52\nTOPIC\n68.68\n62.49\n54.81\n49.99\n45.25\n41.40\n38.35\n35.36\n32.22\n28.31\n26.28\n42.40\nCEC\n75.85\n71.94\n68.50\n63.50\n62.43\n58.27\n57.73\n55.81\n54.83\n53.52\n52.28\n23.57\nMeta-FC\n75.90\n72.41\n68.78\n64.78\n62.96\n59.99\n58.30\n56.85\n54.78\n53.82\n52.64\n23.26\nALICE\n77.40\n72.70\n70.60\n67.20\n65.90\n63.40\n62.90\n61.90\n60.50\n60.60\n60.10\n17.30\nFeSSSS\n79.60\n73.46\n70.32\n66.38\n63.97\n59.63\n58.19\n57.56\n55.01\n54.31\n52.98\n26.62\nPro-KT\n82.90\n74.15\n72.82\n62.46\n59.69\n54.88\n51.56\n48.57\n47.32\n46.86\n47.68\n35.22\nLIMIT\n76.32\n74.18\n72.68\n69.19\n68.79\n65.64\n63.57\n62.69\n61.47\n60.44\n58.45\n17.87\nNC-FSCIL\n80.45\n75.98\n72.30\n70.28\n68.17\n65.16\n64.43\n63.25\n60.66\n60.01\n59.44\n21.01\nMCNet\n77.57\n73.96\n70.47\n65.81\n66.16\n63.81\n62.09\n61.82\n60.41\n60.09\n59.08\n18.49\nSoftNet\n78.07\n74.58\n71.37\n67.54\n65.37\n62.60\n61.07\n59.37\n57.53\n57.21\n56.75\n21.32\nCoSR\n74.87\n73.15\n68.23\n63.50\n62.72\n59.10\n57.46\n55.73\n53.28\n52.31\n51.75\n23.12\nM-FSCIL\n81.04\n79.73\n76.62\n73.30\n71.22\n68.90\n66.87\n65.02\n63.90\n62.49\n60.40\n20.64\nFSIL-GAN\n81.27\n78.03\n75.61\n73.72\n70.49\n68.19\n66.58\n65.63\n63.21\n60.92\n59.43\n21.84\nCPE-CLIP\n81.58\n78.52\n76.68\n71.86\n71.52\n70.23\n67.66\n66.52\n65.09\n64.47\n64.60\n16.98\nOFCL\n76.67\n81.09\n76.80\n76.20\n74.80\n72.44\n71.52\n69.17\n70.33\n68.17\n68.92\n7.74\nTable 3: Averaged classification accuracy (%) on CUB200 (10-way 5-shot).\nMethods\n𝐴𝐶𝐶𝑁in each task (%) ↑\nPD ↓\n0\n1\n2\n3\n4\n5\n6\n7\n8\niCaRL\n61.31\n46.32\n42.94\n37.63\n30.49\n24.00\n20.89\n18.80\n17.21\n44.10\nTOPIC\n61.31\n50.09\n45.17\n41.16\n37.48\n35.52\n32.19\n29.46\n24.42\n36.89\nCEC\n72.00\n66.83\n62.97\n59.43\n56.70\n53.73\n51.19\n49.24\n47.63\n24.37\nMeta-FSCIL\n72.04\n67.94\n63.77\n60.29\n57.58\n55.16\n52.9\n50.79\n49.19\n24.41\nALICE\n80.60\n70.60\n67.40\n64.50\n62.50\n60.00\n57.80\n56.80\n55.70\n24.90\nFeSSSS\n81.50\n77.04\n72.92\n69.56\n67.27\n64.34\n62.07\n60.55\n58.87\n22.63\nPro-KT\n98.59\n79.82\n77.85\n71.82\n70.34\n69.18\n70.13\n69.27\n69.73\n28.86\nLIMIT\n72.32\n68.47\n64.30\n60.78\n57.95\n55.07\n52.70\n50.72\n49.19\n23.13\nNC-FSCIL\n84.02\n76.80\n72.00\n67.83\n66.35\n64.04\n61.46\n59.54\n58.31\n25.71\nMCNet\n72.33\n67.70\n63.50\n60.34\n57.59\n54.70\n52.13\n50.41\n49.08\n23.25\nSoftNet\n76.63\n70.13\n65.92\n62.52\n59.49\n56.56\n53.71\n51.72\n50.48\n26.15\nCoSR\n71.92\n66.91\n62.71\n59.59\n56.63\n53.78\n51.01\n49.23\n47.93\n23.99\nM-FSCIL\n93.45\n91.82\n87.09\n88.07\n86.75\n87.15\n85.68\n84.80\n85.37\n8.08\nFSIL-GAN\n69.87\n62.91\n59.81\n58.86\n57.12\n54.07\n50.64\n48.14\n46.14\n23.73\nCPE-CLIP\n90.23\n89.56\n87.42\n86.80\n86.51\n85.08\n83.43\n83.38\n82.77\n7.46\nOFCL\n96.00\n91.65\n89.17\n84.90\n86.46\n87.18\n87.57\n85.71\n87.12\n10.88\nTable 4: Averaged classification accuracy (%) on MiniImageNet (5-way 5-shot).\nUnknowns\nDetection\nCUB200\nMiniImageNet\n𝐴𝑈𝐶𝑁𝐹𝑃𝑅𝑁𝐴𝑈𝐶𝑁\n𝐹𝑃𝑅𝑁\nw/o selection mechanism 66.68 69.96 75.73\n80.54\nw/o ITA\n67.06 75.83 70.61\n86.71\nw/o AKS\n-\n-\n-\n-\nOFCL\n69.72 69.85 76.20\n71.30\nKnowns\nClassification\nCUB200\nMiniImageNet\n𝐴𝐶𝐶𝑁\nPD\n𝐴𝐶𝐶𝑁\nPD\nw/o selection mechanism 54.52 12.48 61.34\n29.66\nw/o ITA\n50.33 31.14 51.42\n34.58\nw/o AKS\n55.24 28.42 63.02\n34.78\nOFCL\n63.80\n8.20\n65.24\n25.56\nTable 5: Ablation studies.\nFigure 4 (a) shows that using a large selection size 𝐾and a\nlong token length 𝐿𝑃may lead to knowledge under-fitting, whereas\nFigure 4 (b) demonstrates that the impact of 𝑙remains relatively\n0.5    1    1.5    2    2.5\n0.4 0.3 0.2 0.1  0\n(a)\n0.5    1    1.5    2    2.5\n0.4 0.3 0.2 0.1  0\n(b)\nFigure 5: (a) and (b): 𝐴𝑈𝐶𝑁and 𝐹𝑃𝑅𝑁w.r.t constraint governing\ndeviations 𝜎and margin 𝑚(CUB200 dataset).\nstable within the range 5 ≤𝑙≤30. The parameter 𝜎acts as a\nconstraint, governing deviations and regulating boundary violations.\nA smaller 𝜎imposes a more compact boundary. As depicted in\nFigure 5, when 𝑚= 0.5, performance decreases as 𝜎increases.\nHowever, for larger margins, performance improves with increasing\n\n\nImproving Open-world Continual Learning under the Constraints of Scarce Labeled Data\n𝜎, indicating the robustness of open detection is sensitive for the\nmargin 𝑚. Hence, we can safely conclude that our OFCL framework\nexhibits admirable robustness across various configurations.\n5\nCONCLUSIONS AND FUTURE WORK\nIn this paper, we introduced OFCL, a novel framework designed\nto tackle open-world continual learning with scarce labeled data.\nTechnically, the core of our proposed framework lies in its ability to\ndynamically adapt the knowledge space, enabling efficient mining,\nrepresentation, and accumulation of knowledge for both knowns\nand unknowns. Hence, the proposed OFCL not only enhances the\nrepresentation of known classes but also improves the compactness\nof decision boundaries for open detection.\nIn addition, the OFCL framework is designed to be inherently\ncompatible with multiple backbones for getting embeddings, making\nit plug-and-play with generalization capability. Effectively integrat-\ning LLMs into our model is an important direction for future work.\nMoreover, we will focus on enhancing the interpretability and de-\nveloping methods for test-time adaptation to improve knowledge\nrepresentation for unknowns.\nREFERENCES\n[1] Aishwarya Agarwal, Biplab Banerjee, Fabio Cuzzolin, and Subhasis Chaudhuri.\n2022. Semantics-driven generative replay for few-shot class incremental learning.\nIn Proceedings of the 30th ACM International Conference on Multimedia. 5246–\n5254.\n[2] Touqeer Ahmad, Akshay Raj Dhamija, Steve Cruz, Ryan Rabinowitz, Chunchun\nLi, Mohsen Jafarzadeh, and Terrance E Boult. 2022. Few-shot class incremental\nlearning leveraging self-supervised features. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. 3900–3910.\n[3] Touqeer Ahmad, Akshay Raj Dhamija, Mohsen Jafarzadeh, Steve Cruz, Ryan\nRabinowitz, Chunchun Li, and Terrance E Boult. 2022. Variable few shot class\nincremental and open world learning. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (Workshop). 3688–3699.\n[4] Steven Basart, Mazeika Mantas, Mostajabi Mohammadreza, Steinhardt Jacob,\nand Song Dawn. 2022. Scaling Out-of-Distribution Detection for Real-World\nSettings. In International Conference on Machine Learning.\n[5] Abhijit Bendale and Terrance Boult. 2015. Towards open world recognition. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\n1893–1902.\n[6] Robin Chan, Matthias Rottmann, and Hanno Gottschalk. 2021. Entropy max-\nimization and meta classification for out-of-distribution detection in semantic\nsegmentation. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision. 5128–5137.\n[7] Zhixiang Chi, Li Gu, Huan Liu, Yang Wang, Yuanhao Yu, and Jin Tang. 2022.\nMetafscil: A meta-learning approach for few-shot class incremental learning.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition. 14166–14175.\n[8] Yawen Cui, Zitong Yu, Wei Peng, Qi Tian, and Li Liu. 2024. Rethinking Few-Shot\nClass-Incremental Learning With Open-Set Hypothesis in Hyperbolic Geometry.\nIEEE Transactions on Multimedia 26 (2024), 5897–5910.\n[9] Marco D’Alessandro, Alberto Alonso, Enrique Calabrés, and Mikel Galar. 2023.\nMultimodal parameter-efficient few-shot class incremental learning. In Proceed-\nings of the IEEE/CVF International Conference on Computer Vision. 3393–3403.\n[10] Nanyi Fei, Yizhao Gao, Zhiwu Lu, and Tao Xiang. 2021. Z-score normalization,\nhubness, and few-shot learning. In Proceedings of the IEEE/CVF International\nConference on Computer Vision. 142–151.\n[11] Dan Hendrycks, Steven Basart, Mantas Mazeika, Andy Zou, Joseph Kwon, Mo-\nhammadreza Mostajabi, Jacob Steinhardt, and Dawn Song. 2022. Scaling Out-of-\nDistribution Detection for Real-World Settings. In International Conference on\nMachine Learning. PMLR, 8759–8773.\n[12] Dan Hendrycks and Kevin Gimpel. 2016. A Baseline for Detecting Misclas-\nsified and Out-of-Distribution Examples in Neural Networks. In International\nConference on Learning Representations.\n[13] Zhong Ji, Zhishen Hou, Xiyao Liu, Yanwei Pang, and Xuelong Li. 2023. Memo-\nrizing complementation network for few-shot class-incremental learning. IEEE\nTransactions on Image Processing 32 (2023), 937–948.\n[14] KJ Joseph, Salman Khan, Fahad Shahbaz Khan, and Vineeth N Balasubramanian.\n2021. Towards open world object detection. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. 5830–5840.\n[15] Gyuhak Kim, Changnan Xiao, Tatsuya Konishi, Zixuan Ke, and Bing Liu. 2022.\nA theoretical study on solving continual learning. Advances in neural information\nprocessing systems 35 (2022), 5065–5079.\n[16] Gyuhak Kim, Changnan Xiao, Tatsuya Konishi, Zixuan Ke, and Bing Liu. 2023.\nOpen-World Continual Learning: Unifying Novelty Detection and Continual\nLearning. arXiv preprint arXiv:2304.10038 (2023).\n[17] Jinze Li, Yan Bai, Yihang Lou, Xiongkun Linghu, Jianzhong He, Shaoyun Xu, and\nTao Bai. 2022. Memory-Based Label-Text Tuning for Few-Shot Class-Incremental\nLearning. arXiv preprint arXiv:2207.01036 (2022).\n[18] Miaomiao Li, Jiaqi Zhu, Yang Wang, Yi Yang, Yilin Li, and Hongan Wang. 2024.\nRulePrompt: Weakly Supervised Text Classification with Prompting PLMs and\nSelf-Iterative Logical Rules. In Proceedings of the ACM on Web Conference 2024.\n4272–4282.\n[19] Yujie Li, Xin Yang, Hao Wang, Xiangkun Wang, and Tianrui Li. 2024. Learning to\nPrompt Knowledge Transfer for Open-World Continual Learning. In Proceedings\nof the AAAI Conference on Artificial Intelligence, Vol. 38(12). 13700–13708.\n[20] Huiwei Lin, Shanshan Feng, Baoquan Zhang, Hongliang Qiao, Xutao Li, and\nYunming Ye. 2023. UER: A Heuristic Bias Addressing Approach for Online\nContinual Learning. In Proceedings of the 31st ACM International Conference\non Multimedia. Association for Computing Machinery, New York, NY, USA,\n96–104.\n[21] Bing Liu, Sahisnu Mazumder, Eric Robertson, and Scott Grigsby. 2023. AI\nAutonomy: Self-initiated Open-world Continual Learning and Adaptation. AI\nMagazine (2023).\n[22] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. 2020. Energy-based\nout-of-distribution detection. Advances in Neural Information Processing Systems\n33 (2020), 21464–21475.\n[23] Jaewoo Park, Yoon Gyo Jung, and Andrew Beng Jin Teoh. 2023. Nearest neigh-\nbor guidance for out-of-distribution detection. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision. 1686–1695.\n[24] Can Peng, Kun Zhao, Tianren Wang, Meng Li, and Brian C Lovell. 2022. Few-shot\nclass-incremental learning from an open-set perspective. In European Conference\non Computer Vision. Springer, 382–397.\n[25] Chengwei Qin and Shafiq Joty. 2021.\nLfpt5: A unified framework for life-\nlong few-shot language learning based on prompt tuning of t5. arXiv preprint\narXiv:2110.07298 (2021).\n[26] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H\nLampert. 2017. icarl: Incremental classifier and representation learning. In Pro-\nceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\n2001–2010.\n[27] Vikash Sehwag, Mung Chiang, and Prateek Mittal. 2021. Ssd: A unified frame-\nwork for self-supervised. In International Conference on Machine Learning.\n[28] Yiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li. 2022. Out-of-distribution\ndetection with deep nearest neighbors. In International Conference on Machine\nLearning. PMLR, 20827–20840.\n[29] Xiaoyu Tao, Xiaopeng Hong, Xinyuan Chang, Songlin Dong, Xing Wei, and\nYihong Gong. 2020. Few-shot class-incremental learning. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition. 12183–\n12192.\n[30] Daniel J Trosten, Rwiddhi Chakraborty, Sigurd Løkse, Kristoffer Knutsen Wick-\nstrøm, Robert Jenssen, and Michael C Kampffmeyer. 2023. Hubs and hyper-\nspheres: Reducing hubness and improving transductive few-shot learning with\nhyperspherical embeddings. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. 7527–7536.\n[31] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. 2016.\nMatching networks for one shot learning. Advances in Neural Information Pro-\ncessing Systems 29 (2016).\n[32] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie.\n2011. The caltech-ucsd birds-200-2011 dataset.\n[33] Haoqi Wang, Zhizhong Li, Litong Feng, and Wayne Zhang. 2022. Vim: Out-\nof-distribution with virtual-logit matching. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. 4921–4930.\n[34] Xin Wang, Yue Liu, Jiapei Fan, Weigao Wen, Hui Xue, and Wenwu Zhu. 2023.\nContinual Few-shot Learning with Transformer Adaptation and Knowledge Regu-\nlarization. In Proceedings of the ACM Web Conference, Vol. 2023.\n[35] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu\nLee, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, et al. 2022. Dualprompt:\nComplementary prompting for rehearsal-free continual learning. In European\nConference on Computer Vision. Springer, 631–648.\n[36] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren,\nGuolong Su, Vincent Perot, Jennifer Dy, and Tomas Pfister. 2022. Learning to\nprompt for continual learning. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. 139–149.\n[37] Yibo Yang, Haobo Yuan, Xiangtai Li, Zhouchen Lin, Philip Torr, and Dacheng Tao.\n2022. Neural Collapse Inspired Feature-Classifier Alignment for Few-Shot Class-\nIncremental Learning. In The Eleventh International Conference on Learning\nRepresentations.\n\n\nLi and Wang, et al.\n[38] Jaehong Yoon, Sultan Madjid, Sung Ju Hwang, Chang-Dong Yoo, et al. 2023. On\nthe Soft-Subnetwork for Few-Shot Class Incremental Learning. In International\nConference on Learning Representations (ICLR) 2023. International Conference\non Learning Representations.\n[39] Zhiqi Yu, Jingjing Li, Zhekai Du, Fengling Li, Lei Zhu, and Yang Yang. 2023.\nNoise-Robust Continual Test-Time Domain Adaptation. In Proceedings of the\n31st ACM International Conference on Multimedia. Association for Computing\nMachinery, New York, NY, USA, 2654–2662.\n[40] Chi Zhang, Nan Song, Guosheng Lin, Yun Zheng, Pan Pan, and Yinghui Xu. 2021.\nFew-shot incremental learning with continually evolved classifiers. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 12455–\n12464.\n[41] Hanbin Zhao, Yongjian Fu, Mintong Kang, Qi Tian, Fei Wu, and Xi Li. 2021.\nMgsvf: Multi-grained slow vs. fast framework for few-shot class-incremental\nlearning.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n(2021).\n[42] Xiaowei Zhao, Yuqing Ma, Duorui Wang, Yifan Shen, Yixuan Qiao, and Xiang-\nlong Liu. 2023. Revisiting open world object detection. IEEE Transactions on\nCircuits and Systems for Video Technology (2023).\n[43] Da-Wei Zhou, Fu-Yun Wang, Han-Jia Ye, Liang Ma, Shiliang Pu, and De-Chuan\nZhan. 2022. Forward compatible few-shot class-incremental learning. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\n9046–9056.\n[44] Da-Wei Zhou, Han-Jia Ye, Liang Ma, Di Xie, Shiliang Pu, and De-Chuan Zhan.\n2022. Few-shot class-incremental learning by sampling multi-phase tasks. IEEE\nTransactions on Pattern Analysis and Machine Intelligence (2022).\n[45] Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Ziyao Zeng, Zipeng Qin,\nShanghang Zhang, and Peng Gao. 2023. Pointclip v2: Prompting clip and gpt for\npowerful 3d open-world learning. In Proceedings of the IEEE/CVF International\nConference on Computer Vision. 2639–2650.\n\n\n"}
{"text": "“No negatives needed”: weakly-supervised regression for interpretable\ntumor detection in whole-slide histopathology images\nMarina D’Amato1, Jeroen van der Laak1, Francesco Ciompi1\n1Computational Pathology Group, Radboud University Medical Center, Nijmegen, The Netherlands\nCorresponding author: marina.damato@radboudumc.nl\nAbstract\nAccurate tumor detection in digital pathology whole-slide images (WSIs) is crucial for cancer diagnosis and treatment\nplanning. Multiple Instance Learning (MIL) has emerged as a widely used approach for weakly-supervised tumor detection\nwith large-scale data without the need for manual annotations. However, traditional MIL methods often depend on\nclassification tasks that require tumor-free cases as negative examples, which are challenging to obtain in real-world clinical\nworkflows, especially for surgical resection specimens. We address this limitation by reformulating tumor detection as a\nregression task, estimating tumor percentages from WSIs, a clinically available target across multiple cancer types. In this\npaper, we provide an analysis of the proposed weakly-supervised regression framework by applying it to multiple organs,\nspecimen types and clinical scenarios. We characterize the robustness of our framework to tumor percentage as a noisy\nregression target, and introduce a novel concept of “amplification technique” to improve tumor detection sensitivity when\nlearning from small tumor regions. Finally, we provide interpretable insights into the model’s predictions by analyzing\nvisual attention and logit maps.\n1\nIntroduction\nEarly and accurate cancer diagnosis is crucial for success-\nful treatment and improved patient outcomes. Tradition-\nally, histopathological analysis, where pathologists exam-\nine tissue sections under a microscope, serves as the gold\nstandard for diagnosing tumors. However, this process is\nlabor-intensive, time-consuming and subject to inter-reader\nvariability ([30], [21], [24]).\nThe digitization of whole-slide images (WSIs) has enabled\nthe development of automated analysis methods using ar-\ntificial intelligence (AI) and deep learning, which can en-\nhance diagnostic efficiency and accuracy [29]. Automating\ntumor detection can alleviate the workload on pathologists\npotentially improving the diagnostic process and reducing\nturnaround time. Traditional deep learning models for tu-\nmor detection rely heavily on fully-supervised approaches,\nnecessitating detailed pixel- or patch-level annotations ([32],\n[19], [11]). This dependency on extensive annotations poses\nchallenges in scalability and generalizability, primarily due\nto the scarcity of annotated datasets and the limited avail-\nability of expert pathologists.\nWeakly-supervised learning (WSL), which requires only\ncoarse annotations or slide-level labels, has emerged as a\nviable alternative to fully-supervised models, substantially\nreducing the annotation burden. While various WSL ap-\nproaches exist, including those based on image compression\n[28, 2] and streaming techniques [25, 12], Multiple Instance\nLearning (MIL) ([7], [15]) has emerged as the predominant\nframework in digital pathology. MIL treats each WSI as a\n”bag” containing image patches (”instances”), where only\nthe bag-level label is available (e.g., tumor presence or ab-\nsence). MIL has been successfully applied to binary classifi-\ncation tasks across various cancer types, including prostate\ncancer and basal cell carcinoma ([6]), breast cancer ([20]),\ncolorectal cancer ([9]), and skin cancer ([16]), achieving per-\nformance comparable to fully-supervised models without\nthe need for manual annotations.\nDespite its advantages, most investigations have re-\nmained confined to classification problems with categorical\noutcomes, such as tumor presence or absence.\nA signifi-\ncant limitation in tumor detection formulated as a weakly-\nsupervised binary classification task is the need for large\ndatasets containing both positive (tumor present) and neg-\native (tumor absent) cases. However, obtaining WSIs com-\npletely tumor-free in real-world clinical settings can be chal-\nlenging as most resected specimens typically contain at least\nsome degree of tumor.\nWhile biopsies and lymph nodes\ninclude both tumor-positive and tumor-negative cases, fo-\ncusing on these tissues alone introduces biases and limits\nmodel generalizability. Unlike resections, these samples do\nnot capture the full morphological diversity of tumors or\ntheir surrounding microenvironments. Since surgical speci-\nmens offer a broader and more representative range of tu-\nmor variations, they are a more suitable focus for large-scale\napplications. However, the scarcity of completely negative\nresections hinders the scalability of classification-based tu-\nmor detection models.\nTo address this challenge, researchers have explored alter-\nnative targets for weakly-supervised learning, such as tumor\npercentages. In clinical practice, pathologists routinely de-\nlineate the tumor bed on WSIs as a preparatory step for\nmolecular diagnostics, as shown in the first step of Figure1.\nThis involves marking the approximate tumor area, often\nusing a pen marker, and visually estimating the percent-\nage of tumor cells within the annotated region to ensure\nadequate tumor content for molecular testing.\nBuilding on this workflow, [22] proposed a weakly-\nsupervised learning approach that leverages the tumor cell\npercentage provided by pathologists as a proxy to train\na weakly-supervised segmentation algorithm derived from\nMIL. Their method uses the percentage of tumor cells within\nthe tumor bed to assign proxy instance-level labels to im-\nage patches to approximate segmentation maps that match\nthe global percentage estimate.\nHowever, the tumor cell\n1\narXiv:2502.21109v1  [eess.IV]  28 Feb 2025\n\n\nInput WSI\nTissue Segmentation\nMarker Segmentation\nTumor Area\nPercentage Tumor Area: 26.57%\nFigure 1: Visual example of a procedure to extract an approximate tumor percentage area from a slide used in molecular\ndiagnostics procedures. From a coarse annotation of the tumor area, often provided with a pen marker, we can derive\nthe tumor percentage via simple image analysis steps: 1) segmentation of foreground tissue versus background, 2)\nidentification of the pen marker and area filling, 3) intersection of tissue and marker area, 4) calculation of the tumor\npercentage.\npercentage does not always accurately represent the true\nspatial extent of the tumor in the slide. Moreover, their\nwork primarily focused on frozen sections and showed lim-\nited success when applied to data from routine diagnostics\nbased on formalin-fixed, paraffin-embedded (FFPE) tissue\nfrom two test cohorts.\nWe propose to shift the focus from tumor cell percent-\nages to tumor area percentages, under the hypothesis that\ntumor area percentage provides a more informative learn-\ning signal for tumor detection, as it better reflects the actual\ndistribution of the tumor within the tissue. By leveraging\nthe pathologist-annotated tumor areas as a starting point,\nwe employ simple image analysis steps (illustrated in Fig-\nure 1, more information in the Supplementary Material) to\ncalculate the percentage of tumor area relative to the en-\ntire tissue section. We then use this percentage as a direct\ntarget to train a regression MIL framework, an approach\nthat has been previously used for stromal tumor-infiltrating\nlymphocytes ([27]), inferring gene expression profiles ([31],\n[17]), survival prediction ([8]), and molecular biomarker pre-\ndiction ([14]), but never for tumor detection.\n1.1\nOur contributions\nOur key contributions are as follows. First, we introduce a\nnovel application of Multiple Instance Learning (MIL) for\ntumor detection with weakly-supervised regression, circum-\nventing the need for extensive annotations and tumor-free\ncases. We explore the applicability of this approach on var-\nious tissue types and compare various weakly-supervised\nmethods in a regression setting, examining the impact of\ndifferent models and pooling strategies.\nSecond, we acknowledge that visual estimation of the tu-\nmor area made by pathologists is subject to variability and\nnoise. To address this, we conduct a robustness analysis to\nevaluate the performance when building our model under\ndifferent levels of synthetic noise in the tumor percentage\ntargets. This analysis is crucial for ensuring the reliability\nof our approach in real-world clinical settings, where such\nvariability is unavoidable.\nAdditionally, we introduce and analyze the effectiveness\nof a non-linear transformation of the target, which we refer\nto as the “amplification technique” to enhance the training\nprocess of our regression models, particularly when dealing\nwith cases involving small lesions.\nLastly, we assess the interpretability of our models by\ncomparing raw prediction maps based on instance-level pre-\ndictions with attention maps derived from attention scores.\nThese heatmaps provide valuable insights into the model’s\ndecision-making process, aligning with the clinical need for\nFigure 2: Examples of WSIs paired with manual annota-\ntions (when available) or tumor segmentation masks, illus-\ntrating the computed tumor percentages. The first column\npresents the input WSIs, the second column displays either\nmanual annotations (for CAM16) or the raw output of the\nsegmentation algorithm, and the third column showcases\nthe binarized maps highlighting the tumor regions.\nexplainable AI in medical image analysis.\nWe conduct a\nquantitative evaluation of the interpretability performance\nby comparing the heatmaps with ground truth annotations.\n2\nMaterials\nWe evaluated weakly-supervised regression models on five\ndatasets consisting of WSIs stained with hematoxylin and\neosin (H&E), covering a diverse range of specimen types, in-\ncluding surgical resections, lymph nodes, and biopsies from\ndifferent organs. Figure 2 provides a visual overview of the\ninput data and computed tumor percentages.\n2\n\n\nBreast TNBC\nThe TNBC dataset [3] includes 595 cases\nof triple negative breast cancer (TNBC) surgical resections\nfrom an equal amount of patients. This dataset was con-\nstructed from a multicenter, retrospective cohort study and\nencompasses patients diagnosed with TNBC between 2006\nand 2014 from several hospitals in the Eastern Netherlands.\nThe slides were scanned using a Pannoramic 1000 DX scan-\nner (3DHISTECH) at a pixel resolution of 0.24 µm. Tumor\npercentages in this dataset were computed using the pub-\nlicly available HookNet algorithm [26], as manual annota-\ntions were not available. HookNet demonstrated strong seg-\nmentation performance on invasive tumors, achieving Dice\nscores of 0.9 and 0.91 for IDC and ILC, respectively, mak-\ning it well-suited for estimating tumor percentages in this\ndataset. All the slides in this dataset contain cancer with\npercentages ranging from 2% to 66%. The mean and me-\ndian percentage of tumor across the slides are 25% and 24%\nrespectively, indicating a relatively even distribution of tu-\nmor burden without significant skewness.\nCAMELYON16\nThe\nCAMELYON16\ndataset\n([13])\n(CAM16) consists of 399 WSIs of lymph node sections (one\nslide per patient) from two medical centers in the Nether-\nlands: Radboud University Medical Center (RUMC) and\nUniversity Medical Center Utrecht (UMCU). RUMC images\nwere scanned using a digital slide scanner (Pannoramic 250\nFlash II; 3DHISTECH) with a 20× objective lens, result-\ning in a specimen-level pixel size of 0.243 µm × 0.243 µm.\nUMCU images were scanned with a NanoZoomer-XR Dig-\nital slide scanner C12000-01 (Hamamatsu Photonics) using\na 40× objective lens, producing a pixel size of 0.226 µm ×\n0.226 µm. This dataset includes pixel-level annotations for\nboth macro-metastases and micro-metastases which we used\nto compute tumor percentages. Of the 399 slides, 160 con-\ntain metastases while the remaining depict normal tissue.\nThe percentages of tumor in tumor slides range from 0.003%\nto 71%, with 91 slides having a percentage less than 1%.\nThe mean percentage of tumor across the tumorous slides\nis approximately 5% while the median is only 0.4%, indicat-\ning a skewed distribution towards lower tumor percentages,\nhighlighting the challenge of detecting small metastases in\nthis dataset.\nEXAMODE Colon\nThe ExaMode colon dataset [10]\nconsists of 8556 H&E-stained colorectal biopsy WSIs cut\nfrom 6556 paraffin blocks of 3501 patients collected from\nthe Radboud University Medical Center between 2000 and\n2009. As the diagnoses were reported at the block level,\nslides from the same block were combined into a single new\nslide (“packed” slide [1]) by minimizing the area of back-\nground between sections. Pathology reports associated with\nthese slides provide labels for five classes: normal, hyper-\nplastic polyps, low-grade dysplasia (lgd), high-grade dyspla-\nsia (hgd), and cancer. For this study, we grouped normal\nand hyperplastic polyps into a single “normal” class, result-\ning in two classes: normal (4546 cases) and abnormal (lgd,\nhgd, and cancer; 2010 cases). Tumor percentages in abnor-\nmal cases were derived from segmentation maps created us-\ning a colon tissue segmentation algorithm [5] which demon-\nstrated high performance, achieving a Dice score of 0.89 for\ntumor segmentation. The percentages in this dataset range\nfrom 0.033% to 62%, with a mean of 14% and median of\n10%. The small size and sparse distribution of tumor areas\nin these slides pose significant challenges for accurate tumor\ndetection.\nAQUILA Colon\nThe AQUILA colon dataset includes\n571 WSIs of colon tissue from surgical resections, with one\nslide per patient. These slides were scanned at two institu-\ntions: Radboud University Medical Center (RUMC) and\nLaboratorium Pathologie Oost-Nederland (LabPON). As\nfor the EXAMODE cohort, tumor percentages were com-\nputed using the same colon tissue segmentation algorithm\n([5]), grouping the segmentation labels of high-grade dys-\nplasia (HGD), low-grade dysplasia (LGD), and tumor into\na single “abnormal” class, while all other tissue types were\nclassified as “normal.” All the slides in this dataset contain\ncancer with percentages ranging from 0.078% to 62%, with\na mean percentage of 13.33% and a median of 12.27%.\nCOBRA\nThe COBRA dataset ([16]) consists of 5147\nslides from 4066 patients obtained from Radboud Univer-\nsity Medical Center between 2016 and 2020.\nIt includes\npatients diagnosed with Basal Cell Carcinoma (BCC), epi-\ndermal dysplasia (actinic keratosis or Bowen’s disease), or\nbenign conditions.\nAmong the slides, 2661 contain BCC\ntumors, while 2476 are classified as non-BCC. All slides\nwere scanned using a 3DHistech Pannoramic 1000 scanner\nat 20×magnification (pixel resolution 0.24 µm). Tumor per-\ncentages were computed using an in-house skin tumor seg-\nmentation algorithm (more information in the Supplemen-\ntary Material) and range from 0.01% to 89%, with a mean\nof 16% and a median of 10%. This distribution suggests a\nmoderate range of tumor burden within the BCC-positive\nslides.\n3\nMethods\nWe investigated adaptations of the classical MIL classifi-\ncation approach [6] for regression tasks, using the tumor\npercentage within a WSI as the continuous target. In this\nsection, we first retrace the main concepts of MIL, we then\nintroduce the proposed approaches for MIL regression, the\nanalysis that we performed on the impact of noisy labels as\nwell as the motivation and the effect of the “amplification\ntechnique”.\n3.1\nMultiple Instance Learning\nIn Multiple Instance Learning, a WSI is treated as a bag\ncontaining multiple image patches, each referred to as an\ninstance xi, where i = 0, 1, ..., N and N represents the\ntotal number of patches in the slide. The task is to pre-\ndict a bag-level target y based on the instances, where each\npatch xi does not have a direct label but contributes to\nthe overall prediction. Instead of assigning individual la-\nbels to each patch, the model learns a bag-level prediction\nbased on the aggregated information from all patches within\nthe WSI. MIL methods are typically divided into two cate-\ngories: instance-based and embedding-based approaches, de-\npending on how the individual patches are processed and\naggregated. The instance-based method starts by extract-\ning features from each image patch in the WSI. These fea-\ntures are then passed through an instance-level prediction\nlayer, which generates a prediction for each patch indepen-\ndently. After obtaining the instance-level predictions, these\nare combined into a single prediction for the entire slide us-\ning a pooling mechanism, which can be as simple as mean\n3\n\n\npatch-level\nfeature extractor\ninstance-level features\ninstance-level\nprediction layer\n0.63\n0.76\n0.67\n0.72\n0.59\npooling\n0.67\ninstance-level predictions\n...\n...\nattention module\n0.1\n0.02\n... 0.3\n0.08 0.12\n...\nattention scores\npatches\ninput WSI\n🔥\n🔥\nFigure 3: Overview of the instance-based multiple instance learning (MIL) framework. The instance-based approach pro-\ncesses individual patches from the input image independently, making predictions at the instance level before aggregating\nthem.\npooling or more sophisticated, such as attention-weighted\npooling.\nIn this study, we focus on instance-based approaches to\nMIL (see Figure 3) because it aligns with the framework\nused in [22], where proxy-labels are assigned to individual\ninstances, and instance-level predictions are made.\nAd-\nditionally, we prioritize interpretability in our approach,\nwhich is facilitated by instance-based MIL since it allows\nfor direct visualization of individual patch predictions, en-\nabling a clearer understanding of how the model makes its\ndecisions.\n3.2\nMultiple instance learning to regress\ntumor percentage\nWe evaluate four different MIL approaches, each adapted\nto the task of tumor percentage regression and to the\ninstance-based formulation of MIL. These methods vary in\ntheir strategies for aggregating information from individual\npatches to predict the tumor percentage for the entire slide.\nMeanPool\nThe first method, Meanpool MIL (MeanPool),\nis a classical approach that uses the average of all patch\npredictions within a slide as the final tumor percentage,\nproviding a simple and intuitive way to combine information\nfrom all patches.\nAttention-based MIL\nThe second method, Attention-\nbased MIL (ABMIL), introduced by [20], leverages the at-\ntention mechanism as a pooling operator. It assign weights\n(attention scores) to each patch, indicating its relative im-\nportance for the final prediction. Higher scores signify that\nthe model considers the corresponding patch to be more\ninformative about the tumor content. ABMIL uses these\nattention scores to create a weighted average of the patch\npredictions, focusing more on informative regions within the\nWSI.\nCLAM\nBuilding upon ABMIL’s interpretability,\nthe\nthird\nmethod,\nClustering\nConstrained\nAttention\nMIL\n(CLAM), incorporates an instance clustering loss function\n([23]). This loss function encourages the model to identify\na subset of patches with high attention scores (likely con-\ntaining tumor) and a separate subset with low attention\nscores (likely non-tumor). These patches are then used for\nadditional training with pseudo-labels, potentially improv-\ning the model’s ability to differentiate between tumor and\nnon-tumor regions.\nIn this study, we adapt the original\nembedding-based CLAM formulation to an instance-based\napproach.\nWeSEG\nLastly, we examined Weakly Supervised Segmen-\ntation (WeSEG) ([22]). While previous methods employ a\ntwo-step approach, where a frozen encoder maps all patches\nto low-dimensional embeddings, WeSEG trains the encoder\nend-to-end to generate patch-level tumor probability maps\nusing weak labels. It employs a recursive training mech-\nanism to generate proxy labels and iteratively refines the\nmodel’s predictions. At every step, it creates proxy labels by\nassigning label 1 to the p% patches with the highest proba-\nbility and label 0 to the remaining ones, where p is the target\ntumor percentage. While WeSEG was not explicitly opti-\nmized to predict tumor percentages, we can derive percent-\nage estimates from its patch-level predictions. Specifically,\nwe convert patch-level probabilities to binary predictions\nusing a threshold optimized per experiment, then calculate\nthe tumor percentage as the ratio of patches classified as\ntumor to the total number of tissue patches.\nThis post-\nprocessing step allows us to compare the predicted tumor\npercentage directly with the target annotation.\n3.3\nEffect of noisy targets\nPathologists’ annotations of the tumor bed provide a valu-\nable basis for estimating tumor area percentages. However,\nthese annotations are inherently noisy due to their reliance\non coarse manual delineations and visual estimation.\nIn\nthis work, we benefit from more precise tumor area esti-\nmates based on detailed manual annotations or AI-assisted\ndelineation techniques ([26], [5]). To account for the inher-\nent variability in clinical practice, we introduce synthetic\nnoise into the tumor percentage labels during training. This\nreflects the imprecision that naturally arises from visual es-\ntimation, allowing us to assess the robustness of the frame-\nwork to noisy inputs, while testing and validation are per-\nformed using the true, unaltered labels.\nWe simulated three levels of noise, ranging from mild to\nsubstantial deviations, to model potential underestimation\nor overestimation by pathologists in real-world clinical sce-\nnarios. For each training example, we added noise to the\ntrue percentages by sampling from uniform distributions:\n4\n\n\n[-0.1, 0.1] for mild noise (±10%), [-0.3, 0.3] for moderate\nnoise (±30%), and [-0.5, 0.5] for substantial noise (±50%).\nFigure 4: The first panel illustrates the fifth root trans-\nformation, with annotated points demonstrating the effect\nof amplification on selected values, particularly enhancing\nlower percentages. The other five panels display the distri-\nbution of tumor percentages across various cohorts, shown\nonly for tumorous slides (excluding negative cases), before\nand after the amplification technique.\n3.4\nTarget amplification\nIn the context of our regression task for tumor detec-\ntion, cases with small lesions or low tumor percentages\ncan be particularly difficult for the model to distinguish\nfrom tumor-free cases, potentially leading to reduced per-\nformance.\nTo address this challenge, we employ a target\namplification technique designed to enhance the model’s\nsensitivity to small tumor percentages.\nThis technique involves transforming the target labels us-\ning a root transformation ˆy =\nn√y that expands lower-end\nvalues (small tumor percentages), making subtle differences\nbetween normal tissue and areas with small tumor lesions\nmore discernible. For this work, we selected the fifth root\ntransformation (n = 5), based on empirical results demon-\nstrating that it strikes a good balance between amplifying\nsmall tumor values without overly distorting higher tumor\npercentages.\nThis transformation is particularly valuable for datasets\nsuch as CAM16, COBRA, and ExaMode consisting of biop-\nsies and lymph nodes, which usually include either small\ntumor percentages or entirely tumor-free regions. Figure 4\nillustrates the distributions of tumor percentages before and\nafter amplification, demonstrating how the transformation\nexpands the range of small values, enhancing the separa-\ntion between tumor-free and low-tumor cases. On the other\nhand, datasets such as AQUILA and TNBC consist of sur-\ngical resections with no tumor-free cases, where tumor areas\nare relatively large. In these datasets, amplification is un-\nnecessary and was not applied in our analysis, as the clear\ndistinction between tumor regions does not require further\nenhancement.\n4\nExperiments\nData preprocessing\nWe first segmented the tissue from\nthe background using a pre-trained tissue segmentation net-\nwork ([4]). From the segmented tissue regions, we extracted\nnon-overlapping 256x256 patches at a spatial resolution of\n0.5 µm using the hs2p library ([18]).\nModel Training\nWith the exception of WeSEG, all in-\nvestigated methods follow a two-step process involving a\nfrozen convolutional neural network (CNN) encoder and a\nfinal classification layer (see Figure 3).\nIn the first step,\nwe used a modified ResNet50 pretrained on ImageNet as\na patch-level feature extractor, following the approach de-\nscribed by [23], where the model was adapted by applying\nadaptive mean-spatial pooling after the 3rd residual block.\nThe encoder’s weights were frozen, meaning they were not\nupdated during training. The second step involved training\nthe final layer for tumor percentage estimation. The models\nwere trained by minimizing the mean squared error (MSE)\nloss between the target and predicted tumor percentages\nwith the Adam optimizer. Training included L2 weight de-\ncay set at 10−5 and a learning rate of 2 × 10−4 was used.\nTraining proceeded for at least 50 epochs and up to a maxi-\nmum of 200 epochs, with early stopping implemented if the\nvalidation loss plateaued.\nUnlike previous methods that rely on fixed feature extrac-\ntion, WeSEG operates directly on the image patches using a\nmodel that learns to extract features during training. Here,\nthe pre-trained ResNet50 architecture was used for end-to-\nend training, where the feature representations were contin-\nuously refined with each iteration. At each training step, 30\ntiles were randomly sampled from each whole-slide image,\nas done in [22]. Due to computational limitations, we re-\nstricted the sampling to a single whole-slide image per step.\nBased on the original work, data augmentation techniques\nwere applied which included random vertical and horizontal\nflips, and color jittering with defined parameters for bright-\nness, contrast, saturation, and hue (as implemented in [22]).\nThe model adopted the Adam optimizer with binary cross-\nentropy loss, with weight decay of 10−5 and a learning rate\nof 0.0005.\nTraining spanned a maximum of 100 epochs,\nincorporating early stopping with a 50-epoch patience.\nAll experiments were conducted using a 5-fold cross val-\nidation strategy for each dataset to assess model perfor-\nmance generalizability. For each fold, we first selected the\ntest set, which consisted of 20% of the total slides. This en-\nsured that, across all 5 folds, the test sets collectively rep-\nresented the entire dataset, with no overlap between test\nsets. The remaining 80% of the slides were then split into\ntraining (85%) and validation sets (15%). Stratified sam-\npling was employed during the partitioning process to en-\nsure that the distribution of tumor percentages remained\nconsistent across the training, validation, and test sets. For\ncases containing multiple slides, where a ”case” refers to all\nthe slides from a single patient, all slides belonging to the\nsame case were assigned to the same fold. Additionally, for\nCAM16 and COBRA, we performed separate training and\nvalidation on the official sets to obtain results comparable\nto those reported in literature.\nEvaluation metrics\nWe evaluated the regression task us-\ning the Spearman’s and Pearson’s correlation coefficients\nto quantify how closely our models’ predictions align with\nthe reference standard. Additionally, for datasets contain-\ning negative cases, we used receiver operating characteristic\n(ROC) curve analysis with the area under the curve (AUC)\nto assess tumor detection performance. In this analysis, bi-\n5\n\n\nTNBC\nAQUILA\nCAM16\nExaMode\nCOBRA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPearson Correlation\n0.97\n0.96\n0.85\n0.95\n0.96\n0.97\n0.96\n0.81\n0.95\n0.97\n0.97\n0.96\n0.88\n0.95\n0.95\n0.87\n0.81\n0.35\n0.81\n0.89\n(a)\nTNBC\nAQUILA\nCAM16\nExaMode\nCOBRA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSpearman Correlation\n0.97\n0.96\n0.49\n0.73\n0.86\n0.97\n0.96\n0.46\n0.73\n0.84\n0.97\n0.96\n0.51\n0.74\n0.81\n0.88\n0.80\n0.18\n0.57\n0.82\nMethod\nABMIL\nCLAM\nMeanPool\nWeSEG\n(b)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue Positive Rate\nCAM16 Tumor Detection\nABMIL (AUC = 0.708 ± 0.059)\nCLAM (AUC = 0.683 ± 0.058)\nMEANPOOL (AUC = 0.712 ± 0.056)\nWESEG (AUC = 0.580 ± 0.058)\nRandom Chance (AUC = 0.5)\n(c)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue Positive Rate\nExaMode Tumor Detection\nABMIL (AUC = 0.927 ± 0.016)\nCLAM (AUC = 0.927 ± 0.011)\nMEANPOOL (AUC = 0.928 ± 0.006)\nWESEG (AUC = 0.813 ± 0.014)\nRandom Chance (AUC = 0.5)\n(d)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue Positive Rate\nCOBRA Tumor Detection\nABMIL (AUC = 0.909 ± 0.009)\nCLAM (AUC = 0.887 ± 0.023)\nMEANPOOL (AUC = 0.870 ± 0.015)\nWESEG (AUC = 0.895 ± 0.012)\nRandom Chance (AUC = 0.5)\n(e)\nFigure 5: Comparison of bar plots and ROC curves across methods and datasets. (a) Pearson correlation coefficients\nfor different methods across the five datasets, indicating the strength of linear relationships between predicted and true\ntumor percentages. (b) Spearman correlation coefficients across the five datasets, showcasing the rank-based correlation\nbetween predictions and ground truth. (c-e) Receiver Operating Characteristic (ROC) curves for tumor detection with\nAUC scores for CAM16 (c), ExaMode (d), and COBRA (e), illustrating the trade-off between true positive and false\npositive rates. The shaded areas represent the confidence intervals over the 5-fold cross-validation, and the mean AUC\nvalues with standard deviations are reported for each dataset. These results are based on models trained using true\ntumor percentages.\nnary labels (tumor vs. no tumor) were derived from tumor\npercentages to classify slides as positive or negative. Addi-\ntionally, following [22], we used AUC to evaluate the inter-\npretability of attention scores and instance-level predictions\nby comparing them against pathologists’ ground-truth an-\nnotations or tumor segmentation masks. This comparison\nhelps determine if attention heatmaps or instance-level pre-\ndictions offer advantages for interpretability in regression\ntasks.\n5\nResults\n5.1\nAssessing tumor percentage prediction\nFigure 5(a-b) present the Pearson and Spearman correlation\nresults for various methods across multiple datasets. Over-\nall, most methods demonstrate very strong correlations,\nhighlighting their effectiveness in aligning predicted tumor\npercentages with the ground truth. However, WeSEG shows\nnotably poorer performance compared to the other meth-\nods. This difference arises because WeSEG was not explic-\nitly optimized to predict tumor percentages; rather, tumor\npercentages were derived from patch-level predictions using\na binarization step, as described in section 3.2. This addi-\ntional processing step introduces potential inaccuracies, as\nerrors in the segmentation or imprecisions when converting\nto a percentage can impact the final result.\nIn the AQUILA and TNBC datasets, all methods (except\nWeSEG) achieve high Pearson (r > 0.95) and Spearman\n(ρ > 0.95) correlations, demonstrating robust performance\nin both value prediction and rank ordering. Surprisingly,\nMeanPool performs comparably to more complex methods\nsuch as ABMIL and CLAM, suggesting that methods with\nsimpler pooling mechanisms can be effective in contexts\nwhere tumor areas are relatively large, such as in surgical\nresections.\nFor the ExaMode dataset, while Pearson correlations re-\nmain strong (r > 0.95), the Spearman correlations are lower\n(ρ ≈0.73).\nThis discrepancy highlights potential issues\nsuch as the presence of outliers which may cause the Pear-\nson correlation to appear overly optimistic. Specifically, a\ndetailed analysis reveals a significant number of tumor-free\ncases with predicted tumor percentages greater than zero\n(see the scatter plots in the Supplementary Material). For\nCOBRA, all methods show strong performance, with Pear-\nson correlations ranging from 0.95 to 0.97 for the attention-\nbased methods.\nHowever, the Spearman correlations are\nslightly lower (ρ ≈0.85), suggesting that the models may\n6\n\n\n47\n(b)\n(a)\n(c)\nFigure 6: Variation of Pearson, Spearman, and AUC metrics across different noise levels. Subplots (a), (b), and (c)\ndepict line plots showing how the Pearson correlation, Spearman correlation, and AUC, respectively, change as noise\nlevels increase. These metrics highlight the robustness of different methods to increasing levels of synthetic noise.\nnot consistently maintain the correct rank order of tumor\npercentages despite their strong linear relationships.\nThe CAM16 dataset poses unique challenges due to its\nheavily skewed tumor percentage distribution, with numer-\nous samples containing extremely low tumor burdens (per-\ncentages < 1%). This makes it difficult for models to dis-\ncern small differences in tumor percentage values, which is\nreflected in lower Spearman correlations (ρ < 0.51) despite\nstrong Pearson correlations (r > 0.81). WeSEG particularly\nunderperforms on this dataset due to its reliance on post-\nprocessed patch-level predictions, which are less effective in\ndetecting extremely small tumor regions.\n5.2\nAssessing tumor detection capabilities\nIn this subsection, we assess the tumor detection capabil-\nities on CAM16, ExaMode, and COBRA datasets, which\ncontain both tumor and normal cases. The ROC curves and\ncorresponding AUC scores for each method are presented in\nFigure 5(c-d-e).\nOn the CAM16 dataset, ABMIL achieves the strongest\nperformance with an AUC of 0.716, followed by MeanPool\n(0.707) and CLAM (0.698). WeSEG showed notably lower\nperformance with an AUC of 0.588. These results, however,\nare substantially lower compared to the state-of-the-art per-\nformance reported for binary classification approaches on\nthis dataset. This performance gap highlights the increased\ncomplexity of tumor percentage regression when compared\nto traditional binary classification tasks, where learning\nto distinguish between tumor and normal cases is more\nstraightforward.\nMost methods show substantially higher performance on\nthe ExaMode dataset.\nABMIL, CLAM, and MeanPool\nachieve nearly identical performance with AUC scores rang-\ning from 0.926 to 0.928. WeSEG, while showing improved\nabsolute performance compared to CAM16, still underper-\nforms with an AUC of 0.807. This suggests challenges in\nextracting a reliable tumor detection signal from weakly su-\npervised segmentation maps, limiting the performance and\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAUC Score\nABMIL\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCLAM\nDataset\nCAM16\nEXAMODE\nCOBRA\nOriginal\nAmplified\nAmplified+10%\nAmplified+30%\nAmplified+50%\nNoise Level\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAUC Score\nMeanPool\nOriginal\nAmplified\nAmplified+10%\nAmplified+30%\nAmplified+50%\nNoise Level\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWeSeg\nFigure 7: Impact of amplification on model performance.\nLine plot showing AUC variation with amplification across\nnoise levels.\nusability of the method.\nOn the COBRA dataset, all methods achieve strong per-\nformance. ABMIL leads with an AUC of 0.908. Interest-\ningly, WeSEG showed competitive performance here with\nan AUC of 0.889, followed by CLAM (0.880) and MeanPool\n(0.870). While these results are promising, they remain be-\nlow the reported benchmarks for binary tumor detection\ntasks on this dataset using classification labels, consistent\nwith our observations reported on CAM16.\n5.3\nAssessing the robustness to noise\nFigure 6(a-b) present the Pearson and Spearman correlation\nresults across different noise levels, showcasing the models’\nability to predict tumor percentages under increasing noise.\n7\n\n\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue Positive Rate\nCAM16 Tumor Detection\nnormal (AUC = 0.683 ± 0.058)\namplified (AUC = 0.893 ± 0.049)\namp+10% noise (AUC = 0.895 ± 0.023)\namp+30% noise (AUC = 0.881 ± 0.015)\namp+50% noise (AUC = 0.877 ± 0.020)\nRandom Chance (AUC = 0.5)\n(a)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue Positive Rate\nExaMode Tumor Detection\nnormal (AUC = 0.927 ± 0.011)\namplified (AUC = 0.964 ± 0.008)\namp+10% noise (AUC = 0.962 ± 0.008)\namp+30% noise (AUC = 0.961 ± 0.011)\namp+50% noise (AUC = 0.959 ± 0.012)\nRandom Chance (AUC = 0.5)\n(b)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue Positive Rate\nCOBRA Tumor Detection\nnormal (AUC = 0.887 ± 0.023)\namplified (AUC = 0.989 ± 0.003)\namp+10% noise (AUC = 0.993 ± 0.002)\namp+30% noise (AUC = 0.992 ± 0.003)\namp+50% noise (AUC = 0.992 ± 0.002)\nRandom Chance (AUC = 0.5)\nnormal (AUC = 0.887 ± 0.023)\namplified (AUC = 0.989 ± 0.003)\namp+10% noise (AUC = 0.993 ± 0.002)\namp+30% noise (AUC = 0.992 ± 0.003)\namp+50% noise (AUC = 0.992 ± 0.002)\nRandom Chance (AUC = 0.5)\n(c)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue Positive Rate\nCAM16 Tumor Detection Official Test Set\nCLAM - AUC=0.815\nRandom Chance (AUC = 0.5)\nCLAM - AUC=0.815\nRandom Chance (AUC = 0.5)\n(d)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue Positive Rate\nCOBRA Tumor Detection Official Test Set\nCLAM - AUC=0.992\nRandom Chance (AUC = 0.5)\nCLAM - AUC=0.992\nRandom Chance (AUC = 0.5)\n(e)\nFigure 8: (a-c) ROC curves for the CAM16, ExaMode, and COBRA datasets with confidence interval over the 5-fold cross\nvalidation, incorporating both amplification and noise effects. (d-e) ROC curves for the CAM16 and COBRA datasets\nevaluated on the official test sets.\nAs expected, the models trained with the true tumor per-\ncentages exhibit the best overall performance. Despite the\nexpected decrease in performance when noise is added, the\nresults show that the impact of noise is relatively moder-\nate. Notably, performance degradation becomes more pro-\nnounced when 50% noise is introduced, but this represents\nan extreme and unrealistic scenario in real-world applica-\ntions.\nFor the TNBC dataset, all models demonstrated strong\nrobustness to noise, maintaining high correlation scores even\nunder substantial noise.\nABMIL and MeanPool showed\nparticularly robust performance, with Pearson and Spear-\nman correlations decreasing by only 14% at 50% noise and\nmaintaining correlations above 0.83.\nCLAM, despite its\nstrong baseline, showed greater sensitivity, with a 20% de-\ncrease under the same conditions. WeSEG, while starting\nfrom a lower baseline correlation, showed robustness with a\n12% decrease. This indicates that its segmentation quality\nremained effective at identifying tumor areas despite the\nadded noise, indirectly supporting tumor percentage esti-\nmation.\nFor the AQUILA dataset, models were more affected by\nnoise overall. At 30% noise, correlations dropped by 17%\nfor MeanPool (Pearson: 0.964 to 0.804, Sperman: 0.962\nto 0.516) and up to 27% for WeSEG (Pearson: 0.815 to\n0.597, Sperman:\n0.8 to 0.607).\nAt 50% noise, ABMIL\nand MeanPool maintained correlations above 0.5 but ex-\nperienced 41% and 46% declines, respectively.\nWeSEG,\ndespite its lower baseline correlations, performed compara-\nbly to ABMIL and MeanPool under 50% noise, maintaining\ncorrelations around 0.57. Surprisingly, CLAM showed the\ngreatest sensitivity, with correlations declining by approxi-\nmately 80%.\nThe CAM16 dataset presented the most challenging sce-\nnario, with overall lower baseline correlation scores across\nall models. ABMIL and CLAM emerged as the most robust\nmodels, experiencing a 28% reduction in Pearson correla-\ntion at 50% noise (ABMIL: 0.847 to 0.612, CLAM: 0.811\nto 0.596). MeanPool, on the other hand, showed a larger\ndecrease of 37% (0.876 to 0.552), while WeSEG exhibited\nthe weakest performance with correlations dropping by over\n59%, indicating that the quality of the segmentation is more\naffected by the challenges posed by small lesions. Spearman\ncorrelations showed some unexpected trends, with CLAM’s\nperformances remaining stable, while ABMIL and WeSEG\nshowed initial drops at 30%, followed by an increase of per-\nformance at 50% noise.\nFor the ExaMode dataset, models demonstrated strong\nrobustness, with CLAM and MeanPool experiencing a drop\nof 10% at 50% noise (CLAM: 0.95 to 0.849, MeanPool:\n0.953 to 0.854), and ABMIL a drop of 13% (0.95 to 0.827).\nWeSEG showed higher sensitivity to noise, with a small\n6.6% decline at 30% noise but a significant 61% drop at\n50% noise.\nInterestingly, Spearman correlations for AB-\nMIL and CLAM improved with noise, whereas MeanPool\nshowed slight declines.\nLastly, all models performed robustly on the COBRA\ndataset, with MeanPool showing only a 6% drop (0.954\n8\n\n\nto 0.892) at 50% noise. CLAM, WeSEG and ABMIL fol-\nlowed with 15%, 20% and 21% decline, respectively. Spear-\nman correlations remained stable or slightly improved for\nCLAM, ABMIL, and MeanPool, while WeSEG showed mi-\nnor decreases.\nTo further assess robustness, we analyzed AUC scores\nacross noise levels for the CAM16, EXAMODE, and CO-\nBRA datasets (Figure 6c). ABMIL and CLAM consistently\nachieved the highest AUC values, underscoring the robust-\nness of attention-based methods. Interestingly, they showed\nimproved results under noise conditions across the three\ndatasets.\nFor ExaMode, ABMIL’s AUC increased from\n0.927 to 0.947, and CLAM’s from 0.927 to 0.943.\nSimi-\nlarly, in the COBRA dataset, CLAM’s AUC improved from\n0.887 to 0.974, and ABMIL’s improved from 0.909 to 0.963\nat 50% noise. For the CAM16 dataset, ABMIL showed a\nmodest improvement from 0.708 to 0.732, while CLAM’s\nAUC increased from 0.683 to 0.737. In contrast, WeSEG\nshowed a slight drop of performance on the three datasets,\nespecially on ExaMode (0.813 to 0.608) and COBRA (0.895\nto 0.782).\n5.4\nAssessing the role of the amplification\ntechnique\nFigure 7 shows AUC results across datasets and methods\nunder different noise levels, while Figure 8(a-c) highlights\nROC curves for the best-performing model, CLAM. As il-\nlustrated by the AUC results, the amplification technique\nsignificantly improved the performance of attention-based\nmethods such as ABMIL and CLAM. For CAM16, ABMIL’s\nAUC increased from 0.708 to 0.851, and CLAM’s from 0.683\nto 0.893, demonstrating enhanced detection of small lesions\nand better differentiation between tumor and tumor-free\ncases.\nIn contrast, MeanPool and WeSEG showed only\nminor improvements, with AUC increasing from 0.712 to\n0.743 for MeanPool, and from 0.58 to 0.59 for WeSEG.\nThese findings suggest that these models do not benefit\nfrom the amplification technique as much as ABMIL and\nCLAM. We also present the results with the injection of\nnoise, which align with the previous findings where we ob-\nserved that AUC generally remains stable or even improves\nwith added noise. On CAM16, ABMIL achieved its highest\nAUC (0.899) at 30% noise, while CLAM peaked at 0.895\nwith 10% noise.\nFor EXAMODE, the amplification technique similarly\nshowed robust improvements, especially with ABMIL and\nCLAM where AUC increased to 0.962 and 0.964, respec-\ntively.\nMeanPool showed only minor improvements from\n0.928 to 0.955.\nInterestingly, WeSEG experienced a sig-\nnificant drop in AUC to 0.534, indicating that amplifi-\ncation might not be beneficial for all models.\nFor CO-\nBRA, the AUC demonstrated a similar same trend, with\nattention-based methods consistently outperforming, main-\ntaining AUC always above 0.9, and WeSEG showing a de-\ncrease of performance after the amplification.\nTo further evaluate the benefits of using the amplification\ntechnique and provide results comparable to the literature,\nwe conducted an additional experiment on the official test\nsets for CAM16 and COBRA, in contrast to the previous\nresults computed using the combined test sets from 5-fold\ncross-validation. We trained two separate models with am-\nplification using the official splits from the publicly available\ndatasets, and the results are presented in Figure 8(d-e). For\nCOBRA, the amplification method achieved an AUC of 0.99\non the test set, aligning closely with the results reported in\n[16], where CLAM was applied to binary classification tasks.\nSimilarly, despite CAM16 showing the lowest performance\nin the original setting, the amplified model still achieved\nresults comparable to those reported in [12] for the corre-\nsponding classification task with CLAM, reaching an AUC\nof 0.81.\n5.5\nVisual interpretability\nTo evaluate interpretability, we first extract patch-level la-\nbels by assigning binary labels (0 or 1) to image patches\nbased on the percentage of tumor area within each patch.\nA threshold of 50% tumor content is used, where patches\nwith more than 50% tumor area are labeled as tumor. These\npatch-level labels serve as the reference standard, based on\nthe tumor annotations or segmentation masks, to quantify\nthe tumor presence within each patch. We then compare\nthese patch-level labels with the instance logits and atten-\ntion scores, and compute the AUC for each method.\nThe results in Table 1 reveal a significant performance\ngap. Interestingly, instance logits consistently yield much\nhigher AUC scores across various noise levels and datasets\ncompared to attention scores. Attention scores show consid-\nerable variability, with values ranging from low to moderate.\nTo further investigate this phenomenon, we analyzed the\nattention heatmaps and identified an interesting behavior.\nIn some cases, both the attention heatmaps and instance\nlogits heatmaps closely align with the segmentation map,\neffectively focusing on tumor regions, as shown in Figure\n9a. However, in other instances, as shown in Figure 9b, the\ninstance logits correctly focus on the tumor areas, while the\nattention heatmap mistakenly assigns higher scores to nor-\nmal tissue and lower scores to the tumor, producing an ”in-\nverted attention heatmap”. These variations lead to lower\noverall performance for attention scores and highlight their\nlimitations in regression tasks. While attention mechanisms\nhave proven highly effective in classification tasks for similar\ntumor detection scenarios, our findings suggest significant\nlimitations when applied to regression problems.\nAdditionally, we evaluate the impact of amplification\nand noise on interpretability, as shown in Figure 10. Am-\nplified models produce more consistent and better-aligned\nheatmaps, with attention focused more accurately on tu-\nmor regions. Additionally, the issue of ”inverted attention\nheatmaps” observed in some cases, appears to be mitigated\nwith amplification. Importantly, the qualitative results re-\nmain consistent across varying noise levels, with heatmaps\nshowing no degradation even at higher noise levels.\n6\nDiscussion and conclusion\nIn this study, we introduced a novel weakly-supervised re-\ngression framework for tumor detection in whole-slide im-\nages, leveraging tumor percentage as a clinically relevant\ntarget. While precise annotations or AI-generated segmen-\ntation masks were used in this study to compute ground\ntruth tumor percentages, these percentages can be easily\nestimated from the clinically available coarse annotations\nof the tumor burden routinely documented in molecular\npathology workflows. By shifting the focus from traditional\nbinary classification to regression, this framework addresses\n9\n\n\nModel\nTask\nTNBC\nAQUILA\nCAM16\nExaMode\nCOBRA\nAttention\nLogits\nAttention\nLogits\nAttention\nLogits\nAttention\nLogits\nAttention\nLogits\nABMIL\nNo noise\n0.539\n0.939\n0.568\n0.98\n0.511\n0.915\n0.648\n0.863\n0.738\n0.786\n10% noise\n0.383\n0.927\n0.457\n0.967\n0.719\n0.951\n0.591\n0.853\n0.737\n0.781\n30% noise\n0.325\n0.898\n0.219\n0.887\n0.865\n0.959\n0.625\n0.822\n0.727\n0.693\n50% noise\n0.542\n0.879\n0.353\n0.793\n0.763\n0.886\n0.715\n0.8\n0.732\n0.664\nAmplified\n-\n-\n-\n-\n0.841\n0.945\n0.738\n0.786\n0.849\n0.899\nCLAM\nNo noise\n0.554\n0.94\n0.736\n0.982\n0.571\n0.959\n0.774\n0.869\n0.741\n0.754\n10% noise\n0.449\n0.926\n0.619\n0.967\n0.594\n0.953\n0.705\n0.856\n0.733\n0.728\n30% noise\n0.256\n0.899\n0.307\n0.887\n0.669\n0.931\n0.707\n0.814\n0.731\n0.657\n50% noise\n0.272\n0.878\n0.27\n0.636\n0.653\n0.876\n0.756\n0.769\n0.728\n0.578\nAmplified\n-\n-\n-\n-\n0.935\n0.897\n0.741\n0.754\n0.848\n0.895\nMeanPool\nNo noise\n-\n0.936\n-\n0.982\n-\n0.924\n-\n0.863\n-\n0.793\n10% noise\n-\n0.925\n-\n0.974\n-\n0.928\n-\n0.851\n-\n0.787\n30% noise\n-\n0.897\n-\n0.945\n-\n0.933\n-\n0.832\n-\n0.775\n50% noise\n-\n0.873\n-\n0.789\n-\n0.93\n-\n0.828\n-\n0.769\nAmplified\n-\n-\n-\n-\n-\n0.924\n-\n0.793\n-\n0.854\nWeSEG\nNo noise\n-\n0.83\n-\n0.877\n-\n0.68\n-\n0.785\n-\n0.918\n10% noise\n-\n0.885\n-\n0.958\n-\n0.8\n-\n0.67\n-\n0.89\n30% noise\n-\n0.842\n-\n0.904\n-\n0.44\n-\n0.68\n-\n0.889\n50% noise\n-\n0.846\n-\n0.835\n-\n0.66\n-\n0.585\n-\n0.871\nAmplified\n-\n-\n-\n-\n-\n0.74\n-\n0.505\n-\n0.566\nTable 1: This table summarizes the AUC results for attention scores and instance logits compared to the true tumor\nmasks. Results are presented for different methods and datasets for training experiments without noise, with noise, and\nwith amplification.\n(a)\n(b)\nFigure 9: Visualization and comparison of logits heatmaps and attention heatmaps for tumor detection. The first row\nshows examples from the TNBC dataset, while the second row shows examples from the AQUILA dataset. The set of\nimages in column (a) shows good examples where the attention heatmap and logits heatmap align with tumor regions,\nwhile column (b) shows examples with reversed attention heatmaps, where the attention mechanism highlights non-tumor\nregions instead.\nthe challenges posed by datasets with scarce or absent nega-\ntive cases, offering a flexible alternative for tumor detection.\nOur experiments demonstrated that weakly-supervised\nmodels can effectively learn to estimate tumor percentages\nacross diverse datasets. More importantly, we showed that\nthese predicted percentages can serve as a proxy for tumor\ndetection, the primary aim of this framework. However, we\nrecognize that clinical annotations of tumor burden inher-\nently contain noise, introducing variability into the reference\nstandard. To address this, we simulated real-world noise\nin clinical practice by introducing synthetic noise into our\ndata. Our results showed that the regression framework re-\nmained robust even under significant noise levels, especially\nfor attention-based methods (ABMIL and CLAM), demon-\nstrating its potential usability despite noisy annotations.\nHowever, it is worth noting that our noise simulations, con-\nducted using uniform distributions with three noise levels\n(±10%, ±30%, ±50%), may not fully reflect the exact dis-\ntribution of variability found in clinically available annota-\ntions. Future works could aim at better modeling the noise\nencountered in clinical practice and integrating real-world\nnoisy annotations to better understand the model’s robust-\nness.\nAddressing tumor detection in datasets with small lesions\nproved to be challenging due to the difficulty of the regres-\nsion framework in distinguishing between small percentages\nand tumor-free cases, resulting in suboptimal performance\ncompared to classification benchmarks. To mitigate this, we\nintroduced a target amplification technique, which proved\nparticularly valuable for datasets containing biopsies or sub-\ntle lesions like CAM16, ExaMode, and COBRA. While tar-\nget amplification improves the model’s sensitivity to small\ntumor percentages, it introduces a trade-off by compressing\nhigher-end values, which may reduce the model’s precision\nin predicting tumor percentages for cases with extensive le-\nsions. This limitation could affect the accuracy of regres-\nsion predictions for large tumors. However, it is important\nto emphasize that the ultimate goal of this framework is\nrobust tumor detection, not precise tumor percentage re-\ngression.\nThe regression task is primarily used as a tool\n10\n\n\nFigure 10: Visualization and comparison of attention heatmaps for tumor detection, illustrating the effects of the ampli-\nfication technique and noise on the qualitative results. The figure demonstrates how amplification and increasing noise\nlevels affect the attention mechanism’s ability to highlight tumor regions.\nto enable training without relying on negative cases, but\nthe end goal remains the accurate identification of tumor\nregions.\nAn important focus of this study was on model inter-\npretability.\nAttention mechanisms, while widely used in\nMIL models for tumor segmentation in classification tasks,\nhave been less explored in regression tasks. Interestingly, in\nour regression framework, attention scores exhibited a coun-\nterintuitive behavior, often assigning lower scores to tumor\nregions and higher scores to normal regions, resulting in ”in-\nverted attention heatmaps”. Despite this, instance-based\nMIL frameworks allowed us to compare attention scores\nwith patch-level predictions, which proved to be more ef-\nfective in localizing tumor regions. Nevertheless, the inter-\npretability of attention scores for regression models remains\nan open challenge. While the amplification technique im-\nproved the performance of attention heatmaps, potentially\nmitigating the reversed behavior observed in unamplified\nsettings, attention mechanisms may require further refine-\nment and adaptation for use in regression tasks, where the\ntask is to predict a continuous value rather than making\nbinary classifications.\nOur approach has limitations. For this work, we relied on\nImageNet pre-trained encoders.\nAlthough ImageNet pre-\ntraining is a standard practice and has shown general util-\nity in histopathology task, using domain-specific pretrain-\ning could provide a better could provide a better repre-\nsentation of histopathology data.\nFuture work could ex-\nplore leveraging encoders pretrained on large-scale H&E-\nstained histopathology datasets, which may better capture\ndomain-specific features and improve generalizability. An-\nother area for improvement lies in the interpretability of\nattention-based heatmaps. Future research should focus on\ndesigning more robust interpretability techniques tailored\nfor regression, which could provide clearer insights into the\nmodel’s decision-making process.\nAdditionally, the fifth-\nroot transformation used for target amplification was em-\npirically effective but may not be the optimal choice for\nall datasets. Further investigations could explore alterna-\ntive amplification strategies, or even dynamic amplification\nmethods that adapt based on the specific characteristics of\nthe dataset. Finally, while we simulated noise to evaluate\nrobustness, a reader study with multiple pathologists an-\nnotating the tumor burden on the same slides would help\nquantify the actual variability in clinical annotations and\nprovide valuable insights into the practical applicability of\nthis framework.\nAcknowledgments\nWe would like to thank Daan Geijs for the development\nof the skin segmentation model, John-Melle Bokhorst for\nthe development of the colon segmentation model and Mart\nRijthoven for the development of the breast segmentation\nmodel. This project has received funding from the Innova-\ntive Medicines Initiative 2 Joint Undertaking under grant\nagreement No 945358. This Joint Undertaking receives sup-\nport from the European Union’s Horizon 2020 research and\ninnovation program and EFPIA (www.imi.europe.eu).\n7\nDeclaration of Generative AI and\nAI-assisted technologies in the\nwriting process\nDuring the preparation of this work the author(s) used\nChatGPT in order to improve readability and language.\nAfter using this tool/service, the author(s) reviewed and\nedited the content as needed and take(s) full responsibility\nfor the content of the publication.\nReferences\n[1] Aswolinskiy,\nW.,\n2023.\npathology-whole-slide-\npacker.\nhttps://github.com/DIAGNijmegen/\npathology-whole-slide-packer.\n[2] Aswolinskiy, W., Tellez, D., Raya, G., van der Woude,\nL., Looijen-Salamon, M., van der Laak, J., Grunberg,\nK., Ciompi, F., 2021. Neural image compression for\nnon-small cell lung cancer subtype classification in HE\nstained whole-slide images, in: Medical Imaging 2021:\nDigital Pathology, SPIE. pp. 1 – 7. doi:10.1117/12.\n2581943.\n[3] Balkenhol, M.C.A., Vreuls, W., Wauters, C.A.P., Mol,\nS.J.J., van der Laak, J.A.W.M., Bult, P., 2020. His-\ntological subtypes in triple negative breast cancer are\n11\n\n\nassociated with specific information on survival. An-\nnals of Diagnostic Pathology 46, 151490. doi:10.1016/\nj.anndiagpath.2020.151490.\n[4] B´andi, P., Balkenhol, M., van Ginneken, B., van der\nLaak, J., Litjens, G., 2019.\nResolution-agnostic tis-\nsue segmentation in whole-slide histopathology images\nwith convolutional neural networks.\nPeerJ 7, e8242.\ndoi:10.7717/peerj.8242.\n[5] Bokhorst, J., Nagtegaal, I., Fraggetta, F., Vatrano, S.,\nMesker, W., Vieth, M., van der Laak, J.A., Ciompi, F.,\n2023. Deep learning for multi-class semantic segmen-\ntation enables colorectal cancer detection and classifi-\ncation in digital pathology images 13, 8398. doi:10.\n1038/s41598-023-35491-z.\n[6] Campanella, G., Hanna, M.G., Geneslaw, L., Mi-\nraflor, A., Werneck Krauss Silva, V., Busam, K.J.,\nBrogi, E., Reuter, V.E., Klimstra, D.S., Fuchs, T.J.,\n2019.\nClinical-grade computational pathology using\nweakly supervised deep learning on whole slide im-\nages. Nature Medicine 25, 1301–1309. doi:10.1038/\ns41591-019-0508-1.\n[7] Carbonneau,\nM.A.,\nCheplygina,\nV.,\nGranger,\nE.,\nGagnon, G., 2018. Multiple Instance Learning: A Sur-\nvey of Problem Characteristics and Applications. Pat-\ntern Recognition 77, 329–353. doi:10.1016/j.patcog.\n2017.10.009.\n[8] Chen, R.J., Chen, C., Li, Y., Chen, T.Y., Trister, A.D.,\nKrishnan, R.G., Mahmood, F., 2022.\nScaling vision\ntransformers to gigapixel images via hierarchical self-\nsupervised learning, in: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recogni-\ntion (CVPR), pp. 16144–16155.\n[9] Chikontwe,\nP.,\nKim,\nM.,\nNam,\nS.J.,\nGo,\nH.,\nPark,\nS.H.,\n2020.\nMultiple\ninstance\nlearning\nwith center embeddings for histopathology classifica-\ntion, in:\nInternational Conference on Medical Im-\nage Computing and Computer-Assisted Intervention,\nSpringer. pp. 519–528.\ndoi:https://doi.org/10.\n1007/978-3-030-59722-1_50.\n[10] Contreras, N.S.L., Grisi, C., Aswolinskiy, W., Vatrano,\nS., Fraggetta, F., Nagtegaal, I., D’Amato, M., Ciompi,\nF., 2024.\nBenchmarking hierarchical image pyramid\ntransformer for the classification of colon biopsies and\npolyps histopathology images, pp. 1–4. doi:10.1109/\nisbi56570.2024.10635841.\n[11] Cruz-Roa, A., Basavanhally, A., Gonz´alez, F., Gilmore,\nH., Feldman, M., Ganesan, S., Shih, N., Tomaszewski,\nJ., Madabhushi, A., 2014. Automatic detection of inva-\nsive ductal carcinoma in whole slide images with con-\nvolutional neural networks, in: Medical Imaging 2014:\nDigital Pathology, SPIE. p. 904103. doi:10.1117/12.\n2043872.\n[12] Dooper, S., Pinckaers, H., Aswolinskiy, W., Hebeda,\nK., Jarkman, S., van der Laak, J., Litjens, G., 2023.\nGigapixel end-to-end training using streaming and at-\ntention. Medical Image Analysis 88, 102881. doi:10.\n1016/j.media.2023.102881.\n[13] Ehteshami Bejnordi, B., Veta, M., Johannes van Di-\nest, P., van Ginneken, B., Karssemeijer, N., Litjens,\nG., van der Laak, J.A.W.M., the CAMELYON16 Con-\nsortium, Hermsen, M., Manson, Q.F., Balkenhol, M.,\nGeessink, O., Stathonikos, N., van Dijk, M.C., Bult, P.,\nBeca, F., Beck, A.H., Wang, D., Khosla, A., Gargeya,\nR., Irshad, H., Zhong, A., Dou, Q., Li, Q., Chen,\nH., Lin, H.J., Heng, P.A., Haß, C., Bruni, E., Wong,\nQ., Halici, U., ¨Oner, M.¨U., Cetin-Atalay, R., Berseth,\nM., Khvatkov, V., Vylegzhanin, A., Kraus, O., Sha-\nban, M., Rajpoot, N., Awan, R., Sirinukunwattana,\nK., Qaiser, T., Tsang, Y.W., Tellez, D., Annuscheit,\nJ., Hufnagl, P., Valkonen, M., Kartasalo, K., Lato-\nnen, L., Ruusuvuori, P., Liimatainen, K., Albarqouni,\nS., Mungal, B., George, A., Demirci, S., Navab, N.,\nWatanabe, S., Seno, S., Takenaka, Y., Matsuda, H.,\nAhmady Phoulady, H., Kovalev, V., Kalinovsky, A.,\nLiauchuk, V., Bueno, G., Fernandez-Carrobles, M.M.,\nSerrano, I., Deniz, O., Racoceanu, D., Venˆancio, R.,\n2017.\nDiagnostic Assessment of Deep Learning Al-\ngorithms for Detection of Lymph Node Metastases in\nWomen With Breast Cancer. JAMA 318, 2199–2210.\ndoi:10.1001/jama.2017.14585.\n[14] El Nahhas, O.S.M., Loeffler, C.M.L., Carrero, Z.I., van\nTreeck, M., Kolbinger, F.R., Hewitt, K.J., Muti, H.S.,\nGraziani, M., Zeng, Q., Calderaro, J., Ortiz-Br¨uchle,\nN., Yuan, T., Hoffmeister, M., Brenner, H., Brobeil,\nA., Reis-Filho, J.S., Kather, J.N., 2024. Regression-\nbased Deep-Learning predicts molecular biomarkers\nfrom pathology slides.\nNature Communications 15,\n1253. doi:10.1038/s41467-024-45589-1.\n[15] Fatima, S., Ali, S., Kim, H.C., 2023. A Comprehensive\nReview on Multiple Instance Learning. Electronics 12,\n4323. doi:10.3390/electronics12204323.\n[16] Geijs, D.J., Dooper, S., Aswolinskiy, W., Hillen, L.M.,\nAmir, A.L., Litjens, G., 2024. Detection and subtyping\nof basal cell carcinoma in whole-slide histopathology\nusing weakly-supervised learning. Medical Image Anal-\nysis 93, 103063. doi:10.1016/j.media.2023.103063.\n[17] Graziani, M., Marini, N., Deutschmann, N., Janakara-\njan, N., M¨uller, H., Rodriguez Martinez, M., 2022.\nAttention-based Interpretable Regression of Gene Ex-\npression in Histology, Interpretability of Machine In-\ntelligence in Medical Image Computing. pp. 44–60.\ndoi:10.1007/978-3-031-17976-1_5.\n[18] Grisi,\nC.,\n2023.\nhs2p.\nhttps://github.com/\nclemsgrs/hs2p.\n[19] Halicek, M., Shahedi, M., Little, J.V., Chen, A.Y., My-\ners, L.L., Sumer, B.D., Fei, B., 2019. Head and Neck\nCancer Detection in Digitized Whole-Slide Histology\nUsing Convolutional Neural Networks. Scientific Re-\nports 9, 14043. doi:10.1038/s41598-019-50313-x.\n[20] Ilse, M., Tomczak, J.M., Welling, M., 2018. Attention-\nbased Deep Multiple Instance Learning, in: Proceed-\nings of the 35th International Conference on Machine\nLearning, pp. 2127–2136.\n[21] Krieger, N., Hiatt, R.A., Sagebiel, R.W., Clark, W.H.,\nMihm, M.C., 1994.\nInter-observer variability among\n12\n\n\npathologists’ evaluation of malignant melanoma: Ef-\nfects upon an analytic study. Journal of Clinical Epi-\ndemiology 47, 897–902. doi:10.1016/0895-4356(94)\n90193-7.\n[22] Lerousseau, M., Classe, M., Battistella, E., Estienne,\nT., Henry, T., Leroy, A., Sun, R., Vakalopoulou, M.,\nScoazec, J.Y., Deutsch, E., Paragios, N., 2021. Weakly\nsupervised pan-cancer segmentation tool, Medical Im-\nage Computing and Computer Assisted Intervention –\nMICCAI 2021. doi:10.1007/978-3-030-87237-3_24.\n[23] Lu,\nM.Y.,\nWilliamson,\nD.F.,\nChen,\nT.Y.,\nChen,\nR.J.,\nBarbieri,\nM.,\nMahmood,\nF.,\n2021.\nData-\nefficient and weakly supervised computational pathol-\nogy on whole-slide images. Nature Biomedical Engi-\nneering 5, 555–570.\ndoi:https://doi.org/10.1038/\ns41551-020-00682-w.\n[24] Marr´on-Esquivel,\nJ.M.,\nDuran-Lopez,\nL.,\nLinares-\nBarranco, A., Dominguez-Morales, J.P., 2023.\nA\ncomparative study of the inter-observer variability\non Gleason grading against Deep Learning-based ap-\nproaches for prostate cancer. Computers in Biology and\nMedicine 159, 106856.\ndoi:10.1016/j.compbiomed.\n2023.106856.\n[25] Pinckaers, H., Bulten, W., Litjens, G., 2019.\nHigh\nresolution whole prostate biopsy classification using\nstreaming stochastic gradient descent, in:\nMedical\nImaging 2019:\nDigital Pathology.\ndoi:10.1117/12.\n2512817.\n[26] van Rijthoven, M., Balkenhol, M., Silina, K., van der\nLaak, J., Ciompi, F., 2021. Hooknet: Multi-resolution\nconvolutional neural networks for semantic segmenta-\ntion in histopathology whole-slide images. Medical Im-\nage Analysis 68, 101890. doi:10.1016/j.media.2020.\n101890.\n[27] Schirris, Y., Engelaer, M., Panteli, A., Horlings, H.M.,\nGavves, E., Teuwen, J., 2022. WeakSTIL: Weak whole-\nslide image level stromal tumor infiltrating lymphocyte\nscores are all you need, SPIE Medical Imaging 2022.\ndoi:10.1117/12.2611528.\n[28] Tellez, D., Litjens, G., van der Laak, J., Ciompi,\nF., 2021.\nNeural image compression for gigapixel\nhistopathology image analysis. IEEE Transactions on\nPattern Analysis and Machine Intelligence 43, 567–578.\ndoi:10.1109/TPAMI.2019.2936841.\n[29] van der Laak, J., Litjens, G., Ciompi, F., 2021.\nDeep learning in histopathology:\nThe path to the\nclinic.\nNature Medicine 27, 775–784.\ndoi:10.1038/\ns41591-021-01343-4.\n[30] Wang, X., Chen, H., Gan, C., Lin, H., Dou, Q.,\nTsougenis, E., Huang, Q., Cai, M., Heng, P.A., 2020.\nWeakly Supervised Deep Learning for Whole Slide\nLung Cancer Image Analysis.\nIEEE transactions on\ncybernetics 50, 3950–3962. doi:10.1109/TCYB.2019.\n2935141.\n[31] Weitz, P., Wang, Y., Hartman, J., Rantalainen, M.,\n2021.\nAn investigation of attention mechanisms in\nhistopathology whole-slide-image analysis for regres-\nsion objectives, in:\n2021 IEEE/CVF International\nConference on Computer Vision Workshops (ICCVW),\nIEEE, Montreal, BC, Canada. pp. 611–619.\ndoi:10.\n1109/ICCVW54120.2021.00074.\n[32] Xia, T., Kumar, A., Feng, D., Kim, J., 2018. Patch-\nlevel Tumor Classification in Digital Histopathology\nImages with Domain Adapted Deep Learning, in: 2018\n40th Annual International Conference of the IEEE En-\ngineering in Medicine and Biology Society (EMBC),\nIEEE, Honolulu, HI. pp. 644–647. doi:10.1109/EMBC.\n2018.8512353.\n13\n\n\n"}
{"text": "PersuasiveToM: A Benchmark for Evaluating Machine Theory of Mind\nin Persuasive Dialogues\nFangxu Yu1\nLai Jiang 2\nShenyi Huang 3\nZhen Wu1*\nXinyu Dai1\n1National Key Laboratory for Novel Software Technology, Nanjing University, China\n1School of Artificial Intelligence, Nanjing University, China\n2Department of Computer Science and Engineering, Shanghai Jiao Tong University\n3University of California, San Diego, CA, USA\nyufx@smail.nju.edu.cn\njianglai0023-sjth@sjtu.edu.cn\nshh058@ucsd.edu\n{wuz, daixinyu}@nju.edu.cn\nAbstract\nThe ability to understand and predict the mental\nstates of oneself and others, known as the The-\nory of Mind (ToM), is crucial for effective so-\ncial interactions. Recent research has emerged\nto evaluate whether Large Language Models\n(LLMs) exhibit a form of ToM. Although recent\nstudies have evaluated ToM in LLMs, existing\nbenchmarks focus predominantly on physical\nperception with principles guided by the Sally-\nAnne test in synthetic stories and conversations,\nfailing to capture the complex psychological\nactivities of mental states in real-life social in-\nteractions. To mitigate this gap, we propose\nPERSUASIVETOM, a benchmark designed to\nevaluate the ToM abilities of LLMs in persua-\nsive dialogues. Our framework introduces two\ncategories of questions: (1) ToM Reasoning, as-\nsessing the capacity of LLMs to track evolving\nmental states (e.g., desire shifts in persuadees),\nand (2) ToM Application, evaluating whether\nLLMs can take advantage of inferred mental\nstates to select effective persuasion strategies\n(e.g., emphasize rarity) and evaluate the effec-\ntiveness of persuasion strategies. Experiments\nacross eight state-of-the-art LLMs reveal that\nwhile models excel on multiple questions, they\nstruggle to answer questions that need tracking\nthe dynamics and shifts of mental states and\nunderstanding the mental states in the whole\ndialogue comprehensively. Our aim with PER-\nSUASIVETOM is to allow an effective evalua-\ntion of the ToM reasoning ability of LLMs with\nmore focus on complex psychological activities.\nOur code is available at https://github.com/Yu-\nFangxu/PersuasiveToM.\n1\nIntroduction\nTheory of Mind (ToM) involves the ability to rea-\nson about mental states both in oneself and in oth-\ners (Premack and Woodruff, 1978). This capac-\nity strengthens many aspects of human cognition\nand social reasoning, enabling individuals to infer\n*\nCorresponding author.\n……\nHey Alice, how about trying something different this \nweekend like botanical garden tour?\nI don’t know Bob. I really wanted to go shopping this \nweekend. What’s so special about these ﬂowers?\nTurn 1\nHow about this: we go on the botanical garden tour \nin the morning, and then we can go shopping \nafterward? That way, we can enjoy both activities!\nAlright, that seems like a fair compromise. Let’s do \nthe tour and shopping afterward.\nTurn 4\nTheory of Mind Question\nDesire Question\n- Q: Is Alice likely to join \nthe botanical garden tour?\n- A: Not likely to join the \nbotanical garden tour\nTheory of Mind Application\nPrediction Question\nQ: What Strategy will Bob use?\nA: Emphasize rarity\nJudgement Question\n- Q: Is this strategy effective?\n- A: Yes\nBelief Question\n- Q: What will Bob believe Alice \nattitude?\n- A: Alice is hesitant.\nIntention Question\n- Q: What are the intentions of \nAlice?\n- A: Alice wants to go shopping\nBob\nAlice\nBob\nAlice\nTheory of Mind Question\nDesire Question\n- Q: Is Alice likely to join \nthe botanical garden tour?\n- A: Likely to join the \nbotanical garden tour\nBelief Question\n- Q: What will Bob believe \nAlice’s attitude?\n- A: Alice is agreeable.\nIntention Question\n- Q: What are the intentions of \nAlice?\n- A: Alice agrees to do the tour.\nFigure 1: An example in PERSUASIVETOM. Bob is\npersuading Alice to join the botanical garden tour.\nand simulate the mental states of others (Gopnik\nand Wellman, 1992; Baron-Cohen et al., 1985).\nToM is essential for various cognitive and social\nprocesses, including predicting actions (Dennett,\n1988), planning based on others’ beliefs and an-\nticipated behaviors, and facilitating reasoning and\ndecision making (Pereira et al., 2016; Rusch et al.,\n2020).\nRecent advances in Large Language Models\n(LLMs) have demonstrated performance compara-\nble to humans in problem-solving tasks. To assess\n1\narXiv:2502.21017v1  [cs.CL]  28 Feb 2025\n\n\nwhether LLMs exhibit high-level reasoning abili-\nties regarding mental states, various studies have\nproposed benchmarks to evaluate their capacity to\nhandle ToM tasks. A foundational concept in these\nbenchmarks is the Sally-Anne test (Baron-Cohen\net al., 1985), which has inspired the development\nof ToM evaluation frameworks (Gu et al., 2024; He\net al., 2023). In this test, Anne secretly moves an\nobject initially known to both Sally and Anne, lead-\ning Sally to hold a false belief about the object’s\nlocation. The task requires participants to reason\nabout \"Where will Sally look for the object?\". Al-\nthough this test assesses basic awareness, it falls\nshort in capturing the complex dynamics of mental\nstates in real-life social interactions and may not\nfully reflect ToM abilities in practical scenarios. To\nbetter simulate real-world social contexts, some\nbenchmarks have been developed around commu-\nnication scenarios (Kim et al., 2023; Chan et al.,\n2024). However, these benchmarks focus primarily\non inferring the scope of information awareness\nand often involve characters in equal positions en-\ngaged in information exchange. This limits the\nToM evaluation of reasoning psychological states.\nAdditionally, current ToM benchmarks often over-\nlook the critical step of applying ToM reasoning\nto predict actions, which is a key component of\nadvanced social cognition.\nTo address these limitations, we introduce PER-\nSUASIVETOM, a benchmark designed to evalu-\nate LLMs in real-life social scenarios, focusing on\nToM reasoning and its practical applications. PER-\nSUASIVETOM is built around persuasive dialogue,\nplacing LLMs in a realistic social interaction sce-\nnario characterized by asymmetrical social status\nfor complex psychological activities. Unlike tradi-\ntional benchmarks that focus on information aware-\nness (e.g., the location of an object), PERSUASIVE-\nTOM draws inspiration from the Belief-Desire-\nIntention (BDI) model (Bratman, 1987; Georgeff\net al., 1999) to shift the focus from physical percep-\ntion to psychological states, such as a character’s\nattitude toward an event, which are more complex\nto reason about. Furthermore, PERSUASIVETOM\ngoes beyond reasoning about mental states by as-\nsessing how well LLMs can predict actions (e.g.,\npersuasion strategies) based on their understanding\nof mental states and evaluate the effectiveness of\nthese strategies based on the persuadee’s reactions.\nOur evaluation results reveal several key find-\nings: (1) LLMs score significantly lower than hu-\nmans on questions requiring reasoning about dy-\nnamic changes (e.g., the persuadee’s shifting de-\nsires) but perform competitively to humans on\nstatic aspects (e.g., the persuader’s desires). (2)\nWhile Chain-of-Thought (CoT) (Wei et al., 2022)\nprompting does not substantially improve perfor-\nmance on mental state reasoning, it enhances per-\nformance for most LLMs in predicting persuasion\nstrategies. (3) LLMs exhibit distinct error patterns\nwhen reasoning about the persuader versus the per-\nsuadee, even when the question types are identical.\n(4) LLMs struggle to truly understand the dynamics\nof mental states of the whole dialogue, performing\nnotably worse than humans in this regard.\n2\nRelated Works\nTheory of Mind Benchmarks.\nExisting ToM\nevaluation benchmarks for LLMs are mainly text\nstory-based QA forms (Gandhi et al., 2024; Le\net al., 2019; Kim et al., 2023; He et al., 2023; Gu\net al., 2024) with multi-modal extensions (Jin et al.,\n2024a; Shi et al., 2024), which adapt or extend the\nSally-Anne test (Baron-Cohen et al., 1985). The\nquestions in these benchmarks ask a model to select\nthe true hypothesis of a person’s knowledge and\nbelief based on a given premise. However, such\nbenchmarks are inherently suffer from shortcuts\nand spurious correlations (Sclar et al., 2023; Ull-\nman, 2023; Shapira et al., 2023; Ma et al., 2023)\nMoreover, most story-based benchmarks simply\nask characters where the objects are located, which\nlacks a real social interaction scenario. To further\naddress this weakness, (Hou et al., 2024) evalu-\nates the ToM of agents in a situated environment,\n(Chan et al., 2024) evaluates the ability of ToM in a\nnegotiation environment, yet it is limited to bargain-\ning specific resources, water, food, and firewood.\nPersuasion scenarios involve more complex psy-\nchological activities and unequal character relation-\nships that lead to differences in the mental states of\nboth parties. Therefore, this work introduces PER-\nSUASIVETOM, which assesses an LLM’s ability\nto accurately reason the mental states of individ-\nuals in persuasive multi-turn dialogues. We also\nevaluate whether they can appropriately apply the\nunderstanding of mental states for persuasion strat-\negy prediction and evaluation, which connects ToM\nreasoning with decision-making in social scenarios.\nPersuasive Dialogue.\nPersuasive dialogues aim\nto influence the beliefs, attitudes, or behaviors of\nindividuals through communication strategies (Shi\net al., 2020). Recent works have tried to develop\n2\n\n\nType\nPERSUASIVETOM Questions\nDesire Question\nIs <Persuader/Persuadee> likely to <Target of Persuasion> ?\nBelief Question\nWhat will <Persuader/Persuadee> believe <Persuadee/Persuader>’s attitude towards <Target of Persuasion> ?\nIntention Question\nWhat are the intentions of <Persuader/Persuadee> expressed in <Utterance> given the dialogue history?\nPrediction Question\nWhat strategy will the persuader use next?\njudgement Question\n<Persuader> will adopt <Strategy> to persuade <Persuadee> to <Target of Persuasion>. Is this strategy (not) effective?\nToM Reasoning\nToM Application\nDialogue\nDesire\nBelief\nIntention\nPrediction\nJudgement\n1st Turn Dialogue\nBob says: Hey Alice, how about trying something\nPersuade Alice to\nAlice is\nIntent to make the other\nEmphasize rarity\nStrategy \"Emphasize\ndifferent this weekend? The botanical garden tour\njoin the botanical\nhesitant\nperson feel the experience or\nrarity\" is Effective\nis a unique experience, and you’ll get to take\ngarden tour\nobjects are unique or scarce.\nstunning pictures of the exotic flowers on display!\nAlice says: I don’t know, Bob. I really wanted to\nNot likely to join the\nBob is\nAlice wants to go shopping.\ngo shopping this weekend. What’s so special\nbotanical garden tour\nenthusiastic\nabout these flowers?\n2nd Turn Dialogue\nBob says: These flowers are incredibly rare, and\nPersuade Alice to\nAlice is\nIntent to make the other\nMention garden’s\nStrategy \"Mention\nit’s not often that you get to see such a diverse and\njoin the botanical\ncurious\nperson feel the experience or\nhistory\ngarden’s history\" is\nexotic collection up close. Some of these plants are\ngarden tour\nobjects are unique or scarce.\neffective\nnot found anywhere else in the world!\nAlice says: Really? That sounds interesting, but\nNeutral to join the\nBob is\nAlice is curious but not\nI’m not sure if it’s worth giving up shopping for.\nbotanical garden tour\nexcited\nyet convinced.\n3rd Turn Dialogue\nBob says: The botanical garden has a rich history,\nPersuade Alice to\nAlice is\nIntent to demonstrating the\nSuggest shopping\nStrategy \"Suggest\nand the expert guides can teach us so much about\njoin the botanical\nconsidering\nexpertise of the domain\nafterward\nshopping afterward\"\nthe plants and their unique stories. Plus, it’s a great\ngarden tour\nand showing authority.\nis effective\nopportunity to learn something new while enjoying\nnature’s beauty.\nAlice says: Hmm, that does sound intriguing, but I\nNot likely to join the\nBob is\nAlice is considering the idea\nstill want to go shopping.\nbotanical garden tour\ninformative\nbut still wants to shop.\n...\nTable 1: An example dialogue from the PERSUASIVETOM benchmark, illustrates the tracking of mental states\n(desire, belief, intention) and the application of ToM reasoning in predicting and evaluating persuasion strategies\nacross multiple turns. The upper part contains questions in the PERSUASIVETOM benchmark.\ndatasets or facilitate LLMs with persuasion tech-\nniques to achieve specific goals. Previous datasets\nare constructed by crowd-sourcing (Wang et al.,\n2019) or synthesizing with LLMs (Zhou et al.,\n2023; Jin et al., 2024b). Many of the previous\nworks build an effective persuasive dialogue sys-\ntem from emotional influence (Samad et al., 2022),\nsocial facts (Chen et al., 2022), and strategies (Tian\net al., 2020; Jin et al., 2023). Previous persuasion\ntechniques have been widely adopted to jailbreak\nLLMs (Zeng et al., 2024), mislead LLMs (Xu et al.,\n2023), as well as for information retrieval (Furumai\net al., 2024). A similar work (Sakurai and Miyao,\n2024) evaluates the intention detection abilities of\nLLMs in persuasive dialogues; however, PERSUA-\nSIVETOM introduces a more comprehensive bench-\nmark to assess the ToM abilities of LLMs in such\ncontexts.\n3\nPersuasiveToM Benchmark\n3.1\nOverview\nIn constructing the PERSUASIVETOM benchmark,\nwe aim to evaluate the Theory of Mind (ToM) abil-\nities of LLMs in dynamic, multi-turn persuasive di-\nalogues with asymmetric social status, which leads\nto different mental states. Our dataset construction\nfocuses on two primary dimensions: ToM Rea-\nsoning (§3.2) and ToM Application (§3.3). ToM\nReasoning assesses the models’ ability to track and\nunderstand the evolving mental states of both the\npersuader and the persuadee, including desires, be-\nliefs, and intentions. ToM Application evaluates\nwhether LLMs can leverage their inferred under-\nstanding of mental states to select and apply effec-\ntive persuasion strategies, such as predicting the\nnext strategy or judging the effectiveness of a given\nstrategy based on the persuadee’s reactions. There\nare several key considerations when constructing\nPERSUASIVETOM. (1) The dataset should contain\ndiverse persuasive domains (e.g., life, education,\ntechnology, etc.) to ensure a comprehensive eval-\nuation in the social context. (2) The mental states\nshould be changed after multi-turn interactions to\nassess whether LLMs can track the shift in the dia-\nlogue. (3) The mental states of both parties should\nbe asymmetric (e.g., persuaders has relatively sta-\n3\n\n\nble mental states with the guidance of static desire\nfor the goal, yet persuadee’s mental states shift\ndrastically under the proactive persuasion by per-\nsuaders.)\nTable 1 presents an example of PERSUASIVE-\nTOM, illustrating how mental states are tracked\nand how ToM reasoning is applied across multi-\nple turns in a persuasive dialogue. The example\ndemonstrates the dynamic nature of desire shifts,\nbelief updates, and intention inferences, as well as\nthe application of these mental states to predict and\nevaluate persuasion strategies. This example high-\nlights the complex psychological activities of real-\nworld persuasive interactions and the challenges\nLLMs face in accurately reasoning in such social\ncontexts.\n3.2\nToM Reasoning\nIn PERSUASIVETOM, we break down ToM reason-\ning into three core reasoning tasks: Desire Rea-\nsoning, Belief Reasoning, and Intention Reason-\ning for evaluation, which matches Belief-Desire-\nIntention (BDI) modeling (Bratman, 1987). Ques-\ntions are listed in Table 1.\nDesire Reasoning.\nDesire represents a motiva-\ntional state that drives behavior but does not neces-\nsarily imply a firm commitment (Malle and Knobe,\n2001; Kavanagh et al., 2005). Desires are seen\nas either fulfilled or unfulfilled which is different\nform beliefs that are evaluated in terms of truth or\nfalsity. In PERSUASIVETOM, we evaluate LLMs’\nability to comprehend and track the evolution of\ndesires in both persuaders and persuadees. For\nthe persuader, the desire is typically static, repre-\nsenting their goal (e.g., Persuade Alice to join the\nbotanical garden tour). For the persuadee, however,\ndesires are dynamic and shift in response to the\npersuader’s tactics (e.g., Alice’s initial desire to\nshop transforms into a willingness to compromise).\nTo assess this, we design Desire Questions that\nprobe two key aspects: (1) Can LLMs consistently\nidentify the persuader’s static desire throughout\nthe dialogue? (2) Can LLMs track the dynamics\nof the persuadee’s desire shifting from refusal or\ndisinterest to being persuaded?\nBelief Reasoning.\nBelief is a cognitive state\nwhere an individual holds a particular perspective,\nattitude, or viewpoint regarding a given proposi-\ntion or idea. In PERSUASIVETOM, beliefs refer\nto understanding and reasoning the attitudes of the\nPrinciples\nIntentions\nReciprocity\nMake the other person feel\naccepted through concessions,\npromises, or benefits.\nScarcity\nMake the other person feel the\nexperience or objects are unique\nor scarce.\nConsensus\nRefer to what other people are\ndoing, or what they have already\npurchased or done.\nAuthority\nDemonstrating expertise of the\ndomain and showing authority.\nCommitment\n&\nconsistency\nEncourage the other person to\ncommit to take the first step and\nbe consistent.\nLiking\nPraising other people or finding\ncommon characteristics to im-\nprove the other person’s liking.\nTable 2: Intention mapping from the persuasive princi-\nples. Refer to Appendix C for definitions of persuasive\nprinciples.\nopponent toward the goal, which is explicitly or im-\nplicitly expressed in the dialogue. For example, in\nTurn 1, Bob believes Alice is hesitant about the tour,\nwhile Alice believes Bob is enthusiastic. By Turn 3,\nBob’s belief shifts to thinking Alice is considering\nthe idea, while Alice becomes more informed about\nthe garden’s history. Belief Questions ask LLMs\nto infer what the will <persuader/persuadee> be-\nlieve <persuadee/persuader>’s attitude towards the\npersuasion goal. These questions require models\nto understand cues in utterances and update beliefs\ndynamically as the dialogue progresses.\nIntention Reasoning.\nIntentions represent delib-\nerate commitments to pursue specific goals based\non desires and beliefs, often linked to tangible ac-\ntions aimed at achieving those objectives. Inten-\ntions have been extensively studied in psychology\ntests such as action prediction (Phillips et al., 2002)\nand behavioral re-enactment (Meltzoff, 1995). In-\nspired by persuasion principles (Cialdini and Gold-\nstein, 2004; Cialdini and Cialdini, 2007), we de-\nvelop a mapping from persuasion principles to in-\ntentions, as shown in table 2. In persuasive dia-\nlogue, persuasive strategies have a strong associa-\ntion with intentions (Wang et al., 2019). In PER-\nSUASIVETOM, we collect the persuasive strategies\nfrom the PersuasionDaily dataset and their corre-\nsponding utterances for prompting the LLMs to\nchoose the most appropriate intentions from ta-\nble 2. The details of the extraction is recorded in\n4\n\n\nSocial and Welfare\nCulture and Art\nBusiness and Careers\nLife and Leisure\nTech and Innovation\nHealth and Ecology\nWelfare\nCharity\nFamily\nEthics\nLaw\nPolitics\nEducation\nArt\nLiterature\nPhilosophy\nCulture\nFashion\nMedia\nHistory\nCareer\nBusiness\nFinance\nEconomics\nNegotiation\nMarketing\nTravel\nLeisure\nLifestyle\nSport\nDebate\nCraftsmanship\nTechnology\nInnovation\nScience\nResearch\nArchitecture\nHealth\nEcology\nSafety\nPsychology\nFigure 2: Domains of PERSUASIVETOM. Under 6 pri-\nmary topics and 35 domains in total.\nAppendix A. For the persuader, we ask LLMs to\nchoose the most appropriate intention from the six\ndesigned intention choices. For the persuadee, in-\ntentions are extracted by LLMs from utterances\nthat are unrelated to persuasion intention.\n3.3\nToM Application\nWhile ToM reasoning plays a crucial role, it is\nequally important to analyze how LLMs utilize the\nunderstanding of mental states to proactively influ-\nence others’ thoughts and decisions. To this end,\nwe propose to assess LLMs’ ability to leverage\nthe understanding of mental states in a dialogue\nfor identifying the most effective persuasive strate-\ngies and evaluating the effectiveness of persuasive\nstrategies based on the persuadee’s response. These\ntasks test whether LLMs can leverage inferred men-\ntal states to guide strategic decision-making, bridg-\ning the gap between reasoning and action.\nPersuasion Strategy Prediction.\nThis question\ninvolves asking which persuasion strategy the per-\nsuader is likely to employ next from a set of possi-\nble strategies. To answer these questions correctly,\nLLMs need to reason over the dialogue to infer the\nmental states of characters and predict what is the\nlikely next prediction strategy to further influence\nthe persuadee’s beliefs, desires, and intentions, ul-\ntimately achieving the desired persuasion outcome.\nJudgement Question.\nThe judgement question\nspecifies the correct strategy was taken, and asks\nLLMs if the selected strategy is effective for persua-\nsion. Answering such questions requires reasoning\nabout the beliefs and intentions of the persuadee.\nOnly by accurately inferring the persuadee’s mental\n# Domains\n35\n# Dialog instances\n525\n# Avg. Turns Per Dialog\n4.9\n# Avg. Words Per Turn\n61.3\nQuestions\n# Desire (er/ee)\n2568/2459\n# Belief (er/ee)\n2580/2580\n# Intention (er/ee)\n2568/2041\n# Strategy prediction\n2041\n# Strategy judgement\n2041\nTable 3: Statistics of PERSUASIVETOM dataset.\nstate can properly determine whether the persua-\nsion strategies should be employed to convince the\npersuadee.\n3.4\nStatistics and Analysis\nData Source.\nPERSUASIVETOM is annotated on\nthe multi-turn persuasive dialogue dataset DailyPer-\nsuasion (Jin et al., 2024b). Each instance in Dai-\nlyPersuasion is an N-round alternating dialogue\nD = [(ua\n1, ub\n1, sa\n1), (ua\n2, ub\n2, sa\n2), ..., (ua\nN, ub\nN, sa\nN)]\nbetween the persuader a and the perusadee b, and\naccompanied with a persuasion strategy sa\ni . per-\nsuadee b has different desire from a initially, af-\nter multi-turn persuasion, persuadee b changes the\nmind to agree or consider the proposal of a.\nStatistics.\nIn Table 3, we present the data statis-\ntics of PERSUASIVETOM. As shown in Fig-\nure 2, PERSUASIVETOM includes diverse domains.\nThese real-life domains are crucial for compre-\nhensively evaluating LLMs in social interactions.\nWe sample 15 dialogues from each domain to\nform the dataset in PERSUASIVETOM. We create\nmulti-choice questions by either prompting GPT-\n4o to generate semantically different choices or\nrandomly selecting three distractors from the pre-\ndefined pool. Refer to Appendix A for more details.\n4\nExperimental setups\n4.1\nBaseline Models\nWe evaluate PERSUASIVETOM on eight frontier\nLLMs from different sources and with different lev-\nels of capabilities: Llama-3.1-8B-Instruct (Dubey\net al., 2024), Qwen-2.5-7B-Chat (Yang et al.,\n2024),\nGemma-2-9B-it (Team et al., 2024),\nGLM4-9B -Chat(GLM et al., 2024), Mixtral-\n8x7b-Instruct (Jiang et al., 2024), and ChatGPT-\nseries(GPT-4o-mini, GPT-4o-0806). By following\n5\n\n\nToM Reasoning\nToM Application\nPersuader\nPersuadee\nModel\nDesire\nBelief\nIntention\nDesire\nBelief\nIntention\nState Pred\nJudgement\nRandom Guess\n50.00\n25.00\n16.67\n33.33\n25.00\n25.00\n25.00\n50.00\nHuman\n100.00\n92.31\n78.02\n84.62\n87.91\n94.50\n86.81\n97.83\nLLaMa-3.1-8B-Instruct\n58.78\n66.28\n40.85\n69.91\n71.82\n86.77\n62.22\n96.37\nQwen2.5-7B-Chat\n96.33\n82.37\n45.64\n65.15\n79.06\n85.84\n58.94\n82.99\nGemma-2-9b-it\n98.20\n82.52\n45.98\n61.37\n64.07\n82.31\n65.02\n56.93\nGLM4-9B-Chat\n89.45\n73.64\n41.24\n65.23\n66.82\n85.35\n61.29\n91.47\nMixtral-8x7B-Instruct\n92.69\n67.56\n42.95\n69.74\n72.56\n85.10\n61.15\n96.81\nInternLM-2.5-7B-Chat\n83.80\n69.65\n39.87\n71.45\n69.11\n87.31\n65.07\n89.41\nGPT-4o-mini\n82.87\n69.98\n45.66\n70.72\n76.47\n87.56\n65.50\n92.65\nGPT-4o\n98.75\n89.49\n46.02\n50.10\n80.03\n88.78\n73.00\n97.55\nLLaMa-3.1-8B-Instruct + CoT\n59.75\n69.81\n43.11\n68.64\n73.45\n85.35\n61.98\n77.27\nQwen2.5-7B-Chat + CoT\n74.57\n80.30\n46.07\n67.63\n79.40\n84.62\n66.39\n96.22\nGemma-2-9b-it + CoT\n95.91\n83.30\n45.30\n64.98\n66.55\n81.67\n67.11\n66.55\nGLM4-9B-Chat + CoT\n87.46\n69.53\n45.56\n59.94\n71.07\n83.93\n63.98\n91.42\nMixtral-8x7B-Instruct + CoT\n92.64\n72.05\n45.05\n67.81\n71.28\n85.84\n65.90\n92.65\nInternLM-2.5-7B-Chat + CoT\n93.57\n69.92\n39.41\n50.79\n65.34\n84.32\n59.78\n77.90\nGPT-4o-mini + CoT\n93.42\n71.82\n45.55\n66.29\n78.04\n85.55\n66.08\n89.61\nGPT-4o + CoT\n96.51\n83.57\n46.10\n68.72\n83.56\n87.54\n77.06\n95.88\nTable 4: Results of LLMs on PERSUASIVETOM. Bold font and underlining indicate the best and second-best\nperformance respectively.\nthe common practices ((Kim et al., 2023); (Sabour\net al., 2024)), we test these models with two types\nof prompts: (1) vanilla zero-shot prompting directly\nasks LLMs to give a choice without any explana-\ntion; (2) CoT prompting method by following (Ko-\njima et al., 2022) and using the prompt “Let’s think\nstep by step.” to elicit the reasoning process and\nextract the choices by string matching. The tem-\nperature for generating answers is set to 0.71. To\nmeasure the specific performance gap between hu-\nmans and the state-of-the-art machine on the PER-\nSUASIVETOM, we employ three graduate students\nin computer science to complete the human eval-\nuation task. To avoid the bias of LLMs toward\na specific choice letter, we shuffle the choices to\nmaintain a nearly uniform distribution of correct\nchoices over the dataset. Prompts used for vanilla\nzero-shot prompting and CoT prompting are shown\nin Appendix B.2.\n5\nResults and Analysis\n5.1\nMain Results\nThe overall evaluation results on PERSUASIVE-\nTOM for the 8 models are summarized in Table 4,\nincluding all the different questions for the per-\n1LLMs occasionally output with illegal format. We choose\na low but nonzero temperature to resample the answers for\nthese invalid generations.\nsuader and persuadee. We analyze the model’s\nperformance for each type of question below.\nDesire.\nOur results show that smaller models,\nsuch as Gemma-2-9B and Qwen-2.5-7B, can per-\nform reasonably well in inferring the persuader’s\ndesires, achieving an accuracy of over 98%, which\nis competitive with GPT-4o. This suggests that\nmost LLMs can easily discern the desires of the\npersuader. However, when it comes to the desires\nof the persuadee, performance is relatively lower.\nUnlike the static desires of the persuader, the per-\nsuadee’s desires are dynamic, evolving from an\ninitial state to a final state, often with neutral ex-\npressions in between. This lower performance high-\nlights that inferring the dynamic desires of the per-\nsuadee remains a significant challenge for LLMs.\nIn summary, LLMs are better at inferring the de-\nsires of the persuader than those of the persuadee.\nBelief.\nOn belief questions, larger models like\nGPT-4o perform much better than smaller mod-\nels on reasoning the beliefs of both parties. The\nperformance difference between the reasoning per-\nsuader’s beliefs and persuadee’s beliefs is subtle.\nThis is because both parties’ beliefs dynamically\nchange with each other’s speech during the con-\nversation, unlike the desire of the persuader which\nhas a more obvious and precise tendency. The diffi-\n6\n\n\n20%\n40%\n60%\n80%\n100%\nDialogue Progress\n10\n20\n30\n40\n50\n60\nError Ratio\nGemma-2-9B-it\nGPT-4o\nGemma-2-9B-it + CoT\nGPT-4o + CoT\n20%\n40%\n60%\n80%\n100%\nDialogue Progress\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\n25.0\n27.5\nError Ratio\nInternLM-2.5-7B-Chat\nGPT-4o\nInternLM-2.5-7B-Chat + CoT\nGPT-4o + CoT\nFigure 3: Distribution of errors of Desire questions happening in different stages of dialogue progress. The Left\nfigure corresponds to the persuader, and the Right figure corresponds to the persuadee.\n0\n200\n400\n600\n800\nLlama-3.1-8B-Instruct\nLlama-3.1-8B-Instruct + CoT\nQwen2.5-7B-Chat\nQwen2.5-7B-Chat + CoT\nGemma-2-9B-it\nGemma-2-9B-it + CoT\nGLM4-9B-Chat\nGLM4-9B-Chat + CoT\nMixtral-8x7B-Instruct\nMixtral-8x7B-Instruct + CoT\nInternLM-2.5-7B-Chat\nInternLM-2.5-7B-Chat + CoT\nGPT-4o-Mini\nGPT-4o-Mini + CoT\nGPT-4o\nGPT-4o + CoT\nSame Polarity\nOpposite Polarity\nFigure 4: Model errors of belief questions of persuader.\nculty of reasoning persuader and persuadee’s belief\nis similar.\nIntention.\nResults in Table 4 indicate that LLMs\nstruggle to accurately infer the intentions of per-\nsuaders while performing relatively better at rea-\nsoning the intentions of persuadees. The low per-\nformance on persuader-related intention questions\nsuggests that LLMs face challenges in understand-\ning how persuaders aim to influence others. This\nalso indicates a lack of proficiency in persuasive\ntheory, limiting the models’ ability to correctly in-\nterpret and predict the intentions behind the per-\nsuader’s strategies.\nStrategy Prediction and judgement.\nOur re-\nsults reveal that LLMs perform well in evaluating\nthe effectiveness of persuasive strategies aimed for\nchanging the mental states of the persuadee. How-\never, the task of selecting the appropriate strategy\nto persuade is more challenging, particularly for\nsmaller models. This suggests LLMs struggle with\nthe complex reasoning required to determine which\nstrategy to adopt in different persuasive contexts.\nImpact of CoT Reasoning.\nBoth ToM reasoning\nand ToM application tasks indicate that CoT rea-\nsoning has not consistently improved performance,\nyet improve strategy prediction to some extent for\n0\n200\n400\n600\n800\n1000\nLlama-3.1-8B-Instruct\nLlama-3.1-8B-Instruct + CoT\nQwen2.5-7B-Chat\nQwen2.5-7B-Chat + CoT\nGemma-2-9B-it\nGemma-2-9B-it + CoT\nGLM4-9B-Chat\nGLM4-9B-Chat + CoT\nMixtral-8x7B-Instruct\nMixtral-8x7B-Instruct + CoT\nInternLM-2.5-7B-Chat\nInternLM-2.5-7B-Chat + CoT\nGPT-4o-Mini\nGPT-4o-Mini + CoT\nGPT-4o\nGPT-4o + CoT\nSame Polarity\nOpposite Polarity\nFigure 5: Model errors of belief questions of persuadee.\nmost LLMs. We attribute this limited improvement\nto the inherent lack of ToM capability in LLMs.\nThese models struggle to simulate the human cog-\nnitive reasoning process, which affects their ability\nto generate correct answers in tasks requiring a\nnuanced understanding of the mental state.\nComparison with Human Performance.\nTo\nobtain a baseline for human performance, we\nrecruited participants to complete the questions.\nMore details of human evaluation are shown in Ap-\npendix B.1. As shown in Table 4, our human partic-\nipants outperformed LLMs on all tasks. In particu-\nlar, although GPT-4 reaches close performance in\nhumans, it still falls short of understanding and rea-\nsoning the complex dynamics such as the intention\nof persuaders and the desire of persuadee, which\ninvolves complex psychological changes, highlight-\ning a significant gap in current LLMs and humans.\n5.2\nIn-depth Analysis\nTo better understand the limitations of large lan-\nguage models (LLMs) in the PERSUASIVETOM\nbenchmark, we categorized common failure cases\ninto several key error types based on task perfor-\nmance and manual error analysis.\nDesire Reasoning Errors.\nFigure 3 summarizes\nthe distribution of errors of Desire questions hap-\n7\n\n\n0\n20\n40\n60\n80\n100\nRatio (%)\nLlama-3.1-8B-Instruct\nQwen2.5-7B-Chat\nGemma-2-9B-it\nGLM4-9B-Chat\nMixtral-8x7B-Instruct\nInternLM-2.5-7B-Chat\nGPT-4o-Mini\nGPT-4o\nVanilla\nCoT\nFigure 6: Ratio of intention errors misclassified to feel\naccepted through concessions, promises, or benefits.\npening in different stages of dialogue progress with\nand without CoT reasoning. The error distribution\nfor persuader and persuadee has a significant differ-\nence. At the beginning of the dialogue, LLMs may\nnot accurately understand the persuader’s desire,\nbut as the dialogue progresses, the persuader’s de-\nsire becomes relatively easy to identify. However,\nfor the persuadee, the desire to reject at the early\nstage of the dialogue is relatively easy to recog-\nnize. As the persuasion proceeds, the persuadee\nmay begin to contemplate and hesitate over the per-\nsuader’s proposal, leading to complex and nuanced\npsychological activities that make it difficult for\nthe LLM to accurately judge the persuadee’s desire.\nAs the dialogue approaches its end, the persuadee\nshows a tendency to agree, making the reasoning\nof desire easier. This suggests LLMs still fall short\nin ToM reasoning regarding desire shifts.\nBelief Reasoning Errors.\nFigure 4 and 5 summa-\nrize error types of belief questions for each model\nwith and without CoT. We use Distil-BERT2 (Sanh,\n2019) to discriminate whether the choice of LLMs\nhas the same attitude polarity to the ground-truth.\nInterestingly, we found that the proportion of errors\nwith the same polarity is higher when predicting the\npersuader’s beliefs, whereas the opposite trend was\nobserved when predicting the persuadee’s beliefs.\nThis highlights reasoning shifting mental states is\nstill challenging for LLMs.\nIntention Bias.\nGiven the high error rate in in-\ntention questions related to the persuader, we con-\nducted an analysis of the error types. Our find-\nings reveal that most LLMs exhibit a bias toward\npredicting intentions characterized by making the\nother person feel accepted through concessions,\n2https://huggingface.co/distilbert/distilbert-base-uncased-\nfinetuned-sst-2-english\nModel\nDesire\nBelief\nIntention\nLLaMa-3.1-8B-Instruct\n22.31\n21.71\n60.76\nLLaMa-3.1-8B-Instruct + CoT\n19.92\n22.86\n57.52\nQwen2.5-7B-Chat\n19.12\n31.81\n56.95\nQwen2.5-7B-Chat + CoT\n20.52\n24.76\n54.67\nInternLM2.5\n24.70\n20.19\n58.10\nInternLM2.5 + CoT\n6.57\n32.14\n53.71\nGPT-4o-mini\n23.39\n30.77\n60.79\nGPT-4o-mini + CoT\n16.13\n32.14\n56.95\nGPT-4o\n19.35\n36.19\n65.71\nGPT-4o + CoT\n6.57\n45.38\n62.02\nHuman\n61.11\n55.56\n81.82\nTable 5: The consistency (%) of the models for ToM\nreasoning questions of persuadee.\npromises, or benefits. Figure 6 illustrates the pro-\nportion of errors resulting from misclassifying in-\ntentions into this category. We hypothesize that this\nbias may stem from the pretraining phase, partic-\nularly with Reinforcement Learning from Human\nFeedback (RLHF) (Christiano et al., 2017), which\ntends to prioritize safety and politeness. This may\nexplain the models’ bias toward predicting inten-\ntions emphasizing benefits and concessions, even\nwhen misaligned with the dialogue context. We\nprovide a case study in Appendix B.3.\n5.3\nHow Well LLMs Track Mental States of\nPersuadees?\nTo study the ability of LLMs to track mental states\nand completely understand the psychological dy-\nnamics in the dialogue. We evaluate the consis-\ntency of LLMs. To assess consistency, we measure\nwhether the model maintains a stable understanding\nof the persuadee’s beliefs, desires, and intentions\nacross multiple turns in the conversation. Specifi-\ncally, we indicate a success only when all the ques-\ntions in the dialogue are answered correctly.\nAs shown in Table 5, only a small portion of\ndialogues can be fully understood, particularly for\ndesire-related questions. This highlights that LLMs\nstill lack the ability to reason about the dynamics of\nmental states in persuasive dialogues, resulting in a\nsignificant performance gap compared to humans.\n6\nConclusion\nWe introduce PERSUASIVETOM, a benchmark de-\nsigned to evaluate the Machine Theory of Mind\nability of LLMs in persuasive dialogues. The core\nof PERSUASIVETOM lies in the design of ques-\ntions that probe the beliefs, desires, and intentions\nof both parties that have asymmetrical social status\nin real-life social interactions. Furthermore, we\npropose evaluating the ability to persuade based on\n8\n\n\nunderstanding mental states. We conduct extensive\nexperiments and analysis to evaluate the perfor-\nmance of LLMs on PERSUASIVETOM benchmark.\n7\nLimitations\nWhile PERSUASIVETOM offers a comprehensive\nevaluation of the Theory of Mind in real-life social\ninteraction scenarios within persuasive dialogues,\nboth PERSUASIVETOM and previous benchmarks\nstill focus on understanding a character’s mental\nstate from the perspective of an observer. However,\nthe ability to reason about others’ mental states in\npersuasive dialogues can further position LLMs as\nautonomous agents. This capability would enable\nthem to better guide other agents in fulfilling their\nown desires by reasoning about the mental states\nof others. Therefore, future benchmarks should\nestablish environments with multiple LLM agents,\nwhere tasks involve reasoning about the mental\nstates of other agents and proposing persuasion\nstrategies to influence their desires, beliefs, and\nintentions to fulfill the current agent’s target. In\nthis context, agents will develop the management\nskills necessary for effective cooperation and other\napplications.\n8\nSocietal and Ethical Considerations\nWe recognize that the concept of the Theory of\nMind might suggest anthropomorphic qualities\nwhen applied to AI models. However, we want\nto clarify that our work is not intended to anthro-\npomorphize LLMs. Our goal is to examine the\nlimitations in the social and psychological reason-\ning capabilities of existing LLMs. Our results show\nthat current models do not perform genuine The-\nory of Mind reasoning; instead, they generate re-\nsponses primarily based on the literal interpretation\nof the input.\nReferences\nSimon Baron-Cohen, Alan M Leslie, and Uta Frith.\n1985.\nDoes the autistic child have a “theory of\nmind”? Cognition, 21(1):37–46.\nMichael Bratman. 1987. Intention, plans, and practical\nreason.\nChunkit Chan, Cheng Jiayang, Yauwai Yim, Zheye\nDeng, Wei Fan, Haoran Li, Xin Liu, Hongming\nZhang, Weiqi Wang, and Yangqiu Song. 2024. Nego-\ntiationtom: A benchmark for stress-testing machine\ntheory of mind on negotiation surrounding. arXiv\npreprint arXiv:2404.13627.\nMaximillian Chen, Weiyan Shi, Feifan Yan, Ryan Hou,\nJingwen Zhang, Saurav Sahay, and Zhou Yu. 2022.\nSeamlessly integrating factual information and social\ncontent with persuasive dialogue.\narXiv preprint\narXiv:2203.07657.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Mar-\ntic, Shane Legg, and Dario Amodei. 2017. Deep\nreinforcement learning from human preferences. Ad-\nvances in neural information processing systems, 30.\nRobert B Cialdini and Robert B Cialdini. 2007. In-\nfluence: The psychology of persuasion, volume 55.\nCollins New York.\nRobert B Cialdini and Noah J Goldstein. 2004. Social\ninfluence: Compliance and conformity. Annu. Rev.\nPsychol., 55(1):591–621.\nDaniel C Dennett. 1988. Précis of the intentional stance.\nBehavioral and brain sciences, 11(3):495–505.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, et al. 2024. The llama 3 herd of models. arXiv\npreprint arXiv:2407.21783.\nJoseph L Fleiss. 1971. Measuring nominal scale agree-\nment among many raters. Psychological bulletin,\n76(5):378.\nKazuaki Furumai, Roberto Legaspi, Julio Vizcarra, Yu-\ndai Yamazaki, Yasutaka Nishimura, Sina J Semnani,\nKazushi Ikeda, Weiyan Shi, and Monica S Lam. 2024.\nZero-shot persuasive chatbots with llm-generated\nstrategies and information retrieval. arXiv preprint\narXiv:2407.03585.\nKanishk Gandhi, Jan-Philipp Fränken, Tobias Gersten-\nberg, and Noah Goodman. 2024.\nUnderstanding\nsocial reasoning in language models with language\nmodels. Advances in Neural Information Processing\nSystems, 36.\nMichael Georgeff, Barney Pell, Martha Pollack, Milind\nTambe, and Michael Wooldridge. 1999. The belief-\ndesire-intention model of agency.\nIn Intelligent\nAgents V: Agents Theories, Architectures, and Lan-\nguages: 5th International Workshop, ATAL’98 Paris,\nFrance, July 4–7, 1998 Proceedings 5, pages 1–10.\nSpringer.\nTeam GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chen-\nhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu\nFeng, Hanlin Zhao, et al. 2024. Chatglm: A family\nof large language models from glm-130b to glm-4 all\ntools. arXiv preprint arXiv:2406.12793.\nAlison Gopnik and Henry M Wellman. 1992. Why the\nchild’s theory of mind really is a theory.\nYuling Gu, Oyvind Tafjord, Hyunwoo Kim, Jared\nMoore, Ronan Le Bras, Peter Clark, and Yejin Choi.\n2024. Simpletom: Exposing the gap between explicit\ntom inference and implicit tom application in llms.\narXiv preprint arXiv:2410.13648.\n9\n\n\nYinghui He, Yufan Wu, Yilin Jia, Rada Mihalcea, Yu-\nlong Chen, and Naihao Deng. 2023.\nHi-tom: A\nbenchmark for evaluating higher-order theory of\nmind reasoning in large language models.\narXiv\npreprint arXiv:2310.16755.\nGuiyang Hou, Wenqi Zhang, Yongliang Shen, Zeqi Tan,\nSihao Shen, and Weiming Lu. 2024. Entering real\nsocial world! benchmarking the theory of mind and\nsocialization capabilities of llms from a first-person\nperspective. arXiv preprint arXiv:2410.06195.\nAlbert Q Jiang, Alexandre Sablayrolles, Antoine\nRoux, Arthur Mensch, Blanche Savary, Chris Bam-\nford, Devendra Singh Chaplot, Diego de las Casas,\nEmma Bou Hanna, Florian Bressand, et al. 2024.\nMixtral of experts. arXiv preprint arXiv:2401.04088.\nChuanyang Jin, Yutong Wu, Jing Cao, Jiannan Xiang,\nYen-Ling Kuo, Zhiting Hu, Tomer Ullman, Antonio\nTorralba, Joshua B Tenenbaum, and Tianmin Shu.\n2024a. Mmtom-qa: Multimodal theory of mind ques-\ntion answering. arXiv preprint arXiv:2401.08743.\nChuhao Jin, Kening Ren, Lingzhen Kong, Xiting Wang,\nRuihua Song, and Huan Chen. 2024b. Persuading\nacross diverse domains: a dataset and persuasion\nlarge language model. In Proceedings of the 62nd\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1678–\n1706.\nChuhao Jin, Yutao Zhu, Lingzhen Kong, Shijie Li, Xiao\nZhang, Ruihua Song, Xu Chen, Huan Chen, Yuchong\nSun, Yu Chen, et al. 2023. Joint semantic and strategy\nmatching for persuasive dialogue. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2023, pages 4187–4197.\nDavid J Kavanagh, Jackie Andrade, and Jon May. 2005.\nImaginary relish and exquisite torture: the elabo-\nrated intrusion theory of desire. Psychological re-\nview, 112(2):446.\nHyunwoo Kim, Melanie Sclar, Xuhui Zhou, Ronan Le\nBras, Gunhee Kim, Yejin Choi, and Maarten Sap.\n2023. Fantom: A benchmark for stress-testing ma-\nchine theory of mind in interactions. arXiv preprint\narXiv:2310.15421.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. Advances in\nneural information processing systems, 35:22199–\n22213.\nMatthew Le, Y-Lan Boureau, and Maximilian Nickel.\n2019. Revisiting the evaluation of theory of mind\nthrough question answering. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5872–5877.\nAixin Liu, Bei Feng, Bing Xue, Bingxuan Wang,\nBochao Wu, Chengda Lu, Chenggang Zhao, Chengqi\nDeng, Chenyu Zhang, Chong Ruan, et al. 2024.\nDeepseek-v3 technical report.\narXiv preprint\narXiv:2412.19437.\nZiqiao Ma, Jacob Sansom, Run Peng, and Joyce Chai.\n2023. Towards a holistic landscape of situated theory\nof mind in large language models. arXiv preprint\narXiv:2310.19619.\nBertram F Malle and Joshua Knobe. 2001. The distinc-\ntion between desire and intention: A folk-conceptual\nanalysis.\nAndrew N Meltzoff. 1995. Understanding the intentions\nof others: re-enactment of intended acts by 18-month-\nold children. Developmental psychology, 31(5):838.\nGonçalo Pereira, Rui Prada, and Pedro A Santos. 2016.\nIntegrating social power into the decision-making of\ncognitive agents. Artificial Intelligence, 241:1–44.\nAnn T Phillips, Henry M Wellman, and Elizabeth S\nSpelke. 2002. Infants’ ability to connect gaze and\nemotional expression to intentional action. Cogni-\ntion, 85(1):53–78.\nDavid Premack and Guy Woodruff. 1978. Does the\nchimpanzee have a theory of mind? Behavioral and\nbrain sciences, 1(4):515–526.\nTessa Rusch, Saurabh Steixner-Kumar, Prashant Doshi,\nMichael Spezio, and Jan Gläscher. 2020. Theory of\nmind and decision science: Towards a typology of\ntasks and computational models. Neuropsychologia,\n146:107488.\nSahand Sabour, Siyang Liu, Zheyuan Zhang, June M\nLiu, Jinfeng Zhou, Alvionna S Sunaryo, Juanzi\nLi, Tatia Lee, Rada Mihalcea, and Minlie Huang.\n2024. Emobench: Evaluating the emotional intel-\nligence of large language models. arXiv preprint\narXiv:2402.12071.\nHiromasa Sakurai and Yusuke Miyao. 2024. Evaluating\nintention detection capability of large language mod-\nels in persuasive dialogues. In Proceedings of the\n62nd Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n1635–1657.\nAzlaan Mustafa Samad, Kshitij Mishra, Mauajama Fir-\ndaus, and Asif Ekbal. 2022. Empathetic persuasion:\nreinforcing empathy and persuasiveness in dialogue\nsystems. In Findings of the Association for Compu-\ntational Linguistics: NAACL 2022, pages 844–856.\nV Sanh. 2019. Distilbert, a distilled version of bert:\nsmaller, faster, cheaper and lighter. arXiv preprint\narXiv:1910.01108.\nMelanie Sclar, Sachin Kumar, Peter West, Alane Suhr,\nYejin Choi, and Yulia Tsvetkov. 2023. Minding lan-\nguage models’(lack of) theory of mind: A plug-and-\nplay multi-character belief tracker. arXiv preprint\narXiv:2306.00924.\n10\n\n\nNatalie Shapira, Mosh Levy, Seyed Hossein Alavi,\nXuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten\nSap, and Vered Shwartz. 2023.\nClever hans or\nneural theory of mind?\nstress testing social rea-\nsoning in large language models.\narXiv preprint\narXiv:2305.14763.\nHaojun Shi, Suyu Ye, Xinyu Fang, Chuanyang Jin,\nLeyla Isik, Yen-Ling Kuo, and Tianmin Shu. 2024.\nMuma-tom: Multi-modal multi-agent theory of mind.\narXiv preprint arXiv:2408.12574.\nWeiyan Shi, Xuewei Wang, Yoo Jung Oh, Jingwen\nZhang, Saurav Sahay, and Zhou Yu. 2020. Effects\nof persuasive dialogues: testing bot identities and\ninquiry strategies. In Proceedings of the 2020 CHI\nconference on human factors in computing systems,\npages 1–13.\nGemma Team, Morgane Riviere, Shreya Pathak,\nPier Giuseppe Sessa, Cassidy Hardin, Surya Bhupati-\nraju, Léonard Hussenot, Thomas Mesnard, Bobak\nShahriari, Alexandre Ramé, et al. 2024. Gemma 2:\nImproving open language models at a practical size.\narXiv preprint arXiv:2408.00118.\nYouzhi Tian, Weiyan Shi, Chen Li, and Zhou Yu. 2020.\nUnderstanding user resistance strategies in persua-\nsive conversations. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n4794–4798.\nTomer Ullman. 2023. Large language models fail on\ntrivial alterations to theory-of-mind tasks.\narXiv\npreprint arXiv:2302.08399.\nXuewei Wang, Weiyan Shi, Richard Kim, Yoojung Oh,\nSijia Yang, Jingwen Zhang, and Zhou Yu. 2019. Per-\nsuasion for good: Towards a personalized persua-\nsive dialogue system for social good. arXiv preprint\narXiv:1906.06725.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in neural\ninformation processing systems, 35:24824–24837.\nRongwu Xu, Brian S Lin, Shujian Yang, Tianqi Zhang,\nWeiyan Shi, Tianwei Zhang, Zhixuan Fang, Wei\nXu, and Han Qiu. 2023.\nThe earth is flat be-\ncause...: Investigating llms’ belief towards misinfor-\nmation via persuasive conversation. arXiv preprint\narXiv:2312.09085.\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui,\nBo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu,\nFei Huang, Haoran Wei, et al. 2024. Qwen2. 5 tech-\nnical report. arXiv preprint arXiv:2412.15115.\nYi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang,\nRuoxi Jia, and Weiyan Shi. 2024. How johnny can\npersuade llms to jailbreak them: Rethinking persua-\nsion to challenge ai safety by humanizing llms. arXiv\npreprint arXiv:2401.06373.\nXuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang,\nHaofei Yu, Zhengyang Qi, Louis-Philippe Morency,\nYonatan Bisk, Daniel Fried, Graham Neubig, et al.\n2023.\nSotopia: Interactive evaluation for social\nintelligence in language agents.\narXiv preprint\narXiv:2310.11667.\n11\n\n\nA\nData Annotation\nAnnotation.\nIn this section, we outline the anno-\ntation process and the templates utilized for annota-\ntion. Among the various tasks, the intention ques-\ntions of persuaders and the desire questions of per-\nsuadees require annotation. Initially, we recruited\nthree graduate students to annotate 25 dialogues.\nSubsequently, we carefully designed a few-shot\nprompt to guide DeepSeek-V3 (Liu et al., 2024) as\nan annotator, aiming to enhance the alignment be-\ntween the model’s answers and human annotations.\nFollowing this, we employed the LLM to anno-\ntate the remaining questions. To ensure the qual-\nity of the annotations, we randomly sampled 100\ndialogues and calculated the inter-annotator agree-\nment. The Fleiss κ (Fleiss, 1971) was found to be\n76.20% for the desire questions of persuadees and\n78.28% for the intention questions of persuaders.\nThese results are presented in Table 6, which indi-\ncate a high inter-annotator agreement. The detailed\nstatistics and comparison with other ToM datasets\nare shown in Table 7.\nChoices Generation.\nBinary choice questions,\nincluding those related to the desires of the per-\nsuader and judgment questions, do not require ad-\nditional choice generation. For belief-related ques-\ntions for both parties, we adapt the tones from the\nPersuasionDaily dataset. We also create lists of at-\ntitudes—positive, neutral, and negative—and man-\nually remove any items that have semantics too\nclose to the ground truths. From each attitude list,\nwe then randomly sample one word to generate the\nfour choices.\nFor intention questions concerning the persuader,\nwe directly use the intention options outlined in\nTable 2. For the persuadee’s intention questions,\nwe leverage DeepSeek-V3 and employ a few-shot\nprompt (as shown in Figure 7) to extract the per-\nsuadee’s intention. Subsequently, we design an-\nother prompt (as shown in Figure 8) to generate\nthree incorrect intention choices.\nSince DailyDialogue provides persuasion strate-\ngies for each utterance, we construct the choices by\nincluding the correct strategy and three alternative\nstrategies that appear in other turns of the same\ndialogue.\nQuestions\nFleiss’s Kappa (%)\nDesire (Persuader)\n76.20\nIntention (Persuadee)\n78.28\nTable 6: Inter-rater agreement in terms of Fleiss’s κ on\ndesire and intention questions.\nB\nAppendix for Experiments\nB.1\nHuman Performance\nTo measure the performance gap between humans\nand the state-of-the-art LLMs on PERSUASIVE-\nTOM, we recruited three graduate student work-\ners majoring in computer science to complete the\nquestions. Each question is shown to the workers\nwith identical prompts which are used for evaluat-\ning LLMs. We then compute the majority vote on\nthe labels assigned. Student workers solve 50 dia-\nlogues in total. For a question where three people\nhave different answers, we randomly select one of\ntheir answers as the answer for human evaluation.\nB.2\nPrompts used for evaluation\nHere we show the prompts used for vanilla zero-\nshot prompting and CoT prompting for generating\nanswers for all the ToM Reasoning and ToM Ap-\nplication questions. We only need to fill in the\ncontent to \"<>\" for evaluating different questions.\nThe vanilla zero-shot prompt is shown in Figure 9\nand the CoT prompt is shown in Figure 10\nB.3\nCase study on Persuader’s intention\nHere we present an example of common mistakes\nmade by GPT-4o that misclassifying intentions to\nmake the other person feel accepted through con-\ncessions, promises, or benefits., as shown in Ta-\nble 8. We believe these errors can be attributed to\nthe RLHF which highlights the benefits for humans,\nas well as the potential unfamilirity of persuasion\ntheory for LLMs.\nC\nDetails on Persuasive Principles\nRobert Cialdini’s six principles of persuasion, out-\nlined in his book Influence: The Psychology of\nPersuasion, are foundational concepts in social psy-\nchology. They explain how people can be per-\nsuaded or influenced by others. We include an\noverview for each of the principle in Table 9.\n12\n\n\nDataset\nTotal #Questions\nAvg. #Questions per Context\nAvg. #Turns (Full)\nAvg. Turn Length\nToMi\n6K\n6.0\n4.9\n4.7\nFanToM\n10K\n12.9\n24.5\n21.9\nNegotiationToM\n13K\n7.0\n6.0\n42.2\nPERSUASIVETOM\n19K\n8.0\n4.9\n61.3\nTable 7: Statistics of PERSUASIVETOM and other recent benchmarks.\nUtterance\nBob: I understand your love for Paris, but Bali also offers a thrilling adventure! We can go\nwhite water rafting, hike to volcanoes, and explore hidden waterfalls. It’s a perfect destination\nfor creating unforgettable memories together.\nQuestion\nWhat is the intention of Bob?\nGPT-4o\n(A) Intent to make the other person feel accepted through concessions, promises, or benefits.\nLabel\n(B) Intent to make the other person feel the experience or objects are unique or scarce.\nTable 8: Common observed mistakes in our experiments. Green and Red indicate the correct answer and GPT-4o’s\nanswer, respectively.\nPrinciple\nDescription\nReciprocity principle\nAssist others or provide them with gifts, creating a sense of obligation to return the favor.\nFor instance, giving away free trials, discount coupons, or complimentary gifts can enhance\npersuasion.\nScarcity principle\nWhen a resource or opportunity is scarce, people are more inclined to take action.\nHighlighting urgency and scarcity can motivate the audience to respond quickly.\nConsensus principle\nPeople often follow the actions of others, especially in uncertain situations. Provide\ninformation such as successful cases of others, positive reviews, or the number of\nsupporters to increase persuasiveness.\nAuthority principle\nPeople are more likely to trust and follow guidance from authoritative figures. Citing expert\nopinions, research findings, or endorsements from reputable institutions can enhance\ncredibility and persuasiveness.\nCommitment and\nPeople are inclined to stick to their past commitments and behave consistently.\nconsistency principle\nEncouraging them to express support or make a small commitment increases the\nchances of them taking further action later.\nLiking principle\nPeople are more easily influenced by those they like, admire, or find relatable.\nFor instance, a salesperson who shares common interests with a customer is\nmore likely to make a sale.\nTable 9: Explanations for Robert Cialdini’s six principles of persuasion.\nPrompt for extracting intention of persuadee.\nYou are a skilled intent understanding expert. You will be given a sentence describing <persuadee’s name>’s intent. Please\nonly return the intent without any explanation.\nCase 0:\nSentence: Mary wants excitement, so I’ll appeal to her sense of adventure and describe how exploring the ruins can be\nthrilling.\nIntent: Mary wants excitement\nCase 1:\nSentence: Oliver is concerned about failure, so discussing the financial benefits of starting an e-commerce business could\nhelp alleviate his worries.\nIntent: Oliver is concerned about failure\nCase 2:\nSentence: Olivia seems intrigued by the idea of personalization. I’ll explain how we can incorporate it into our subscription\nmodel.\nIntent: Olivia seems intrigued by the idea of personalization.\nCase 3:\nSentence: <Utterance of persuadee>\nIntent:\nFigure 7: Prompt template for extracting intention of persuadee.\n13\n\n\nPrompt for choice generation of intention questions of persuadee\nYou are an expert in multiple-choice question-making. You will be given a correct choice. Please generate three plausible\nbut incorrect choices without any explanation. Only return the incorrect choices for the last case.\nCase 0:\nCorrect Intent: Mr. Chen needs further persuasion\nAnalysis: To further persuade Mr. Chen, Li Na should share success stories of other books that have benefited from\nincorporating literary criticism in their marketing strategies. Providing concrete examples will make her argument more\nconvincing.\nIncorrect Intent 1: Mr. Chen is interested in literary criticism.\nIncorrect Intent 2: Mr. Chen is looking for success stories.\nIncorrect Intent 3: Mr. Chen prefers concrete examples.\nCase 1:\nCorrect Intent: James is more open to the idea.\nAnalysis: James is now more open to the idea, so I’ll outline the implementation plan and emphasize the program’s\nflexibility to address any concerns about disruptions.\nIncorrect Intent 1: James is concerned about disruptions.\nIncorrect Intent 2: James is looking for a detailed implementation plan.\nIncorrect Intent 3: James is hesitant about the program’s flexibility.\nCase 2:\nCorrect Intent: <correct intent>\nAnalysis: <analysis>\nIncorrect Intent 1:\nIncorrect Intent 2:\nIncorrect Intent 3:\nFigure 8: Prompt template for choice generation of intention questions of persuadee.\nPrompt for vanilla zero-shot prompting.\nHere is a persuasive dialogue. There are two agents, the persuader and the persuadee. The persuader is trying to persuade\nthe persuadee to do something. Please answer the following questions using A, B, C, D, E, F, without any explanation.\nDialogue History:\n<dialogue>\nQuestion:\n<Question>\nChoices:\n<Choice A>\n<Choice B>\n<Choice C>\n<Choice D>\nAnswer:\nFigure 9: Prompt template for vanilla zero-shot prompting.\nPrompt for CoT prompting.\nHere is a persuasive dialogue. There are two agents, the persuader and the persuadee. The persuader is trying to persuade\nthe persuadee to do something. Think step by step to answer the question.\nEnding with \"The answer is A, B, C, D, E, F\". For example, if the most likely answer option is ’A. considering’, then end\nyour response with ’The answer is A’.\nDialogue History:\n<dialogue>\nQuestion:\n<Question>\nChoices:\n<Choice A>\n<Choice B>\n<Choice C>\n<Choice D>\nAnswer: Let’s think step by step.\nFigure 10: Prompt template for CoT prompting.\n14\n\n\n"}
{"text": "Robust and Efficient Writer-Independent\nIMU-Based Handwriting Recognization\nJindong Li1[0000−0002−3550−1660], Tim Hamann2[0000−0003−3562−6882], Jens\nBarth2[0000−0003−3967−9578], Peter Kaempf2, Dario Zanca1[0000−0001−5886−0597],\nand Bjoern Eskofier1[0000−0002−0417−0336]\n1 Machine Learning and Data Analytics Lab, Friedrich-Alexander-Universität\nErlangen-Nürnberg, Germany\n2 STABILO International GmbH, Germany\nAbstract. Online handwriting recognition (HWR) using data from in-\nertial measurement units (IMUs) remains challenging due to variations\nin writing styles and the limited availability of high-quality annotated\ndatasets. Traditional models often struggle to recognize handwriting from\nunseen writers, making writer-independent (WI) recognition a crucial\nbut difficult problem. This paper presents an HWR model with an encoder-\ndecoder structure for IMU data, featuring a CNN-based encoder for fea-\nture extraction and a BiLSTM decoder for sequence modeling, which\nsupports inputs of varying lengths. Our approach demonstrates strong\nrobustness and data efficiency, outperforming existing methods on WI\ndatasets, including the WI split of the OnHW dataset and our own\ndataset. Extensive evaluations show that our model maintains high accu-\nracy across different age groups and writing conditions while effectively\nlearning from limited data. Through comprehensive ablation studies, we\nanalyze key design choices, achieving a balance between accuracy and\nefficiency. These findings contribute to the development of more adapt-\nable and scalable HWR systems for real-world applications. The code is\navailable at https://github.com/jindongli24/REWI.\nKeywords: Online Handwriting Recognition · Time-series Analysis ·\nInertial Measurement Unit\n1\nIntroduction\nHandwriting has been an essential way of recording and sharing information\nthroughout human history. With advancements in technology, the demand for\ndigitizing handwriting has increased. Handwriting recognition (HWR), a method\nfor turning handwritten symbols into computer-readable text, has become an\nimportant area of research.\nHWR is generally divided into two types: offline HWR and online HWR.\nOffline HWR, also known as optical character recognition, identifies handwriting\nfrom static images of handwritten text. This approach is widely used in various\nfields, including historical research [18] and healthcare [7]. On the other hand,\narXiv:2502.20954v1  [cs.LG]  28 Feb 2025\n\n\n2\nJ. Li et al.\nonline HWR works with time-series data that captures dynamic handwriting\nfeatures such as strokes, positions, directions, and speeds. This data is usually\ncollected using touch screens and styluses on mobile devices [3], which limits the\nwriting surface users can write on.\nAnother approach to online HWR uses pens equipped with inertial mea-\nsurement units (IMUs) [21,1,14]. These sensors, including accelerometers and\ngyroscopes, capture pen movement without relying on the exact position of the\ntip, allowing the pen to function independently of external devices and on any\nsurface. As IMU sensor costs continue to decrease, this method shows great po-\ntential for the application in online HWR. However, variations in handwriting\nstyles can impact recognition accuracy, and sensor noise from rough surfaces,\ntemperature changes, and digitization artifacts add further challenges. More sig-\nnificantly, gravitational acceleration also introduces noise, making motion track-\ning less accurate and complicating reliable HWR.\nThis paper introduces a sequence-to-sequence model for IMU-based online\nHWR that addresses challenges related to handwriting style variations and sen-\nsor noise. The model uses an encoder-decoder structure that combines a con-\nvolutional neural network (CNN) with a bidirectional long short-term memory\n(BiLSTM) network and integrates recent advancements in deep learning. We\nevaluate the proposed model by comparing it with several mainstream models\nand existing IMU-based HWR methods. Experimental results on datasets col-\nlected with an IMU-based pen show that our model outperforms others in terms\nof accuracy, data efficiency, robustness, and flexibility in WI HWR.\nSection 2 reviews related work on IMU-based HWR and advanced main-\nstream models. Section 3 describes the datasets, the CNN-BiLSTM model, and\nthe data augmentation methods that we used. Section 4 explains the training\nprocess and presents the experimental results in detail. Section 5 discusses key\nfactors that improve IMU-based HWR. Finally, Section 6 provides the conclu-\nsions and suggests directions for future research.\n2\nRelated Works\n2.1\nIMU-based HWR\nIMU-based HWR has been an area of research for decades. Early studies, such\nas [4,9], used dynamic-time warping-based algorithms to recognize digit data\ncollected with IMU-based pens, achieving recognition rates above 90%. Later\nworks, including [13,15], investigated LSTM-based models for recognizing indi-\nvidual English characters, reaching recognition accuracies of up to 99.68% and\n79.01% on their respective datasets. While these methods achieved high accu-\nracy, they rely on isolated character recognition, processing entire input signals\nto classify single characters. This character-by-character approach disrupts natu-\nral writing flow, making it less practical for real-world applications where people\ntypically write word-by-word.\nTo handle more complex tasks, [22,16] employed CNN-LSTM models with\nconnectionist temporal classification (CTC) [6] to recognize English and Ger-\n\n\nRobust and Efficient Writer-Independent IMU-based HWR\n3\nman characters in a sequence-to-sequence format. These models were tested on\ndatasets collected using the IMU-based pen developed by STABILO [21]. Al-\nthough the methods achieved character error rates (CERs) of 27.8% and 17.97%,\nwhich are worse than earlier approaches, they paved the way for recognizing full\nwords and even sentences, taking a step closer to practical applications.\n2.2\nAdvancements in Deep Learning Architectures\nIn recent years, the introduction of ResNet [8] and Transformers [20] has signif-\nicantly advanced the development of deep learning models. ResNet tackled the\nvanishing gradient problem using skip connections, making it possible to train\nvery deep neural networks effectively. Transformers introduced self-attention\nmechanisms, which improved parallelization and enhanced the ability to model\nlong-range dependencies compared to convolutional or recurrent neural networks.\nBuilding on these innovations, Vision Transformer [5] applied a transformer-\nbased architecture to image recognition by treating images as sequences of\npatches and leveraging self-attention to achieve state-of-the-art results. MLP-\nMixer [19] showed that strong performance in vision tasks could be achieved\nwithout convolutions or self-attention, relying solely on multi-layer perceptrons\n(MLPs) to mix spatial and channel information. Swin Transformers [11,10] intro-\nduced a hierarchical architecture with shifted window-based self-attention, en-\nabling efficient multi-scale modeling for various vision tasks. ConvNeXt [12] com-\nbined ideas from CNNs and Transformers, achieving state-of-the-art performance\nwhile maintaining the simplicity and efficiency of traditional CNNs. xLSTM [2]\nenhanced conventional LSTMs with memory augmentation and cross-layer pa-\nrameter sharing, improving their ability to handle long-term dependencies and\nprocess sequential data effectively.\nAlthough these models were originally designed for different tasks, their core\nprinciples e.g. self-attention, hierarchical structures, and memory augmentation,\nare well-suited for time-series data. Applying these techniques to IMU-based\nHWR could potentially enhance performance and open new research opportuni-\nties in the field.\n3\nMethods\n3.1\nDatasets\nThis paper uses a dataset collected with the IMU-based pen developed by\nSTABILO [21]. The pen is equipped with two accelerometers, one at each end,\nalong with a gyroscope, a magnetometer, and a force sensor, generating 13 out-\nput channels at a sampling rate of 100 Hz. Data collection included 984 recording\nsessions with participants of different ages and handednesses, resulting in 54,666\nsamples of English and German words of varying lengths. These words cover\n59 character categories, including both upper- and lowercase letters for both\nlanguages.\n\n\n4\nJ. Li et al.\nThe datasets are evaluated using two configurations: writer-independent (WI)\nand random splits, both following a 5-fold cross-validation approach. In the WI\nsplit, data are divided so that no writer appears in both the training and test-\ning sets, ensuring handwriting styles remain independent. In the random split,\nsamples are assigned randomly without considering writer identity.\nTo evaluate model robustness, we analyze subsets of the WI dataset based\non participants’ ages. Since children are still developing their handwriting skills,\ntheir handwriting patterns differ significantly from those of adults, particularly\nin areas such as writing speed and discontinuity due to high cognitive load. These\npatterns typically stabilize around 14 years old [17]. Based on this, we classify\nparticipants aged 12 and under as children to ensure that most writers represent\ntypical children’s handwriting patterns, and those aged 18 and older as adults.\nWe assess model performance on both groups. Participants aged 13 to 17 were\nexcluded because some had already developed mature and fluent handwriting,\nmaking them less representative of either the children or adult groups. Notably,\nthe adult subset (47,992 samples) is significantly larger than the children subset\n(6,070 samples).\nTo evaluate data efficiency, we reduce the training sets to 50% and 25%\nof their original size while keeping the testing sets unchanged. Due to the sig-\nnificant imbalance between right- and left-handed samples (51,854 vs. 2,812,\nrespectively), we exclude handedness-related comparisons from our analysis.\nFor commercial reasons, this dataset will not be publicly available. How-\never, we also use the writer-dependent (WD) subset, where the WD subset is\ndivided by words, and the WI subset only from the right-handed portion of the\nOnHW-words500 dataset [16] to further benchmark our model against various\nmainstream models. Due to the significant imbalance between right-handed and\nleft-handed samples (25,218 vs. 1,000, respectively), left-handed portion of the\nOnHW dataset are excluded from our experiments.\n3.2\nHWR Model\nThe sequence-to-sequence model uses an encoder-decoder structure, as shown in\nFig 1. The encoder is a CNN that efficiently extracts and embeds input features,\nwhile the decoder is a BiLSTM network that captures contextual information\nin both directions. Both the CNN and BiLSTM can process inputs of varying\nlengths, providing flexibility for different handwriting data.\nThe encoder has three stages, each with a patch embedding layer followed by\nthree convolutional blocks. The patch embedding layer uses a 1-D convolutional\nlayer with a kernel size of 2 and a stride of 2, followed by a 1-D instance normal-\nization layer. The first patch embedding layer converts the 13-channel input data\ninto 128 channels, and each subsequent layer doubles the number of channels.\nThe convolutional blocks are based on the ConvNeXt block, which improves\nefficiency using grouped convolution and inverted bottlenecks. To reduce compu-\ntational cost without sacrificing performance, each block includes a depth-dilated\n1-D separable convolution. This layer uses a 1-D depthwise convolution with a\nkernel size of 5 that doubles the input dimension, followed by a 1-D pixelwise\n\n\nRobust and Efficient Writer-Independent IMU-based HWR\n5\n×3\nInstanceNorm\nConv1D\nDepth dilated \n1-D separable \nconvolution \nInstanceNorm\nGELU\nDropout\nBi-LSTM\n×3\n×3\nGGGGGG|ooooo|ooooo|ddddddd\nIMU signal\nDecoder\nEncoder\nText\nFig. 1. Model architecture\nconvolution that reduces it back. This design creates a larger hidden space for\nbetter feature representation compared to standard depthwise separable con-\nvolutions while reducing complexity by omitting an extra layer found in the\nConvNeXt block. Each depth-dilated 1-D separable convolution is followed by\na 1-D instance normalization layer, a GELU activation function, and a dropout\nlayer with a drop rate of 0.2.\nThe decoder has three BiLSTM layers, each with a hidden size of 128 and\na dropout rate of 0.2. These layers are followed by a fully connected layer and\na Softmax layer for pixelwise classification. The Softmax outputs are then pro-\ncessed by a greedy CTC decoder to produce the final results.\n3.3\nData Augmentation\nTo improve the model’s resistance to noise, we used four data augmentation\ntechniques: AddNoise, Drift, Dropout, and TimeWarp. Each technique has a\n25% chance of being applied to a given signal. TimeWarp only affects the time\ndimension, while the other methods are applied multiplicatively to keep the\naugmented signals’ magnitude similar to the original signals. Examples of these\naugmentations are shown in Fig 2. After augmentation, the signals are also\nnormalized.\n– AddNoise: Adds Gaussian noise to the original signals.\n– Drift: Divides the original signals into sections and applies random drift\nwithin each section.\n\n\n6\nJ. Li et al.\n– Dropout: Randomly replaces small segments of the signal with the last\nvalue preceding each segment.\n– TimeWarp: Randomly adjusts the speed of sections of the original signal.\n0\n5\n10\n15\n20\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAddNoise\n0\n5\n10\n15\n20\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nDrift\n0\n5\n10\n15\n20\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nDropout\n0\n5\n10\n15\n20\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTimeWarp\nFig. 2. Data augmentations\n4\nExperiments\nThe experiments are divided into five parts: comparison with previous methods,\nbenchmarking against mainstream models, robustness evaluation, data efficiency\nevaluation, and an ablation study. In each part, models are trained using 5-fold\ncross-validation, and the best results from each fold’s test set are used to calculate\nthe final results.\nExcept for the CLDNN [22], which is trained using the method described\nin its original paper, all other models, including those from previous studies\nand mainstream models, are trained using the same following approach. We use\nthe AdamW optimizer with a learning rate of 0.001 for 300 epochs. To improve\nconvergence, a linear learning rate warm-up starts at 0.0001 for the first 30\nepochs, followed by a cosine annealing schedule for the remaining epochs. The\n\n\nRobust and Efficient Writer-Independent IMU-based HWR\n7\ntraining process uses the CTC loss function and a batch size of 64. Models\nare saved and evaluated every 5 epochs during training. All experiments are\nconducted on a computer with an AMD Ryzen 9 5950X processor and a GeForce\nRTX 3090 graphics card, using the PyTorch library for model implementation.\nTo assess model performance, we use two metrics: character error rate (CER)\nand word error rate (WER). These measure the proportion of errors, including\nsubstitutions, deletions, and insertions, relative to the total number of characters\nin the reference text at the character and word levels, respectively. Additionally,\nwe evaluate model size and computational requirements by measuring the num-\nber of parameters (number of Params) and multiply and accumulate operations\n(MACs).\n4.1\nComparison with Previous Works\nIn this section, we compare our model with previous works using both our dataset\nand the OnHW dataset [16]. For our dataset, we evaluate performance on the\nrandom and WI splits, while for the OnHW dataset, we use the WD and WI\nsplits of the right-handed words500 subset. The comparison includes three mod-\nels: CNN+BiLSTM [16], CLDNN, and our proposed model. Since the code for\nthe previous models is not publicly available, we re-implemented them based on\ntheir published descriptions using PyTorch. We replicated the training and pre-\nprocessing strategy for CLDNN as described. However, the training details for\nCNN+BiLSTM were not clearly specified, making it difficult to reproduce the\nreported performance. Therefore, we trained CNN+BiLSTM using our training\npipeline and data augmentation strategy, which may result in differences from\nthe original reported results. Additionally, we include the previously published\nresults for CNN+BiLSTM on the OnHW dataset for reference.\nTable 1. Comparison with previous works on our dataset.\nModels\nRandom\nWI\n#Params\nMACs\nCER\nWER\nCER\nWER\nCNN+BiLSTM\n0.0710\n0.3142\n0.1512\n0.5213\n0.40M\n153M\nCLDNN\n0.0784\n0.3291\n0.1533\n0.5112\n0.75M\n291M\nOurs\n0.0367\n0.1703\n0.0692\n0.2676\n3.89M\n600M\nAs shown in Table 1, our model outperforms previous works, achieving the\nlowest CER and WER on both the random and WI splits of our dataset. Specif-\nically, it achieves a CER of 0.0367 and a WER of 0.1703 on the random split,\nand a CER of 0.0692 and a WER of 0.2676 on the WI split. In comparison, the\nerror rates of the CNN+BiLSTM and CLDNN models are approximately twice\nas high. However, our model has a larger number of parameters and higher\ncomputational costs than these models.\n\n\n8\nJ. Li et al.\nTable 2. Comparison with previous works on OnHW dataset [16].\nModels\nWD\nWI\n#Params\nMACs\nCER\nWER\nCER\nWER\nCNN+BiLSTM (orig.)\n0.1716\n0.5195\n0.2780\n0.6091\n0.40M\n153M\nCNN+BiLSTM\n0.1597\n0.5243\n0.1716\n0.4240\n0.40M\n153M\nCLDNN\n0.1696\n0.5404\n0.1715\n0.3948\n0.75M\n291M\nOurs\n0.1546\n0.4551\n0.0746\n0.1559\n3.89M\n600M\nAs shown in Table 2, on the OnHW dataset, our model achieves the best\nperformance compared to previous works. It achieves the lowest CER and WER\nacross both the WD and WI splits, with a CER of 0.1546 and a WER of 0.4551\nfor the WD split, and a CER of 0.0746 and a WER of 0.1559 for the WI split.\nCompared to the original CNN+BiLSTM results reported in the paper, our reim-\nplementation using the same model architecture but our training pipeline and\ndata augmentation strategy leads to better convergence and lower error rates.\nIn the WI split, our model reduces the error rates by more than half, highlight-\ning its effectiveness in handling challenges associated with unseen handwriting\nstyles.\n4.2\nBenchmark against Mainstream Models\nIn this section, we benchmark our proposed model against various models using\nthe same datasets as in the previous experiments. To efficiently extract high-\nlevel features while reducing output size and computational cost for the decoder,\nwe use ResNet, ConvNeXt, MLP-Mixer, and Swin Transformer V2 as encoders\ncombined with our BiLSTM decoder. To capture long-range dependencies, we\nuse the mLSTM module of xLSTM as a decoder alongside our CNN encoder. For\na fair comparison, we implement the mLSTM module in a bidirectional manner,\nallowing it to access both past and future information. Since the Transformer\narchitecture can capture high-level information and has a global receptive field\nover the input, we evaluate it as an encoder, a decoder, and as a standalone\nHWR model. Since these models were not originally designed for HWR and\nvary in size, we adjust their hyperparameters to better suit the HWR task while\nmaintaining a similar number of parameters across models. Additionally, because\nTransformer, MLP-Mixer, Swin Transformer V2, and Bi-mLSTM require a fixed\ninput length, all inputs for these models are zero-padded to a length of 1024.\nTable 3 shows that our model achieves the best CER and WER on the WI\nsplit, with values of 0.0692 and 0.2676, respectively, demonstrating its strong\nability to generalize to unseen handwriting styles. It also delivers competitive\nperformance on the random split, achieving a CER of 0.0367 and a WER of\n0.1703. Compared to other models, our approach provides well-balanced per-\nformance across both splits, outperforming most other models. While the MLP-\nMixer achieves the lowest error rates on the random split, it requires significantly\n\n\nRobust and Efficient Writer-Independent IMU-based HWR\n9\nTable 3. Benchmark against mainstream models on our dataset.\nModels\nRandom\nWI\n#Params\nMACs\nCER\nWER\nCER\nWER\nTransformer\n0.0443\n0.2082\n0.1163\n0.4042\n3.96M\n509M\nResNet (enc.)\n0.0306\n0.1456\n0.0805\n0.3026\n3.97M\n591M\nConvNeXt (enc.)\n0.0355\n0.1696\n0.0834\n0.3205\n3.86M\n600M\nMLP-Mixer (enc.)\n0.0274\n0.1339\n0.1025\n0.3624\n3.90M\n802M\nTransformer (enc.)\n0.0422\n0.1975\n0.1002\n0.3572\n3.71M\n477M\nSwinV2 (enc.)\n0.0303\n0.1491\n0.0746\n0.2924\n3.88M\n601M\nTransformer (dec)\n0.0387\n0.1822\n0.0828\n0.3183\n3.82M\n590M\nBi-mLSTM (dec.)\n0.0476\n0.2192\n0.0914\n0.3484\n4.10M\n625M\nOurs\n0.0367\n0.1703\n0.0692\n0.2676\n3.89M\n600M\nhigher computational resources, with the highest MACs among all models. In-\nterestingly, the MLP-Mixer ranks second worst on the WI split. This highlights\nthe efficiency and robustness of our model in achieving strong performance while\nmaintaining a favorable balance between accuracy and computational cost.\nTable 4. Benchmark against mainstream models on OnHW dataset.\nModels\nWD\nWI\n#Params\nMACs\nCER\nWER\nCER\nWER\nTransformer\n0.2615\n0.6283\n0.1139\n0.2573\n3.96M\n509M\nResNet (enc.)\n0.1294\n0.4164\n0.0850\n0.1846\n3.97M\n591M\nConvNeXt (enc.)\n0.1515\n0.4657\n0.0812\n0.1791\n3.86M\n600M\nMLP-Mixer (enc.)\n0.1438\n0.4659\n0.0964\n0.2149\n3.90M\n802M\nTransformer (enc.)\n0.1817\n0.5242\n0.1060\n0.2303\n3.71M\n477M\nSwinV2 (enc.)\n0.1750\n0.4983\n0.0820\n0.1814\n3.88M\n601M\nTransformer (dec)\n0.1396\n0.4407\n0.0864\n0.1881\n3.82M\n590M\nBi-mLSTM (dec.)\n0.2267\n0.5749\n0.0841\n0.1803\n4.10M\n625M\nOurs\n0.1546\n0.4551\n0.0746\n0.1559\n3.89M\n600M\nTable 4 compares the performance of different models on the OnHW dataset.\nOur model delivers strong results, with a CER of 0.1546 and a WER of 0.4551\non the WD split, and the lowest CER of 0.0746 and WER of 0.1559 on the\nWI split. CNN-based models, such as ResNet and ConvNeXt, also perform well,\nwith ResNet achieving the lowest CER and WER on the WD split. These models\nshow better performance on the unseen WI split compared to Transformer-based\nmodels, which generally have higher CER and WER. These results highlight the\nadvantage of CNN-based models in handling unseen handwriting styles while\nmaintaining a good balance between performance and efficiency.\n\n\n10\nJ. Li et al.\n4.3\nRobustness Evaluation\nIn this section, we assess the robustness of the models on the adult (Adult) and\nchildren (Children) subsets of our WI dataset. Additionally, we evaluate how\nmodels trained on the adult subset perform when tested on the children’s subset\n(Adult2Child). All models are tested using the same configurations as in the\nprevious experiments.\nTable 5. Robustness evaluation.\nModels\nAdults\nChildren\nAdult2Child\nCER\nWER\nCER\nWER\nCER\nWER\nCNN+BiLSTM\n0.1525\n0.5086\n0.6545\n0.9669\n0.6678\n0.9982\nCLDNN\n0.1479\n0.4793\n0.4704\n0.8331\n0.3716\n0.7783\nTransformer\n0.1247\n0.4199\n0.7584\n0.9881\n0.3350\n0.7466\nResNet (enc.)\n0.0891\n0.3194\n0.1775\n0.4934\n0.2841\n0.6607\nConvNeXt (enc.)\n0.0910\n0.3330\n0.3102\n0.6557\n0.2801\n0.6612\nMLP-Mixer (enc.)\n0.1118\n0.3715\n0.7732\n0.9835\n0.3044\n0.6824\nTransformer (enc.)\n0.1115\n0.3727\n0.4559\n0.8390\n0.2888\n0.6681\nSwinV2 (enc.)\n0.0842\n0.3109\n0.5400\n0.8634\n0.2637\n0.6368\nTransformer (dec.)\n0.0891\n0.3252\n0.2462\n0.5431\n0.2807\n0.6630\nBi-mLSTM (dec.)\n0.0974\n0.3524\n0.3079\n0.6769\n0.2995\n0.7018\nOurs\n0.0752\n0.2772\n0.1748\n0.4493\n0.2678\n0.6273\nTable 5 shows the robustness of our proposed model on the adult and children\nsubsets of the WI split of our dataset. Our model achieves the best results on\nboth subsets, with a CER of 0.0752 and a WER of 0.2772 for adults, and a CER\nof 0.1748 and a WER of 0.4493 for children. When trained on the adult subset,\nalthough our model is slightly behind the Swin Transformer V2, it still achieves\nthe lowest WER and ranks among the top performers on the children subset.\nThis demonstrates its ability to learn robust features and adapt to different\nhandwriting styles across age groups. Notably, models like the Transformer and\nMLP-Mixer struggle with the children subset, likely due to the challenge of\ngeneralizing to different handwriting styles when trained on the adult dataset\nand the smaller size of the children subset.\n4.4\nData Efficiency Evaluation\nIn this section, we assess the data efficiency of the models by training them on\ndifferent portions of the training set. Using 5-fold cross-validation, each training\nset consists of four groups of data. To evaluate performance under varying data\nconditions, we train all models on 100% (four groups), 50% (two groups), and\n25% (one group) of the original training set from our WI dataset.\nTable 6 shows the data efficiency of various models when trained on different\nproportions of the WI dataset. Our model consistently outperforms all others\n\n\nRobust and Efficient Writer-Independent IMU-based HWR\n11\nTable 6. Data efficiency evaluation.\nModels\n100%\n50%\n25%\nCER\nWER\nCER\nWER\nCER\nWER\nCNN+BiLSTM\n0.1512\n0.5213\n0.2733\n0.6833\n0.3877\n0.8208\nCLDNN\n0.1533\n0.5112\n0.1868\n0.5665\n0.2688\n0.7096\nTransformer\n0.1163\n0.4042\n0.1765\n0.5318\n0.1910\n0.5615\nResNet (enc.)\n0.0805\n0.3026\n0.1256\n0.4135\n0.1978\n0.5730\nConvNeXt (enc.)\n0.0834\n0.3205\n0.1297\n0.4336\n0.2005\n0.5869\nMLP-Mixer (enc.)\n0.1025\n0.3624\n0.1471\n0.4622\n0.2223\n0.6213\nTransformer (enc.)\n0.1002\n0.3572\n0.1476\n0.4692\n0.2272\n0.6295\nSwinV2 (enc.)\n0.0746\n0.2924\n0.1191\n0.4081\n0.1929\n0.5728\nTransformer (dec.)\n0.0828\n0.3183\n0.1253\n0.4244\n0.1910\n0.5615\nBi-mLSTM (dec.)\n0.0914\n0.3484\n0.1356\n0.4526\n0.2077\n0.5935\nOurs\n0.0692\n0.2676\n0.1102\n0.3763\n0.1802\n0.5235\nacross all training set sizes, achieving a CER of 0.0692 and WER of 0.2676 on the\nfull dataset (100%), 0.1102 and 0.3763 on 50%, and 0.1802 and 0.5235 on 25%.\nThese results highlight our model’s ability to maintain high accuracy even with\nless training data. While ResNet and SwinV2 perform well on larger training\nsets, their accuracy drops more noticeably as the dataset size decreases. Other\nmodels, such as the Transformer and MLP-Mixer, show even larger performance\ngaps, emphasizing the robustness and efficiency of our model in making effective\nuse of training data.\n4.5\nAblation Study\nIn this section, we conduct an ablation study on our WI dataset using CLDNN\nas the baseline, as it has a similar architecture to our model. We evaluate the\nperformance improvements achieved through incremental changes, including op-\ntimized training strategies, architectural enhancements, hyperparameter tuning,\nand data augmentation.\nTable 7 summarizes the ablation study results, showing performance im-\nprovements from incremental modifications. Starting from the CLDNN baseline\nwith a CER of 0.1533 and WER of 0.5112, each enhancement improves accuracy.\nScaling the CNN and BiLSTM reduces error rates at the cost of more parameters\nand MACs. Optimized training strategies, instance normalization, GELU activa-\ntion, and dropout rate adjustment further boost performance without increasing\ncomputational overhead. Notably, the dilated-depth 1-D separable convolution\nimproves accuracy while reducing parameters and MACs. Data augmentation\ntechniques, such as add dropout and time warping, also play a key role, lowering\nCER from 0.0839 to 0.0745 (-11.2%) and WER from 0.3098 to 0.2810 (-9.3%),\nachieving significant gains despite limited room for improvement. With all en-\nhancements and fine-tuned dropout, the final model achieves a CER of 0.0692\nand a WER of 0.2676, demonstrating the effectiveness of these modifications. In\n\n\n12\nJ. Li et al.\nTable 7. Ablation study.\nModels\nCER\nWER\n#Params\nMACs\nCLDNN\n0.1533\n0.5112\n0.75M\n291M\n+ Training strategy\n0.1305\n0.4500\n0.75M\n291M\n+ Reverse dimension order\n0.1261\n0.4470\n0.92M\n214M\n+ 3× deeper CNN\n0.1070\n0.3924\n3.05M\n989M\n+ Standalone embedding layer\n0.1064\n0.3851\n3.95M\n687M\n+ Dilated-depth separable convolution\n0.0997\n0.3630\n2.85M\n467M\n+ Instance normalization\n0.0960\n0.3635\n2.85M\n465M\n+ GELU\n0.0934\n0.3526\n2.85M\n465M\n+ 2× wider BiLSTM\n0.0860\n0.3203\n3.52M\n551M\n+ 3-layer BiLSTM\n0.0830\n0.3048\n3.91M\n602M\n+ Remove hidden layer\n0.0839\n0.3098\n3.89M\n600M\n+ Data augmentation\n0.0745\n0.2810\n3.89M\n600M\n+ Dropout rate 0.2\n0.0692\n0.2676\n3.89M\n600M\nsummary, our final model reduces CER by 55% and WER by 48% compared to\nthe baseline CLDNN. To achieve this performance improvement, it uses approx-\nimately 5.2 times more parameters (increasing from 0.75M to 3.89M) and 2.1\ntimes more MACs (rising from 291M to 600M).\n5\nDiscussion\nThis section examines the key factors behind our model’s strong performance,\nwith a focus on robustness, data efficiency, and flexibility.\n5.1\nRobustness\nHandwriting styles vary widely among individuals, creating significant challenges\nfor HWR. Since it is impossible to train an HWR model on data that includes\nevery handwriting style in the world, evaluating models on WI datasets provides\na more realistic measure of performance than WD or random-split datasets. WI\ndatasets ensure that the handwriting styles in the training set do not overlap\nwith those in the test set, better simulating real-world scenarios where a model\nmust generalize to unseen writers.\nAs children are beginners in handwriting, their handwriting styles differ from\nthose of adults, making HWR more challenging. However, they should not be\nexcluded as potential users of HWR systems. Therefore, developing a solution\nthat works well for both adults and children is essential for the success of an\nHWR system.\nAdditionally, while noise is common in IMU data and poses a challenge for\nHWR, this issue can be mitigated by manually introducing noise during training\nto improve the model’s resistance to noise.\n\n\nRobust and Efficient Writer-Independent IMU-based HWR\n13\n5.2\nData Efficiency\nMore data generally improves the performance of deep learning models, including\nHWR. However, supervising contributors and removing faulty samples, such as\ntypographical mistakes, makes collecting handwriting IMU data time-consuming\nand costly. Therefore, the ability to extract features efficiently from smaller\ndatasets is crucial for real-world applications. This can be achieved through\ndata augmentation, regularization, normalization, and learning rate scheduling,\nwhich help models converge better and perform well with limited training data.\n5.3\nFlexibility\nIn real-world deployment, handwriting inputs vary in size, posing challenges\nfor models that require fixed input dimensions. Transformer-based models, such\nas Swin Transformer V2 and xLSTM, rely on predefined input sizes, requiring\npadding or compression to meet these constraints. However, padding increases\ncomputational overhead, while compression can lead to information loss. In con-\ntrast, the CNN-BiLSTM design processes inputs of any size without resizing,\nimproving computational efficiency and adaptability, making it more suitable\nfor real-world HWR.\n6\nConclusion & Outlook\nIn conclusion, our experiments show that our model consistently outperforms\ncompetitors on WI datasets, demonstrating strong robustness, data efficiency,\nand flexibility in HWR. These strengths make it well-suited for real-world de-\nployment.\nHowever, our evaluation is limited by the dataset, as we have not compared\nperformance on left- and right-handed data and have observed significant im-\nprovements with larger datasets. Additionally, we have not investigated whether\nthere are patterns of errors that could provide insights for further reducing the\nerror rate. We also have yet to explore performance on sentence-level handwrit-\ning, which better reflects real-world use and could offer valuable directions for\nfuture research.\nSince HWR is closely related to language, incorporating natural language pro-\ncessing techniques, such as multimodal pretraining, could potentially encourage\nmodels to learn more semantic features and further enhance performance. Ex-\nploring these approaches should lead to even more robust and intelligent HWR\nsystems.\nReferences\n1. Alemayoh, T.T., Shintani, M., Lee, J.H., Okamoto, S.: Deep-learning-based char-\nacter recognition from handwriting motion data captured using imu and force\nsensors. Sensors 22(20) (2022). https://doi.org/10.3390/s22207840, https://www.\nmdpi.com/1424-8220/22/20/7840\n\n\n14\nJ. Li et al.\n2. Beck, M., Pöppel, K., Spanring, M., Auer, A., Prudnikova, O., Kopp, M., Klam-\nbauer, G., Brandstetter, J., Hochreiter, S.: xlstm: Extended long short-term\nmemory. In: Thirty-eighth Conference on Neural Information Processing Systems\n(2024), https://arxiv.org/abs/2405.04517\n3. Carbune, V., Gonnet, P., Deselaers, T., Rowley, H.A., Daryin, A., Calvo, M., Wang,\nL.L., Keysers, D., Feuz, S., Gervais, P.: Fast multi-language lstm-based online hand-\nwriting recognition. International Journal on Document Analysis and Recognition\n(IJDAR) 23(2), 89–102 (Jun 2020). https://doi.org/10.1007/s10032-020-00350-4,\nhttps://doi.org/10.1007/s10032-020-00350-4\n4. Choi, S.d., Lee, A.S., Lee, S.y.: On-line handwritten character recognition with 3d\naccelerometer. In: 2006 IEEE International Conference on Information Acquisition.\npp. 845–850 (2006). https://doi.org/10.1109/ICIA.2006.305842\n5. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.:\nAn image is worth 16x16 words: Transformers for image recognition at scale. In:\nInternational Conference on Learning Representations (2021), https://openreview.\nnet/forum?id=YicbFdNTTy\n6. Graves, A., Fernández, S., Gomez, F., Schmidhuber, J.: Connectionist tempo-\nral classification: labelling unsegmented sequence data with recurrent neural net-\nworks. In: Proceedings of the 23rd International Conference on Machine Learning.\npp. 369–376. ICML ’06, Association for Computing Machinery, New York, NY,\nUSA (2006). https://doi.org/10.1145/1143844.1143891, https://doi.org/10.1145/\n1143844.1143891\n7. Hassan, E., Tarek, H., Hazem, M., Bahnacy, S., Shaheen, L., Elashmwai, W.H.:\nMedical prescription recognition using machine learning. In: 2021 IEEE 11th An-\nnual Computing and Communication Workshop and Conference (CCWC). pp.\n0973–0979 (2021). https://doi.org/10.1109/CCWC51732.2021.9376141\n8. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR) (June 2016)\n9. Jeen-Shing, W., Yu-Liang, H., Cheng-Ling, C.: Online handwriting recognition\nusing an accelerometer-based pen device. In: Proceedings of the 2nd International\nConference on Advances in Computer Science and Engineering (CSE 2013). pp.\n231–234. Atlantis Press (2013/07). https://doi.org/10.2991/cse.2013.52, https://\ndoi.org/10.2991/cse.2013.52\n10. Liu, Z., Hu, H., Lin, Y., Yao, Z., Xie, Z., Wei, Y., Ning, J., Cao, Y., Zhang, Z., Dong,\nL., Wei, F., Guo, B.: Swin transformer v2: Scaling up capacity and resolution. In:\nInternational Conference on Computer Vision and Pattern Recognition (CVPR)\n(2022)\n11. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin\ntransformer: Hierarchical vision transformer using shifted windows. In: Proceedings\nof the IEEE/CVF International Conference on Computer Vision (ICCV) (2021)\n12. Liu, Z., Mao, H., Wu, C.Y., Feichtenhofer, C., Darrell, T., Xie, S.: A convnet for\nthe 2020s. Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR) (2022)\n13. Lopez-Rodriguez, P., Avina-Cervantes, J.G., Contreras-Hernandez, J.L., Cor-\nrea, R., Ruiz-Pinales, J.: Handwriting recognition based on 3d accelerometer\ndata by deep learning. Applied Sciences 12(13) (2022). https://doi.org/10.3390/\napp12136707, https://www.mdpi.com/2076-3417/12/13/6707\n\n\nRobust and Efficient Writer-Independent IMU-based HWR\n15\n14. Meißl, F., Eibensteiner, F., Petz, P., Langer, J.: Online handwriting recognition\nusing lstm on microcontroller and imu sensors. In: 2022 21st IEEE International\nConference on Machine Learning and Applications (ICMLA). pp. 999–1004 (2022).\nhttps://doi.org/10.1109/ICMLA55696.2022.00167\n15. Meißl, F., Eibensteiner, F., Petz, P., Langer, J.: Online handwriting recognition\nusing lstm on microcontroller and imu sensors. In: 2022 21st IEEE International\nConference on Machine Learning and Applications (ICMLA). pp. 999–1004 (2022).\nhttps://doi.org/10.1109/ICMLA55696.2022.00167\n16. Ott, F., Rügamer, D., Heublein, L., Hamann, T., Barth, J., Bischl, B., Mutschler,\nC.: Benchmarking online sequence-to-sequence and character-based handwriting\nrecognition from imu-enhanced pens. Int. J. Doc. Anal. Recognit. 25(4), 385–\n414 (Dec 2022). https://doi.org/10.1007/s10032-022-00415-6, https://doi.org/10.\n1007/s10032-022-00415-6\n17. Pontart, V., Bidet-Ildei, C., Lambert, E., Morisset, P., Flouret, L., ALA-\nMARGOT, D.: Influence of handwriting skills during spelling in primary\nand lower secondary grades. Frontiers in Psychology 4 (2013). https://doi.\norg/10.3389/fpsyg.2013.00818, https://www.frontiersin.org/journals/psychology/\narticles/10.3389/fpsyg.2013.00818\n18. Seuret, M., van der Loop, J., Weichselbaumer, N., Mayr, M., Molnar, J., Hass, T.,\nChristlein, V.: Combining ocr models for reading early modern books. In: Fink,\nG.A., Jain, R., Kise, K., Zanibbi, R. (eds.) Document Analysis and Recognition -\nICDAR 2023. pp. 342–357. Springer Nature Switzerland, Cham (2023)\n19. Tolstikhin, I.O., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner,\nT., Yung, J., Steiner, A., Keysers, D., Uszkoreit, J., Lucic, M., Dosovitskiy, A.:\nMlp-mixer: An all-mlp architecture for vision. In: Ranzato, M., Beygelzimer, A.,\nDauphin, Y., Liang, P., Vaughan, J.W. (eds.) Advances in Neural Information\nProcessing Systems. vol. 34, pp. 24261–24272. Curran Associates, Inc. (2021)\n20. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\nL.u., Polosukhin, I.: Attention is all you need. In: Guyon, I., Luxburg, U.V., Bengio,\nS., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R. (eds.) Advances in\nNeural Information Processing Systems. vol. 30. Curran Associates, Inc. (2017)\n21. Wehbi, M., Hamann, T., Barth, J., Eskofier, B.: Digitizing handwriting with a\nsensor pen: A writer-independent recognizer. In: 2020 17th International Con-\nference on Frontiers in Handwriting Recognition (ICFHR). pp. 295–300 (2020).\nhttps://doi.org/10.1109/ICFHR2020.2020.00061\n22. Wehbi, M., Hamann, T., Barth, J., Kaempf, P., Zanca, D., Eskofier, B.: Towards an\nimu-based pen online handwriting recognizer. In: Lladós, J., Lopresti, D., Uchida,\nS. (eds.) Document Analysis and Recognition – ICDAR 2021. pp. 289–303. Springer\nInternational Publishing, Cham (2021)\n\n\n"}
{"text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n1\nJiTTER: Jigsaw Temporal Transformer for Event\nReconstruction\nfor Self-Supervised Sound Event Detection\nHyeonuk Nam, member, IEEE, Yong-Hwa Park, member, IEEE,\nAbstract—Sound event detection (SED) has significantly ben-\nefited from self-supervised learning (SSL) approaches, partic-\nularly masked audio transformer for SED (MAT-SED), which\nleverages masked block prediction to reconstruct missing audio\nsegments. However, while effective in capturing global dependen-\ncies, masked block prediction disrupts transient sound events and\nlacks explicit enforcement of temporal order, making it less suit-\nable for fine-grained event boundary detection. To address these\nlimitations, we propose JiTTER (Jigsaw Temporal Transformer for\nEvent Reconstruction), an SSL framework designed to enhance\ntemporal modeling in transformer-based SED. JiTTER intro-\nduces a hierarchical temporal shuffle reconstruction strategy,\nwhere audio sequences are randomly shuffled at both the block-\nlevel and frame-level, forcing the model to reconstruct the correct\ntemporal order. This pretraining objective encourages the model\nto learn both global event structures and fine-grained transient\ndetails, improving its ability to detect events with sharp onset-\noffset characteristics. Additionally, we incorporate noise injection\nduring block shuffle, providing a subtle perturbation mechanism\nthat further regularizes feature learning and enhances model\nrobustness. Experimental results on the DESED dataset demon-\nstrate that JiTTER outperforms MAT-SED, achieving a 5.89%\nimprovement in PSDS, highlighting the effectiveness of explicit\ntemporal reasoning in SSL-based SED. Our findings suggest\nthat structured temporal reconstruction tasks, rather than simple\nmasked prediction, offer a more effective pretraining paradigm\nfor sound event representation learning.\nIndex Terms—Sound event detection, self-supervised learning,\ntemporal modeling, transformer, hierarchical shuffle\nI. INTRODUCTION\nSound event detection (SED) is a fundamental task in\nmachine listening and plays a crucial role in auditory intel-\nligence, enabling applications in AI-driven perception, smart\nenvironments, and bioacoustic monitoring [1]–[6]. In addition\nto SED, various research efforts have focused on speech and\nspeaker recognition [7]–[16], sound event recognition [17]–\n[21], sound event localization and detection (SELD) [22]–\n[24], automated audio captioning (AAC) [25]–[27], few-shot\nbioacoustic detection [28], [29], human auditory perception\n[30], [31], highlighting the broad impact of auditory perception\nin real-world applications. Furthermore, recent advancements\nin sound synthesis [32]–[34] have explored the generative\nmodeling of sound events from textual descriptions, providing\nnew perspectives in sound representation learning.\nThis paper was produced by the IEEE Publication Technology Group. They\nare in Piscataway, NJ.\nManuscript received April 19, 2021; revised August 16, 2021.\nFig. 1.\nIllustration of the hierarchical temporal shuffle strategy in JiTTER,\ndesigned to improve temporal modeling for self-supervised SED. (a) Block-\nLevel Shuffle: The input audio sequence is divided into non-overlapping\nblocks, and a portion of these blocks (in darker grey) is randomly shuffled\nalong the time axis. This disrupts global event dependencies while preserving\nintra-block structures, forcing the model to reconstruct event sequences at\na higher level. (b) Frame-Level Shuffle: A subset of blocks is randomly\nselected, and within each selected block (in blue and orange), a fraction\nof frames (in darker blue and orange) is randomly shuffled. This introduces\nfine-grained perturbations while maintaining the overall event order, helping\nthe model learn transient sound characteristics. Together, these two levels of\nshuffle perturbations encourage the model to reconstruct the correct temporal\norder, improving both global event structure comprehension and fine-grained\nboundary detection in SED.\nSED aims to classify sound events while precisely lo-\ncalizing their temporal boundaries, making it a fundamental\nresearch area in auditory intelligence. Recent advances in\ndeep learning have significantly improved SED performance\n[35]–[42], driven by both architectural innovations and in-\nsights from auditory cognition [43]–[47]. Early approaches\nprimarily relied on convolutional neural networks (CNNs) to\nmodel spectral and temporal patterns in audio signals [48]–\n[50]. While CNN-based models demonstrated strong feature\nextraction capabilities, they struggled to capture long-range\ndependencies, which are essential for distinguishing between\nsequential and overlapping sound events. To overcome this\nlimitation, transformer-based architectures pretrained on large-\nscale datasets such as AudioSet have been explored, enabling\nmore effective temporal modeling [38]–[40], [51], [52]. These\ntransformer-based models leverage self-attention mechanisms\nto capture global contextual information, leading to significant\nimprovements in SED accuracy and robustness.\nAmong recent transformer-based approaches, the masked\n0000–0000/00$00.00 © 2025 IEEE\narXiv:2502.20857v1  [eess.AS]  28 Feb 2025\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n2\naudio transformer for SED (MAT-SED) introduced a self-\nsupervised learning (SSL) strategy on full transformer model\nknown as masked block prediction [41]. This method in-\nvolves masking temporal blocks of audio input and training\nthe model to reconstruct the missing features, improving its\nability to learn meaningful audio representations. MAT-SED\nemploys the patchout fast spectrogram transformer (PaSST)\nas an encoder network and a transformer with relative po-\nsitional encoding (RPE) as a context network, forming a\nfully transformer-based SED model [53]. The prototype-based\nmasked audio model (PMAM) further extended this approach\nby introducing Gaussian mixture model (GMM)-based proto-\ntypical representations as semantically rich frame-level pseudo\nlabels [42]. While these methods enhance the model’s ability\nto capture long-range dependencies and improve generaliza-\ntion, they exhibit two fundamental limitations: transient event\nloss and lacking explicit temporal order constraints.\nTo overcome these challenges, we propose JiTTER (Jig-\nsaw Temporal Transformer for Event Reconstruction), a self-\nsupervised learning framework that introduces hierarchical\ntemporal shuffle reconstruction to improve temporal modeling\nin SED. Unlike masked block prediction, which removes infor-\nmation, JiTTER shuffles audio segments at different temporal\nscales, forcing the model to learn to reconstruct their correct\norder. As shown in Figure 1, JiTTER applies two levels of\nperturbation:\n• Block-Level Shuffle: Randomly shuffles non-overlapping\nblocks of audio, disrupting global event structures while\npreserving local coherence.\n• Frame-Level Shuffle: Randomly selects blocks and shuf-\nfles a fraction of frames within each selected block,\nperturbing fine-grained temporal information.\nBy solving this hierarchical jigsaw-like reconstruction task,\nJiTTER explicitly enforces global event continuity and tran-\nsient event representation, enhancing the model’s ability to\nrecognize event boundaries while capturing long-range de-\npendencies. Additionally, we introduce noise injection during\nblock shuffle to provide a controlled level of information\ncorruption. Unlike masked block prediction, which completely\nremoves event cues, noise injection partially obscures informa-\ntion while retaining weak structural signals, promoting more\nrobust feature learning.\nOur experimental results on the Domestic Environment\nSound Event Detection (DESED) dataset demonstrate that\nJiTTER achieves a 5.89% improvement in PSDS over MAT-\nSED, highlighting the importance of explicitly modeling tem-\nporal dependencies in self-supervised pretraining for SED.\nMoreover, ablation studies confirm the optimal shuffle config-\nurations, suggesting that block-level shuffle primarily captures\nlong-range temporal dependencies, while frame-level shuffle\nenhances short-range temporal structures. Multitask learning,\nwhich combines both shuffle strategies, yields the highest\nperformance gains, further indicating that these perturbations\noperate at different temporal scales. This highlights the ben-\nefit of integrating multiple levels of temporal reordering to\nstrengthen event representations.\nThe key contributions of this paper are:\n1) We introduce JiTTER, a self-supervised pretraining\nframework that enhances SED performance by recon-\nstructing temporally shuffled sequences.\n2) We propose hierarchical temporal shuffle reconstruc-\ntion, combining block-level and frame-level shuffling\nto improve global event continuity and transient event\nrecognition.\n3) We incorporate noise injection during block shuffle to\nenhance feature learning while preserving weak struc-\ntural cues.\n4) Extensive ablation studies validate the effectiveness of\ndifferent shuffle configurations and multitask learning in\nimproving SED performance.\n5) Our experiments on the DESED dataset demonstrate that\nJiTTER outperforms conventional masked prediction\nmethods, achieving a 5.89% PSDS improvement over\nMAT-SED.\nThe official implementation code is available on GitHub1.\nII. RELATED WORKS\nA. Self-Supervised Learning for Sound Recognition\nSSL has been widely explored for audio tagging and\ngeneral-purpose audio representation learning. SSL enables\nmodels to learn meaningful features without requiring explicit\nhuman annotations, making it particularly valuable in domains\nwith limited labeled data. Several SSL approaches have been\nproposed to enhance audio tagging, focusing on capturing\nhigh-level acoustic representations.\nBYOL-A [54] applies contrastive learning to augmented\naudio segments, ensuring consistency across different transfor-\nmations. SSAST, an extension of the audio spectrogram trans-\nformer (AST), combines discriminative and generative masked\nspectrogram patch modeling to improve generalization across\ndiverse audio tasks [20], [55]. AudioMAE employs a masked\nautoencoder framework to reconstruct spectrogram patches\nusing a transformer encoder-decoder [56], while BEATs itera-\ntively optimizes an acoustic tokenizer alongside an SSL model\nto refine bidirectional audio representations [21].\nWhile these SSL methods have shown success in general-\npurpose audio learning and audio tagging, they are not specifi-\ncally designed for SED, which requires precise event boundary\nlocalization. Unlike audio tagging, where the objective is to\nclassify the presence of sound categories, SED demands ac-\ncurate onset-offset detection and a structured understanding of\nsequential dependencies. Many existing SSL models prioritize\ngeneral acoustic feature extraction but lack mechanisms to\ncapture fine-grained temporal structures necessary for event-\nlevel modeling.\nTo address these challenges, SSL techniques specifically\ndesigned for SED have been proposed. MAT-SED introduced\nmasked block prediction, where randomly selected temporal\nblocks in an audio sequence are masked, and the model is\ntrained to reconstruct the missing features [41]. This approach\nenhances the model’s ability to capture global contextual de-\npendencies and improves its robustness to missing information.\n1https://github.com/frednam93/JiTTER-SED\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n3\nThe prototype-based masked audio model (PMAM) further\nextended this technique by incorporating Gaussian mixture\nmodel (GMM)-based pseudo labels, providing additional se-\nmantic guidance during reconstruction [42]. These methods\nhave demonstrated state-of-the-art performance in modeling\nlong-range dependencies for self-supervised SED.\nB. Shuffle-Based Learning in Representation Learning\nSegment reordering tasks have been widely explored in\nself-supervised representation learning, particularly in vision\nand video processing. Jigsaw pretext tasks involve permuting\nimage patches and training models to recover the correct order,\nleading to stronger spatial feature learning [57], [58]. In the\nvideo domain, Shuffle and Learn demonstrated that training\nmodels to predict the correct sequence of shuffled frames\nenhances motion feature learning [59].\nIn audio processing, shuffle-based learning has been largely\nunexplored, with existing self-supervised approaches like\nTERA and BYOL-A focusing on temporal coherence rather\nthan explicit sequence reconstruction. TERA promotes tempo-\nral consistency by reconstructing spectrograms altered along\ntime, frequency, and magnitude axes, ensuring robustness to\ndistortions but without enforcing strict sequential dependen-\ncies [60]. BYOL-A enhances temporal coherence through\nrandom resized cropping (RRC), which approximates pitch\nshifting and time stretching by randomly resizing and crop-\nping spectrogram segments. This forces the model to learn\nrepresentations that remain stable across variations in time and\nfrequency [54]. However, neither method explicitly enforces\nevent ordering, which is critical for SED. In contrast, JiTTER\nextends shuffle-based learning to SED by introducing hier-\narchical temporal shuffle reconstruction, where models must\nrecover the correct sequence after structured perturbations\nat both block and frame levels. Unlike contrastive learning,\nJiTTER does not rely on negative pairs but directly optimizes\nfor sequence restoration, making it more suitable for fine-\ngrained temporal reasoning in SED.\nC. Temporal Modeling for Sound Event Detection\nTemporal modeling is critical in SED, where accurately\ncapturing event onsets and offsets is essential. Transformer-\nbased architectures have demonstrated strong capabilities in\nmodeling long-range dependencies. Patchout fast spectrogram\ntransformer (PaSST) [53] improves computational efficiency\nwhile preserving contextual modeling, making it a strong\nbackbone for SED. More recently, studies have explored\nrelative positional encoding (RPE) [61], which helps capture\nfine-grained temporal relationships.\nA key challenge in SED is distinguishing overlapping\nevents from sequentially occurring ones. Previous works\nhave attempted to improve event boundary detection through\nclassification-based segmentation [62] and data augmentation\ntechniques [7], [37], but these approaches lack a structured\nlearning objective for temporal order recovery. JiTTER ad-\ndresses this gap by enforcing explicit temporal reasoning\nthrough hierarchical shuffle-based reconstruction, leading to\nimproved event boundary detection and reduced reliance on\npost-processing techniques.\nIII. METHODOLOGY\nJiTTER (Jigsaw Temporal Transformer for Event Recon-\nstruction) is a self-supervised learning framework designed\nto improve temporal representation learning for SED. Unlike\nmasked prediction, JiTTER introduces a hierarchical temporal\nshuffle reconstruction strategy, challenging the model to re-\ncover the correct sequence of shuffled segments. This enhances\nthe model’s ability to capture both local transient structures\nand long-term event dependencies, leading to improved sound\nevent boundary detection.\nA. Limitations of Masked Block Prediction\nMasked block prediction, as employed in MAT-SED, re-\nmoves entire audio segments, leading to the loss of transient\nsound events such as footsteps, door slams, and alarms, which\nare brief and temporally localized [41]. Since these masked\nsegments are absent during training, the model is forced to\nreconstruct them solely from surrounding context. This often\nresults in inaccurate reconstructions, as the model may over-\nrely on background noise or unrelated acoustic cues instead of\nlearning to capture fine-grained transient event structures. The\nabsence of direct supervision on these short-duration events\nreduces the model’s ability to accurately detect event onsets\nand offsets, which is crucial for SED.\nAdditionally, masked block prediction does not explicitly\nenforce temporal order learning, making it less effective in\ndistinguishing between overlapping and sequential events.\nTransformers inherently model attention-based relationships,\nbut without explicit temporal constraints, they may struggle to\nrecover the correct event sequence when multiple events occur\nin succession. This limitation weakens the model’s ability\nto differentiate between events that share similar spectral\ncharacteristics but differ in their temporal positioning. As a\nresult, event boundaries may become blurred, reducing the\nprecision of SED predictions.\nTo address these issues, JiTTER introduces hierarchical\ntemporal shuffle reconstruction, which perturbs audio se-\nquences at both the block and frame levels while preserving\nall content. Unlike masked block prediction, which removes\ninformation, JiTTER retains all temporal data but disrupts its\norder, forcing the model to reconstruct the correct sequence.\nThis structured perturbation explicitly enforces temporal co-\nherence, improving two critical aspects of SED: event bound-\nary precision and transient event recognition.\nBy preserving all acoustic information while shuffling its\norder, JiTTER eliminates the risk of interpolation artifacts and\nensures that the model learns event-level temporal dependen-\ncies rather than relying on surrounding context for reconstruc-\ntion. This approach enhances both global event continuity and\nfine-grained transient modeling, leading to more accurate SED\nperformance compared to masked block prediction.\nB. Hierarchical Temporal Shuffle Reconstruction\nJiTTER consists of two levels of temporal perturbation:\nblock-level shuffle and frame-level shuffle, which are applied\nin parallel within a single forward pass. These perturbations\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n4\noperate at different temporal scales: block-level shuffle dis-\nrupts global event structures, while frame-level shuffle intro-\nduces local variations. The objective is to learn robust repre-\nsentations by solving a temporal jigsaw puzzle that requires\nthe model to infer both long-range and fine-grained event\ndependencies.\n1) Block-Level Shuffle: Block-level shuffle (Figure 1a) par-\ntitions an audio sequence into non-overlapping blocks and\nrandomly shuffles a portion (pb), keeping the first and last\nblocks fixed as anchors.\nGiven an input audio sequence X of length T, it is divided\ninto B blocks:\nX = {B1, B2, ..., BB}\n(1)\nwhere each block Bi consists of multiple consecutive frames.\nOne example of a randomly shuffled sequence is:\n˜Xb = {B1, B2, BB−3, B4, ..., B3, BB}\n(2)\nwhere\n˜Xb represents a sequence perturbed by block-level\nshuffling. The model is pretrained to recover the correct order\nX from\n˜Xb, which encourages the transformer to capture\nglobal contextual dependencies.\nInspired by masked prediction, we introduce Gaussian noise\ninjection into shuffled blocks, partially obscuring information\nwithout full masking [41], [42]. The goal is to obscure infor-\nmation slightly rather than removing it completely, allowing\nthe model to retain weak structural cues while still reinforcing\nits ability to restore the original sequence.\nGiven a shuffled block Bi, noise injection is performed as:\nBnoise\ni\n= Bi + λNi,\nNi ∼N(0, I)\n(3)\nwhere Ni is Gaussian noise sampled from a normal distribu-\ntion with zero mean and unit variance. Here, Ni has the same\ndimensions as Bi, ensuring that noise is applied consistently\nacross the entire block. The scaling factor λ controls the\nintensity of perturbation, and we set λ = 0.1 to introduce\nsubtle distortion while preserving core temporal structures.\nThis additional perturbation encourages the model to be more\nrobust to small variations in real-world recordings while still\nmaintaining the integrity of the shuffled reconstruction task.\n2) Frame-Level Shuffle: As shown in Figure 1 (b), frame-\nlevel shuffling randomly selects a subset of blocks and applies\nintra-block frame shuffling. The hyperparameters pfb and pff\ndenote the proportion of blocks undergoing frame shuffling\nand the proportion of frames shuffled within each selected\nblock, respectively.\nFor a chosen block Bi = {fi1, fi2, ..., fiF }, a portion of its\nframes is randomly permuted as follows:\n˜Bi = {fi3, fi2, fiF , fi4, ..., fi1}\n(4)\nwhere ˜Bi represents an example of a shuffled block. The\nperturbed sequence ˜Xf is then reconstructed by combining\nall modified blocks:\n˜Xf = { ˜B1, B2, ..., ˜Bm, Bm+1, ..., BB}\n(5)\nwhere m represents the number of shuffled blocks and ˜Xf rep-\nresents an example of a frame-level shuffled sequence. Unlike\nblock shuffle, this preserves global order while introducing\nlocal perturbations, enhancing fine-grained temporal modeling.\nC. Training Objective\nJiTTER is trained using a reconstruction loss that explicitly\nenforces temporal structure learning by requiring the model\nto restore the correct event sequence from perturbed versions\n˜Xb and ˜Xf. Unlike masked block prediction, which removes\ninformation entirely, JiTTER preserves all temporal data but\ndisrupts their order, making reconstruction a more structured\nlearning objective.\nThe reconstruction loss is formulated as follows:\nLrec( ˜X, X) =\nT\nX\nt=1\n||Fθ( ˜X)(t) −X(t)||2\n(6)\nwhere X(t) represents the t-th frame of the original sequence,\nand Fθ is the transformer network parameterized by θ, which\naims to reconstruct X from its shuffled counterpart ˜X. The\nmodel is optimized to recover both global event continuity\nand local transient details, leading to stronger representations\nfor SED.\nJiTTER’s overall training objective is defined as:\nLJiTTER = Lrec( ˜Xb, X) + Lrec( ˜Xf, X)\n(7)\nwhere ˜Xb and ˜Xf correspond to block-shuffled and frame-\nshuffled sequences, respectively.\nThis structured loss function ensures that:\n• Block-level shuffle encourages learning long-range tem-\nporal dependencies by forcing the model to recover the\ncorrect high-level sequence order.\n• Frame-level shuffle improves fine-grained event local-\nization by requiring the model to reconstruct disrupted\ntransient event patterns.\nMultitask learning, which combines both perturbations, al-\nlows the model to integrate global and local event structures\neffectively, leading to more robust temporal reasoning in SED.\nIV. EXPERIMENTAL SETUPS\nThis section describes the dataset, input feature extraction\nprocess, model architecture, training procedure, and evaluation\nmetrics used in our experiments.\nA. Dataset\nWe train, validate, and evaluate our models using the Do-\nmestic Environment Sound Event Detection (DESED) dataset\n[2], a widely used benchmark for domestic sound event\ndetection. The dataset consists of 10-second-long audio clips\nrecorded at a sampling rate of 16 kHz. It includes both\nsynthetic and real recordings that simulate common household\nacoustic environments, covering ten sound event classes such\nas alarms, speech, running water, and vacuum cleaners.\nThe DESED dataset is divided into four subsets:\n• Strongly labeled synthetic data: Includes precise onset\nand offset annotations for each event.\n• Weakly labeled real data: Indicates only the presence of\nevents in each clip without specifying time boundaries.\n• Unlabeled real data: Used in a self-supervised and semi-\nsupervised learning setup to improve generalization.\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n5\n• Strongly labeled real validation data: Reserved for model\nevaluation.\nTo ensure a fair comparison, we follow the standard Detection\nand Classification of Acoustic Scenes and Events (DCASE)\n2021 Task 4 data partitioning scheme [2]. The strongly labeled\nsynthetic data is used for supervised learning, while weakly\nlabeled and unlabeled data are leveraged in semi-supervised\ntraining. We also include the real strongly labeled training data\nfrom the DCASE 2022 Task 4 Challenge.\nB. Input Feature Extraction\nRaw audio waveforms are first normalized to a maximum\nabsolute value of one to standardize input levels. We then\nextract log-mel spectrograms as input features using a short-\ntime Fourier transform (STFT) with FFT size of 1024, Hop\nlength of 320 (corresponding to 20 ms at 16 kHz), Hanning\nwindowing function and 128 mel frequency bins. The resulting\nlog-mel spectrograms serve as input to JiTTER’s transformer-\nbased architecture.\nC. Data Augmentation\nTo enhance the model’s robustness against environmental\nvariability, we apply a diverse set of augmentation techniques\nfor fine-tuning stages:\n• Frame shift [2]: Shifts the audio features by a small\nnumber of frames to introduce temporal variations.\n• Mixup [63]: Linearly interpolates pairs of spectrograms\nand labels, encouraging smoother decision boundaries.\n• Time masking [7]: Randomly masks sections of the time\naxis, simulating missing event cues.\n• FilterAugment [43]: Randomly reweights frequency re-\ngions to simulate different acoustic environments.\n• Frequency Distortion [41]: Perturbs frequency compo-\nnents to improve spectral robustness.\nWe apply Mixup to both strongly and weakly labeled datasets,\nwhile time masking is applied jointly to the input spectrogram\nand its corresponding labels to maintain consistency.\nD. Model Architecture\nJiTTER extends the MAT-SED framework by replacing\nmasked block prediction with hierarchical temporal shuffle\nreconstruction. The model consists of:\n• A patchout fast spectrogram transformer (PaSST) en-\ncoder, which extracts rich spectral-temporal representa-\ntions [53].\n• A Transformer-based context network with relative po-\nsitional encoding (RPE), designed to capture long-range\ntemporal dependencies [61].\n• A fully connected (FC) classification head, which predicts\nframe-wise event occurrences.\nUnlike conventional masked prediction, JiTTER applies block-\nlevel and frame-level shuffle perturbations before input se-\nquences enter the transformer model. The network is then\npretrained to reconstruct the original sequence, enforcing\ntemporal structure learning.\nE. Training Procedure\nJiTTER follows a three-stage training paradigm to progres-\nsively refine temporal representations for SED:\n1) Pretraining: The context network, consisting of the\ntransformer with relative positional encoding (RPE), is\ntrained using the hierarchical temporal shuffle strategy\nto reconstruct shuffled sequences. During this phase, the\ncontext network is updated, while the encoder network\n(PaSST) remains frozen. This stage runs for 6000 steps.\n2) Feature adaptation: The pretrained transformer re-\nmains fixed, while the SED and AT heads are trained\nseparately using the SED objective. This step allows\nthe classification layers to adapt to structured temporal\nrepresentations. Training runs for an additional 6000\nsteps.\n3) Fine-tuning: The entire model, including the trans-\nformer, PaSST encoder, SED, and AT heads, is jointly\noptimized in an end-to-end manner with the SED objec-\ntive to refine overall event detection performance. This\nfinal stage runs for another 6000 steps.\nAll training is conducted on NVIDIA RTX 3090 GPUs using\nthe AdamW optimizer with a weight decay of 1 × 10−4.\nF. Loss Function\nJiTTER is trained using a multi-stage loss function that\ncombines self-supervised reconstruction loss for pretraining\nand semi-supervised classification loss for SED fine-tuning.\n1) Pretraining Loss: During the pretraining stage, JiTTER\nis optimized with a reconstruction loss that encourages the\nmodel to infer the original sequence from temporally shuffled\nversions. The objective is formulated as:\nLrec( ˜X, X) =\nT\nX\nt=1\n||Fθ( ˜X)(t) −X(t)||2\n(8)\nwhere X(t) is the t-th frame of the original sequence, and\nFθ( ˜X)(t) is the model’s reconstructed prediction. JiTTER\noptimizes both block-shuffled and frame-shuffled sequences:\nLJiTTER = Lrec( ˜Xb, X) + Lrec( ˜Xf, X)\n(9)\nwhere\n˜Xb and\n˜Xf denote the block-level and frame-level\nshuffled sequences, respectively.\n2) SED Fine-Tuning Loss: After pretraining, JiTTER is\noptimized using a supervised classification loss for polyphonic\nSED. Following standard SED frameworks [41], [42], the loss\nfunction consists of:\n• Strong classification loss (Ls): Applied to strongly la-\nbeled data using binary cross-entropy (BCE).\n• Weak classification loss (Lw): Applied to weakly labeled\ndata using BCE.\n• Consistency loss (Lc): Ensures consistency between the\nstudent and teacher models using mean square error\n(MSE).\nThe overall fine-tuning loss is defined as:\nLSED = B(SP , ls) + wW B(WP , lw) + wCLcons\n(10)\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n6\nwhere SP , WP are the strong and weak predictions from the\nstudent model, ls, lw are the corresponding ground-truth labels,\nand wW and wC are weighting factors for weak classification\nand consistency losses.\nThe consistency loss Lcons is given by:\nLcons = M(SP , sg(ST\nP )) + M(WP , sg(W T\nP ))\n(11)\nwhere sg(·) denotes a stop-gradient operation, and ST\nP , W T\nP\nare the strong and weak predictions from the teacher model.\nThis multi-stage optimization strategy ensures that JiTTER\nfirst learns robust temporal representations via self-supervised\npretraining, which are later refined with supervised event\nclassification.\nG. Post-Processing\nTo refine event predictions, we apply a post-processing\npipeline consisting of two steps. First, weak prediction mask-\ning is applied, where strong predictions are retained only if\ntheir confidence surpasses the corresponding weak predictions\n[37]. This ensures that detected events are supported by\nglobal event presence information. Next, median filtering is\nused to smooth predictions and reduce spurious detections.\nSpecifically, we apply median filters with a window size of\n5 for transient sound events and a window size of 20 for\nstationary sound events. This strategy helps to suppress short-\nduration false positives for transient events while preventing\nabrupt fluctuations in stationary event predictions.\nH. Evaluation Metrics\nTo evaluate SED performance, the polyphonic sound detec-\ntion score (PSDS) was used [5]. PSDS is a metric specifically\ndesigned for evaluating polyphonic SED systems, addressing\nthe limitations of conventional collar-based event F-scores and\nevent error rates by incorporating intersection-based event de-\ntection. In addition, unlike traditional metrics, PSDS considers\nthe full polyphonic receiver operating characteristic (ROC)\ncurve, summarizing system performance across multiple oper-\nating points. This allows for a more robust and comprehensive\nassessment of an SED system’s capabilities, making it less\nsensitive to subjective annotation errors and more suitable for\nreal-world applications. Additionally, PSDS provides better\ninsights into classification stability across different sound\nclasses and dataset biases.\nFor the DCASE Challenge 2021, 2022, and 2023 Task 4\nbenchmarks [2], two types of PSDS were employed: PSDS1\nand PSDS2. Among them, PSDS2 is more aligned with the\naudio tagging task rather than SED, as it emphasizes the\nclassification of event presence rather than precise temporal\nlocalization [37], [64]. Therefore, we report only PSDS1 in\nthis work, as it directly measures an SED system’s ability to\ndetect sound events with accurate onset and offset timings.\nPSDS values reported in the tables represent the best scores\nobtained from six independent training runs, ensuring that the\nevaluation reflects a reliable and well-optimized performance\nestimate of the proposed model.\nTABLE I\nABLATION STUDY ON JITTER USING BLOCK-LEVEL SHUFFLE,\nFRAME-LEVEL SHUFFLE, AND MULTITASK LEARNING. PSDS VALUES ARE\nAVERAGED OVER SIX INDEPENDENT TRAINING RUNS.\nMethod\npb\npfb\npff\nPSDS (↑)\nMAT-SED (Baseline)\n-\n-\n-\n0.543\nBlock-Level Shuffle\nJiTTER (Block Shuffle)\n0.25\n-\n-\n0.566\nJiTTER (Block Shuffle)\n0.5\n-\n-\n0.569\nJiTTER (Block Shuffle)\n0.75\n-\n-\n0.570\nFrame-Level Shuffle\nJiTTER (Frame Shuffle)\n-\n0.25\n0.25\n0.560\nJiTTER (Frame Shuffle)\n-\n0.25\n0.5\n0.563\nJiTTER (Frame Shuffle)\n-\n0.25\n0.75\n0.563\nJiTTER (Frame Shuffle)\n-\n0.5\n0.25\n0.564\nJiTTER (Frame Shuffle)\n-\n0.5\n0.5\n0.562\nJiTTER (Frame Shuffle)\n-\n0.5\n0.75\n0.563\nJiTTER (Frame Shuffle)\n-\n0.75\n0.25\n0.563\nJiTTER (Frame Shuffle)\n-\n0.75\n0.5\n0.562\nJiTTER (Frame Shuffle)\n-\n0.75\n0.75\n0.562\nMultitask Learning (Block + Frame-Level Shuffle)\nJiTTER (Multitask) - Best\n0.75\n0.5\n0.25\n0.574\nJiTTER (Multitask)\n0.5\n0.5\n0.25\n0.570\nJiTTER (Multitask)\n0.75\n0.25\n0.25\n0.567\nJiTTER (Multitask)\n0.75\n0.75\n0.25\n0.571\nJiTTER (Multitask)\n0.75\n0.5\n0.5\n0.573\nV. RESULTS AND DISCUSSION\nTo evaluate the effectiveness of the proposed JiTTER frame-\nwork, we compare it with the baseline MAT-SED model and\nconduct an ablation study on block-level shuffle, frame-level\nshuffle, and multitask learning. Additionally, we analyze the\nimpact of block flipping and noise injection, then compare the\nbest JiTTER with MAT-SED under controlled experimental\nconditions.\nA. Block-Level Shuffle\nBlock-level shuffle aims to disrupt global temporal de-\npendencies by perturbing the order of event segments while\npreserving all acoustic information. Unlike masked block\nprediction, which removes entire segments and requires the\nmodel to hallucinate missing content, block shuffle enforces\nexplicit temporal reasoning by requiring the model to recover\nthe correct event sequence rather than merely interpolating\ngaps. To evaluate its impact, we vary the block shuffle rate pb\nwhile keeping frame-level shuffle disabled (pfb = 0, pff = 0).\nEach audio sequence consists of 100 time frames, which are\npartitioned into 20 non-overlapping blocks of size 5. The\nresults are presented in Table I.\nBlock shuffle consistently improves PSDS over the baseline,\nwith the highest improvement of 4.97% at pb = 0.75. This\nsuggests that reconstructing shuffled event blocks forces the\nmodel to capture long-range temporal dependencies, which\nplay a crucial role in transformer pretraining. The performance\ndifference across different shuffle rates is relatively small,\nindicating that block shuffling provides stable improvements.\nHowever, excessive shuffling (pb = 1.0) leads to performance\ndegradation, likely due to the introduction of excessive tem-\nporal disorder, making event reconstruction overly difficult.\nOne limitation of masked block prediction in MAT-SED is\nthat it removes entire blocks, forcing the model to hallucinate\nmissing content based on surrounding context [41]. This can\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n7\nlead to unintended interpolation artifacts, where the model\nlearns to reconstruct missing regions by exploiting statistical\ncorrelations rather than truly modeling event sequences. In\ncontrast, JiTTER’s block shuffle retains all acoustic infor-\nmation while disrupting event ordering, requiring the model\nto reason explicitly about temporal coherence. This explicit\nenforcement of sequence reconstruction provides a more ef-\nfective pretraining signal for SED.\nB. Frame-Level Shuffle\nFrame-level shuffle introduces fine-grained temporal pertur-\nbations, allowing the model to learn local transient depen-\ndencies. Unlike block-level shuffle, which primarily alters the\nevent sequence at a coarse level, frame shuffle operates within\nindividual event segments, perturbing intra-event structures\nwhile preserving the overall event order. This enables the\nmodel to refine event localization and better capture transient\nevent characteristics. To analyze its effect, we vary the frame\nblock selection rate (pfb) and the fraction of shuffled frames\nper block (pff), while keeping block-level shuffle disabled\n(pb = 0). Each audio sequence consists of 100 time frames,\nwhich are divided into 5 non-overlapping blocks of size 20.\nThe results are summarized in Table I.\nFrame shuffle improves performance over the baseline,\nthough its impact is less significant compared to block\nshuffling. The best configuration (pfb = 0.5, pff = 0.25)\nleads to a 3.8% improvement. This suggests that while local\ntemporal order is important, it has a smaller effect than\nglobal event structure. The results indicate that SED models\nbenefit more from capturing event boundary structures at a\ncoarser granularity (block level) rather than relying solely on\nfine-grained perturbations (frame level). Nevertheless, frame\nshuffle enhances event localization by introducing intra-block\nvariability, which aids in transient event detection.\nUnlike block shuffle, which restructures event sequences,\nframe shuffle focuses on intra-event variability by perturbing\nlocal structures without affecting the sequence at a higher\nlevel. This distinction suggests that block shuffle plays a more\ndominant role in shaping global event representations, whereas\nframe shuffle refines detailed event characteristics.\nC. Multitask Learning of Block and Frame-Level Shuffle\nWe evaluate multitask learning, where block and frame-\nlevel shuffle are applied in separate training iterations. The\nresults are presented in Table I. We experimented with the\ncombination of the best block shuffle setting and the best frame\nsetting, along with additional variations.\nMultitask learning yields the highest PSDS of 0.574, im-\nproving by 5.71% over the baseline. This suggests that\ncombining global (block-level) and local (frame-level) per-\nturbations enables more effective pretraining, as the model\nlearns both event-level continuity and finer transient vari-\nations. Furthermore, the best-performing model arises not\nfrom simply stacking perturbations, but from a combination\nof optimal block0lvel and frame-level configurations. These\nfindings highlight the importance of designing task-specific\nTABLE II\nEFFECT OF BLOCK FLIP AUGMENTATION IN JITTER MULTITASK\nLEARNING.\nMethod\nflip rate\nPSDS (↑)\nJiTTER (Multitask)\n-\n0.574\nJiTTER (Multitask + Flip)\n0.25\n0.572\nJiTTER (Multitask + Flip)\n0.5\n0.570\nJiTTER (Multitask + Flip)\n0.75\n0.571\npretraining objectives rather than arbitrarily applying multiple\naugmentations.\nRather than relying solely on contextual interpolation, as\nseen in MAT-SED’s masked block prediction, JiTTER explic-\nitly enforces temporal order reconstruction by reconstructing\nshuffled sequences. This prevents the model from over-relying\non surrounding context and instead optimizes it to capture both\nfine-grained temporal order and global event structures. The\nresults demonstrate that integrating perturbations at multiple\ntemporal scales improves the model’s ability to generalize\nacross various sound event patterns. This highlights the advan-\ntage of explicitly modeling hierarchical temporal dependencies\nin SED pretraining.\nD. Block Flip in Block-level Shuffle\nTo further assess the impact of temporal transformations,\nwe apply block flipping to the best multitask configuration. In\nthis augmentation, shuffled blocks are reversed along the time\naxis with a probability defined by the hyperparameter flip rate.\nThe results are presented in Table II. Contrary to expectations,\nblock flipping does not improve performance and leads to a\nslight drop in PSDS. This suggests that excessive disruption\nof transient structures reduces the model’s ability to capture\nglobal temporal dependencies. One possible explanation is that\nblock flipping distorts natural event progression by reversing\nlocalized patterns within audio sequences.\nFrom an auditory perception perspective, humans rarely\nencounter temporally inverted sound patterns in real-world\nscenarios. As a result, models trained with block flipping may\nlearn non-representative patterns that do not generalize well to\nnatural sound event structures. Additionally, phase incoherence\nintroduced by time-reversed blocks may disrupt the model’s\nability to capture spectral-temporal relationships, which are\ncrucial for precise event boundary detection.\nMoreover, the pretext task of temporal reconstruction be-\ncomes significantly more challenging when flipped blocks\nare introduced. Unlike shuffled blocks, where the original\norder can be inferred through contextual reasoning, flipped\nblocks fundamentally alter the spectral envelope and transient\nstructures of events, making it difficult for the model to\nlearn meaningful representations. Similar findings have been\nobserved in prior works on time-reversed signal processing,\nwhere artificially reversing signals degraded classification per-\nformance in audio and speech tasks.\nE. Noise Injection in Block-level Shuffle\nTo explore a softer form of perturbation compared to block\nflipping, we investigate the impact of injecting Gaussian noise\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n8\nTABLE III\nEFFECT OF NOISE INJECTION ON JITTER MULTITASK LEARNING.\nMethod\nNoise Scale λ\nPSDS (↑)\nJiTTER (Multitask)\n-\n0.574\nJiTTER (Multitask + Noise)\n0.05\n0.570\nJiTTER (Multitask + Noise)\n0.1\n0.575\nJiTTER (Multitask + Noise)\n0.2\n0.567\nJiTTER (Multitask + Noise)\n0.4\n0.569\ninto shuffled blocks. Unlike masked prediction tasks, which\nremove entire regions of input audio, noise injection slightly\nobscures information while preserving weak structural cues.\nThis allows the model to enhance feature robustness without\nfully disrupting temporal reconstruction.\nWe apply Gaussian noise sampled from N(0, I) to shuffled\nblocks, scaling its magnitude by λ. As shown in Table III,\nintroducing mild noise (λ = 0.1) improves PSDS to 0.575,\nsurpassing the baseline multitask configuration (0.574). How-\never, increasing λ beyond this threshold leads to performance\ndegradation, with PSDS dropping to 0.567 at λ = 0.2 and\n0.569 at λ = 0.4. These results suggest that a small degree of\ninformation corruption can regularize training and encourage\nbetter generalization. When applied in moderation, noise injec-\ntion acts as a form of stochastic feature smoothing, preventing\nthe model from overfitting to minute acoustic details that may\nnot generalize well.\nThis finding aligns with prior work in robust speech recog-\nnition and audio classification, where slight perturbations in\nthe feature space have been shown to enhance generalization\nby encouraging invariance to minor spectral variations. The\ntheoretical basis for this effect lies in stochastic regularization,\nwhere controlled noise injection forces the model to rely\non robust acoustic features rather than overfitting to specific\nwaveform characteristics.\nHowever, excessive noise injection disrupts key acoustic\npatterns, making it harder for the model to reconstruct mean-\ningful temporal structures. As the noise magnitude increases,\nthe signal-to-noise ratio (SNR) decreases, and event bound-\naries become harder to distinguish, leading to a decline in\ndetection accuracy. This phenomenon aligns with perspectives\nin information theory, where too much noise obscures discrim-\ninative features necessary for accurate classification, ultimately\ndegrading performance.\nThe results from both experiments indicate that block flip-\nping and noise injection have fundamentally different effects\non JiTTER’s pretraining dynamics. Block flipping disrupts the\nfundamental auditory structure by inverting event sequences,\nmaking it difficult for the model to reconstruct coherent time-\naligned representations. This forces the model to develop se-\nquence reordering capabilities but can also introduce unnatural\nphase distortions that degrade generalization. Noise injection,\nin contrast, preserves event order while slightly perturbing\nfeature representations. This acts as a form of robustness\nenhancement rather than a restructuring task, promoting gen-\neralization without forcing the model to learn unrealistic event\nsequences.\nThese findings highlight an important consideration for\ndesigning self-supervised pretraining objectives: perturbations\nTABLE IV\nCOMPARISON OF MAXIMUM PSDS SCORES ACROSS TRAINING RUNS.\nMethod\nMax PSDS (↑)\nMAT-SED [41]\n0.587\nMAT-SED (Reproduced)\n0.552\nJiTTER (Best)\n0.584\nshould balance informative learning signals with the risk of\nintroducing non-naturalistic distortions. Our study suggests\nthat structured shuffling, such as JiTTER’s hierarchical shuffle\nstrategy, is a more effective SSL task than aggressive augmen-\ntations like full inversion or high-magnitude noise injection.\nWhile noise injection provides a useful regularization mech-\nanism, its impact remains secondary to structured temporal\nperturbations like block and frame-level shuffle. Future re-\nsearch could explore adaptive noise scheduling strategies or\ncontext-aware noise injection to enhance its benefits while\nminimizing potential degradation.\nF. Comparison with MAT-SED with Maximum PSDS\nTo ensure a fair comparison, we report the maximum\nPSDS scores measured across multiple training runs in Ta-\nble IV. While JiTTER achieves a maximum PSDS of 0.584,\nslightly below the originally reported MAT-SED result (0.587),\nit outperforms our reproduced MAT-SED baseline (0.552),\ndemonstrating the robustness of our approach. The discrepancy\nbetween our reproduced MAT-SED score and the originally\nreported result can be attributed to three main factors:\n1) Differences in training infrastructure, such as GPU mod-\nels, software versions, and CUDA configurations.\n2) Heuristically optimized hyperparameters may not gen-\neralize consistently across different environments.\n3) Random variations in the generation of the synthetic\nstrongly labeled DESED dataset used for training.\nThese factors underscore the challenges of reproducing\nresults in deep learning and emphasize the need for fair\ncomparisons under controlled settings.\nVI. CONCLUSION\nIn this work, we introduced JiTTER (Jigsaw Tempo-\nral Transformer for Event Reconstruction), a self-supervised\nlearning framework designed to enhance temporal represen-\ntation learning for SED. Unlike masked block prediction,\nwhich removes entire segments and forces interpolation, JiT-\nTER preserves all information while enforcing explicit event\nreordering through hierarchical temporal shuffle reconstruc-\ntion, enabling the model to capture both global event struc-\ntures and fine-grained transient details. Experiments on the\nDESED dataset show that JiTTER achieves a 5.89% PSDS\nimprovement over MAT-SED, with ablation studies confirming\nthat block-level shuffle strengthens long-range dependencies,\nframe-level shuffle enhances transient event detection, and\nmultitask learning combining both perturbations yields the\nhighest gains. Additional investigations reveal that block flip-\nping disrupts essential event ordering, degrading performance,\nwhile moderate noise injection acts as a useful regulariza-\ntion mechanism. These findings highlight the importance of\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n9\nstructured pretraining objectives that maintain event integrity\nwhile enforcing temporal reasoning. Beyond SED, JiTTER’s\napproach is applicable to broader auditory tasks such as audio\ncaptioning, speaker recognition, and bioacoustic monitoring.\nOur results demonstrate that temporal order reconstruction is\na crucial pretraining signal for event-aware SSL, providing a\nstronger alternative to conventional masking strategies.\nREFERENCES\n[1] T. Virtanen, M. D. Plumbley, and D. Ellis, Computational Analysis\nof Sound Scenes and Events, 1st ed.\nSpringer Publishing Company,\nIncorporated, 2017, pp. 3–11, 71–77.\n[2] N. Turpault, R. Serizel, A. P. Shah, and J. Salamon, “Sound event\ndetection in domestic environments with weakly labeled data and\nsoundscape synthesis,” in DCASE Workshop, 2019.\n[3] E. C¸ akır, G. Parascandolo, T. Heittola, H. Huttunen, and T. Virtanen,\n“Convolutional recurrent neural networks for polyphonic sound event\ndetection,” IEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, vol. 25, no. 6, pp. 1291–1303, 2017.\n[4] A. Mesaros, T. Heittola, and T. Virtanen, “Metrics for polyphonic sound\nevent detection,” Applied Sciences, vol. 6, no. 6, 2016.\n[5] C¸ . Bilen, G. Ferroni, F. Tuveri, J. Azcarreta, and S. Krstulovi´c, “A\nframework for the robust evaluation of sound event detection,” in\nICASSP, 2020, pp. 61–65.\n[6] H. Nam, S.-H. Kim, B.-Y. Ko, D. Min, and Y.-H. Park, “Study on\nfrequency dependent convolution methods for sound event detection,”\nin Proc. INTER-NOISE, 2024.\n[7] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk,\nand Q. V. Le, “SpecAugment: A Simple Data Augmentation Method\nfor Automatic Speech Recognition,” in Proc. Interspeech, 2019.\n[8] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han,\nS. Wang, Z. Zhang, Y. Wu, and R. Pang, “Conformer: Convolution-\naugmented Transformer for Speech Recognition,” in Proc. Interspeech,\n2020.\n[9] H. Nam and Y.-H. Park, “Coherence-based phonemic analysis on the ef-\nfect of reverberation to practical automatic speech recognition,” Applied\nAcoustics, vol. 227, p. 110233, 2025.\n[10] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0: A\nframework for self-supervised learning of speech representations,” in\nAdvances in Neural Information Processing Systems, 2020.\n[11] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and\nA. Mohamed, “Hubert: Self-supervised speech representation learning\nby masked prediction of hidden units,” IEEE/ACM Transactions on\nAudio, Speech, and Language Processing, 2021.\n[12] K. Okabe, T. Koshinaka, and K. Shinoda, “Attentive statistics pooling\nfor deep speaker embedding,” in Proc. Interspeech, 2018.\n[13] W. Cai, J. Chen, and M. Li, “Exploring the encoding layer and loss\nfunction in end-to-end speaker and language recognition system,” in\nProc. Interspeech, 2018.\n[14] S.-H. Kim, H. Nam, and Y.-H. Park, “Analysis-based optimization of\ntemporal dynamic convolutional neural network for text-independent\nspeaker verification,” IEEE Access, vol. 11, 2023.\n[15] J. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating fre-\nquency translational invariance in tdnns and frequency positional in-\nformation in 2d resnets to enhance speaker verification,” in Proc.\nInterspeech, 2021.\n[16] J. Li, Y. Tian, and T. Lee, “Convolution-based channel-frequency atten-\ntion for text-independent speaker verification,” in ICASSP, 2023.\n[17] Q. Kong, Y. Cao, T. Iqbal, Y. Wang, W. Wang, and M. D. Plumbley,\n“Panns: Large-scale pretrained audio neural networks for audio pattern\nrecognition,” IEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, 2020.\n[18] G.-T. Lee, H. Nam, S.-H. Kim, S.-M. Choi, Y. Kim, and Y.-H. Park,\n“Deep learning based cough detection camera using enhanced features,”\nExpert Systems with Applications, vol. 206, 2022.\n[19] S.-H. Kim, H. Nam, S.-M. Choi, and Y.-H. Park, “Real-time sound\nrecognition system for human care robot considering custom sound\nevents,” IEEE Access, vol. 12, 2024.\n[20] Y. Gong, Y.-A. Chung, and J. Glass, “Ast: Audio spectrogram trans-\nformer,” in Proc. Interspeech, 2021.\n[21] S. Chen, Y. Wu, C. Wang, S. Liu, D. Tompkins, Z. Chen, W. Che,\nX. Yu, and F. Wei, “Beats: Audio pre-training with acoustic tokenizers,”\nin ICML, 2023.\n[22] A. Politis, A. Mesaros, S. Adavanne, T. Heittola, and T. Virtanen,\n“Overview and evaluation of sound event localization and detection in\ndcase 2019,” IEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, vol. 29, 2020.\n[23] A. Politis, K. Shimada, P. Sudarsanam, S. Adavanne, D. Krause,\nY. Koyama, N. Takahashi, S. Takahashi, Y. Mitsufuji, and T. Virtanen,\n“STARSS22: A dataset of spatial recordings of real scenes with spa-\ntiotemporal annotations of sound events,” in DCASE Workshop, 2022.\n[24] B.-Y. Ko, H. Nam, S.-H. Kim, D. Min, S.-D. Choi, and Y.-H. Park,\n“Data augmentation and squeeze-and-excitation network on multiple\ndimension for sound event localization and detection in real scenes,”\nDCASE Challenge, Tech. Rep., 2022.\n[25] K. Drossos, S. Adavanne, and T. Virtanen, “Automated audio captioning\nwith recurrent neural networks,” in IEEE Workshop on Applications of\nSignal Processing to Audio and Acoustics, 2017.\n[26] K. Drossos, S. Lipping, and T. Virtanen, “Clotho: an audio captioning\ndataset,” in ICASSP, 2020.\n[27] I. Choi, H. Nam, D. Min, S.-D. Choi, and Y.-H. Park, “Chatgpt caption\nparaphrasing and fense-based caption filtering for automated audio\ncaptioning,” DCASE Challenge, Tech. Rep., 2024.\n[28] J. Liang, I. Nolasco, B. Ghani, H. Phan, E. Benetos, and D. Stowell,\n“Mind the Domain Gap: a Systematic Analysis on Bioacoustic Sound\nEvent Detection,” arXiv preprint arXiv:2403.18638, 2024.\n[29] D. Min, H. Nam, and Y.-H. Park, “Few-shot bioacoustic event detection\nutilizing spectro-temporal receptive field,” in Proc. INTER-NOISE, 2024.\n[30] B.-Y. Ko, G.-T. Lee, H. Nam, and Y.-H. Park, “Prtfnet: Hrtf individual-\nization for accurate spectral cues using a compact prtf,” IEEE Access,\nvol. 11, 2023.\n[31] B.-Y. Ko, Y.-H. Park, G.-T. Lee, and H. Nam, “Deep learning based\nprediction of human auditory brainstem response for sound localization\nin median plane,” in International Congress on Acoustics (ICA), 2022.\n[32] H. Liu, Z. Chen, Y. Yuan, X. Mei, X. Liu, D. Mandic, W. Wang,\nand M. D. Plumbley, “AudioLDM: Text-to-audio generation with latent\ndiffusion models,” in ICML, 2023.\n[33] F. Kreuk, G. Synnaeve, A. Polyak, U. Singer, A. D´efossez, J. Copet,\nD. Parikh, Y. Taigman, and Y. Adi, “Audiogen: Textually guided audio\ngeneration,” in International Conference on Learning Representations\n(ICLR), 2023.\n[34] J. Lee, H. Nam, and Y.-H. Park, “Vifs: An end-to-end variational\ninference for foley sound synthesis,” DCASE Challenge, Tech. Rep.,\n2023.\n[35] K. Miyazaki, T. Komatsu, T. Hayashi, S. Watanabe, T. Toda, and\nK. Takeda, “Convolution-augmented transformer for semi-supervised\nsound event detection,” DCASE Challenge, Tech. Rep., 2020.\n[36] X. Zheng, H. Chen, and Y. Song, “Zheng ustc team’s submission for\ndcase2021 task4 – semi-supervised sound event detection,” DCASE\nChallenge, Tech. Rep., 2021.\n[37] H. Nam, B.-Y. Ko, G.-T. Lee, S.-H. Kim, W.-H. Jung, S.-M. Choi, and\nY.-H. Park, “Heavily augmented sound event detection utilizing weak\npredictions,” DCASE Challenge, Tech. Rep., 2021.\n[38] J. W. Kim, S. W. Son, Y. Song, H. K. Kim, I. H. Song, and J. E. Lim,\n“Semi-supervised learning-based sound event detection using frequency\ndynamic convolution with large kernel attention for DCASE challenge\n2023 task 4,” DCASE Challenge, Tech. Rep., 2023.\n[39] F. Schmid, P. Primus, T. Morocutti, J. Greif, and G. Widmer, “Improving\naudio spectrogram transformers for sound event detection through multi-\nstage training,” DCASE2024 Challenge, Tech. Rep., 2024.\n[40] N. Shao, X. Li, and X. Li, “Fine-tune the pretrained atst model for sound\nevent detection,” in ICASSP, 2024.\n[41] P. Cai, Y. Song, K. Li, H. Song, and I. McLoughlin, “Mat-sed: A\nmasked audio transformer with masked-reconstruction based pre-training\nfor sound event detection,” in Proc. Interspeech, 2024.\n[42] P. Cai, Y. Song, N. Jiang, Q. Gu, and I. McLoughlin, “Prototype\nbased masked audio model for self-supervised learning of sound event\ndetection,” arXiv preprint arXiv:2409.17656, 2024.\n[43] H. Nam, S.-H. Kim, and Y.-H. Park, “Filteraugment: An acoustic\nenvironmental data augmentation method,” in ICASSP, 2022.\n[44] H. Nam, S.-H. Kim, B.-Y. Ko, and Y.-H. Park, “Frequency Dynamic\nConvolution: Frequency-Adaptive Pattern Recognition for Sound Event\nDetection,” in Proc. Interspeech, 2022.\n[45] H. Nam, S.-H. Kim, D. Min, B.-Y. Ko, and Y.-H. Park, “Towards\nunderstanding of frequency dependence on sound event detection,” arXiv\npreprint arXiv:2502.07208, 2025.\n[46] D. Min, H. Nam, and Y.-H. Park, “Application of spectro-temporal re-\nceptive field on soft labeled sound event detection,” DCASE Challenge,\nTech. Rep., 2023.\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n10\n[47] ——, “Auditory neural response inspired sound event detection based\non spectro-temporal receptive field,” in DCASE Workshop, 2023.\n[48] H. Nam, S.-H. Kim, D. Min, and Y.-H. Park, “Frequency & channel\nattention for computationally efficient sound event detection,” in DCASE\nWorkshop, 2023.\n[49] H. Nam, S.-H. Kim, D. Min, J. Lee, and Y.-H. Park, “Diversifying\nand expanding frequency-adaptive convolution kernels for sound event\ndetection,” in Proc. Interspeech, 2024.\n[50] H. Nam and Y.-H. Park, “Pushing the limit of sound event detec-\ntion with multi-dilated frequency dynamic convolution,” arXiv preprint\narXiv:2406.13312, 2024.\n[51] J. F. Gemmeke, D. P. W. Ellis, D. Freedman, A. Jansen, W. Lawrence,\nR. C. Moore, M. Plakal, and M. Ritter, “Audio set: An ontology and\nhuman-labeled dataset for audio events,” in ICASSP, 2017.\n[52] H. Nam, D. Min, I. Choi, S.-D. Choi, and Y.-H. Park, “Self training\nand ensembling frequency dependent networks with coarse prediction\npooling and sound event bounding boxes,” in DCASE Workshop, 2024.\n[53] K. Koutini, J. Schl¨uter, H. Eghbal-zadeh, and G. Widmer, “Efficient\ntraining of audio transformers with patchout,” in Proc. Interspeech, 2022.\n[54] D. Niizumi, D. Takeuchi, Y. Ohishi, N. Harada, and K. Kashino, “Byol\nfor audio: Self-supervised learning for general-purpose audio represen-\ntation,” in 2021 International Joint Conference on Neural Networks\n(IJCNN), 2021.\n[55] Y. Gong, C.-I. Lai, Y.-A. Chung, and J. Glass, “Ssast: Self-supervised\naudio spectrogram transformer,” Proceedings of the AAAI Conference\non Artificial Intelligence, 2022.\n[56] P.-Y. Huang, H. Xu, J. Li, A. Baevski, M. Auli, W. Galuba, F. Metze,\nand C. Feichtenhofer, “Masked autoencoders that listen,” in Advances\nin Neural Information Processing Systems, 2022.\n[57] P. Chen, S. Liu, and J. Jia, “Jigsaw clustering for unsupervised visual\nrepresentation learning,” in IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), 2021.\n[58] Y. Chen, X. Shen, Y. Liu, Q. Tao, and J. A. Suykens, “Jigsaw-vit:\nLearning jigsaw puzzles in vision transformer,” Pattern Recognition\nLetters, 2023.\n[59] I. Misra, C. L. Zitnick, and M. Hebert, “Shuffle and learn: Unsupervised\nlearning using temporal order verification,” in ECCV, 2016.\n[60] A. T. Liu, S.-W. Li, and H.-y. Lee, “Tera: Self-supervised learning of\ntransformer encoder representation for speech,” IEEE/ACM Transactions\non Audio, Speech, and Language Processing, 2021.\n[61] Z. Dai*, Z. Yang*, Y. Yang, W. W. Cohen, J. Carbonell, Q. V. Le, and\nR. Salakhutdinov, “Transformer-XL: Language modeling with longer-\nterm dependency,” in International Conference on Learning Represen-\ntations (ICLR), 2019.\n[62] S. Venkatesh, D. Moffat, and E. R. Miranda, “You only hear once: a\nyolo-like algorithm for audio segmentation and sound event detection,”\nApplied Sciences, 2022.\n[63] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, “mixup: Beyond\nempirical risk minimization,” in International Conference on Learning\nRepresentations (ICLR), 2018.\n[64] J. Ebbers, F. G. Germain, G. Wichern, and J. L. Roux, “Sound event\nbounding boxes,” in Proc. Interspeech, 2024.\nHyeonuk Nam received B.S. and M.S. degrees in\nmechanical engineering from Korea Advanced Insti-\ntute of Science and Technology, Daejeon, Korea, in\n2018 and 2020 respectively. He is currently pursuing\nthe Ph.D. degree in mechanical engineering at the\nsame institute. His research interest includes various\nauditory intelligence themes including sound event\ndetection, sound event localization and detection,\nautomatic audio captioning, sound scene synthesis\nand human auditory perception.\nYong-Hwa Park received BS, MS, and PhD in\nMechanical Engineering from KAIST in 1991,\n1993, and 1999, respectively. In 2000, he joined to\nAerospace Department at the University of Colorado\nat Boulder as a research associate. From 2003-2016,\nhe worked for Samsung Electronics in the Visual\nDisplay Division and Samsung Advanced Institute\nof Technology (SAIT) as a Research Master in the\nfield of micro-optical systems with applications to\nimaging and display systems. From 2016, he joined\nKAIST as professor of NOVIC+ (Noise & Vibration\nControl Plus) at the Department of Mechanical Engineering devoting to\nresearch on vibration, acoustics, vision sensors, and condition monitoring with\nAI. His research fields include structural vibration; condition monitoring from\nsound and vibration using AI; health monitoring sensors; and 3D sensors,\nand lidar for vehicles and robots. He is the conference chair of MOEMS\nand miniaturized systems in SPIE Photonics West since 2013. He is a vice-\npresident of KSME, KSNVE, KSPE, and member of IEEE and SPIE\n\n\n"}
{"text": "DISTRIBUTION STATEMENT A: Approved for public release: distribution unlimited. \n \nHybrid Team Tetris: A New Platform For \nHybrid Multi-Agent, Multi-Human \nTeaming \nKaleb MCDOWELLa,1, Nick WAYTOWICH a, Javier GARCIAa, Stephen GORDONb, \nBryce BARTLETTb, and Jeremy GASTON a \na\n Army Research Laboratory \nb\n DCS Corporation \nAbstract. Metcalfe et al (1) argue that the greatest potential for human-AI \npartnerships lies in their application to highly complex problem spaces. Herein, we \ndiscuss three different forms of hybrid team intelligence and posit that across all \nthree forms, the hybridization of man and machine intelligence can be effective \nunder the right conditions. We foresee two significant research and development \n(R&D) challenges underlying the creation of effective hybrid intelligence. First, \nrapid advances in machine intelligence and/or fundamental changes in human \nbehaviors or capabilities over time can outpace R&D. Second, the future conditions \nunder which hybrid intelligence will operate are unknown, but unlikely to be the \nsame as the conditions of today. Overcoming both of these challenges requires a \ndeep understanding of multiple human-centric and machine-centric disciplines that \ncreates a large barrier to entry into the field. Herein, we outline an open, shareable \nresearch platform that creates a form of hybrid team intelligence that functions under \nrepresentative future conditions. The intent for the platform is to facilitate new forms \nof hybrid intelligence research allowing individuals with human-centric or machine-\ncentric backgrounds to rapidly enter the field and initiate research. Our hope is that \nthrough open, community research on the platform, state-of-the-art advances in \nhuman and machine intelligence can quickly be communicated across what are \ncurrently different R&D communities and allow hybrid team intelligence research \nto stay at the forefront of scientific advancement.  \nKeywords. Hybrid Intelligence, Team Intelligence, Human-Machine Teaming, \nResearch Platform, Research Environment \n1. Introduction \nIncreased hybridization of intelligence between humans and artificial capabilities is \nenvisioned arising from decades of rapid, pervasive, and disruptive technological \nevolution (see 2,3). Herein, we focus on hybrid team intelligence, which we define as the \nability to learn or use skilled reasoning (intelligence) that arises from multi-agent, multi-\n \n1 Kaleb McDowell, Army Research Laboratory, FCDD-RLH 7101 Mulberry Point Rd., Aberdeen \nProving Ground, Maryland 21005, United States; E-mail: kaleb.g.mcdowell.civ@army.mil \n \nMcDowell, K., Waytowich, N. Garcia, J. Gordon, S., Bartlett, B., & Gaston, J. (2022). “Hybrid Team Tetris: \nA New Platform for Hybrid Multi-Agent, Multi-Human Teaming.” Paper presented at Human-Centered \nDesign of Symbiotic Hybrid Intelligence Workshop. Hybrid Human-Artificial Intelligence Conference. \nAmsterdam, The Netherlands. June 13-17.  \n\n\nDISTRIBUTION STATEMENT A: Approved for public release: distribution unlimited. \n \nhuman systems (teams) where there is a deliberate blending of the processing between \nat least two disparate disciplines, cultures, or entities (hybridization). In this definition, \nany specific entity does not need to be intelligent. Rather, the mixed human-agent system \noverall must present the ability to learn or use skilled reasoning.  \nNumerous scientific and commercial breakthroughs point to disruptive \ntechnological evolution already forming the basis for hybrid team intelligence.  We give \nexamples of three forms of human-technology interaction: technology as a tool, \ntechnology as a human decision enhancer, and human-guided artificial learning.  Initial \ndisruptions came in the form of humans using tools that solve complex problems. The \ncreation of consumer Global Positioning System (GPS) navigation devices pioneered by \nGarmin in the 1990s exemplifies how complex problem-solving tool use can reduce, but \nnot eliminate, human cognitive demand associated with specifics aspects of navigation \n(see 4).  Extending this form of hybrid intelligence to teams, the development of weather \nforecasting models has led to a dramatically altered approach to weather forecasting \ninvolving numerous complex models with numerous human users to generate hybrid \nforecasts with less errors than models or humans alone can produce (e.g., see National \nHurricane Center). Technological advances have gone beyond human use of tools to \nrelationships where technologies integrate, modify, and extend human decisions. The \ncreation of Foldit for predicting protein structures provides an example of this form of \nrelationship by merging crowdsourcing with intelligent technologies for ideation and \ndiscovery (5). The creation of cortically coupled computer vision technologies provides \nanother example of integrated human inputs by combining brain-computer interfaces \n(BCI) along with computer vision algorithms to achieve extremely fast object detection \nat human-level performance, allowing for the triage of large image databases (e.g., 6,7).  \nMost recently, human-guided machine learning techniques have evolved that shift \nhumans from the roles of actors to that of technology teachers.  Warnell et al (8) \ndemonstrate this form of human-machine interaction via having a human trainer teach a \ndeep learning agent to play Atari games using evaluative human feedback called \nTAMER (Teaching Agents Manually through Evaluative Reinforcement).  Using Deep \nTAMER, Atari games can be taught in less than 15 mins; a task that has proven difficult \nfor state-of-the-art reinforcement learning methods. At a group level, this form of \ninteraction is also observed in the commercial sector in selector systems that use inputs, \nsuch as thumbs up or thumbs down, to prioritize playlists, advertisements, etc. A key \nfinding across all three forms of human-technology relationship described herein is the \ndemonstration that hybrid intelligence can outperform a machine-only or human-only \napproaches under the right conditions.  \nFocusing on future human-AI ecosystems, Metcalfe et al. (1) argue that the greatest \npotential for human-AI partnerships lies in their application to highly complicated \nproblem spaces. Herein, we embrace a vision of hybrid team intelligence applied in such \ncomplex problem spaces. Our first aim is to articulate three factors driving the types of \nproblem spaces hybrid team intelligence may solve.  Our second aim is to present a vision \nfor enabling community research with a novel hybrid team intelligence research platform \nto enable that vision.   \n\n\nDISTRIBUTION STATEMENT A: Approved for public release: distribution unlimited. \n \n2. A Complex Problem Space for the Future Hybrid Team Intelligence \nThe rapid evolution of machine intelligence presents fundamental challenges to the \nresearch and development community. First, advances in machine intelligence can \noutpace the rate at which the community develops its understanding of relationship \nbetween intelligent technologies and humans; that is, the interactions or relationships \nbetween humans and technology studied for a particular state-of-the-art technology can \nquickly be rendered irrelevant by either the significant advancement of the technology \nor the rise of a superior machine intelligence. Second, the advances in intelligent \ntechnologies are correlated with changes in human behaviors and potentially human \ncapabilities; for example, there are numerous negative short- and long-term effects on \nhuman spatial reasoning resulting from GPS device use (4,9) as well as changes in human \nroles (human knowledge, skills, and behavior requirements) in the aforementioned \nweather forecasting and human-guided machine learning examples. This correlation \neffectively points to the potential for human-machine intelligence research to be studying \nthe “wrong people,” which can directly impact overall performance as well as underlying \nissues such as system usability and trust (see 10). We posit that overcoming these \nchallenges will require the combined expertise of multidisciplinary teams (of humans \nand technology) with an emphasis on combining human-centric and technology-centric \nexpertise.  \nConsistent with the literature on multidisciplinary teaming (see 1, 11, 12), we have \nobserved that specialized perspectives, communications, priorities, and assumptions of \nhuman-centric versus intelligent technology-centric researchers and developers can \nmake it quite difficult for teams to function effectively together.  Our approach to \novercoming the challenges of developing high performing mixed human- and machine \nintelligence-centric research and development teams starts with attempting to create a \nshared vision for the future. While numerous future environments are plausible, we \nconstrain our vision through three “driving factors” that we extract from current \ntechnological and sociotechnical trends:  \n \nAdvanced Intelligence: Machine intelligence technologies that sense, perceive, \nreason, and/or learn are expected to continue to become increasingly capable, \nmore pervasive in society, and potentially more “alien” to humans. These \ntechnologies will adapt to and learn from a variety of sources including humans, \nother technologies, as well as the environment. While not as dramatic, human \nintelligence and its application is also expected to become refined with the \ncontinued development of cognitive enhancement technologies, training \napproaches, and continued exposure to advanced sociotechnical ecosystems. \nSample Challenge: Create future human-technology interactions for human \nand machine intelligence that does not yet exist. \n \n“Superhuman” Capabilities: The continued evolution and growth of \ninterconnected sociotechnical ecosystems combined with more capable data \nprocessing and interpretation capabilities will continue to create conditions \nwhere decisions are made faster and/or on more data than humans can \neffectively process through traditional means. Sample Challenge: Create future \nhuman-technology interactions that allow for humans to influence \n“superhuman” decisions and actions without limiting overall performance. \n \nRapid Technological Change: Continued decreases in the time from ideation \nto fielding of new technologies combined with the fact that these technologies \n\n\nDISTRIBUTION STATEMENT A: Approved for public release: distribution unlimited. \n \nhave become increasingly pervasive in society has created an environment of \nconstant and rapid technological change. Sample Challenge: Enable humans \nand machines to have effective and stable in-field adaptations to predicted and \nunforeseen changes in the sociotechnical environment. \nCombined, these three interrelated “driving factors” constrain the complex problem \nspace in which we expect hybrid team intelligence to be critical. However, we have \nobserved that this level of constraint is still insufficient to bring together high performing \nmixed human- and intelligent technology-centric research and development teams. To \nhelp multidisciplinary teams further create a shared vision, we have attempted to \ninstantiate this future problem space in the simplified, toy research platform of Hybrid \nTeam Tetris. \n3. Hybrid Team Tetris \nThe primary aim of Hybrid Team Tetris is to create hybrid team intelligence within the \nenvisioned complex problem space described above and to do so in a manner that \nmakes the platform easily accessible to a broad community of human-centric and \nmachine intelligence-centric researchers and developers. Hybrid Team Tetris is a \nfunctional multi-agent, multi-human open-source research platform that allows for \nusers with different knowledge, skills and behaviors to experience key elements of \nhybrid team intelligence as outlined below. The intent of the platform is to enable \nproductive multidisciplinary discussions through the creation of critical technical and \nresearch challenges and ongoing community contributions.  While this paper provides \nonly a brief introduction to Hybrid Team Tetris, downloadable software, as well as, a \ndescription of an upcoming open community Hybrid Team Tetris-based hack-a-thon \n(see ‘Human System Adaptation Paradigm Challenge 2022’) can be found on the \nGitHub website as of Jun 17, 2022 (https://github.com/DCSHGAI/HGAITetris).  \n3.1. Why Tetris \nWhile numerous experimental and gaming paradigms could serve as the basis for our \ncommunity platform (see ‘Human System Adaptation Paradigm Challenge 2022’ for \nopportunities to present alternative platforms), the choice to use Tetris as a basis for the \nplatform arose from three primary reasons: \n \nTetris has been successfully deployed as a basic form of hybrid intelligence in \nthe past (13, 14) \n \nTetris is well-defined and has been used as a platform for both human-centric \nand machine intelligence-centric research and development for over two \ndecades (15-17; Current Tetris AI Record set by G Cannon in 2021) \n \nTetris offers experimental flexibility and accessibility (for multiple versions and \nconfigurations see 18-20). \nIn addition, Tetris is one the of the best-selling games of all time (>100M copies \nsold), although we acknowledge that Hybrid Team Tetris has very different game play \nthan that originally designed by Alexey Pazhitnov in 1984.  \n\n\nDISTRIBUTION STATEMENT A: Approved for public release: distribution unlimited. \n \n3.2. Basic Multi-Agent, Multi-Player Gameplay \nAt its core, Hybrid Team Tetris uses a human-guided machine learning approach similar \nto Knox & Stone’s (13), but updated with Deep TAMER (see 8). That is, as the machine \nagent drops a “tetrimino” into a row, a human has an option to press or not press “ENTER” \nto indicate what they view as a “positive” move.  The Deep TAMER uses the reward \nsignal to relate the tetrimino shape and dropped position to the state of the game board \nand update its game play.  This basic mechanism allows humans, who may have no \nsubject matter expertise in intelligent technologies, to train agents to play the game.  \nTo extend the game to multi-agent, multi-player gameplay, three mechanisms are \nemployed.  First, a single player is presented with multiple games simultaneously.  That \nis, the player can use keystrokes (“1-2”) to toggle between games and then, press \n“ENTER” to indicate a positive move in the selected game.  This allows the player to \ndevelop independent agents.  Second, a multi-player mode allows additional players to \nalso play simultaneous games and develop other independent agents, using the \naforementioned mechanism.  Third, to develop interactions across the team, additional \ngames are played by machine agents that learn from all or a sub-set of the player directed \ngames. This creates agent learning that is dependent on team decisions.  A functional \nversion of the game with 2 players each guiding 2 agents with one dependent agent \nlearning from the 4 player-guided games is depicted in figure 1 and is located on the \nGitHub website (see Base platform). \n \n \nFigure 1. Basic Hybrid Team Tetris gameplay.  Human Player A guides machine Agents 1 & 2. Human \nPlayer B guides machine Agents 3 & 4.  Machine Agent 5 learns from machine Agents 1, 2, 3, and 4. \nThe number of total numbers of players, agents, and the network interconnectivity \nof players and agents can be manipulated to study issues that range from team \ncollaboration to distributed learning.  \n3.3. Advanced Intelligence Gameplay \nA critical challenge for studying future human-technology hybridization is to create a \nresearch environment that is representative of the advances in human and machine \n\n\nDISTRIBUTION STATEMENT A: Approved for public release: distribution unlimited. \n \nintelligence that do not yet exist. Hybrid Team Tetris approaches this challenge through \nthe use of “Hidden Rules” that are only initially presented to a single or a sub-set players \nor agents. For example, if the “Hidden Rule” is that anytime a row is cleared with >3 \nyellow squares, a “favorable” tetrimino is presented as the next piece (see Spiel et al \n2017 for adapting Tetris difficulty via adjusting the next available Tetrimino) and the \nagent receives a “positive” reward even if the human-player does not actively press \n“ENTER”. Alternatively, whenever the row with >3 yellow squares is cleared, a \n“10*bonus score” could be awarded immediately on screen if the player is being \npresented the rule or as an end of game bonus if the agent is being presented the rule.  A \nfunctional version of the basic game with the later “Hidden Rule” is located on the \nGitHub website (See Advanced_Intelligence platform). \n3.4. “Superhuman” Gameplay \nA second critical challenge for human-technology hybridization is effectively allowing \nhumans to influence “superhuman” decisions and actions. The original game of Tetris \nnaturally generates this challenge by ramping up the speed at which Tetriminos are \ndropped. Hybrid Team Tetris ramps up this challenge by adding multiple simultaneous \ngames per player (up to 10 selected via keystrokes “0-9”) in addition to speed \nmodifications. Extending the Knox & Stone (2009) version of human-guided machine \nlearning Tetris allows players to overcome this challenge by granting access to discrete \nmoments in time rather than continuously making decisions. A functional version of the \nbasic game with “superhuman” challenges is located on the GitHub website (See \nSuperhuman platform). \n3.5. Rapid Technological Change Gameplay \nA third critical challenge for human-technology hybridization is to enable humans and \nmachines to have effective and stable in-field adaptations to predicted and unforeseen \nchanges in the sociotechnical environment. Hybrid Team Tetris allows for this challenge \nby introducing in-game changes that require substantive changes in strategy to maintain \ngameplay and serves as a proxy for rapid technological change.  For example, the version \non GitHub combines the introduction of novel Tetrimino pieces and alters the rules \nawarding points as game time elapses (see Rapid_Technological_Change platform).  \n3.6. Overall Gameplay \nWhile all the above examples of Hybrid Team Tetris are functional, our goal is to \ndemonstrate a hybrid intelligence suitable research platform that represents our \nenvisioned future complex problem space, which combines advanced intelligence, \n“superhuman” capabilities, and rapid technological change. A functional version of the \ncombined platform is also available on GitHub (See Integrated platform).  \n4. Conclusions \nHybrid Team Tetris is an open, shareable research platform that aims to lower the bar \nfor entry into hybrid intelligence by allowing individuals to rapidly experience hybrid \n\n\nDISTRIBUTION STATEMENT A: Approved for public release: distribution unlimited. \n \nteams and lowers the multidisciplinary knowledge requirement to initiate research. \nMoreover, community contribution on this open-source platform allows for increasing \nsophistication of both human-centric and machine-centric interactions.  This \nsophistication across R&D communities allows for effective cross-community state-of-\nthe-art collaboration and advancement of hybrid intelligence principles and insights. \nReferences \n[1]  Metcalfe JS, Perelman BS, Boothe DL, & McDowell K. Systemic oversimplification limits the potential \nfor human-AI partnerships. IEEE Access 2021; 9: 70242-60. \n[2]    Kurzweil R. The Singularity Is Near: When Humans Transcend Biology. 2005 Sep; Menlo Park, CA. \n[3]    Pelton JN. Preparing for Next Cyber Revolution: How Our World Will Be Radically Transformed Again. \n2019; Cham, Switzerland: Springer, 2019 [online] Available: http://link.springer.com/10.1007/978-3-\n030-02137-5. \n[4]    Dahmani L, & Bohbot VD. Habitual use of GPS negatively impacts spatial memory during self-guided \nnavigation. Scientific Reports 2020;10: 6310. \n[5]  Cooper S, Khatib F, Treuille A, Barbero J, Lee J, Beenen M, Leaver-Fay A, Baker D, Popović Z, &  \nFoldit players. Predicting protein structures with a multiplayer online game. Nature 2010; 466, 756-760. \nURL http://dx.doi.org/10.1038/nature09304 \n[6]    Gerson AD, Parra LC, Sajda P. Cortically coupled computer vision for rapid image search. IEEE Trans \nNeural Syst Rehabil Eng 2006 Jun; 14(2):174-9.    \n[7]    Pohlmeyer EA, Wang J, Jangraw DC, Lou B, Chang SF, Sajda P. Closing the loop in cortically-coupled \ncomputer vision: a brain–computer interface for searching image databases. Jour of Neural Eng 2011; \n8(3): 036025. \n[8]    Warnell G, Waytowich N, Lawhern V, & Stone P. Deep tamer: Interactive agent shaping in high-\ndimensional state spaces. Proceedings of the AAAI Conference on Artificial Intelligence; 2018: 32 (1). \n[9]    Gardony AL, Brunyé TT, Mahoney CR, & Taylor HA. How navigational aids impair spatial memory: \nEvidence for divided attention. Spat. Cognition Computation 2013; 13:319–50. \n[10] de Winter JCF. Pitfalls of automation: a faulty narrative? Commentary on Hancock (2019) Some pitfalls \nin the promises of automated and autonomous vehicles. Ergonomics 2019; 62(4): 505-8.  \n[11] Aagaard-Hansen J, The challenges of cross-disciplinary research. Social Epistemol., 2007; 21 (4): 425-\n8. \n[12] Monteiro M & Keating E. Managing misunderstandings: The role of language in interdisciplinary \nscientific collaboration. Sci. Commun 2009 Sep; 31 (1): 6-28. \n[13] Knox WB & Stone P. Interactive Shaping of a Tetris Agent Using the TAMER Framework. Technical \nreport. In Proceedings of the IJCAI Robot Workshop, 2009 Jul. \n[14] Lin J, Zhang Q, Gomez R, Nakamura K, He B, & Li G. Human Social Feedback for Efficient Interactive \nReinforcement Agent Learning. In 2020 29th IEEE International Conference on Robot and Human \nInteractive Communication (RO-MAN) 2020 Aug-Sep; 706-12. \n[15] Haier RJ, Siegel B, Tang C, Abel L, Buchsbaum MS. Intelligence and changes in regional cerebral \nglucose metabolic rate following learning. Intelligence Jul 1992 Jul 1;16(3-4):415-26. \n[16] Burgiel H. How to lose at Tetris. The Mathematical Gazette. 1997 Jul;81(491):194-200. \n[17] Agren T, Hoppe JM, Singh L, Holmes EA, & Rosén J. The neural basis of tetris gameplay: implicating \nthe role of visuospatial processing. Current Psychology 2021 Aug; 1-8. \n[18] Lindstedt JK, Gray WD. Meta-T: TetrisⓇ as an experimental paradigm for cognitive skills research. \nBehavior research methods. 2015 Dec;47(4):945-65. \n[19] Spiel K, Bertel S, Kayali F. \" Not another Z piece!\" Adaptive Difficulty in TETRIS. In Proceedings of \nthe 2017 CHI Conference on Human Factors in Computing Systems 2017 May 2; 5126-31. \n[20] Pires G, Torres M, Casaleiro N, Nunes U, Castelo-Branco M. Playing Tetris with non-invasive BCI. In \n2011 IEEE 1st international conference on serious games and applications for health 2011 Nov: 1-6. \nIEEE. \n \n\n\n"}
{"text": "MITracker: Multi-View Integration for Visual Object Tracking\nMengjie Xu1∗\nYitao Zhu1∗\nHaotian Jiang1\nJiaming Li1\nZhenrong Shen2\nSheng Wang1,2\nHaolin Huang1\nXinyu Wang1\nQing Yang1,3\nHan Zhang1,3\nQian Wang1,3†\n1School of Biomedical Engineering & State Key Laboratory of\nAdvanced Medical Materials and Devices, ShanghaiTech University\n2School of Biomedical Engineering, Shanghai Jiao Tong University\n3Shanghai Clinical Research and Trial Center\n{xumj2023, zhuyt, jianght2023, lijm2024}@shanghaitech.edu.cn\n{zhenrongshen, wsheng}@sjtu.edu.cn\n{huanghl2023, wangxy42023, yangqing, zhanghan2, qianwang}@shanghaitech.edu.cn\nAbstract\nMulti-view object tracking (MVOT) offers promising solu-\ntions to challenges such as occlusion and target loss, which\nare common in traditional single-view tracking.\nHow-\never, progress has been limited by the lack of comprehen-\nsive multi-view datasets and effective cross-view integra-\ntion methods. To overcome these limitations, we compiled\na Multi-View object Tracking (MVTrack) dataset of 234K\nhigh-quality annotated frames featuring 27 distinct objects\nacross various scenes. In conjunction with this dataset, we\nintroduce a novel MVOT method, Multi-View Integration\nTracker (MITracker), to efficiently integrate multi-view ob-\nject features and provide stable tracking outcomes.\nMI-\nTracker can track any object in video frames of arbitrary\nlength from arbitrary viewpoints. The key advancements of\nour method over traditional single-view approaches come\nfrom two aspects:\n(1) MITracker transforms 2D image\nfeatures into a 3D feature volume and compresses it into\na bird’s eye view (BEV) plane, facilitating inter-view in-\nformation fusion; (2) we propose an attention mechanism\nthat leverages geometric information from fused 3D fea-\nture volume to refine the tracking results at each view. MI-\nTracker outperforms existing methods on the MVTrack and\nGMTD datasets, achieving state-of-the-art performance.\nThe code and the new dataset will be available at mii-\nlaboratory.github.io/MITracker.\n1. Introduction\nVisual object tracking, a core computer vision task, involves\nestimating class-agnostic target positions across video se-\n* These authors contributed equally. † Corresponding author.\n1\n𝐾\n…\n3D Feature Volume Space\nTarget Visible\nProjection\nRefinement\nTrajectory\nFigure 1. Overview of MITracker’s multi-view integration mech-\nanism. Given K camera views, our method projects features from\nviews with visible targets into a 3D feature volume space, which is\nthen used to refine tracking in views where the target is occluded.\nquences.\nThis technique is crucial for applications such\nas augmented reality and autonomous driving, where it\nis essential to continuously monitor and predict the tra-\njectories of various objects within dynamic environments.\nDespite notable advances in single-view tracking through\nSiamese networks [10, 25] and transformers [3, 9, 46],\nsignificant challenges persist – particularly occlusions, ap-\npearance changes, and target loss. While approaches like\nRTracker [22] attempt to address these challenges by deter-\nmining target loss and detection mechanisms, the inherent\nlimitations of single viewpoint information remain a funda-\nmental constraint.\nMulti-camera systems offer a promising solution by\nleveraging complementary viewpoints to maintain contin-\nuous tracking, particularly for handling occlusions through\ncamera overlap [48]. However, the development of effective\narXiv:2502.20111v1  [cs.CV]  27 Feb 2025\n\n\nmulti-view object tracking (MVOT) faces several critical\nchallenges. First, existing multi-view datasets are largely\nrestricted to specific object categories like humans or birds\n[15, 40], limiting their applicability for generic object track-\ning. Second, current MVOT approaches [17, 19, 42] primar-\nily focus on tracking specific categories of objects using de-\ntection and re-identification methods, which are not suitable\nfor class-agnostic object tracking. Even when attempting\nto track generic objects across multiple views, researchers\nhave to rely on single-view datasets for training due to the\nabsence of comprehensive multi-view data [38]. This limi-\ntation severely restricts models’ ability to understand com-\nplex spatial relationships and appearance variations across\ndifferent viewpoints.\nTo address these challenges, we first construct a Multi-\nView object Tracking (MVTrack) dataset.\nMVTrack\ndataset contains 234K frames captured from 3-4 cameras,\nwith precise bounding box (BBox) annotations covering 27\ndistinct objects across 9 challenging tracking attributes such\nas occlusion and deformation. Unlike existing datasets such\nas GMTD [38] which only provides testing data, MVTrack\ndataset offers both training and evaluation sets, enabling de-\nvelopment and validation of MVOT models.\nTo effectively utilize MVTrack dataset, we propose\na novel MVOT method named Multi-View Integration\nTracker (MITracker) for tracking any object in video\nframes of arbitrary length from arbitrary viewpoints. As\nillustrated in Figure 1, MITracker can integrate multi-view\nfeatures into a unified 3D feature volume and further refine\ntracking in occluded views, thus producing robust tracking\noutcomes. The framework of MITracker consists of two im-\nportant modules: View-Specific Feature Extraction and\nMulti-View Integration. The first module employs a Vi-\nsion Transformer (ViT) [12] to extract view-specific fea-\ntures of the target object from the current search frame in\na streaming manner, where the target object is indicated by\na reference frame. The second module constructs a 3D fea-\nture volume by fusing 2D features from multiple views and\nleveraging bird’s eye View (BEV) guidance, which signif-\nicantly enhances the model’s spatial understanding. This\n3D feature volume is then deployed in spatial-enhanced at-\ntention to improve tracking accuracy. MITracker allows for\nthe maintenance of stable tracking results and demonstrates\nstrong recovery capabilities in challenging cases such as oc-\nclusions and out of view objects.\nIn summary, our main contributions are as follows:\n• We introduce MVTrack, a large-scale multi-view track-\ning dataset containing 234K frames from 3-4 calibrated\ncameras. It has precise BBox annotations of 27 object\ncategories across 9 challenging tracking attributes, which\nprovides the first comprehensive benchmark for train-\ning class-agnostic MVOT methods and enriches the ap-\nproaches for evaluating these methods.\n• We propose MITracker, a novel multi-view tracking\nmethod that constructs BEV-guided 3D feature volumes\nto enhance spatial understanding and utilize a spatial-\nenhanced attention mechanism to enable robust recovery\nfrom target loss in specific views.\n• Our extensive experiments demonstrate that MITracker\nachieves state-of-the-art (SOTA) performance on both\nMVTrack and GMTD datasets, improving recovery rate\nfrom 56.7% to 79.2% to reduce target loss in challenging\nscenarios.\n2. Related Work\n2.1. Visual Object Tracking\nVisual object tracking has garnered significant research in-\nterest, leading to many breakthroughs. Numerous single-\nview datasets [13, 20, 21, 23, 24, 27, 29, 36, 39] span a wide\nrange of categories, aimed at enhancing models’ ability to\ntrack arbitrary objects. With the expansion of these datasets,\nsingle-view tracking methods have also advanced rapidly.\nEarly approaches based on Siamese networks [10, 25] use\nCNNs to extract features from reference and search regions,\nestablishing a linear relationship between them. More re-\ncent works have incorporated transformers for enhanced\nfeature extraction [9, 43], while others introduce attention\nmodules to enable nonlinear relationships [5]. However,\nthese methods lack temporal continuity as they process each\nframe independently. Algorithms like dynamic template up-\ndating [6] and spatio-temporal trajectory tracking [37, 46]\nhave shown promising results in addressing this issue. De-\nspite these advancements, recovering from target loss re-\nmains a significant challenge.\nTo re-track the target after a tracking failure, RTracker\n[22] leverages a tree-structured memory system to detect\ntarget loss and a dedicated detector for self-recovery. How-\never, this approach is constrained by its complex design\nand the detector’s reliance on specific categories. Single-\nview tracking suffers from inherent limitations due to its\nrestricted field of view, which is an inevitable challenge. In\ncontrast, GMT [38] incorporates multi-view tracking within\na single-view training framework.\nThis limits its capac-\nity to effectively model the intricate relationships between\nmulti-view appearances and background contexts in the real\nworld.\n2.2. Multi-View Object Tracking\nMVOT provides more comprehensive information about the\ntarget, effectively addressing issues such as occlusion. To\nleverage multi-view information, various fusion strategies\nhave been developed for target association across view-\npoints. Some approaches establish multi-view relationships\nby projecting detection results onto a BEV plane [42]. How-\never, this method is prone to detection errors, especially\n\n\nBenchmark\nAim\nCamera\nClass\nTotal\nVideos\nMean\nAbsent\nAtt.\nOverlap\nMove\nCalib.\nFrames\nFrames\nlabel\nOTB 2015 [39]\nEva.\n1\n16\n59K\n100\n590\n✗\n11\n-\n-\n-\nNfS [23]\nEva.\n1\n17\n383K\n100\n3,830\n✗\n9\n-\n-\n-\nVOT 2017 [24]\nEva.\n1\n24\n21K\n60\n356\n✗\n24\n-\n-\n-\nTrackingNet [27]\nTra./Eva.\n1\n27\n14.43M\n30,643\n471\n✗\n15\n-\n-\n-\nLaSOT [13]\nTra./Eva.\n1\n70\n3.52M\n1,400\n2,053\n✓\n14\n-\n-\n-\nGOT-10k [21]\nTra./Eva.\n1\n563\n1.45M\n9,935\n149\n✓\n6\n-\n-\n-\nTNL2K [36]\nTra./Eva.\n1\n-\n1.24M\n2,000\n622\n✓\n17\n-\n-\n-\nVideoCube [20]\nTra./Eva.\n1\n89\n7.46M\n500\n4,008\n✓\n12\n-\n-\n-\nVastTrack [29]\nTra./Eva.\n1\n2,115\n4.20M\n50,610\n83\n✓\n10\n-\n-\n-\nCAMPUS† [42]\nEva.\n4\n1\n83K\n16\n5,188\n-\n-\n✓\n✗\n✗\nWildtrack† [4]\nEva.\n7\n1\n2.80K\n7\n401\n-\n-\n✓\n✗\n✓\nMMPTRACK† [15]\nTra./Eva.\n4-6\n1\n2.98M\n-\n-\n-\n-\n✓\n✗\n✓\nDIVOTrack† [16]\nTra./Eva.\n3\n1\n81K\n75\n1,080\n-\n-\n✓\n✓\n✗\nGMTD [38]\nEva.\n2-3\n8\n18K\n23\n764\n✗\n6\n✓\n✓\n✗\nMVTrack (Ours)\nTra./Eva.\n3-4\n27\n234K\n260\n901\n✓\n9\n✓\n✗\n✓\nTable 1. Comparison of current datasets for object tracking. The upper part of the table focuses on single-view datasets, while the lower\npart is dedicated to multi-view datasets. Datasets marked with † are designed for multi-object tracking, the others are for visual object\ntracking. ‘Tra.’ and ‘Eva.’ indicate training and evaluation, respectively. ‘-’ denotes not available, ‘Att.’ stands for attributes, ‘Overlap’\nrefers to the overlapping of multi-view images, ‘Move’ indicates the movement of the camera position, and ‘Calib.’ represents calibration.\nwith occlusion. To tackle this issue, methods [18, 19] im-\nprove it by incorporating multi-view information at the de-\ntection stage (known as early fusion) through feature pro-\njection onto the ground plane, enabling the model to cap-\nture richer interaction information across views. Building\non these approaches, methods such as [17, 33, 34] map fea-\ntures into 3D space at multiple heights, which reduces dis-\ntortions caused by ground plane projection.\nWhile these methods enhance multi-view tracking, ex-\nisting multi-view datasets are relatively scarce and often\nlimited to specific target categories, such as pedestrians\n[4, 15, 16]. This results in a reliance on detection outcomes,\nlimiting the ability to track arbitrary objects. GMTD [38]\nexpands the target to multiple categories, but its scale re-\nmains small, primarily designed for evaluation purposes.\nThere is a pressing need for a multi-view tracking dataset\ncapable of handling arbitrary objects.\n3. MVTrack Dataset\nMVTrack dataset is designed to fill the gaps in the field of\nMVOT and has received approval for data collection from\nan Institutional Review Board. As shown in Table 1, com-\npared to single-view datasets, we maintain competitive class\ndiversity while adding multi-view capabilities. Compared\nto MVOT datasets, we provide significantly richer object\ncategories (27 vs 1-8 classes) and more videos (260) with\npractical camera setups (3-4 views). MVTrack dataset is the\nonly dataset that combines multi-view tracking, rich object\ncategories, absent label annotations, and calibration infor-\nmation.\nData Collection.\nWe employ a multi-camera system\n(a) umbrella1-1: Deformation, Aspect Ratio Change and Scale Variation.\n(b) phone3-3: Low Resolution, Fully Occlusion and Partial Occlusion.\n(c) tenis5-1: Out of View, Motion Blur and Background Clutter.\nFigure 2. Example sequences, annotations, and their correspond-\ning tracking attributes in the MVTrack dataset.\nfor data collection, consisting of 3 or 4 time-synchronized\nAzure Kinect cameras. All video sequences are recorded at\na resolution of 1920×1080 with 30 FPS. These cameras are\npositioned to ensure multiple overlapping views, and their\nintrinsic parameters are provided by the manufacturer. The\nextrinsic parameters are obtained through calibration and\nfinely adjusted using MeshLab [7, 8]. With this calibration\ninformation, we set the central point of the scene as the ori-\ngin of the world coordinate system, aligning all viewpoints\nto this unified coordinate system.\nData Annotation.\nMVTrack dataset provides frame-\nlevel annotations, including 2D object BBoxes and ground\ncoordinate annotations in a unified coordinate system (i.e.,\n\n\nBEV annotations). Following an annotation strategy simi-\nlar to LaSOT [13], where for each visible frame, an axis-\naligned BBox tightly encloses the target, and an ‘invisi-\nble’ label is assigned for the invisible target. The BBox\nannotations are generated semi-automatically, with trackers\n[6, 41, 46] used for initial labeling. The machine-generated\nannotations are then manually adjusted and double-checked\nfor accuracy. Subsequently, using camera calibration pa-\nrameters, the 2D object BBoxes from multiple viewpoints\nare projected into the unified coordinate system to compute\nthe BEV coordinates.\nChallenging Attributes. In our dataset, we particularly\nfocus on 9 common tracking challenges to better assess\ntracker performance: Background Clutter, Motion Blur,\nPartial Occlusion, Full Occlusion, Out of View, Deforma-\ntion, Low Resolution, Aspect Ratio Change, and Scale Vari-\nation.\nMore specifically, Figure 2 illustrates three challenging\nsamples in the MVTrack dataset. Figure 2a shows signif-\nicant deformation and scale changes of an umbrella being\nopened. Figure 2b demonstrates the tracking of small, low-\nresolution objects like a mobile phone under full and partial\nocclusions. Figure 2c highlights the impact of fast motion\ncausing blur when tracking a tennis ball. These attributes\ncan significantly aid in training the model to achieve more\nrobust results.\nStatistical Analysis. MVTrack dataset consists of five\nindoor scenes, captured with a total of ten sets of calibration\nparameters. It covers 27 everyday objects, ranging from\nsmall objects like pens to larger objects such as umbrellas.\nThe dataset includes 68 sets of multi-view data, comprising\n260 videos and a total of 234,430 frames.\nWe divide the dataset into training, validation, and test-\ning sets. The training set consists of 196 videos and 180K\nframes, while the validation set contains 30 videos and\n28K frames. The testing set comprises 34 videos and 26K\nframes. We include an unseen scene in the validation and\ntesting sets that are distinct from the scenes in the training\nset. Furthermore, the testing set includes both object cate-\ngories that appear in the training set and new object cate-\ngories not present during training. This enables evaluation\nof the model’s performance across various targets and set-\ntings.\nMore details about MVTrack dataset are provided in the\nAppendix.\n4. MITracker\nWe propose MITracker, a novel multi-view tracking frame-\nwork that robustly tracks class-agnostic objects across mul-\ntiple camera views. As illustrated in Figure 3, MITracker\nconsists of two main components: (1) a view-specific fea-\nture extraction module (Sec. 4.1) that encodes frame fea-\ntures and generates single-view tracking results in a stream-\ning fashion, and (2) a multi-view integration module (Sec.\n4.2) that fuses multi-view features with BEV guidance and\nrefines view-specific feature with a spatial-enhanced atten-\ntion mechanism.\n4.1. View-Specific Feature Extraction\nAs shown in Figure 3a, this module processes the video\nstream from a specific viewpoint k, and extracts target-\naware features in the search frame at a timepoint t based\non the reference frame that indicates the target object.\nView-Specific Encoder. We employ ViT as the back-\nbone of our view-specific encoder.\nThe visual inputs of\nthe view-specific encoder consist of a search frame S ∈\nR3×Hs×Ws and a reference frame R ∈R3×Hr×Wr. As the\ntransformer block processes a series of tokens, we segment\nthe frames into non-overlapping patches with p × p resolu-\ntion. The search and reference frames are individually em-\nbedded into a token sequence, represented by IS ∈RNs×D\nand IR ∈RNr×D, where D is the hidden dimension, Ns =\nHsWs\np2\nis the number of search tokens, and Nr = HrWr\np2\nis\nthe number of reference tokens.\nTo ensure temporal continuity between frames, akin to\nthe method utilized in ODTrack [46], two specialized tem-\nporal tokens are also included in the inputs of the view-\nspecific encoder to facilitate the propagation of temporal\ninformation. Specifically, at any given time t, a learnable to-\nken Tt is randomly initialized, which is designed to capture\ntemporal information of the current frame. Concurrently,\nwe incorporate a token Tt−1 that carries temporal informa-\ntion from the preceding frame, which leverages historical\nfeatures to enhance tracking accuracy and continuity. The\ninput token sequence of our view-specific encoder can be\nformulated as the composition of the visual and temporal\ntokens f = [Tt, Tt−1, IR, IS], while the output token se-\nquence is denoted as f ′ = [T ′\nt, T ′\nt−1, I′\nR, I′\nS].\nAfter obtaining f ′, T ′\nt is used to compute attention\nweights in conjunction with I′\nS to utilize temporal informa-\ntion for adjustments, which can be described as follows:\nIU = I′\nS · (I′\nS × (T ′\nt)⊤),\n(1)\nwhere IU represents the extracted feature that encapsulates\nattention focused on the target object in the search frame.\nSingle-View Tracking Result. We employ a BBox head\nbased on the CenterNet architecture [47] to output tracking\nresults from the extracted feature IU. This head comprises\nthree distinct sub-networks, each designed to compute the\nclassification score map, BBox dimensions, and offset sizes,\nrespectively. The highest-scoring position on the classifi-\ncation score map is identified as the target location. This\nconfiguration establishes a robust framework capable of ef-\nfectively handling single-view visual object tracking tasks.\nTo facilitate further multi-view integration, we also ap-\nply convolutional layers to map IU to a 2D feature map\n\n\n𝐸\nViT\n𝐼𝑅\nView-Specific Feature Extraction\nMulti-View Integration\nPervious \nTemporal Token\n𝑇𝑡−1\nReference Frame\nCurrent \nSearch Frame\n(c) Spatial-Enhanced Attention\nBBox\nHead\nCNN\nAttn.\nUnrefined Result\nBEV Head\n(a)  View-Specific Encoder\n𝐹2𝐷\n1\n𝐹2𝐷\n2\n𝐹2𝐷\n𝐾\n(b) 3D Feature Proj. and Agg.\nPatch & Pos embed\n…\nTransformer Blocks\n…\n𝐼𝑆\n𝑇𝑡\n𝐼𝑆\n′\n𝐼𝑅\n′\n𝑇𝑡−1\n′\n𝑇𝑡\n′\n𝐼𝑈\n𝐹3𝐷\n𝐹3𝐷\n′\nRefined Results\nBEV Map\n𝑇3𝐷\n𝐼𝑈\n1\n𝐼𝑈\n2\n𝐼𝑈\n𝐾\nBBox Head\nAggregation\nProjection\nView k, Time t\n𝐹2𝐷\n𝑘\nCNN\nView k\nView 1\nView 2\nView K\nFigure 3. The framework of MITracker. (a) The view-specific feature extraction module employs a ViT that utilizes temporal tokens to\nprocess each view independently, outputting unrefined results that can be further improved by multi-view information. The multi-view\nintegration module contains (b) 3D feature volume construction that aggregates features into 3D space with BEV guidance and (c) spatial-\nenhanced attention that refines tracking results by 3D spatial information.\nwith original image size, denoted as F2D ∈R32×Hs×Ws.\nThis establishes a pixel-wise correspondence between the\nextracted feature and the search image, which is crucial for\nreconstructing the 3D feature space in the following section.\n4.2. Multi-View Integration\nTo effectively integrate 2D feature maps F 1\n2D, F 2\n2D, ..., F K\n2D\nfrom K viewpoints, we project them into a 3D feature\nspace and then aggregate them under the supervision of\nBEV guidance.\nFinally, we embed the aggregated fea-\nture to a 3D-aware token to refine all view-specific features\nI1\nU, I2\nU, ..., IK\nU via spatial-enhanced attention, thus produc-\ning stable tracking results across different viewpoints.\n3D Feature Projection. As illustrated in Figure 3b, we\nconstruct a 3D feature volume of size X × Y × Z, where\n(X, Y ) represents the horizontal plane and Z axis denotes\nthe vertical direction following [17, 44]. For a viewpoint k,\nwe project the (u, v) coordinates in F k\n2D to (x, y, z) coordi-\nnates in the 3D feature volume by the formula below:\n\n\nu\nv\n1\n\n= CK[CR|Ct]\n\n\n\n\nx\ny\nz\n1\n\n\n\n,\n(2)\nwhere CK represents the camera’s intrinsic matrix, CR de-\nnotes the rotation matrix describing the camera’s orienta-\ntion, and Ct is the translation vector specifying the camera’s\nposition in space. Upon establishing the mapping matrix,\nwe implement bilinear sampling to populate the 3D feature\nvolume. In scenarios that involve multiple viewpoints, we\ncompute the average of the mapped values from each view\nto ensure consistency. Consequently, we derive a 3D feature\nvolume represented as F3D ∈R32×X×Y ×Z.\n3D Feature Aggregation. To better integrate multi-view\nspatial information, we apply 1D convolutional layers to\naggregate features along the Z-axis of F3D, resulting in\nF ′\n3D ∈R32×X×Y , thereby consolidating spatial informa-\ntion within the (X, Y ) plane. Subsequently, a classifica-\ntion head (i.e., BEV head) is employed to generate a BEV\nscore map from F ′\n3D. This BEV map delineates the ob-\nject positions on the horizontal plane, thereby imposing su-\npervision constraints on information fusion across multiple\nviewpoints. This integrative approach allows for precise lo-\ncalization and mapping within multi-view scenarios.\nSpatial-Enhanced Attention.\nBEV guidance for the\naggregated 3D feature F ′\n3D only implicitly constrains the\noriginal single-view output, but it is insufficient to address\nthe potential target loss issue due to the lack of direct\nsupervision on tracking results.\nTo remedy this, we in-\ntroduce spatial-enhanced attention to explicitly incorporate\nF ′\n3D into the tracking process as shown in Figure 3c.\nWe first use convolutional layers to embed F ′\n3D into a\n3D-aware token T3D ∈R1×D, which inherits multi-view\nspatial information.\nFor all the K viewpoints, we then\nindividually concatenate T3D with their unrefined features\nI1\nU, I2\nU, ..., IK\nU produced by the view-specific encoder. For a\nviewpoint k, a series of transformer blocks take in its com-\nposite token sequence (T3D, Ik\nU) and refine them using at-\ntention mechanisms that leverage fused 3D spatial informa-\ntion. A final BBox head outputs the refined tracking results,\nwhere potential errors such as target loss are corrected.\n\n\nDataset\nMVTrack\nGMTD\nMethod\nSingle-View\nMulti-View\nSingle-View\nAUC(%)\nPNorm(%)\nP(%)\nAUC(%)\nPNorm(%)\nP(%)\nAUC(%)\nPNorm(%)\nP(%)\nDiMP [1]\n43.14\n59.52\n53.13\n35.77\n49.04\n51.65\n52.71\n68.24\n66.04\nPrDiMP [10]\n48.61\n66.09\n58.93\n38.49\n54.68\n57.95\n57.76\n76.21\n70.49\nTrDiMP [35]\n50.54\n67.67\n60.44\n39.71\n55.31\n58.52\n59.51\n78.94\n73.48\nMixFormer [9]\n57.59\n75.44\n67.72\n43.29\n58.07\n62.70\n62.03\n82.60\n78.48\nOSTrack [43]\n60.04\n77.72\n70.06\n49.10\n65.19\n67.34\n58.44\n77.37\n73.23\nGRM [14]\n52.53\n69.91\n62.31\n41.47\n57.33\n58.76\n55.67\n74.02\n70.27\nSeqTrack [6]\n58.37\n76.63\n69.03\n43.88\n59.11\n63.60\n62.97\n83.20\n79.32\nARTrack [37]\n53.23\n70.25\n62.49\n42.52\n58.00\n60.50\n59.56\n78.12\n74.23\nHIPTrack [2]\n60.45\n78.92\n70.53\n48.43\n63.69\n66.26\n62.20\n80.62\n76.94\nEVPTrack [32]\n61.37\n79.76\n71.97\n46.36\n61.84\n67.20\n63.89\n83.76\n79.93\nAQATrack [41]\n61.93\n80.00\n72.69\n45.24\n59.76\n65.33\n63.57\n83.04\n79.44\nODTrack [46]\n63.36\n82.25\n74.46\n48.05\n63.55\n67.70\n61.43\n82.37\n78.35\nSAM2∗[30]\n46.49\n63.12\n56.82\n39.08\n53.49\n57.34\n59.88\n74.66\n73.25\nSAM2Long∗[11]\n55.30\n72.84\n67.40\n45.40\n59.12\n65.30\n62.80\n78.60\n77.40\nMITracker\n68.57\n88.77\n80.93\n71.13\n91.87\n83.95\n65.96\n87.05\n82.07\nTable 2. Comparison with SOTA methods on the MVTrack and GMTD datasets. MITracker is for multi-view tracking, while others are\nsingle-view methods. Methods with ∗use pre-trained weights without fine-tuning. Best results are bolded, second-best are underlined.\n5. Experiments\n5.1. Dataset\nIn addition to MVTrack dataset, we use two external\ndatasets for training and evaluation, which are detailed as\nfollows.\n• GOT10K. GOT-10K [21] is a large and diverse dataset\nwith a wide range of object categories. Its training set\ncontains 9,335 videos across 480 moving object cate-\ngories.\n• GMTD. GMTD [38] is a multi-view tracking test set with\n10 scenes, captured by 2-3 uncalibrated cameras in indoor\nand outdoor settings. It includes 6 target types and various\ntracking challenges.\n5.2. Implementation Details\nLoss Function. For the BBox head, we employ a weighted\nfocal loss [26] Lcls for classification, along with the gener-\nalized intersection over union loss [31] Lgiou and L1 loss for\nBBox regression. Additionally, a focal loss Lbev is utilized\nfor BEV map supervision. The overall loss function of the\nmodel is formulated as follows:\nLtrack = Lcls + λgiouLgiou + λL1L1 + λbevLbev,\n(3)\nwhere λgiou = 5, λL1 = 2, and λbev = 0.1 are the coeffi-\ncients that balance the contributions from each loss .\nTraining Setup. We initialize our view-specific encoder\nwith pre-trained DINOv2 [28] parameters using the ViT-\nbase model [12]. For the visual inputs, we set the refer-\nence frame with 182 × 182 pixels, and the search frame\nwith 364 × 364 pixels. We utilize the camera parameters\nfrom the dataset for projection, with the 3D feature volume\nhaving dimensions X = 200, Y = 200, and Z = 3.\nOur training process consists of two stages. In the first\nstage, we only train the view-specific feature extraction\nmodule. Specifically, we train the view-specific encoder\nand BBox head using single-view inputs from GOT-10K\nand MVTrack datasets until convergence. For each view-\npoint, we include one reference frame and two random\nsearch frames from 200 frame interval in each iteration,\nthus promoting temporal information propagation between\nframes. In the second stage, we fine-tune the view-specific\nencoder and train the entire framework using multi-view\ndata from the MVTrack dataset. For each training sample,\nwe randomly select 2 to 4 viewpoints with one reference and\ntwo search frames in each iteration. All the training proce-\ndures are conducted on 2 NVIDIA A100 80GB GPUs.\nFor detailed implementation and training specifics,\nplease refer to the Appendix.\n5.3. Evaluation Metrics\nWe evaluate our method using three standard performance\nmeasures from the single-view tracking benchmark [13, 27,\n39]: Area Under Curve (AUC), Precision (P), and Normal-\nized Precision (PNorm):\n• AUC: The Intersection over Union (IoU) measures the\noverlap between predicted and ground truth BBoxes in\neach frame. The AUC metric is calculated by varying the\nIoU threshold to evaluate the area error in the tracking\nregion.\n\n\n0\n0.2\n0.4\n0.6\n0.8\n1\nOverlap threshold\n0\n20\n40\n60\n80\n100\nSuccess rate (%)\n[71.1] MITracker\n[63.4] ODTrack\n[61.9] AQATrack\n[61.4] EVPTrack\n[60.5] HIPTrack\n[60.0] OSTrack\n[58.4] SeqTrack\n[57.6] MixFormer\n[55.3] SAM2Long\n[53.2] ARTrack\n[52.5] GRM\n[50.5] TrDiMP\n[48.6] PrDiMP\n[46.5] SAM2\n[43.1] DiMP\n(a) Success plot.\n0\n5\n10\n15\n20\n25\n30\nFrame numbers needed for recovery\n0\n20\n40\n60\n80\n100\nRecovery rate (%)\n[79.2] MITracker\n[56.7] SAM2Long\n[56.7] ODTrack\n[54.2] EVPTrack\n[54.2] SeqTrack\n[50.8] OSTrack\n[48.3] HIPTrack\n[44.2] MixFormer\n[43.3] AQATrack\n[35.8] SAM2\n[35.0] TrDiMP\n[34.2] GRM\n[27.5] ARTrack\n[26.7] PrDiMP\n[25.8] DiMP\n(b) Recovery ability plot.\n1.5\n2\n2.5\n3\nRestart count\n400\n450\n500\n550\n600\nMax length\nMITracker\nODTrack\nAQATrack\nMixFormer\nEVPTrack\nSeqTrack\nOSTrack\nARTrack\nHIPTrack\nGRM\n(c) Robust tracking plot.\nFigure 4. General experiments on the MVTrack dataset evaluate tracking robustness. MITracker provides multi-view results, while other\nmethods yield single-view results. In (a), methods are ranked by AUC and noted in the legend. For (b), the numbers in the legend represent\nthe method’s recovery rate within 10 frames after the target disappears.\n• P: Precision is defined as the distance between the pre-\ndicted and ground truth BBox centers. This metric is used\nto assess the positional error in tracking.\n• PNorm: To mitigate biases due to variations in BBox size,\nwe normalize the center point by the width and height of\nthe ground truth BBox. This adjustment provides a more\naccurate metric.\n5.4. Comparison with Existing Methods\nSOTA Performance on Benchmark. We evaluate tracking\nperformance with single-view visual object tracking meth-\nods, training all models (except SAM2 and SAM2Long) on\nthe GOT10K and MVTrack datasets. The models are tested\non both the MVTrack and GMTD datasets under single-\nview and multi-view settings.\nHowever, single-view methods cannot handle multi-view\ninputs or generate multi-view predictions. To address this,\nwe employ a post-fusion strategy to obtain multi-view re-\nsults.\nSpecifically, single-view predictions are first pro-\njected into the 3D world coordinate system. The region\nwith maximum overlap is identified as the target position,\nwhich is then reprojected onto the 2D image plane of each\nviewpoint to generate the fused multi-view tracking results.\nAs shown in Table 2, MITracker achieves superior per-\nformance in both multi- and single-view tracking across dif-\nferent datasets. In multi-view scenarios with 3-4 cameras,\nMITracker outperforms other methods that rely on post-\nprocessing for multi-view fusion, surpassing the second-\nbest method OSTrack by approximately 26% in PNorm.\nIn single-view settings, MITracker surpasses SOTA meth-\nods on the MVTrack dataset, achieving an AUC of 68.57%,\nwhich outperforms ODTrack by approximately 5%.\nNotably, MITracker exhibits strong generalization ca-\npabilities by achieving exceptional performance on the\nGMTD, despite it not being included in the training data.\nThis demonstrates the robustness of our multi-view ap-\nproach even in single-view scenarios. We attribute these\nimprovements to our multi-view training strategy, which en-\nables the model to better understand spatial relationships\ncrucial for precise tracking.\nIt is also noteworthy that\npost-processing degrades the performance of all single-view\nmethods. This indicates a substantial distribution gap in\nview-independent feature detection across models, making\neffective fusion through geometric projections challenging.\nStable Continuous Tracking Capability.\nTo further\nevaluate tracking robustness, we conducted three compara-\ntive experiments on the MVTrack dataset. Only MITracker\nutilizes multi-view inputs, other methods use single-view\ninputs and generate BBoxes independently.\nFirst, we analyzed tracking success rates across various\nIoU thresholds, as shown in Figure 4a. MITracker con-\nsistently outperforms competing methods regardless of the\nthreshold value.\nSecond, we evaluated the recovery capability after the\ntarget was invisible by measuring the proportion of suc-\ncessful tracking resumption within given frame intervals\n[22]. As illustrated in Figure 4b, with a 10-frame interval,\nMITracker achieves a high success rate of 79.2% in these\nrecovery tests. In comparison, SAM2Long only achieves\na 56.7% recovery rate under the same setting, highlight-\ning our method’s exceptional ability to quickly reestablish\ntracking after the target dissapears.\nIn practical applications, users can manually intervene\nto restart the model’s tracking by providing an accurate ini-\ntial position. In this experimental setup, we measured the\nmaximum continuous tracking length of video frames and\nthe average number of restarts (triggered when target loss\nexceeds 10 frames, using ground truth for repositioning)\n[45]. As shown in Figure 4c, MITracker achieves nearly\n100 frames longer tracking duration than ODTrack while\n\n\nrequiring fewer restart counts.\n5.5. Ablation Study\nResults in Table 3 demonstrate that BEV Loss, which pro-\nvides implicit multi-view information feedback, signifi-\ncantly enhances model performance. This improvement is\nattributed to its ability to augment spatial awareness during\nsingle-view feature extraction. The Spatial attention, which\nutilizes fused information to adjust outputs from single-\nview perspectives, also contributes to notable performance\nimprovements in the model.\nBEV Loss\nSpatial Attention\nAUC(%)\nPNorm(%)\nP(%)\n63.99\n82.82\n75.00\n✓\n69.64\n89.85\n82.01\n✓\n✓\n71.13\n91.87\n83.95\nTable 3. Ablation study for multi-view evaluation on MVTrack\ndataset.\n5.6. Visualization Comparison\nOur qualitative evaluation focuses on the influence of occlu-\nsion and fast motion. In the upper part of Figure 5, we se-\nlect two viewpoints from the MVTrack dataset and evaluate\nthem on MITracker and ODTrack, which has the second-\nbest performance on this dataset. The gray areas in the\ngraph represent periods when the object is out of view or\nfully occluded by other objects. We can easily observe that\nMITracker is able to re-track the object shortly after it reap-\npears, whereas ODTrack tends to continue in a lost state.\nFor instances #405 and #515 in V2, even when the ob-\nject reappears in the frame, ODTrack still mistakenly locks\nonto the wrong object. The bottom of Figure 5 presents\ntests conducted on the GMTD, where we also selected the\nsecond-best method, EVPTrack, for comparison with MI-\nTracker. When a pedestrian reappears after being obscured\nby a pillar, EVPTrack mistakenly locks onto the wrong tar-\nget, whereas MITracker is able to maintain stable and con-\ntinuous tracking.\nWe also visualize the predicted BEV trajectories from\nMITracker in Figure 6. Referencing the ground-truth trajec-\ntories, MITracker effectively integrates multi-view features\nand provides accurate 3D spatial information.\n6. Discussion\nIn previous sections, we provide a detailed introduction to\nthe MVTrack dataset and demonstrate the outstanding per-\nformance of MITracker. However, there are some areas that\ncould be improved in future work.\nLimitations. Although MVTrack dataset includes a di-\nverse set of scenes, it currently consists of indoor environ-\nments only, potentially limiting the generalization of meth-\nEVPTrack\nTarget Invisible\nODTrack\nMITracker\nGT\n#405\nV1 #405\n#421\n#472\n#414\n#700\nV2 #414\nV2 #515\nV1 #414\nV1 #515\nV2 #405\n0\n1\nIoU\n0\n1\nIoU\n0\n1\nIoU\n#414\n#515\n#405 #414\n#515\n#421\n#472\n#700\nFigure 5.\nQualitative comparison results.\nComparison of our\ntracker with two SOTA methods on MVTrack dataset (top) and\nGMTD (bottom).\nEach frame is cropped for better visualiza-\ntion. IoU curves of each method’s prediction and ground truth\nare shown above, where IoU reflects tracking quality. MITracker\ndemonstrates superior re-tracking performance upon target reap-\npearance.\nEnd\nEnd\nStart\nStart\nMITracker\nGT\nFigure 6.\nVisualization of BEV trajectories on the MVTrack\ndataset. Left: scene captured by four cameras. Right: scene cap-\ntured by three cameras.\nods trained on it to outdoor settings.\nAdditionally, MI-\nTracker relies on camera calibration for multi-view fusion,\nwhich may restrict its applicability in scenarios where cali-\nbration is challenging or infeasible.\nFuture work. We plan to extend MVTrack dataset by\nincluding outdoor scenes and a wider range of tracking\nobjects to enable the development of more generalizable\n\n\nmulti-view tracking algorithms. Furthermore, we aim to\nenhance MITracker by reducing its dependency on precise\ncamera calibration, making it more adaptable to scenarios\nwhere accurate calibration is challenging.\n7. Conclusion\nIn this study, we address key challenges such as occlu-\nsion and target loss in MVOT by making two significant\ncontributions: (1) MVTrack, a comprehensive dataset with\n234K high-quality annotations across diverse scenes and\nobject categories, and (2) MITracker, a novel visual track-\ning method that effectively integrates multi-view object fea-\ntures. MITracker achieves SOTA results on MVTrack and\nGMTD datasets, demonstrating its ability to provide stable\nand reliable tracking across different viewpoints and video\ndurations. Our contributions lay the foundation for future\nadvancements in MVOT, enabling the development of more\nrobust and accurate tracking systems for real-world scenar-\nios.\n8. Acknowledgments\nThis work was partially supported by STI 2030-Major\nProjects (2022ZD0209000) and HPC Platform of Shang-\nhaiTech University.\nReferences\n[1] Goutam Bhat, Martin Danelljan, Luc Van Gool, and Radu\nTimofte.\nLearning discriminative model prediction for\ntracking.\nIn Proceedings of the IEEE/CVF international\nconference on computer vision, pages 6182–6191, 2019. 6\n[2] Wenrui Cai, Qingjie Liu, and Yunhong Wang.\nHiptrack:\nVisual tracking with historical prompts. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2024. 6\n[3] Yidong Cai, Jie Liu, Jie Tang, and Gangshan Wu. Robust\nobject modeling for visual tracking. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 9589–9600, 2023. 1\n[4] Tatjana Chavdarova, Pierre Baqu´e, St´ephane Bouquet, An-\ndrii Maksai, Cijo Jose, Timur Bagautdinov, Louis Lettry,\nPascal Fua, Luc Van Gool, and Franc¸ois Fleuret.\nWild-\ntrack: A multi-camera hd dataset for dense unscripted pedes-\ntrian detection. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 5030–5039,\n2018. 3\n[5] Xin Chen, Bin Yan, Jiawen Zhu, Dong Wang, Xiaoyun Yang,\nand Huchuan Lu. Transformer tracking. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 8126–8135, 2021. 2\n[6] Xin Chen, Houwen Peng, Dong Wang, Huchuan Lu, and Han\nHu. Seqtrack: Sequence to sequence learning for visual ob-\nject tracking. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 14572–\n14581, 2023. 2, 4, 6\n[7] Paolo Cignoni, Alessandro Muntoni, Guido Ranzuglia, and\nMarco Callieri. MeshLab. 3\n[8] Paolo Cignoni, Marco Callieri, Massimiliano Corsini, Mat-\nteo Dellepiane, Fabio Ganovelli, and Guido Ranzuglia.\nMeshLab:\nan Open-Source Mesh Processing Tool.\nIn\nEurographics Italian Chapter Conference. The Eurographics\nAssociation, 2008. 3\n[9] Yutao Cui, Cheng Jiang, Limin Wang, and Gangshan Wu.\nMixformer: End-to-end tracking with iterative mixed at-\ntention.\nIn Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 13608–\n13618, 2022. 1, 2, 6\n[10] Martin Danelljan, Luc Van Gool, and Radu Timofte. Prob-\nabilistic regression for visual tracking.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 7183–7192, 2020. 1, 2, 6\n[11] Shuangrui Ding, Rui Qian, Xiaoyi Dong, Pan Zhang, Yuhang\nZang, Yuhang Cao, Yuwei Guo, Dahua Lin, and Jiaqi\nWang.\nSam2long: Enhancing sam 2 for long video seg-\nmentation with a training-free memory tree. arXiv preprint\narXiv:2410.16268, 2024. 6\n[12] Alexey Dosovitskiy.\nAn image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 2, 6\n[13] Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia\nYu, Hexin Bai, Yong Xu, Chunyuan Liao, and Haibin Ling.\nLasot: A high-quality benchmark for large-scale single ob-\nject tracking. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 5374–5383,\n2019. 2, 3, 4, 6\n[14] Shenyuan Gao, Chunluan Zhou, and Jun Zhang. Generalized\nrelation modeling for transformer tracking. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 18686–18695, 2023. 6\n[15] Xiaotian Han, Quanzeng You, Chunyu Wang, Zhizheng\nZhang, Peng Chu, Houdong Hu, Jiang Wang, and Zicheng\nLiu. Mmptrack: Large-scale densely annotated multi-camera\nmultiple people tracking benchmark. In Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision, pages 4860–4869, 2023. 2, 3\n[16] Shengyu Hao, Peiyuan Liu, Yibing Zhan, Kaixun Jin,\nZuozhu Liu, Mingli Song, Jenq-Neng Hwang, and Gaoang\nWang.\nDivotrack: A novel dataset and baseline method\nfor cross-view multi-object tracking in diverse open scenes.\nInternational Journal of Computer Vision, 132(4):1075–\n1090, 2024. 3\n[17] Adam W Harley, Zhaoyuan Fang, Jie Li, Rares Ambrus, and\nKaterina Fragkiadaki. Simple-bev: What really matters for\nmulti-sensor bev perception?\nIn 2023 IEEE International\nConference on Robotics and Automation (ICRA), pages\n2759–2765. IEEE, 2023. 2, 3, 5\n[18] Yunzhong Hou and Liang Zheng. Multiview detection with\nshadow transformer (and view-coherent data augmentation).\nIn Proceedings of the 29th ACM International Conference on\nMultimedia, pages 1673–1682, 2021. 3\n[19] Yunzhong Hou, Liang Zheng, and Stephen Gould. Multi-\nview detection with feature perspective transformation. In\n\n\nComputer Vision–ECCV 2020: 16th European Conference,\nGlasgow, UK, August 23–28, 2020, Proceedings, Part VII\n16, pages 1–18. Springer, 2020. 2, 3\n[20] Shiyu Hu, Xin Zhao, Lianghua Huang, and Kaiqi Huang.\nGlobal instance tracking:\nLocating target more like hu-\nmans. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 45(1):576–592, 2022. 2, 3\n[21] Lianghua Huang, Xin Zhao, and Kaiqi Huang. Got-10k: A\nlarge high-diversity benchmark for generic object tracking in\nthe wild. IEEE transactions on pattern analysis and machine\nintelligence, 43(5):1562–1577, 2019. 2, 3, 6\n[22] Yuqing Huang, Xin Li, Zikun Zhou, Yaowei Wang, Zhenyu\nHe, and Ming-Hsuan Yang. Rtracker: Recoverable track-\ning via pn tree structured memory.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 19038–19047, 2024. 1, 2, 7\n[23] Hamed Kiani Galoogahi, Ashton Fagg, Chen Huang, Deva\nRamanan, and Simon Lucey.\nNeed for speed: A bench-\nmark for higher frame rate object tracking. In Proceedings of\nthe IEEE international conference on computer vision, pages\n1125–1134, 2017. 2, 3\n[24] Matej Kristan, Jiri Matas, Aleˇs Leonardis, Tom´aˇs Voj´ıˇr, Ro-\nman Pflugfelder, Gustavo Fernandez, Georg Nebehay, Fatih\nPorikli, and Luka ˇCehovin. A novel performance evaluation\nmethodology for single-target trackers.\nIEEE transactions\non pattern analysis and machine intelligence, 38(11):2137–\n2155, 2016. 2, 3\n[25] Bo Li, Junjie Yan, Wei Wu, Zheng Zhu, and Xiaolin Hu.\nHigh performance visual tracking with siamese region pro-\nposal network. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 8971–8980,\n2018. 1, 2\n[26] T Lin. Focal loss for dense object detection. arXiv preprint\narXiv:1708.02002, 2017. 6\n[27] Matthias Muller, Adel Bibi, Silvio Giancola, Salman Al-\nsubaihi, and Bernard Ghanem. Trackingnet: A large-scale\ndataset and benchmark for object tracking in the wild. In\nProceedings of the European conference on computer vision\n(ECCV), pages 300–317, 2018. 2, 3, 6\n[28] Maxime Oquab, Timoth´ee Darcet, Th´eo Moutakanni, Huy\nVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.\nDinov2: Learning robust visual features without supervision.\narXiv preprint arXiv:2304.07193, 2023. 6\n[29] Liang Peng, Junyuan Gao, Xinran Liu, Weihong Li, Shaohua\nDong, Zhipeng Zhang, Heng Fan, and Libo Zhang. Vast-\ntrack: Vast category visual object tracking. arXiv preprint\narXiv:2403.03493, 2024. 2, 3\n[30] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang\nHu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman\nR¨adle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junt-\ning Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-\nYuan Wu, Ross Girshick, Piotr Doll´ar, and Christoph Feicht-\nenhofer. Sam 2: Segment anything in images and videos.\narXiv preprint arXiv:2408.00714, 2024. 6\n[31] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir\nSadeghian, Ian Reid, and Silvio Savarese. Generalized in-\ntersection over union: A metric and a loss for bounding\nbox regression. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 658–666,\n2019. 6\n[32] Liangtao Shi, Bineng Zhong, Qihua Liang, Ning Li, Sheng-\nping Zhang, and Xianxian Li. Explicit visual prompts for\nvisual object tracking. In AAAI, 2024. 6\n[33] Liangchen Song, Jialian Wu, Ming Yang, Qian Zhang, Yuan\nLi, and Junsong Yuan. Stacked homography transformations\nfor multi-view pedestrian detection. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 6049–6057, 2021. 3\n[34] Torben Teepe, Philipp Wolters, Johannes Gilg, Fabian Her-\nzog, and Gerhard Rigoll. Earlybird: Early-fusion for multi-\nview tracking in the bird’s eye view. In Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision, pages 102–111, 2024. 3\n[35] Ning Wang, Wengang Zhou, Jie Wang, and Houqiang Li.\nTransformer meets tracker: Exploiting temporal context for\nrobust visual tracking.\nIn Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npages 1571–1580, 2021. 6\n[36] Xiao Wang, Xiujun Shu, Zhipeng Zhang, Bo Jiang, Yaowei\nWang, Yonghong Tian, and Feng Wu. Towards more flexible\nand accurate object tracking with natural language: Algo-\nrithms and benchmark.\nIn Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npages 13763–13773, 2021. 2, 3\n[37] Xing Wei, Yifan Bai, Yongchao Zheng, Dahu Shi, and Yi-\nhong Gong. Autoregressive visual tracking. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 9697–9706, 2023. 2, 6\n[38] Minye Wu, Haibin Ling, Ning Bi, Shenghua Gao, Qiang\nHu, Hao Sheng, and Jingyi Yu. Visual tracking with mul-\ntiview trajectory prediction.\nIEEE Transactions on Image\nProcessing, 29:8355–8367, 2020. 2, 3, 6\n[39] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Object track-\ning benchmark. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 37:1–1, 2015. 2, 3, 6\n[40] Shiting Xiao, Yufu Wang, Ammon Perkes, Bernd Pfrommer,\nMarc Schmidt, Kostas Daniilidis, and Marc Badger. Multi-\nview tracking, re-id, and social network analysis of a flock\nof visually similar birds in an outdoor aviary. International\nJournal of Computer Vision, 131(6):1532–1549, 2023. 2\n[41] Jinxia Xie, Bineng Zhong, Zhiyi Mo, Shengping Zhang,\nLiangtao Shi, Shuxiang Song, and Rongrong Ji.\nAutore-\ngressive queries for adaptive tracking with spatio-temporal\ntransformers. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 19300–\n19309, 2024. 4, 6\n[42] Yuanlu Xu, Xiaobai Liu, Yang Liu, and Song-Chun Zhu.\nMulti-view people tracking via hierarchical trajectory com-\nposition.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 4256–4265,\n2016. 2, 3\n[43] Botao Ye, Hong Chang, Bingpeng Ma, Shiguang Shan, and\nXilin Chen. Joint feature learning and relation modeling for\ntracking: A one-stream framework. In European Conference\non Computer Vision, pages 341–357. Springer, 2022. 2, 6\n\n\n[44] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenyu Liu,\nand Wenjun Zeng.\nVoxeltrack: Multi-person 3d human\npose estimation and tracking in the wild. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 45(2):2613–\n2626, 2022. 5\n[45] Xin Zhao, Shiyu Hu, Yipei Wang, Jing Zhang, Yimin Hu,\nRongshuai Liu, Haibin Ling, Yin Li, Renshu Li, Kun Liu,\net al. Biodrone: A bionic drone-based single object track-\ning benchmark for robust vision.\nInternational Journal of\nComputer Vision, 132(5):1659–1684, 2024. 7\n[46] Yaozong Zheng, Bineng Zhong, Qihua Liang, Zhiyi Mo,\nShengping Zhang, and Xianxian Li. Odtrack: Online dense\ntemporal token learning for visual tracking. In Proceedings\nof the AAAI Conference on Artificial Intelligence, pages\n7588–7596, 2024. 1, 2, 4, 6\n[47] Xingyi Zhou, Dequan Wang, and Philipp Kr¨ahenb¨uhl. Ob-\njects as points. arXiv preprint arXiv:1904.07850, 2019. 4\n[48] Yitao Zhu, Sheng Wang, Mengjie Xu, Zixu Zhuang, Zhixin\nWang, Kaidong Wang, Han Zhang, and Qian Wang. Muc:\nMixture of uncalibrated cameras for robust 3d human body\nreconstruction. arXiv preprint arXiv:2403.05055, 2024. 1\n\n\nMITracker: Multi-View Integration for Visual Object Tracking\nSupplementary Material\nSection 9 provides additional information on the MV-\nTrack dataset, while Section 10 includes further implemen-\ntation details and experimental results of MITracker.\n9. Dataset Details\nData Annotation. In the BEV annotations, the MVTrack\ndataset covers an 8m × 8m area. Ground truth labels are\nprojected onto a 400 × 400 grid, where each cell is 2cm ×\n2cm in size.\nAttributes Definition. MVTrack dataset contains nine\nattributes to assess tracking robustness, as shown in Table\n4. We provide frame-level binary labels for five attributes:\nBackground Clutter (BC), Motion Blur (MB), Partial Oc-\nclusion (POC), Full Occlusion (FOC), and Out of View\n(OV). These are manually annotated for each frame. De-\nformation (DEF) is labeled according to whether the tracked\ntarget deforms. Low Resolution (LR), Aspect Ratio Change\n(ARC), and Scale Variation (SV) are automatically com-\nputed from changes in the BBox size.\nAtt.\nDefinition\nBC\nThe background has similar appearance as the\ntarget\nMB\nThe target region is blurred due to target motion\nPOC\nThe target is partially occluded in the frame\nFOC\nThe target is fully occluded in the frame\nOV\nThe target completely leaves the video frame\nDEF\nThe target is deformable during tracking\nLR\nThe target BBox is smaller than 1000 pixels\nARC\nThe ratio of BBox aspect ratio is outside the\nrange [0.5, 2]\nSV\nThe ratio of BBox is outside the range [0.5, 2]\nTable 4. Description of 9 attributes in MVTrack dataset.\nStatistical Details. The MVTrack dataset contains 260\nvideos averaging around 900 frames each, as shown in Fig-\nure 7a. As illustrated in Figure 7b, a key challenge is oc-\nclusion, which often results from subject-object interactions\nthat cause partial or complete occlusion.\nConsequently,\ntracking models need to manage occlusion to perform ro-\nbustly and adeptly on this dataset.\n10. Experiment Details\n10.1. Training and Resource Analysis\nTraining Details. We process the visual inputs by cropping\nthe reference frame to 2 times the target’s BBox size and\n0\n5\n10\n15\n20\n25\n500\n700\n900\n1100\n1300\nProbability [%]\nFrames\n(a) Frames distribution.\n65\n179\n260\n219\n50\n109\n233\n253 260\n0\n50\n100\n150\n200\n250\n300\nBC\nMB POC FOC OV DEF LR ARC SV\nNumber of Videos\nAttribute\n(b) Attributes distribution.\nFigure 7. Distribution of sequences in each attribute and length in\nour MVTrack dataset.\nresizing it to 182 × 182 pixels. The search frame is cropped\nat 4.5 times the target box area and resized to 364 × 364\npixels to expand the search region. During projection, we\ntransform the camera intrinsic matrix CK accordingly and\nadd noise to the translation vector Ct to prevent overfitting\nin multi-view fusion.\nTraining consists of two stages. In the first stage, we\noptimize the view-specific encoder using AdamW with a\nlearning rate of 1 × 10−5 and the rest of the model at\n1 × 10−4. We train for 50 epochs, sampling 10,000 image\npairs per epoch with a batch size of 32. In the second stage,\nwe fine-tune the encoder at 1 × 10−6 while keeping other\ncomponents at 1×10−4. We use the MVTrack dataset, sam-\npling 2,500 multi-view image pairs per epoch for 40 epochs\nwith a batch size of 4. AdamW is used throughout.\nComputational Resource. We evaluate MITracker and\nthe single-view model ODTrack under the same input (4\nviews) on an NVIDIA A100, as summarized in Table 5.\nAlthough multi-view fusion introduces additional compu-\ntational overhead, it remains within an acceptable range.\nMethod\nParameters (M)\nGRAM (MB)\nFPS\nODTrack\n92.12\n365.82\n18.78\nMITracker\n101.65\n407.78\n14.08\nTable 5. Comparison of computational complexity and resource.\n\n\n0\n10\n20\n30\n40\n50\nLocation error threshold\n0\n20\n40\n60\n80\n100\nPrecision (%)\n[83.9] MITracker\n[74.5] ODTrack\n[72.7] AQATrack\n[72.0] EVPTrack\n[70.5] HIPTrack\n[70.1] OSTrack\n[69.0] SeqTrack\n[67.7] MixFormer\n[67.4] SAM2Long\n[62.5] ARTrack\n[62.3] GRM\n[60.4] TrDiMP\n[58.9] PrDiMP\n[56.8] SAM2\n[53.1] DiMP\n(a) Precision plot on MVTrack dataset.\n0\n0.1\n0.2\n0.3\n0.4\n0.5\nNormalized location error threshold\n0\n20\n40\n60\n80\n100\nNormalized precision (%)\n[91.9] MITracker\n[82.2] ODTrack\n[80.0] AQATrack\n[79.8] EVPTrack\n[78.9] HIPTrack\n[77.7] OSTrack\n[76.6] SeqTrack\n[75.4] MixFormer\n[72.8] SAM2Long\n[70.2] ARTrack\n[69.9] GRM\n[67.7] TrDiMP\n[66.1] PrDiMP\n[63.1] SAM2\n[59.5] DiMP\n(b) Normalized precision plot on MVTrack dataset.\n0\n10\n20\n30\n40\n50\nLocation error threshold\n0\n20\n40\n60\n80\n100\nPrecision (%)\n[82.1] MITracker\n[79.9] EVPTrack\n[79.4] AQATrack\n[79.3] SeqTrack\n[78.5] MixFormer\n[78.4] ODTrack\n[77.4] SAM2Long\n[76.9] HIPTrack\n[74.2] ARTrack\n[73.5] TrDiMP\n[73.2] SAM2\n[73.2] OSTrack\n[70.5] PrDiMP\n[70.3] GRM\n[66.0] DiMP\n(c) Precision plot on GMTD.\n0\n0.1\n0.2\n0.3\n0.4\n0.5\nNormalized location error threshold\n0\n20\n40\n60\n80\n100\nNormalized precision (%)\n[87.0] MITracker\n[83.8] EVPTrack\n[83.2] SeqTrack\n[83.0] AQATrack\n[82.6] MixFormer\n[82.4] ODTrack\n[80.6] HIPTrack\n[78.9] TrDiMP\n[78.6] SAM2Long\n[78.1] ARTrack\n[77.4] OSTrack\n[76.2] PrDiMP\n[74.7] SAM2\n[74.0] GRM\n[68.2] DiMP\n(d) Normalized precision plot on GMTD.\n0\n0.2\n0.4\n0.6\n0.8\n1\nOverlap threshold\n0\n20\n40\n60\n80\n100\nSuccess rate (%)\n[66.0] MITracker\n[63.9] EVPTrack\n[63.6] AQATrack\n[63.0] SeqTrack\n[62.8] SAM2Long\n[62.2] HIPTrack\n[62.0] MixFormer\n[61.4] ODTrack\n[59.9] SAM2\n[59.6] ARTrack\n[59.5] TrDiMP\n[58.4] OSTrack\n[57.8] PrDiMP\n[55.7] GRM\n[52.7] DiMP\n(e) Success plot on GMTD.\n1.5\n2\n2.5\n3\nRestart count\n450\n500\n550\n600\nMax length\nMITracker\nODTrack\nMixFormer\nHIPTrack\nAQATrack\nSeqTrack\nEVPTrack\nOSTrack\nARTrack\nGRM\n(f) Robust tracking plot on GMTD.\nFigure 8. Comparative results across MVTrack and GMTD datasets, with rankings noted in the legends. Parts (a) and (c) sort methods by\nP with a 20-pixel threshold, parts (b) and (d) by Pnorm with a 0.2 threshold, and part (e) by AUC.\n\n\nTarget Invisible\n1 View Input\nGT\n#326\n0\n1\nIoU\n#326\n#455\n#621\n#455\n#621\n3 Views Input\nFigure 9. Qualitative comparison results on the impact of different numbers of input views. For a specific view, we compare the effects of\nusing only that view versus including two additional overlapping views.\n10.2. Comparison on Benchmark Details\nIn Figure 8, we provide further quantitative evaluations of\nthe AUC, P, and Pnorm across various threshold settings for\nboth the MVTrack and GMTD datasets. In most settings,\nMITracker consistently outperforms other methods.\nDuring zero-shot testing on the GMTD, SAM2 and\nSAM2Long perform better under lenient threshold condi-\ntions but lacks the ability to localize objects precisely. Fur-\nthermore, as shown in Figure 8f, MITracker sustains longer\ntracking durations with fewer reinitializations on this un-\nseen dataset.\n10.3. More Ablation Study\nImpact of Input Views. To assess the importance of the\nnumber of views for tracking, we select a fixed camera\nfrom each scenario in the testing set. We then examine how\nmodel performance changes as we increase the number of\nadditional cameras. The results in Table 6 highlight the ben-\nefits of adding more cameras.\nFigure 9 illustrates the challenges faced by the single-\nview model: after a prolonged target disappearance, it mis-\ntracks a white bottle. In contrast, the multi-view model ini-\ntially mistakes a white trash can for the target but quickly\nrecovers and maintains stable tracking with the aid of addi-\ntional views.\nInput views\nAUC(%)\nPNorm(%)\nP(%)\n1\n62.27\n84.71\n73.92\n2\n63.97\n87.07\n76.30\n3\n67.97\n91.50\n80.73\n3/4\n68.65\n92.37\n81.55\nTable 6. Ablation study for the impact of different numbers of\ninput views on MVTrack dataset.\nImpact of Multi-View Training. Our experiment shows\nthat multi-view training improves single-view performance\nby exposing the model to richer spatial information, which\nenhances its ability to handle occlusion and reappearance.\nTable 7 compares results with MITracker SV trained under\nsingle-view settings, highlighting the advantages of multi-\nview training even for single-view scenarios.\nMethod\nAUC (%)\nPNorm (%)\nP (%)\nMITracker SV\n63.42\n82.97\n79.67\nMITracker\n65.96\n87.05\n82.07\nTable 7. Zero-shot performance of single-view results on GMTD.\nImpact of Temporal Token. The temporal token incor-\nporates tracking information from previous frames, Table 8\nhighlights the improvements achieved through the temporal\ntoken.\nTemporal Token\nAUC (%)\nPNorm (%)\nP (%)\n69.30\n89.62\n81.60\n✓\n71.13\n91.87\n83.95\nTable 8. Ablation study for temporal token.\n10.4. More Visualization Results\nWe provide additional visual comparison results as illus-\ntrated in Figure 10 and Figure 11 from the MVTrack\ndataset, and Figure 12 from the GMTD. MITracker exhibits\nenhanced re-tracking capabilities both in multi-view and\nsingle-view scenarios. Furthermore, multi-view informa-\ntion assists in correcting instances of mistracking. To facil-\nitate better visualization, each frame is cropped to a fixed\narea. The IoU curves above further illustrate the tracking\naccuracy by comparing each method’s predictions to the\nground truth.\n\n\n#79\n0\n1\nIoU\n#177\n#290\n#79\n0\n1\nIoU\n#177\n#290\nV1#79\nV1#177\nV1#290\nV4#79\nV4#177\nV4#290\n(a) Two views: pingpong5-1 and pingpong5-4. ODTrack tends to lose track after extended periods of target disappearance, whereas MI-\nTracker demonstrates robust recovery capabilities.\n#415\n0\n1\nIoU\n#521\n#598\nV1#415\nV1#521\nV1#598\n#415\n#521\n#598\nV2#415\nV2#521\nV2#598\n0\n1\nIoU\n(b) Two views: umbrella2-1 and umbrella2-2. Under the interference of a similar object, ODTrack fails to re-track the correct target. In\ncontrast, with the aid of multi-view assistance, MITracker can correct tracking errors from frame V1#415 to #521.\nTarget Invisible\nODTrack\nGT\nMITracker\nFigure 10. Qualitative comparison results on the MVTrack dataset using ODTrack.\n\n\n#493\n0\n1\nIoU\n#562\n#636\nV1#493\nV1#562\nV1#636\n#493\n0\n1\nIoU\n#562\n#636\nV2#493\nV2#562\nV2#636\n#493\n0\n1\nIoU\n#562\n#636\nV4#493\nV4#562\nV4#636\nTarget Invisible\nODTrack\nGT\nMITracker\n(a) Three views: bottle3-1, bottle3-2 and bottle3-4. In V2 #493, MITracker momentarily mistracks a similar object as the target but success-\nfully re-tracks the target by #562. In contrast, ODTrack struggles to recover once it mistracks.\n#242\n#295\n#559\n0\n1\nIoU\nV4#242\nV4#295\nV4#559\nTarget Invisible\nGT\nMITracker\nSAM2Long\n(b) Sequence: book4-4. SAM2Long completely loses the target following disappearances at frames #242 and #295. Upon re-tracking, it\nfails to adapt to target deformation, resulting in diminished IoU by frame #559.\nFigure 11. Qualitative comparison results on the MVTrack dataset using ODTrack and SAM2Long.\n\n\n#1000\n#1260\n#1650\n0\n1\nIoU\n#1000\n#1260\n#1650\n(a) Sequence: cola-2. MITracker demonstrates faster re-tracking capabilities than EVPTrack upon target reappearance.\n#350\n#500\n#550\n0\n1\nIoU\n#350\n#500\n#550\n(b) Sequence: manInOffice-2. EVPTrack fails to correct after mistracking. In contrast, MITracker exhibits superior recovery capabilities, as\ndemonstrated between frames #500 and #550.\nTarget Invisible\nEVPTrack\nGT\nMITracker\nFigure 12. Qualitative comparison results on the GMTD using EVPTrack.\n\n\n"}
{"text": "Joint Fusion and Encoding:\nAdvancing Multimodal Retrieval from the Ground Up\nLang Huang∗Qiyu Wu*†, Zhongtong Miao, Toshihiko Yamasaki\nThe University of Tokyo, Tokyo, Japan\nAbstract\nInformation retrieval is indispensable for to-\nday’s Internet applications, yet traditional se-\nmantic matching techniques often fall short in\ncapturing the fine-grained cross-modal interac-\ntions required for complex queries. Although\nlate-fusion two-tower architectures attempt to\nbridge this gap by independently encoding vi-\nsual and textual data before merging them at\na high level, they frequently overlook the sub-\ntle interplay essential for comprehensive un-\nderstanding. In this work, we rigorously as-\nsess these limitations and introduce a unified\nretrieval framework that fuses visual and tex-\ntual cues from the ground up, enabling early\ncross-modal interactions for enhancing con-\ntext interpretation. Through a two-stage train-\ning process—comprising post-training adapta-\ntion followed by instruction tuning—we adapt\nMLLMs as retrievers using a simple one-tower\narchitecture. Our approach outperforms con-\nventional methods across diverse retrieval sce-\nnarios, particularly when processing complex\nmulti-modal inputs. Notably, the joint fusion\nencoder yields greater improvements on tasks\nthat require modality fusion compared to those\nthat do not, underscoring the transformative po-\ntential of early integration strategies and point-\ning toward a promising direction for contextu-\nally aware and effective information retrieval.\n1\nIntroduction\nRetrieval is a cornerstone task in modern Inter-\nnet systems and artificial intelligence, tradition-\nally built on semantic matching to identify rele-\nvant information from vast datasets. Early retrieval\nmethods (Robertson et al., 1995; Gao et al., 2021;\nWu et al., 2022) focused exclusively on single\nmodalities–primarily text–which yielded effective\nsystems in limited settings but inherently lacked the\n*Equal contribution.\n†Work done in a personal capacity.\n†Correspondence to: {langhuang, yamasaki}@cvm.t.u-\ntokyo.ac.jp and wuqiyu576@gmail.com\nRequire \nFusion \nNo Fusion \n10\n50\n30\n70\n70\n70\n70\n30\n30\n30\nFigure 1: Multi-modal retrieval results on a collection\nof retrieval tasks, categorized as 1) Single-modal, 2)\nCross-modal, 3) Mixed-modal, 4) Multi-modal, and 5)\nConditional. CLIP (Radford et al., 2021) is two-tower\nmodel, UniIR (Wei et al., 2024) is two-legs model with\nlate-fusion, and ours is built as one-tower with early\nfusion. Our approach obtains moderate improvements\nin retrieval tasks requiring no modality fusion compared\nto state-of-the-art two-tower methods, the gains become\nprominent when the tasks need modality fusion that\ninvolve multiple modalities or conditional information\nin the query or candidate. Specific results are in §4.\ncapacity to process multi-modal inputs. As applica-\ntions evolved to require richer, cross-modal infor-\nmation, such as integrating visual content alongside\ntext, the limitations of these conventional meth-\nods became increasingly evident. In response, re-\nsearchers adapted traditional approaches (Radford\net al., 2021) using late-fusion (Wei et al., 2024),\ntwo-tower architectures, where each modality is\nencoded separately and only fused at a high level.\nAlthough this strategy bridges the gap between\nmodalities, it falls short in capturing the nuanced,\nearly interactions, and fine-grained details essential\nfor fully understanding complex user intentions. In\nemerging applications such as retrieval-augmented\ngeneration (Lewis et al., 2020; Gao et al., 2023),\ncontext-aware search (Han et al., 2017; Liu et al.,\n1\narXiv:2502.20008v1  [cs.CV]  27 Feb 2025\n\n\n“Closest upper \ntaxonomy of \nthis plant”\n(a) Two Towers\n(b) Two Legs\n(c) One Tower \nText \nEncoder\nImage \nEncoder\nFusion \nNetwork\n“Closest upper \ntaxonomy of \nthis plant”\n+\nText \nEncoder\nImage \nEncoder\n“Closest upper \ntaxonomy of \nthis plant”\nEarly \nFusion\nDirect Joint-space \nContrastive Learning\nImage Embedding\nText Embedding\nText + Image Embedding\nContrastive Learning\nJoint Fusion\nEncoder\nStage 1. Post-training \nAdaptation\nStage 2. Instruction \nTuning\nFigure 2: Conceptual illustration of multi-modal retrieval frameworks. (a) Traditional two-tower methods encode\ntext and images separately, limiting early cross-modal interactions and often overlooking nuanced user queries. (b)\nTwo-leg methods introduce an additional fusion network to merge single-modal embeddings, enabling more complex\ntasks yet still deferring cross-modal interplay until later stages. (c) Our proposed Joint Fusion Encoder (JFE), an\none-tower method, integrates visual and textual cues from the ground up, unifying the embedding space for a direct\ncontrastive learning and facilitating fine-grained multi-modal understanding. This approach, including two stages:\npost-training adaptation and instruction tuning, simplifies and improves the retriever when complex multi-modal\nunderstanding is required.\n2021a), and conditional search (Vaze et al., 2023),\nthe simple high-level fusion of independent repre-\nsentations is insufficient. Instead, these scenarios\ndemand an integrated approach that can blend vi-\nsual and textual cues from the ground up, thereby\nenabling more precise interpretation of intricate,\nmulti-modal queries.\nIn this work, we propose a unified multi-modal\nretrieval framework, Joint Fusion Encoder (JFE),\nthat seamlessly interweaves encoding and fusion\ninto a single process, fundamentally rethinking the\nway retrieval systems handle complex, heteroge-\nneous queries. As shown in Figure 2, conventional\napproaches retrofit single-modal systems to accom-\nmodate multi-modal tasks by independently en-\ncoding each modality and subsequently merging\ntheir representations–a late-fusion strategy that of-\nten fails to capture the nuanced interplay between\ntextual and visual cues. In contrast, one-tower ar-\nchitecture embraces a joint fusion and encoding\nparadigm, wherein fusion occurs concurrently with\nencoding. This early integration addresses the in-\ntrinsic challenge of harmonizing the disparate char-\nacteristics of multi-modal data, enabling the cap-\nture of fine-grained, inter-modal interactions and\npreserving the modality-specific nuances critical\nfor deciphering intricate user intentions.\nBesides above fundamental benefits (Jang et al.,\n2023; Li and Tang, 2024) of such one-tower\nparadigm for multi-modal representation, recent ad-\nvent of large-scale pre-trained large language mod-\nels (LLMs) and multi-modal LLMs (MLLMs) built\non them, has made the one-tower design become\neven more compelling. By repurposing MLLMs\nand adapting them into effective encoders through\ncontrastive learning and instruction-tuning, our ap-\nproach not only simplifies the retrieval pipeline\nby eliminating the need for separate fusion mod-\nules but also produces unified, powerful embed-\ndings that significantly enhance performance across\na range of multi-modal retrieval tasks. The pro-\nposed approach consists of two key stages: 1)\nPost-training adaptation: We begin by adapting\nthe MLLM, which was originally pre-trained as\nan autoregressive decoder, to function as an en-\ncoder. This is achieved through post-training using\ncontrastive learning loss, allowing the model to\neffectively process inputs into meaningful embed-\ndings; 2) Instruction tuning We then fine-tune the\nmodel with instructive information specific to vari-\nous cross-modal retrieval tasks. This step enables\nthe model to better understand and follow instruc-\ntions in both textual and visual contexts.\nTo thoroughly evaluate the capabilities of multi-\nmodal retrieval models, we have organized exten-\nsive experiments across a comprehensive set of\nbenchmarks, categorized by their input and output\nmodalities: 1) single-modal, both the query and\n2\n\n\ncandidate are in the same modality; 2) cross-modal,\nthe query and candidate are in different modali-\nties; 3) mixed-modal, either query and candidate is\nmulti-modal and the other remains single-modal;\n4) multi-modal, both query and candidate are multi-\nmodal; and 5) conditional, the query is supple-\nmented with additional contextual or conditional\ninformation. As shown in Figure 1, we can observe\nthat while our joint fusion and encoding method\nJFE obtains moderate improvements in standard\nsingle-modal and cross-modal retrieval compared\nto state-of-the-art two-tower methods, the gains\nbecome even prominent when the query and/or can-\ndidate involve multiple modalities or conditional\ninformation, the scenarios attaining mounting atten-\ntion as massive multi-modal contents and instruc-\ntions are produced every second.\n2\nRelated Work\nMulti-modal retrieval serves as a cornerstone for\nmulti-modal information systems. Exisiting studies\nin this field have been mostly based on two-towers\nor two-legs architectures (Radford et al., 2021; Bal-\ndrati et al., 2022; Liu et al., 2023; Koukounas et al.,\n2024) as we shown in Figure 2. A notable ex-\nample is CLIP (Radford et al., 2021), which has\nbeen widely adopted as a foundational representa-\ntion model in multi-modal tasks, including retrieval.\nCLIP employs separate encoders for text and im-\nages and aligns them within a shared embedding\nspace. Building on CLIP, Pic2Word (Saito et al.,\n2023) leverages pseudo language tokens to train a\nmapping network for zero-shot composed image\nretrieval, while SEARLE (Baldrati et al., 2023)\nadopts a similar strategy by pre-training a textual\ninversion network for zero-shot composed image\nretrieval. Additionally, UniIR (Wei et al., 2024) uti-\nlizes score-level fusion and feature-level fusion as\na comprehensive exploration of two-legs architec-\ntures, and improve CLIP/BLIP-based multi-modal\ninformation retrieval systems.\nMeanwhile, there are latest works applying one-\ntower visual LLM, given the strong multi-modal\nunderstanding ability from large-scale pre-training.\nJiang et al. (2024) and Zhang et al. (2024b) em-\nploy visual-language models as its backbone to\nfuse textual and visual spaces into a unified vision-\nlanguage embedding model, and they also intro-\nduces a comprehensive embedding dataset for eval-\nuation on such a complex multimodal scenairo.\nMeanwhile,\nLin et al. (2024) adopts a similar\napproach using a visual-language model for re-\ntrieval, but it focuses on hard-negative sampling\nand leverages an LLM for reranking. Both of these\nconcurrent works leverage visual-language mod-\nels to fuse text and image inputs—thereby adopt-\ning a one-tower architecture similar to ours. They\nimplicitly share our advocation that a one-tower\narchitecture is particularly effective for complex\nvision-language representation and retrieval tasks.\n3\nMethodology\n3.1\nMulti-modal retrieval\nThis section describes multi-modal retrieval, a\ntask designed to retrieve relevant information from\nmulti-modal databases by matching queries and\ncandidates across textual, visual, or combined\nmodalities. Formally, the query set Q and can-\ndidate set C can be defined as follows,\nQ = {q | q ∈{qi, qt, {qi, qt}}}\nC = {c | q ∈{ci, ct, {ci, ct}}}.\n(1)\nDepending on application scenarios as introduced\nin §1, the query can be a text query qt, an im-\nage query qi or a pair of a text and an image, i.e.,\n{qi, qt}. Similarly, ct, ci, {ci, ct} represent text can-\ndidate, image candidate, and Multi-modal candi-\ndate, respectively.\nConventional two-tower models.\nA conven-\ntional two-tower model, e.g., CLIP (Radford et al.,\n2021), encodes the inputs into meaningful embed-\ndings using encoders for texts and images sepa-\nrately, as follows,\nhq-i = fθi(qi) ∈RD\nhq-t = fθt(qt) ∈RD\n(2)\nIn the case where the inputs contain both texts\nand images, a combiner module, notated as gθc, is\ntypically needed to combine the features for tasks,\nsuch as composed image retrieval. The process can\nbe defined as follows,\nhq =\n\n\n\n\n\ngθc(hq-i, hq-t),\nif q = {qi, qt}\nhq-i,\nif q = qi\nhq-t,\nif q = qt\n(3)\nhq ∈RD is the embedding representing the query\nand the embedding of a candidate hc ∈RD can be\nobtained in the same way with identical f and g.\nOptimization objective.\nGiven a batch of pairs,\nB = {{qi, ci}}|B|\ni=1, where qi ∈Q and ci ∈C are\n3\n\n\ntermed query and the targeted candidate. As intro-\nduced in Equations (1), (2) and (3), we obtain em-\nbeddings as {{hq\ni, hc\ni}}|B|\ni=1. The set of parameters\nto be optimized is Θ = {θt, θi, θc}. Contrastive\nlearning objective (Oord et al., 2018) is used to op-\ntimize parameters Θ by minimizing the following\nInfoNCE loss,\nL = −1\n|B|\nX\n1≤i≤|B|\nlog\nexp(hq\ni · hc\ni/τ)\nP\n1≤j≤|B| exp(hq\ni · hc\nj/τ),\n(4)\nwhere τ is a temperature term controlling the dis-\ncrimination to the negative samples.\n3.2\nJoint Fusion Encoder\nWhile conventional two-tower models have signifi-\ncantly transformed the landscape of image-to-text\nand text-to-image retrieval and perform well on\nsimple text-image matching tasks, these models\ncan still be limited because\n• The inability to jointly model visual and language\ndata because of their separate encoding processes.\n• The requirement for task-specific combiners to\nhandle more complex tasks, such as composed\nimage retrieval, due to the single-modal input and\nembeddings.\nUnified encoding and embedding process.\nWe\npropose Joint Fusion Encoder (JFE), which builds\non an MLLM, e.g., (Beyer et al., 2024), as its\nbackbone encoder, unifying the encoding process\nfor both queries and candidates. Given an input,\neither a query q ∈Q or a candidate c ∈C, that\nmay contain multi-modal information, we augment\nthe token sequence by appending a special token\n[Emb] to its end. For example, for a query we\nform the augmented input xq = [q; [Emb]] which\nis processed by the MLLM’s shared Transformer\nencoder:\neq = fθ(xq) = {eq,1, eq,2, . . . , eq,Nq},\n(5)\nwhere Nq is the total number of tokens in the aug-\nmented sequence. We then extract the hidden state\ncorresponding to the [Emb] token as the final query\nembedding:\nhq ≜eq,Nq ∈RD.\n(6)\nSimilarly, for a candidate, we obtain its embedding\nby hc ≜ec,Nc ∈RD. These embeddings eq and ec\nserve as the representations for the contrastive loss\nin Equation (4), which encourages corresponding\nquery–candidate pairs to have similar embeddings\nwhile pushing apart those of unrelated pairs.\nAlthough MLLMs are originally trained on gen-\nerative tasks (e.g., visual question answering) that\nfocus on next-token prediction, they are not natu-\nrally designed to extract discriminative represen-\ntations for retrieval. To address this limitation,\nwe first perform post-training adaptation on the\nbackbone MLLM using large-scale paired data,\nfollowed by instruction tuning. Unlike the con-\nventional two-tower model described in §3.1, this\nunified encoding process eliminates the need for an\nadditional combiner—even when handling multi-\nmodal queries such as in composed image retrieval.\nPost-training Adaptation.\nIn this stage, we fine-\ntune the backbone MLLM using the image-caption\ndatasets to generate retrieval-specific embeddings.\nFor each image-caption pair, we create two training\ninstances by swapping roles: one instance treats\nthe image as the query and the caption as the can-\ndidate, while the other reverses these roles. Each\ninstance is processed by appending a special token\n(e.g., [Emb]) to the input sequence to designate the\nlocation for extracting the final embedding. The\nMLLM then encodes these augmented inputs, and\na contrastive learning objective aligns embeddings\nfrom matching image–caption pairs while distin-\nguishing non-matching pairs. This process ensures\nthat both images and captions are effectively repre-\nsented for retrieval tasks.\nInstruction Tuning.\nRecent studies in text\nretrieval have explored integrating instructions\ninto retrievers to better align with users’ inten-\ntions (Asai et al., 2023; Su et al., 2023). This\nchallenge is more pronounced in Multi-modal re-\ntrieval, where instructive information can be pre-\nsented in both textual and visual modalities for\ndifferent tasks. This complexity necessitates the\nsimultaneous processing of textual and visual in-\nstructive inputs during encoding. To further re-\nfine the MLLM for vision-language retrieval and\nbetter align it with human intent, we incorporate\nexplicit task-specific instructions into the input se-\nquences. In this stage, we leverage the instruction\ndata from UniIR (Wei et al., 2024) to tune the uni-\nfied MLLM for retrieval tasks. Specifically, for an\ninput query q (which may represent either visual or\ntextual content) and its corresponding instruction i,\nwe construct an augmented sequence by appending\nthe instruction to the context along with a special\n4\n\n\ntoken for embedding extraction:\nq′ = [q; i; [Emb]].\n(7)\nHere, [Emb] marks the position from which the\nfinal embedding is extracted. The MLLM pro-\ncesses this combined sequence as a single input\nas in Equations (5) and (6), thereby jointly encod-\ning the primary content and the instructive cues.\nThe training objective (as defined in Equation (4))\nthen aligns the embeddings of matching pairs while\ndistinguishing those of non-matching pairs. This\nintegrated learning approach enables the model to\neffectively interpret and execute retrieval tasks in\naccordance with human instructions.\nData sampling strategy.\nOur instruction data\noriginates from multiple datasets and tasks, each\nexhibiting unbalanced data volumes. Since con-\ntrastive learning is sensitive to both intra-dataset\nand inter-dataset batch composition—relying on\nthe effective mining of negative examples—we de-\nsign a sampling scheme that carefully controls the\nnumber of datasets included in each batch. Em-\npirically, we observe that limiting the number of\ndatasets per batch improves overall performance.\nTo achieve this, we sample the number of datasets\nper batch from a normal distribution (with round-\ning), Nd ∼N(4, 1), thereby ensuring that each\nbatch contains a small, balanced subset of datasets.\nThis strategy helps mitigate data imbalance while\nmaintaining a rich set of negative samples, ulti-\nmately enhancing the robustness of the contrastive\nlearning process.\nSummary.\nThrough the post-training adapta-\ntion and instruction tuning steps, we transform the\nMLLM into a powerful encoder for retrieval tasks.\nThis adapted model excels in encoding multi-modal\ninputs into meaningful embeddings. Our approach\nleverages the MLLM’s inherent ability to under-\nstand multi-modal instructive information, result-\ning in a unified encoding process that seamlessly\nhandles various input types - be it text-only, image-\nonly, or a combination of both.\n4\nExperiments\n4.1\nDatasets\nCC3M (Sharma et al., 2018).\nWe performed\nthe post-training adaption on the CC3M dataset\n(Sharma et al., 2018), which consists of 3.3 mil-\nlion image-text pairs from the web. Using the\nimg2dataset toolbox (Beaumont, 2021), we down-\nload the dataset based on the provided URL-caption\npairs, resulting in approximately 2.8 million image-\ntext pairs due to some expired links.\nM-BEIR (Wei et al., 2024).\nWe instruction-\ntune the models on the M-BEIR dataset, which is\na multimodal retrieval dataset encompassing eight\ntasks and ten datasets across domains like every-\nday imagery, fashion, Wikipedia, and news. It in-\ncludes 1.5 million queries and 5.6 million retrieval\ncandidates, despite being originally designed for\nvarious purposes. These include retrieval-focused\ndatasets (e.g., OVEN (Hu et al., 2023), CIRR (Liu\net al., 2021b), FashionIQ (Wu et al., 2021)),\nimage-caption datasets (e.g., MS-COCO (Lin et al.,\n2014), Fashion200K (Han et al., 2017)), an image-\nsimilarity dataset (NIGHTS\n(Fu et al., 2023)),\nand retrieval-based VQA datasets (InfoSeek (Chen\net al., 2023), WebQA (Chang et al., 2022)). For\neach dataset above, Wei et al. (2024) generated 4 in-\nstructions that describe a multimodal retrieval task\nby intent, domain, query modality, and target can-\ndidate modality. Thanks to its diverse input/output\nformats, MBEIR provides a suitable platform to\ntrain and evaluate multimodal retrieval systems.\n4.2\nTraining setups\nFor most of the experiments in this paper, we de-\nfault to PaliGemma (Beyer et al., 2024) as the\nchoice of MLLMs because of its relatively compact\nsize (∼3B) and competitive performance on var-\nious vision-language understanding benchmarks.\nFollowing the original recipe, the image input of\nthe model is simply scaled to size 256 × 256 and\nfed into a SigLIP vision encoder (a ViT with patch\nsize of 16 × 16) to obtain 256 vision tokens; the\ntext input is tokenized by the sentence piece tok-\nenizer. For training efficiency and reducing GPU\nmemory consumption, we truncate the input token\nwhen the total number of tokens is larger than 384,\nwhich means the maximal length of textual tokens\n(including those of textual instruction) is 128 when\nthe input contains an image or 378 otherwise.\nFor the post-training adaption, we train the\nMLLMs on CC3M (Sharma et al., 2018) datasets\nfor 1 epoch using Low-Rank Adapters (LoRA) (Hu\net al., 2022) with r = 128, α = 256, and a dropout\nprobability 0.05. We use the AdamW (Loshchilov\nand Hutter, 2017) optimizer with a learning rate\nof 2e-4, a batch size of 2048, and no weight de-\ncay for the LoRA training. The learning rate is\n5\n\n\nTable 1: Retrieval results on M-BEIR benchmark (Wei et al., 2024).\nTask\nDataset\nSoTA Zero-Shot\nSingle-task FT\nMulti-task (w/ instruction)\nCLIP\nSigLIP\nBLIP\nBLIP2\nCLIPSF\nBLIPFF\nCLIPSF\nBLIPFF\nOurs\nqt →ct\nWebQA\n36.2\n39.8\n44.9\n38.6\n81.7\n67.5\n84.1\n79.2\n88.7\nqi →ci\nNIGHTS\n26.1\n28.9\n27.4\n25.4\n33.5\n30.4\n31.1\n31.7\n27.8\nSINGLE-MODAL AVERAGE\n31.2\n34.4\n36.2\n32\n57.6\n49.0\n57.6\n55.5\n58.3\nqt →ci\nVisualNews\n43.3\n30.1\n16.4\n16.7\n43.5\n20.0\n42.5\n22.9\n34.6\nMSCOCO\n61.1\n75.7\n74.4\n63.8\n80.4\n77.3\n80.7\n79.5\n78.5\nFashion200K\n6.6\n36.5\n15.9\n14.0\n10.7\n17.1\n18.1\n26.2\n37.2\nqi →ct\nVisualNews\n41.3\n30.8\n17.2\n15.0\n42.7\n22.4\n42.5\n23.1\n33.1\nMSCOCO\n79.0\n88.2\n83.2\n80.0\n89.8\n86.0\n91.8\n90.8\n90.0\nFashion200K\n7.7\n34.2\n19.9\n14.2\n12.0\n15.6\n18.3\n28.6\n36.9\nCROSS-MODAL AVERAGE\n39.8\n49.3\n37.8\n34.0\n46.5\n39.7\n49.0\n45.2\n51.7\nqt →(ci, ct)\nEDIS\n43.3\n27.0\n26.8\n26.9\n58.8\n38.2\n53.6\n49.9\n54.3\nWebQA\n45.1\n43.5\n20.3\n24.5\n76.3\n67.8\n78.3\n78.1\n82.4\n(qi, qt) →ct\nOVEN\n24.2\n29.7\n16.1\n12.2\n45.4\n33.8\n46.0\n42.7\n46.0\nInfoSeek\n20.5\n25.1\n10.2\n5.5\n23.5\n18.5\n27.4\n23.3\n35.6\n(qi, qt) →ci\nFashionIQ\n7.0\n14.4\n2.3\n4.4\n25.9\n3.0\n24.8\n29.2\n31.8\nCIRR\n13.2\n22.7\n10.6\n11.8\n52.0\n13.9\n44.6\n50.7\n54.0\nMIXED-MODAL AVERAGE\n25.6\n27.1\n14.4\n14.2\n47.0\n29.2\n45.8\n45.7\n50.7\n(qi, qt) →(ci, ct)\nOVEN\n38.8\n41.7\n27.4\n27.3\n66.2\n49.9\n68.7\n56.5\n72.7\nInfoSeek\n26.4\n27.4\n16.6\n15.8\n47.4\n32.3\n48.8\n30.4\n61.1\nMULTI-MODAL AVERAGE\n32.6\n34.6\n22.0\n21.55\n56.8\n41.1\n58.8\n43.5\n66.9\nALL AVERAGE\n32.5\n37.2\n26.8\n24.8\n49.4\n37.1\n50.1\n46.4\n54.0\nlinearly warmed-up for the first 3% of training to\nthe specified value and then decayed using a cosine\nannealing schedule (Loshchilov and Hutter, 2016).\nFor the instruction-tuning stage, we first merge\nthe LoRA of the first stage to the base MLLM and\nthen reinitialize and train a new of new LoRA based\non the merged weights. We use r = 256, α =\n512, and a dropout probability 0.3 for LoRA at\nthis stage because we find it beneficial to use more\nparameters to enhance the instruction-following\ncapability (see Tab. 5). We train the LoRA with a\nbatch size of 1024, no weight decay, and a learning\nrate of 2e-4 which is warmed-up and decayed as in\nthe first stage.\n4.3\nEvaluation setups\nWe mainly evaluate JFE on the M-BEIR bench-\nmark (Wei et al., 2024) because it contains a di-\nverse set of input and target modalities and provides\nlarge-scale query and candidate sets (190K queries\nand 5.6M candidates) for reliable evaluations. We\nadopt the settings that perform retrieval from a\ntask-specific pool provided by the original dataset,\nenabling comparison with non-instruction-tuned re-\ntrievers. We report the Recall@5 for all the datasets\nexcept FashionIQ and Fashion 200K, where Re-\ncall@10 is used following Wu et al. (2021).\nIn addition, we also evaluate JFE on conditional\nimage similarity, which measures the capability\nof models not only in encoding the content of the\nquery but also in understanding users’ intent or\nconditions. We use the GeneCIS benchmark (Vaze\net al., 2023), an image-to-image retrieval task con-\nditioned on several keywords. GeneCIS consists\nof four sub-tasks about focusing or changing on a\nspecific attribute or object. For instance, for the\nsub-task about focusing on an object, the models\nneed to find the most relevant image with the same\nobject (specified in the condition) as the query.\n4.4\nSingle-modal and cross-modal retrieval\nWe begin by evaluating our method in single-modal\nand cross-modal settings, where both the query\nand candidate consist of a single modality. Al-\nthough the baseline models are specifically de-\nsigned to handle single-modal inputs, our multi-\nmodal input method slightly outperforms them in\nsingle-modal retrieval, achieving an average score\nof 58.3 compared to 57.6 for CLIPSF and 49.0 for\nBLIPFF. Similarly, in cross-modal retrieval, our\n6\n\n\nTable 2: Conditional Retrieval on GeneCIS benchmark (Vaze et al., 2023).\nMethod\nFocus Attribute\nChange Attribute\nFocus Object\nChange Object\nAvg\nR@1 R@2 R@3 R@1 R@2 R@3 R@1 R@2 R@3 R@1 R@2 R@3 R@1\nPic2Word (Saito et al., 2023)\n12.5\n23.4\n33.7\n11.7\n21.9\n30.9\n9.9\n19.3\n27.4\n8.6\n18.2\n26.1\n10.7\nSEARLE (Baldrati et al., 2023)\n16.3\n29.4\n40.7\n16.2\n27.3\n35.5\n10.8\n18.2\n27.9\n8.3\n15.6\n25.8\n12.9\nCompoDiff (Gu et al., 2023)\n14.3\n26.7\n38.4\n19.7\n28.8\n37.4\n9.2\n19.1\n25.8\n18.7\n31.7\n40.2\n15.5\nCIReVL (Karthik et al., 2024)\n20.5\n34.0\n44.5\n16.1\n28.6\n39.4\n14.7\n25.2\n33.0\n18.1\n31.2\n41.0\n17.4\nLinCIR (Gu et al., 2024)\n19.1\n33.0\n42.3\n17.6\n30.2\n38.1\n10.1\n19.1\n28.1\n7.9\n16.3\n25.7\n13.7\nMagicLens (Zhang et al., 2024a)\n16.6\n28.7\n39.3\n16.0\n27.5\n36.5\n15.7\n27.6\n37.3\n18.7\n31.7\n40.2\n16.7\nCLIPSF (Wei et al., 2024)\n21.1\n33.9\n44.6\n15.1\n27.6\n37.8\n15.0\n25.3\n35.0\n13.6\n24.8\n35.7\n16.2\nBLIPFF (Wei et al., 2024)\n19.4\n32.3\n44.0\n15.8\n26.9\n36.0\n18.0\n28.4\n37.0\n18.5\n29.4\n39.1\n17.9\nOurs\n18.9\n29.6\n40.7\n15.7\n28.0\n36.7\n21.5\n32.7\n40.5\n24.1\n37.9\n48.4\n20.1\nTable 3: Influence of two-stage training.\nStage 1\nStage 2\nRetrieval recall\nSingle\nCross\nMixed\nAverage\n✗\n✗\n6.7\n0.1\n0.0\n1.4\n✓\n✗\n59.3\n54.3\n6.7\n36.3\n✗\n✓\n84.8\n80.5\n44.3\n66.9\n✓\n✓\n88.2\n84.3\n42.3\n68.3\nmethod attains an average score of 51.7, surpass-\ning single-task fine-tuned models (46.5 for CLIPSF\nand 39.7 for BLIPFF) and multi-task models (49.0).\nThese results indicate that even in tasks where\ntwo-tower-based methods typically excel, our uni-\nfied multi-modal approach delivers competitive and\neven slightly superior performance.\n4.5\nMixed-modal and multi-modal retrieval\nWe further evaluate our model in mixed- and multi-\nmodal settings, where both queries and candidates\ncan be arbitrary combinations of images and text.\nFor mixed-modal retrieval, JFE achieves an av-\nerage score of 50.7, significantly outperforming\nmulti-task baselines (45.8 and 45.7) and single-task\nBLIPFF (29.2). In multi-modal tasks, our model\nachieves an average score of 66.9, surpassing both\nsingle-task FT (56.8) and multi-task models (58.8)\nby 8 points. These results demonstrate that, un-\nlike two-tower-based baselines which struggle to\ninterpret multi-modal inputs, our approach effec-\ntively integrates cross-modal and multi-task sig-\nnals, obtaining superior performance in complex\nmulti-modal retrieval scenarios. These experiments\nreiterate the importance of unified models, like JFE,\nfor vision-language retrieval.\n4.6\nConditional retrieval\nFollowing Vaze et al. (2023), we report the Re-\ncall@K, K = {1, 2, 3} for all four sub-tasks, as\nwell as the averaged Recall@1 in Tab. 2. We con-\nduct in-depth comparisons with various state-of-\nthe-art methods, all following the two-tower fash-\nion and potentially with a combiner. This includes\n1) Pic2Word (Saito et al., 2023), SEARLE (Baldrati\net al., 2023) and LinCIR (Gu et al., 2024) that map\nimages into a special text token inserted to the con-\ndition prompts; 2) CIReVL (Karthik et al., 2024)\nthat first captions the image and then merges the\ncaption and the condition using LLMs to a textual\nquery; 3) CompoDiff (Gu et al., 2023) and Magi-\ncLens (Zhang et al., 2024a) which curate or synthe-\nsize composed image retrieval data for training a\ntwo-tower model; and 4) UniIR variants (CLIPSF\nand BLIPFF) that are trained on the same data as\nours. From the table, we observe that JFE deliv-\ners competitive performance across all tasks com-\npared to SOTA methods, with exceptional results\nin object-centric conditions. JFE achieves an aver-\naged Recall@1 of 20.1, significantly outperforming\nall other methods by a large margin, despite not re-\nlying on any task-specific design for conditional\nretrieval. This underscores the effectiveness of us-\ning a unified model to jointly comprehend vision\nand language information. Additionally, although\nBLIPFF shows relatively competitive performance\nwith an average Recall@1 of 17.9, it still falls sig-\nnificantly short of the robustness demonstrated by\nour approach, especially in subtasks involving ob-\nject modification or composition. This reinforces\nthat the improvements achieved by JFE are primar-\nily due to its architectural design rather than any\nadvantage from the data.\n4.7\nAblation analysis\nImpact of Two-stage Training.\nIn Tab. 3, we\nstudy the impact of the two-stage training on re-\ntrieval performance using a subset of M-BEIR.\n7\n\n\nTable 4: Influence of data sampling strategy.\n#Dataset/Batch\nRetrieval recall\nSingle\nCross\nMixed\nAverage\nN/A\n81.1\n78.4\n37.9\n62.8\n2\n75.4\n81.5\n39.5\n63.4\n4\n84.5\n82.0\n41.3\n66.2\n8\n81.8\n77.9\n40.9\n63.9\nN(4, 1)\n84.8\n80.5\n44.3\n66.9\nTable 5: Influence of the hyper-parameters rank (r), α,\nand dropout probability d in LoRA.\nLoRA hyper-param.\nRetrieval recall\nr\nα\nd\nSingle\nCross\nMixed\nAverage\n16\n32\n0.05\n78.3\n77.7\n35.9\n61.1\n128\n256\n0.05\n81.5\n80.2\n38.3\n63.7\n128\n256\n0.3\n81.9\n83.8\n37.3\n64.8\n256\n512\n0.3\n84.8\n80.5\n44.3\n66.9\nWithout any training, the performance of the orig-\ninal MLLMs is no better than random guessing,\nindicating the necessity of carefully designed adap-\ntion steps. Applying stage 1 alone yields a signif-\nicant performance improvement, but falls behind\nstage 2 training alone, which is reasonable con-\nsidering the discrepancy between the CC3M and\nM-BEIR. The combination of both stages results\nin the best retrieval recall especially in the case of\nsingle-modal retrieval, suggesting the benefits of\nthe post-training adaption for tasks involving only\nsingle-modality query/candidate.\nBenefits of data sampling.\nTab. 4 investigates\nhow different data sampling strategies influence\nretrieval performance. Training without any sam-\npling strategy underperforms (62.8 average recall)\nthose with sampling operations, emphasizing the\nnecessity of batch diversity. As the dataset count\nper batch increases, performance improves up to\na peak at 4 datasets per batch. The use of Gaus-\nsian sampling N(4, 1) further improves this result,\nachieving an averaged score of 66.9. This sug-\ngests that properly balancing the data source within\nbatches benefits generalization.\nImpact of the LoRA hyper-parameters and\nbatch size.\nTabs. 5 and 6 explore the impor-\ntance of LoRA hyper-parameters and batch size\non retrieval performance. For LoRA, larger rank\nand scaling factors (r = 256, α = 512) consis-\ntently yield performance gains, achieving the best\nretrieval recall of 66.9. Additionally, moderate\ndropout regularization (0.3) outperforms smaller\nrates (0.05), suggesting that balancing parameter\nTable 6: Influence of the number of training batch size.\nBatch Size\nRetrieval recall\nSingle\nCross\nMixed\nAverage\n256\n80.2\n77.8\n41.4\n63.7\n512\n77.5\n82.9\n41.1\n65.1\n1024\n84.8\n80.5\n44.3\n66.9\nTable 7: Influence of the number of training epochs.\nNum. Epochs\nRetrieval recall\nSingle\nCross\nMixed\nAverage\n2\n87.1\n80.8\n44.6\n67.6\n3\n88.7\n84.3\n42.9\n68.6\n5\n86.9\n84.2\n44.0\n68.6\ncomplexity and overfitting is crucial for robust per-\nformance. Meanwhile, batch size plays a critical\nrole, with a larger batch size of 1024 reaching 66.9\naveraged score, showing that batch size is partic-\nularly impactful as larger batches typically lead\nto better negative sampling and stronger represen-\ntation learning. However, due to GPU memory\nlimitations, batch sizes beyond 1024 could not be\ntested. We would expect performance to further\nimprove with larger batch sizes.\nScaling the number of training epochs.\nTab. 7\nanalyzes how the number of training epochs affects\nretrieval performance. From the table, we can see\nthat the performance considerably increases from\n67.6 at 2 epochs to 68.6 at 3 epochs. Beyond this,\nthe performance plateaus, as both 3 and 5 epochs\nyield the same overall score (68.6 recall). These re-\nsults suggest that training saturation occurs beyond\n3 epochs, where additional epochs provide dimin-\nishing returns on retrieval effectiveness. We default\nto 3 epochs in our experiments for a good trade-off\nbetween performance and training efficiency.\n5\nConclusion\nIn this work, we introduced JFE, a one-tower\nmulti-modal retrieval framework that integrates fu-\nsion directly into the encoding process. By adapt-\ning MLLMs into effective encoders through post-\ntraining adaptation and instruction-tuning, JFE cap-\ntures fine-grained interactions between visual and\ntextual cues from the ground up. Our extensive\nevaluations demonstrate that JFE achieves moder-\nate gains in standard single-modal and cross-modal\nretrieval, its performance improvements become\nparticularly pronounced in complex scenarios in-\nvolving multi-modal or conditional queries, where\nthe modality fusion is required. Overall, the find-\n8\n\n\nTable 8: Retrieval results on M-BEIRglobal\nZero-shot\nMulti-task (w/ instruction)\nMulti-task (w/o instruction)\nTask\nDataset\nBLIP2\nCLIPSF\nBLIPFF\nCLIPSF\nBLIPFF\nOurs\n1. qt →ci\nVisualNews\n0.0\n12.7\n8.3\n42.2\n22.5\n31.5\nMSCOCO\n0.0\n27.3\n27.7\n71.4\n65.3\n62.1\nFashion200K\n0.0\n5.9\n9.0\n18.0\n26.1\n35.6\n2. qt →ct\nWebQA\n35.2\n82.3\n76.1\n83.5\n78.5\n87.6\n3. qt →(ci, ct)\nEDIS\n0.0\n41.1\n36.0\n52.7\n49.3\n51.7\nWebQA\n0.0\n68.2\n74.7\n77.5\n77.1\n81.0\n4. qi →ct\nVisualNews\n0.0\n12.1\n4.9\n38.8\n21.1\n30.3\nMSCOCO\n0.0\n84.6\n76.9\n91.4\n89.8\n89.0\nFashion200K\n0.0\n1.2\n3.6\n18.2\n27.4\n30.9\n5. qi →ci\nNIGHTS\n24.0\n31.0\n31.3\n39.5\n31.6\n27.8\n6. (qi, qt) →ct\nOVEN\n0.0\n36.8\n37.7\n22.2\n39.5\n42.4\nInfoSeek\n0.0\n18.3\n17.8\n24.6\n19.8\n31.9\n7. (qi, qt) →ci\nFashionIQ\n3.9\n22.8\n28.1\n43.1\n28.9\n31.1\nCIRR\n6.2\n32.0\n45.1\n59.8\n48.3\n50.4\n8. (qi, qt) →(ci, ct)\nOVEN\n13.8\n58.7\n51.6\n44.3\n55.9\n69.1\nInfoSeek\n11.4\n42.3\n25.4\n44.3\n26.2\n57.4\nAverage\n5.9\n36.1\n34.6\n47.4\n44.2\n50.6\nings underscore the superiority of joint fusion and\nencoding for advanced multi-modal retrieval appli-\ncations with inputs requiring complex multi-modal\nunderstanding.\nLimitations\nThis work is built upon large-scale pre-trained mod-\nels rather than developing two-tower and one-tower\narchitectures from scratch. Although these pre-\ntrained models have been widely adopted in the\ncommunity, their use introduces two primary limi-\ntations. First, the influence of the pre-trained mod-\nels cannot be fully isolated—since both the pre-\ntraining approach and the underlying data have not\nbeen entirely publicly disclosed, their contributions\nremain a confounding factor. Second, there are effi-\nciency concerns, particularly for retrieval tasks that\ndemand fast online inference. These issues could\nbe mitigated by further advances in efficient large\nmodels and the development of more streamlined\nbackbone architectures.\nEthical Statement\nThis research focuses on the daily task of informa-\ntion retrieval, which in itself does not pose ethi-\ncal concerns. Our approach employs an encoding\nmodel to compress information, thereby mitigat-\ning the risk of inappropriate data generation. All\ndatasets and pre-trained checkpoints used in this\nstudy are publicly available with free use for re-\nsearch and remain unaltered. However, as is com-\nmon in much of today’s AI research, the perfor-\nmance of large AI models is not yet fully under-\nstood. Our evaluation is limited to academic bench-\nmarks, and we do not endorse their deployment in\npractical applications at this stage.\nAppendix\nA\nAdditional Experimental Results\nWe also adopt an alternative retrieval setting as\ndescribed in (Wei et al., 2024), which conducts re-\ntrieval from a pool of 5.6 million candidates aggre-\ngated from eight tasks across ten M-BEIR datasets.\nAs demonstrated in Table 8, despite the signif-\nicantly varied evaluation settings, JFE not only\nachieves moderate improvements over baselines in\nsingle-modal and cross-modal retrieval but also de-\nlivers substantial gains in mixed-modal and multi-\nmodal scenarios—settings that are increasingly rel-\nevant in our multi-modal content-rich world. These\nresults further corroborate our findings in §4 and\nunderscore the advantages of the Joint Fusion and\nEncoding paradigm.\n9\n\n\nReferences\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen,\nGautier Izacard, Sebastian Riedel, Hannaneh Ha-\njishirzi, and Wen-tau Yih. 2023. Task-aware retrieval\nwith instructions. In Findings of the Association for\nComputational Linguistics: ACL 2023, pages 3650–\n3675, Toronto, Canada. Association for Computa-\ntional Linguistics.\nAlberto Baldrati, Lorenzo Agnolucci, Marco Bertini,\nand Alberto Del Bimbo. 2023. Zero-shot composed\nimage retrieval with textual inversion. In Proceed-\nings of the International Conference on Computer\nVision.\nAlberto Baldrati, Marco Bertini, Tiberio Uricchio, and\nAlberto Del Bimbo. 2022. Effective conditioned and\ncomposed image retrieval combining clip-based fea-\ntures. In 2022 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 21434–\n21442.\nRomain Beaumont. 2021. img2dataset: Easily turn\nlarge sets of image urls to an image dataset. https:\n//github.com/rom1504/img2dataset.\nLucas Beyer, Andreas Steiner, André Susano Pinto,\nAlexander Kolesnikov, Xiao Wang, Daniel Salz,\nMaxim Neumann, Ibrahim Alabdulmohsin, Michael\nTschannen, Emanuele Bugliarello, et al. 2024.\nPaligemma: A versatile 3b vlm for transfer. arXiv\npreprint arXiv:2407.07726.\nYingshan Chang, Mridu Narang, Hisami Suzuki, Gui-\nhong Cao, Jianfeng Gao, and Yonatan Bisk. 2022.\nWebqa: Multihop and multimodal qa. In Proceed-\nings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 16495–16504.\nYang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit\nChangpinyo, Alan Ritter, and Ming-Wei Chang. 2023.\nCan pre-trained vision and language models answer\nvisual information-seeking questions? In Proceed-\nings of the 2023 Conference on Empirical Methods in\nNatural Language Processing, pages 14948–14968.\nStephanie Fu, Netanel Y Tamir, Shobhita Sundaram,\nLucy Chai, Richard Zhang, Tali Dekel, and Phillip\nIsola. 2023. Dreamsim: learning new dimensions\nof human visual similarity using synthetic data. In\nAdvance on Neural Information Processing Systems,\npages 50742–50768.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimCSE: Simple contrastive learning of sentence em-\nbeddings. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 6894–6910, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,\nJinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen\nWang. 2023. Retrieval-augmented generation for\nlarge language models: A survey. arXiv preprint\narXiv:2312.10997.\nGeonmo Gu, Sanghyuk Chun, Wonjae Kim, , Yoohoon\nKang, and Sangdoo Yun. 2024. Language-only train-\ning of zero-shot composed image retrieval. In Pro-\nceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition.\nGeonmo Gu, Sanghyuk Chun, Wonjae Kim, HeeJae Jun,\nYoohoon Kang, and Sangdoo Yun. 2023. Compod-\niff: Versatile composed image retrieval with latent\ndiffusion. arXiv preprint arXiv:2303.11916.\nXintong Han, Zuxuan Wu, Phoenix X Huang, Xiao\nZhang, Menglong Zhu, Yuan Li, Yang Zhao, and\nLarry S Davis. 2017. Automatic spatially-aware fash-\nion concept discovery. In Proceedings of the IEEE\nInternational Conference on Computer Vision, pages\n1463–1471.\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu,\nYuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,\net al. 2022. Lora: Low-rank adaptation of large lan-\nguage models. In International Conference on Learn-\ning Representations.\nHexiang Hu, Yi Luan, Yang Chen, Urvashi Khandel-\nwal, Mandar Joshi, Kenton Lee, Kristina Toutanova,\nand Ming-Wei Chang. 2023. Open-domain visual\nentity recognition: Towards recognizing millions of\nwikipedia entities. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages\n12065–12075.\nJiho Jang, Chaerin Kong, Donghyeon Jeon, Seonhoon\nKim, and Nojun Kwak. 2023.\nUnifying vision-\nlanguage representation space with single-tower\ntransformer. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 37, pages 980–988.\nZiyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz,\nYingbo Zhou, and Wenhu Chen. 2024. Vlm2vec:\nTraining\nvision-language\nmodels\nfor\nmassive\nmultimodal embedding tasks.\narXiv preprint\narXiv:2410.05160.\nShyamgopal Karthik, Karsten Roth, Massimiliano\nMancini, and Zeynep Akata. 2024.\nVision-by-\nlanguage for training-free compositional image re-\ntrieval. In International Conference on Learning\nRepresentation.\nAndreas Koukounas, Georgios Mastrapas, Bo Wang,\nMohammad\nKalim\nAkram,\nSedigheh\nEslami,\nMichael Günther, Isabelle Mohr, Saba Sturua, Scott\nMartens, Nan Wang, et al. 2024. jina-clip-v2: Multi-\nlingual multimodal embeddings for text and images.\narXiv preprint arXiv:2412.08802.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459–9474.\nSongtao Li and Hao Tang. 2024. Multimodal align-\nment and fusion:\nA survey.\narXiv preprint\narXiv:2411.17040.\n10\n\n\nSheng-Chieh Lin, Chankyu Lee, Mohammad Shoeybi,\nJimmy Lin, Bryan Catanzaro, and Wei Ping. 2024.\nMm-embed: Universal multimodal retrieval with\nmultimodal llms. arXiv preprint arXiv:2411.02571.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C Lawrence Zitnick. 2014.\nMicrosoft coco:\nCommon objects in context. In Computer Vision–\nECCV 2014: 13th European Conference, Zurich,\nSwitzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740–755. Springer.\nZhenghao Liu, Chenyan Xiong, Yuanhuiyi Lv, Zhiyuan\nLiu, and Ge Yu. 2023. Universal vision-language\ndense retrieval: Learning a unified representation\nspace for multi-modal retrieval. In The Eleventh In-\nternational Conference on Learning Representations.\nZheyuan Liu, Cristian Rodriguez-Opazo, Damien Teney,\nand Stephen Gould. 2021a. Image retrieval on real-\nlife images with pre-trained vision-and-language\nmodels. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision (ICCV), pages\n2125–2134.\nZheyuan Liu, Cristian Rodriguez-Opazo, Damien Teney,\nand Stephen Gould. 2021b.\nImage retrieval on\nreal-life images with pre-trained vision-and-language\nmodels. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 2125–\n2134.\nIlya Loshchilov and Frank Hutter. 2016. Sgdr: Stochas-\ntic gradient descent with warm restarts.\narXiv\npreprint arXiv:1608.03983.\nIlya Loshchilov and Frank Hutter. 2017.\nDecou-\npled weight decay regularization.\narXiv preprint\narXiv:1711.05101.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018.\nRepresentation learning with contrastive predictive\ncoding. arXiv preprint arXiv:1807.03748.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from\nnatural language supervision. In International con-\nference on machine learning, pages 8748–8763.\nStephen E Robertson, Steve Walker, Susan Jones,\nMicheline M Hancock-Beaulieu, Mike Gatford, et al.\n1995. Okapi at trec-3. Nist Special Publication Sp,\n109:109.\nKuniaki Saito, Kihyuk Sohn, Xiang Zhang, Chun-Liang\nLi, Chen-Yu Lee, Kate Saenko, and Tomas Pfister.\n2023. Pic2word: Mapping pictures to words for zero-\nshot composed image retrieval. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern\nRecognition.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\nRadu Soricut. 2018. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic im-\nage captioning. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 2556–2565.\nHongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang,\nYushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A.\nSmith, Luke Zettlemoyer, and Tao Yu. 2023. One\nembedder, any task: Instruction-finetuned text em-\nbeddings. In Findings of the Association for Compu-\ntational Linguistics: ACL 2023, pages 1102–1121,\nToronto, Canada. Association for Computational Lin-\nguistics.\nSagar Vaze, Nicolas Carion, and Ishan Misra. 2023.\nGenecis: A benchmark for general conditional image\nsimilarity. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition.\nCong Wei, Yang Chen, Haonan Chen, Hexiang Hu,\nGe Zhang, Jie Fu, Alan Ritter, and Wenhu Chen.\n2024.\nUniir: Training and benchmarking univer-\nsal multimodal information retrievers. In European\nConference on Computer Vision, volume 15145 of\nLecture Notes in Computer Science, pages 387–404.\nSpringer.\nHui Wu, Yupeng Gao, Xiaoxiao Guo, Ziad Al-Halah,\nSteven Rennie, Kristen Grauman, and Rogerio Feris.\n2021. Fashion iq: A new dataset towards retrieving\nimages by natural language feedback. In Proceedings\nof the IEEE/CVF Conference on Computer Cision\nand Pattern Recognition, pages 11307–11317.\nQiyu Wu, Chongyang Tao, Tao Shen, Can Xu, Xiubo\nGeng, and Daxin Jiang. 2022. PCL: Peer-contrastive\nlearning with diverse augmentations for unsupervised\nsentence embeddings. In Proceedings of the 2022\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 12052–12066, Abu Dhabi,\nUnited Arab Emirates. Association for Computa-\ntional Linguistics.\nKai Zhang, Yi Luan, Hexiang Hu, Kenton Lee, Siyuan\nQiao, Wenhu Chen, Yu Su, and Ming-Wei Chang.\n2024a. Magiclens: Self-supervised image retrieval\nwith open-ended instructions. In International Con-\nference on Machine Learning.\nXin Zhang, Yanzhao Zhang, Wen Xie, Mingxin Li, Ziqi\nDai, Dingkun Long, Pengjun Xie, Meishan Zhang,\nWenjie Li, and Min Zhang. 2024b. Gme: Improving\nuniversal multimodal retrieval by multimodal llms.\narXiv preprint arXiv:2412.16855.\n11\n\n\n"}
{"text": "1\nSixth-Sense: Self-Supervised Learning\nof Spatial Awareness of Humans from a Planar Lidar\nSimone Arreghini, Nicholas Carlotti, Mirko Nava, Antonio Paolillo, and Alessandro Giusti\nAbstract—Localizing humans is a key prerequisite for any\nservice robot operating in proximity to people. In these scenarios,\nrobots rely on a multitude of state-of-the-art detectors usually\ndesigned to operate with RGB-D cameras or expensive 3D\nLiDARs. However, most commercially available service robots\nare equipped with cameras with a narrow field of view, making\nthem blind when a user is approaching from other directions,\nor inexpensive 1D LiDARs whose readings are difficult to inter-\npret. To address these limitations, we propose a self-supervised\napproach to detect humans and estimate their 2D pose from\n1D LiDAR data, using detections from an RGB-D camera as a\nsupervision source. Our approach aims to provide service robots\nwith spatial awareness of nearby humans. After training on 70\nminutes of data autonomously collected in two environments, our\nmodel is capable of detecting humans omnidirectionally from 1D\nLiDAR data in a novel environment, with 71% precision and\n80% recall, while retaining an average absolute error of 13 cm\nin distance and 44◦in orientation.\nIndex Terms—Self-Supervised Learning, Human Perception,\nService Robotics, Human-Robot Interaction.\nI. INTRODUCTION\nS\nERVICE\nrobots\noperating\nhuman-populated\nenviron-\nments [1]–[3] must be capable to perceive humans, predict\ntheir behavior [4] and even intentions [5]. Indeed, humans’\ndetection and localization are crucial for Human-Robot Inter-\naction (HRI) applications where humans and robots are close,\nto ensure safe cooperation, improve navigation in crowded\nspaces, and provide insightful social cues. A typical sensing\nsetup for service robots includes a combination of a wide Field\nof View (FOV) laser sensor, often seeing 360◦around the\nrobot, and a narrow FOV RGB-D camera. Despite 3D LiDAR\nsensors offering rich information about the surroundings [6],\ntheir use in service robots is limited to a small percent-\nage of high-end platforms. Instead, the majority is equipped\nwith simpler 1D planar LiDAR sensors, typically located at\nwheel height, striking a balance between production costs and\nrichness of information. Examples span from large research\nrobots [7] to those commonly present in many households\nsuch as robot vacuum cleaners, or lawnmowers [8].\nDeep learning models capable of reliably detecting and\nlocalizing humans from RGB-D data have been extensively\ninvestigated in literature [9], [10]; however, doing the same\nwith 1D laser sensors remains an open problem due to\nthe sparse nature and complex interpretation of the sensor’s\nAll authors are with the Dalle Molle Institute for Artificial Intelligence\n(IDSIA), USI-SUPSI, Lugano, Switzerland. Corresponding author: Simone\nArreghini, simone.arreghini@supsi.ch\nThis work was supported by the European Union through the project SER-\nMAS, by the Swiss State Secretariat for Education, Research and Innovation\n(SERI) under contract number 22.00247, and by the Swiss National Science\nFoundation, grant number 213074.\nFig. 1.\nOur approach uses a human detector from the narrow FOV Azure\nKinect as a source of labels to train a 1D FCN that, given planar LiDAR\nscans, predicts the presence and relative 2D pose of humans around the robot.\nThe training approach relies only on hardware onboard the robot and can\nautonomously collect data in any environment. In this environment (Lab), a\nMotion Capture system collects ground truth used for evaluation purposes.\nreadings. Notably, the typical environment for a service robot\nmay feature structures that result in readings mimicking the\nprofile of human beings, such as the legs of tables, bars\nin railings, or pets; differentiating humans from the rest\nrequires recognizing subtle geometric and dynamic patterns.\nNonetheless, awareness of the presence and direction of nearby\nhumans significantly improves the robot’s behavior in social\ncontexts [11], even when these perceptions are uncertain and\npossibly inaccurate. Indeed, inspired by animal perception\nwhere peripheral vision and hearing direct visual focus towards\nareas of interest [12], a robot could use uncertain detections\nfrom 1D LiDAR data to trigger further sensing by the more\nreliable RGB-D camera. Unlike previous approaches that rely\non 3D LiDAR [13], [14], we handle a less informative sensing\nmodality, leading to inherently lower detection accuracy but\nwith a potential widespread adoption by many robotic plat-\nforms.\nOur approach relies on a deep learning model that can\nbe trained, or fine-tuned, directly by the robot during its\ndeployment using self-supervision: an off-the-shelf detector\nreceiving data from the onboard RGB-D camera is used to\nprovide detections of humans considered as training labels,\nas shown in Fig. 1. This is an instance of the general class\nof approaches using one sensor to supervise the training of a\nmodel which interprets data from a different sensor [15], [16];\nthe same paradigm has been applied to skeleton joint pose esti-\narXiv:2502.21029v1  [cs.RO]  28 Feb 2025\n\n\n2\nmation from 3D LiDAR, using an image-based human detector\nproviding the 2D skeleton joints pose as supervision [17].\nOur model receives as input a moving time window of\n1D LiDAR scans and predicts the presence and 2D pose\n(2D position and relative bearing) of humans around the\nrobot. Specifically, we employ a loss function to minimize the\ndistance between the model’s predicted detections from 1D\nLiDAR and those coming from the detector, enforced only\nwhere the two fields of view overlap. In this context, the self-\nsupervised learning paradigm: allows the model to adapt to the\nspecific deployment environment and sensor characteristics;\neliminates the need for large-scale pre-collected datasets; and\nis robust to cluttered environments that generate scans falsely\nmimicking human presence. Further, we adopt a 1D FCN [18]\nand leverage its translation-invariance to extend the detection\nability to the wider FOV of the LiDAR sensor, even in\ndirections never covered by the camera.\nWe describe our main contribution in Section II: a practical\nmethodology and open-source implementation for training\nand running a human detector and pose estimator from 1D\nLiDAR data using camera detections as self-supervision. As a\nsecondary contribution, datasets collected using this setup, as\nwell as the pre-trained models resulting from our approach, are\nmade publicly available. Section III details the experimental\nsetup comprising our platform, sensing setup, data collection,\nand network architecture. Section IV presents the quantitative\nanalysis of our approach’s performance against the ground\ntruth, while deployed on a TIAGo robot equipped with an\nAzure Kinect camera and two 1D LiDAR sensors. Section V\nconcludes the article with final remarks and a discussion on\nfuture research directions.\nII. METHODOLOGY\nWe train a 1D FCN model on the task of estimating 2D\nposes of humans around the robot, giving as input a time\nwindow of readings from a planar LiDAR sensor located at\nthe center of the robot base with a uniform angular resolution\nacross the entire FOV. In practice, we use a tensor of 360\nelements representing rays equally spaced around the robot\nwith a resulting angular resolution of 1◦, and having n\nchannels as the time window history length. We reproject past\nmeasurements as if they were captured from the robot’s current\nposition accounting for the robot’s motion as estimated by its\nFig. 2.\nWalking people and static structures as perceived by the LiDAR:\nlighter shades of green indicate older scans in the temporal window, whereas\nblack arrows indicate the people’s instantaneous orientation.\nFig. 3. Training data is autonomously collected by the robot in a University\nCorridor, on the top, and a Break Area, on the bottom.\nodometry. As a result, static obstacles yield overlapping points\nacross the channels, while moving obstacles leave trail-like\npoint patterns, as shown in Fig. 2. The model is tasked to\npredict the presence of humans in the environment, and their\ndistance and bearing relative to the robot. For each ray, our\nmodel predicts: the scalar ˆp ∈[0, 1] representing the likelihood\nof human presence; the relative distance ˆd ∈[dmin, dmax],\nwhere dmin and dmax are the working range of the sensor;\nand the sine and cosine of the relative bearing ˆo ∈[−π, π]\nexpressed as the difference between the person’s orientation\nand the direction parallel to the ray, with zero indicating a\nperson directly facing the robot. During inference, a discrete\nset of detections is obtained by thresholding and applying Non-\nmaxima Suppression (NMS) to the model’s predicted presence.\nWe aim to extend the detection ability of the model from\nthe narrow area covered by the camera’s FOV to the wider\nLiDAR’s FOV, including areas where the supervision is scarce\nor absent, such as behind the robot. To this end, we rely on\nthe translational invariance of convolutions, in which patterns\nare detected regardless of their position within the input.\nThe self-supervision signal is derived from the front-facing\nonboard camera providing the 3D pose of people’s joints in\nthe camera’s FOV. Among the joints, we specifically select the\none located at the pelvis as it’s closely tied to the legs’ motion,\nand project its pose onto the horizontal plane to get 2D poses.\nLabels for human presence p, distance d, and relative bearing\no are obtained from such poses for each ray of the LiDAR\nscan intersecting a person. The presence p is set to 1 when a\nperson is detected along the ray, or 0 otherwise; accordingly,\nthe relative distance and bearing labels of people are assigned\nto rays in which they are detected, or left undefined otherwise.\nOur model is trained to regress the three components using\na masked loss, considering only errors generated from the\nrays corresponding to the area covered by the camera’s FOV.\nAdditionally, distance and orientation losses are computed\nonly for rays in which the supervision labels indicate the\npresence of humans.\n\n\n3\nIII. EXPERIMENTAL SETUP\nA. Hardware\nWe use a customized version of the PAL Robotics TIAGo\nrobot composed of a differential drive base, a torso with a\nprismatic joint, a 7 degrees-of-freedom manipulator, and a\nhead that can pan in the range ±75◦and tilt from −60◦to\n45◦w.r.t. the horizontal plane. Our TIAGo is equipped with\nadditional sensors to better suit HRI applications: a Microsoft\nAzure Kinect RGB-D camera located in the head reliably\ntracks humans up to 6 m [10], has a narrow horizontal FOV\nof 65◦, and frame rate of 15 Hz; a secondary LiDAR sensor\nmounted on the back of the robot’s base, in addition to the\nbuilt-in one located on the front, as shown in Fig. 1. The\nfront-facing LiDAR is at an height of 95 mm, has a FOV of\n190◦, and scan rate of 15 Hz; the back-facing one is at an\nheight of 329 mm, FOV of 255◦, and scan rate of 10 Hz; both\nsensors’ operating range goes from 0.05 m to 10 m. To obtain a\nsingle, omnidirectional, and radially symmetric sensor, we fuse\nthe two physical LiDARs into a virtual one: the two sensors’\nreadings are time-synchronized at a rate of 10 Hz, projected\nonto the 2D plane, expressed in the frame of the robot base,\nand aggregated into 1◦-wide bins; each bin is assigned the\nvalue of the closest point among its members; when there are\nno members, a default value of 10 m is used.\nB. Dataset\nWe collected data across 9 days in three environments:\nUniversity Corridor (shown in Fig. 3, top), a public transit\narea between classrooms with study desks on the side and\npassers-by (36k samples); Break Area (Fig. 3, bottom), a\nlarge room with tables and chairs where expert individuals\ninteract with the robot (12k samples); and Lab (Fig. 1), a\nlaboratory setting with expert individuals interacting with the\nrobot (7k samples). During data collection, the robot’s base\nmotion and the head panning are randomized to increase the\ndata variability and area covered by the camera. The robot\nbase is manually controlled in the University Corridor for\nsecurity reasons whereas, in the other environments, it moves\nautonomously following random trajectories while avoiding\ncollisions. In all environments, we record body joints from the\nAzure Kinect and scans from the two LiDARs. Additionally,\nthe Lab environment provides ground truth poses for people\nand the robot at 100 Hz from an OptiTrack motion capture\nsystem featuring 18 cameras. The data is split into a training\nset composed of all samples from University Corridor and half\nof those from Break Area, totaling 42k samples; the remaining\n6k samples from Break Area are used as validation set, and\nthe 7k samples from Lab are used as the test set.\nC. Model architecture\nOur model, depicted in Fig. 4, is composed of 7 1D\nconvolutional layers, each with 32 output channels and layer\nnormalization. It features dilated circular convolutions with in-\ncreasing kernel dimension from 3 to 7, resulting in a receptive\nfield of 43◦. Specifically, dilation is used to have a low model\ncomplexity while achieving a receptive field large enough to\nLaser Sensor Readings \nRobot Front / \nAngle\n7 Convolutional Layers\nti\nti-n+1\n65° Camera FOV\nMasked MSE loss\n43° Receptive Field\n Presence\n Distance\n Cosine\n Sine\n Presence\n Distance\n Cosine\n Sine\nOutputs\nLabels\nAngle\nFig. 4.\nOur model uses a temporal window of n LiDAR scans to predict\nthe presence p of nearby people, their distance d, and relative bearing o\n(represented by sine and cosine). Dilated circular convolutions handle omni-\ndirectional scans and yield a 43◦receptive field. A masked MSE loss is only\nenforced on predictions that overlap with the camera FOV (red shaded area).\nFig. 5. Left: Precision-Recall curve for detection. Right: relative bearing error\ndistribution. Results computed against mocap ground truth in the Lab test set.\neffectively capture human motion. During training, the model\nminimizes a squared error between predictions and label values\nof each output. We train our model for 500 epochs at a constant\nlearning rate of 3e−4 using the Adam optimizer [19] and select\nthe model weights resulting in the lowest validation loss. We\napply additive Gaussian noise and mirroring to the input as\ndata augmentations during model training.\nIV. RESULTS\nWe compare our model using the last n = 30 scans collected\nat 10 Hz over three seconds, called History, with an ablated\nbaseline named No History that uses only the current sensor\nreading as input (n = 0). All models are tested using the\nground truth poses collected in the Lab environment, as shown\nin Fig. 1. On the left of Fig. 5, we report the Precision-Recall\ncurve illustrating the human detection performance of our\nmodels. This curve is generated by progressively increasing the\ndetection threshold applied to the presence output ˆp. Human’s\npredicted positions are obtained by projecting a point in the\ndirection of the corresponding ray at the estimated distance\nˆd. A prediction is considered a match (true positive) when it\nhas an Euclidean distance smaller than 50 cm w.r.t. the ground\n\n\n4\n2 m\nHumans\nMotion Capture\nAzure Kinect\nTrue Positive (TP)\nTP\nTP\nTP\nFalse Positive (FP)\n2 m\nSensor\nLiDAR\nAzure Kinect FOV\nModel Output\nRejected\nAccepted\nTP\nFP\nFN\nFN\nFig. 6. Results on the test set: third-person view of two frames (left) and corresponding top view (right) depicting the LiDAR scan; camera FOV and detected\npose arrows; Motion Capture ground truth pose arrows. The predicted presence ˆp is shown as a gray line when below the detection threshold of 90% (dashed\ncircle centered on the robot), or black otherwise. Predictions are represented by arrows colored differently for true positives (TP), and false positives (FP).\ntruth; the orientation component does not influence the match-\ning procedure. The plot shows that, for recall values above\n60%, the History model (in purple) consistently outperforms\nNo History (in light blue). On the right of Fig. 5, we compare\nthe orientation error distributions. We include a Dummy model\nthat always returns the average ground truth orientation and\ndistance in the test set. Errors are computed only for matched\npredictions (true positives), following the same approach used\nfor the Precision-Recall curve; for the Dummy model, instead,\nwe consider the whole test set, i.e., assuming an ideal detec-\ntion. Results show that temporal information is required for\nrelative bearing estimation: History yields a mean absolute\norientation error of 44◦compared to No History with 74◦\nand Dummy with 75◦; this result confirms the findings in the\nliterature on the human motion model [20], [21].\nThe qualitative performance of our approach on detection\nand 2D pose estimation are shown in Fig. 6, with the TIAGo\nrobot deployed in the Lab environment. Failed detections\nderive from the choice of threshold yielding false negatives for\nhigh predictions that fall below the threshold. Table I summa-\nrizes the models’ performance with the following metrics: for\nhuman detection, we report the precision on the true positive\npredictions corresponding to the threshold yielding a recall of\n80%, represented as P80, where the History model outperforms\nNo History by scoring P80 = 70.6% against P80 = 60.7%. On\nthe true positive predictions, we additionally measure the mean\norientation Eo and distance Ed absolute errors.\nTABLE I\nMODELS’ PERFORMANCE ON HUMAN DETECTION P80, MEAN RELATIVE\nBEARING ERROR EO, AND MEAN DISTANCE ERROR ED ON THE TEST SET.\nModel\nP80\nEo\nEd\nBarplot for P80 [%] →\n[%] ↑\n[deg] ↓\n[cm] ↓\nDummy\n−\n75\n64\n40\n60\n80\n100\nNo History\n60.7\n74\n10\nHistory\n70.6\n42\n13\nV. CONCLUSION\nWe presented a novel approach for human detection and\npose estimation in service robots using 1D LiDAR sensors. It\nleverages a state-of-the-art detector from an RGB-D camera\nas the source of self-supervision, requiring no pre-collected\ndatasets. This approach can adapt to different sensing setups,\nassuming only to have a precise albeit narrow source of\nsupervision for interpreting readings from a much wider FOV,\npossibly omnidirectional sensor. The code required to collect\ndata, train models, and run them in real time is made publicly\navailable for the benefit of the community; we also provide\npre-trained model weights and datasets. Future work will focus\non the use of pretext tasks to leverage unlabeled data at training\ntime, and extend the model predictions with more complex\nsocial cues, such as the intention to interact, which could be\nhighly beneficial in human-robot interaction scenarios.\n\n\n5\nREFERENCES\n[1] F. B. V. Benitti, “Exploring the educational potential of robotics in\nschools: A systematic review,” Comp. & Education, vol. 58, no. 3, pp.\n978–988, 2012.\n[2] C. S. Gonz´alez-Gonz´alez, V. Violant-Holz, and R. M. Gil-Iranzo, “Social\nrobots in hospitals: a systematic review,” Appl. Sci., vol. 11, no. 13, p.\n5976, 2021.\n[3] M. M. O. Youngjoon Choi, Miju Choi and S. S. Kim, “Service robots\nin hotels: understanding the service quality perceptions of human-robot\ninteraction,” J. of Hospitality Marketing & Management, vol. 29, no. 6,\npp. 613–635, 2020.\n[4] A. Zaraki, M. Giuliani, M. B. Dehkordi, D. Mazzei, A. D’ursi, and\nD. De Rossi, “An RGB-D based social behavior interpretation system\nfor a humanoid social robot,” in RSI/ISM Int. Conf. on Robot. and\nMechatronics, 2014, pp. 185–190.\n[5] S. Arreghini, G. Abbate, A. Giusti, and A. Paolillo, “Predicting the\nintention to interact with a service robot: the role of gaze cues,” in\nIEEE Int. Conf. Robot. and Autom., 2024, pp. –.\n[6] R. Martin-Martin, M. Patel, H. Rezatofighi, A. Shenoi, J. Gwak,\nE. Frankel, A. Sadeghian, and S. Savarese, “Jrdb: A dataset and\nbenchmark of egocentric robot visual perception of humans in built\nenvironments,” IEEE Trans. on Pattern Anal. and Machine Intell.,\nvol. 45, no. 6, pp. 6748–6765, 2021.\n[7] J. Pages, L. Marchionni, and F. Ferro, “TIAGo: the modular robot that\nadapts to different research needs,” in International workshop on robot\nmodularity, IROS, vol. 290, 2016.\n[8] H. Mahdi, S. A. Akgun, S. Saleh, and K. Dautenhahn, “A survey on the\ndesign and evolution of social robots—past, present and future,” Elsevier\nRobotics and Autonomous Systems, vol. 156, p. 104193, 2022.\n[9] M. Paul, S. M. Haque, and S. Chakraborty, “Human detection in\nsurveillance videos and its applications-a review,” Springer Journal on\nAdvances in Signal Processing, vol. 2013, no. 1, pp. 1–16, 2013.\n[10] M. T¨olgyessy, M. Dekan, and L. Chovanec, “Skeleton tracking accuracy\nand precision evaluation of kinect v1, kinect v2, and the azure kinect,”\nMDPI Applied Sciences, vol. 11, no. 12, p. 5756, 2021.\n[11] N. K. Dhiman, D. Deodhare, and D. Khemani, “Where am i? creating\nspatial awareness in unmanned ground robots using slam: A survey,”\nSpringer Sadhana, vol. 40, pp. 1385–1433, 2015.\n[12] M. D. Vernon, “The peripheral perception of movement,” Cambridge\nUniversity Press British Journal of Psychology, vol. 23, no. 3, p. 209,\n1933.\n[13] Z. Yan, T. Duckett, and N. Bellotto, “Online learning for 3d lidar-based\nhuman detection: experimental analysis of point cloud clustering and\nclassification methods,” Springer Autonomous Robots, vol. 44, no. 2,\npp. 147–164, 2020.\n[14] J. N. Hayton, T. Barros, C. Premebida, M. J. Coombes, and U. J. Nunes,\n“Cnn-based human detection using a 3d lidar onboard a uav,” in IEEE\nInt. Conf. Auton. Robot. Sys. and Comp., 2020, pp. 312–318.\n[15] M. Nava, J. Guzzi, R. O. Chavez-Garcia, L. M. Gambardella, and\nA. Giusti, “Learning long-range perception using self-supervision from\nshort-range sensors and odometry,” IEEE Robot. and Autom. Lett., vol. 4,\nno. 2, pp. 1279–1286, 2019.\n[16] M. Nava, A. Paolillo, J. Guzzi, L. M. Gambardella, and A. Giusti,\n“Uncertainty-aware self-supervised learning of spatial perception tasks,”\nIEEE Robot. and Autom. Lett., vol. 6, no. 4, pp. 6693–6700, 2021.\n[17] P. Cong, Y. Xu, Y. Ren, J. Zhang, L. Xu, J. Wang, J. Yu, and Y. Ma,\n“Weakly supervised 3d multi-person pose estimation for large-scale\nscenes based on monocular camera and single lidar,” in AAAI Conference\non Artificial Intelligence, vol. 37, no. 1, 2023, pp. 461–469.\n[18] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks\nfor semantic segmentation,” in IEEE/CVF Conf. on Comp. Vision and\nPattern Recogn., 2015, pp. 3431–3440.\n[19] D. P. Kingma and J. Ba, “Adam: a method for stochastic optimization,”\nin Int. Conf. on Learn. Represent., 2015.\n[20] W. Liu, Y. Zhang, S. Tang, J. Tang, R. Hong, and J. Li, “Accurate\nestimation of human body orientation from rgb-d sensors,” IEEE Trans.\non Cybernetics, vol. 43, no. 5, pp. 1442–1452, 2013.\n[21] Z. Luo, S. A. Golestaneh, and K. M. Kitani, “3d human motion\nestimation via motion compression and refinement,” in Asian Conference\non Computer Vision, 2020.\n\n\n6\nTABLE II\nDESCRIPTION OF SUPPLEMENTARY MATERIALS\nSupplementary material\nDescription\nZenodo Dataset Link\nLink to the Zenodo page for the publicly available dataset. The excessive size of our\ndataset prevents direct upload. Further dataset explanation is available at the linked page.\nURL: https://zenodo.org/records/14936069\nIEEE DataPort Link\nLink to the IEEE DataPort page for the publicly available dataset. URL: https://ieee-\ndataport.org/documents/sixth-sense-indoor-human-spatial-awareness-dataset\n\n\n"}
{"text": "Detecting Linguistic Diversity on Social Media\nSidney Wong, Benjamin Adams, and Jonathan Dunn\nAbstract This chapter explores the efficacy of using social media data to examine\nchanging linguistic behaviour of a place. We focus our investigation on Aotearoa\nNew Zealand where official statistics from the census is the only source of language\nuse data. We use published census data as the ground truth and the social media sub-\ncorpus from the Corpus of Global Language Use as our alternative data source. We\nuse place as the common denominator between the two data sources. We identify\nthe language conditions of each tweet in the social media data set and validated\nour results with two language identification models. We then compare levels of\nlinguistic diversity at national, regional, and local geographies. The results suggest\nthat social media language data has the possibility to provide a rich source of spatial\nand temporal insights on the linguistic profile of a place. We show that social media\nis sensitive to demographic and sociopolitical changes within a language and at\nlow-level regional and local geographies.\n1 Introduction\nNational censuses across the globe have been criticised for their “high cost, low\nfrequency, publication lag, limited geographic detail and limited breakdown of pop-\nulations classifications into sub-categories” [10]. Policy makers, non-government\norganisations, and researchers all use census data to determine the demographic\nprofile of places and communities. In Aotearoa New Zealand (New Zealand), census\ninformation is the only source of data on languages for social scientists, linguists, and\nSidney Wong\nGeospatial Research Institute, e-mail: sidney.wong@pg.canterbury.ac.nz\nBenjamin Adams\nUniversity of Canterbury, e-mail: benjamin.adams@canterbury.ac.nz\nJonathan Dunn\nUniversity of Illinois Urbana-Champaign, e-mail: jedunn@illinois.edu\n1\narXiv:2502.21224v1  [cs.CL]  28 Feb 2025\n\n\n2\nSidney Wong, Benjamin Adams, and Jonathan Dunn\nthose working in language revitalisation, retention, and maintenance programmes\n[1]. This reliance on a single data source is a risk leading us to consider alternative\ndata sources and methodologies to collect this information.\nIn this chapter, we investigate the feasibility of using social media data as an\nalternative data source within the context of New Zealand. Based on current research,\nit is possible to observe linguistic behaviour of an underlying population using social\nmedia [6]. Therefore, we use social media language data to examine the linguistic\nsituation of New Zealand. We investigate the following research questions in our\nanalysis:\n•\nHow do census data and social media data compare in terms of their basic\ncharacteristics and what they might tell us about language use variation over\nspace and time?\n•\nAnd more importantly, how might we use social media data in place of official\nstatistics when performing analyses based on language use information?\nWe analyse data from two sources: the New Zealand Census of Population and\nDwellings (the census) and the web-based Corpus of Global Language Use (CGLU)\n[2]. We consider census as the ground truth on the linguistic situation of New Zealand\nand the social media language data from the CGLU is our alternative data source. We\ncan compare the two data sets as they are both organised spatially. We use language\nidentification models to identify the languages present in our social media language.\nWe compare the efficacy of the two language identification models in our analysis.\nFollowing this data analysis step, we compute measures of linguistic diversity from\nour ground truth data and our alternative data source at national, regional, and\nlocal geographic levels. This allows us to understand the similarities and differences\nbetween census and social media language data.\n2 Background\n2.1 Linguistic Situation of New Zealand\nAoteaora is an island country located in the South Pacific Ocean. New Zealand has\ntwo official languages: te reo M¯aori and New Zealand Sign Language (NZSL). These\nlanguages accounts for 4.0% and 0.5% of languages used in New Zealand as of the\n2018 Census. However, the most commonly spoken language in New Zealand is\nEnglish which is the de facto official language and accounts for 95.4% of languages\nspoken in the population [26]. The dominance of English across New Zealand society\nis a result of British colonisation and settlement beginning in the 19th Century and\nsubsequent impacts of globalisation [13].\nGovernment preference for migrants from the United Kingdom and Australia and\ndiscriminatory legislation limiting migration from Asia has meant New Zealand\nsociety remained largely linguistically homogenous throughout the 19th and 20th\n\n\nDetecting Linguistic Diversity on Social Media\n3\nTable 1 Most common languages spoken in New Zealand in 2001 and 2018\nRank 2001 Census\n2018 Census\n1\nEnglish\nEnglish\n2\nte reo M¯aori\nte reo M¯aori\n3\nSamoan\nSamoan\n4\nFrench\nNorthern Chinese\n5\nYue\nHindi\n6\nGerman\nFrench\n7\nNZSL\nYue\n8\nNorthern Chinese Sinitic not further defined\n9\nDutch\nTagalog\n10\nTongan\nGerman\nCenturies. The demographic make-up of New Zealand significantly changed as a\nresult of the Immigration Policy Review in 1986, which has allowed an increased\nnumber of migrants from non-Anglophone countries to immigrate to New Zealand\n[11].\nThis demographic change is evident in the most common languages used across\nNew Zealand. The top ten most spoken languages as of the 2001 Census [29] and\nthe 2018 Census [26] are presented in Table 1. While the top three most common\nlanguages (English, te reo M¯aori, and Samoan) remained the same, over half of the\nmost common languages spoken in New Zealand originate from Asian countries\nsuch as Northern Chinese (including Mandarin), Hindi, Yue (Cantonese), Sinitic not\nfurther defined (which is used to signify an unspecified Sinitic (Chinese) language),\nand Tagalog. Respondents who did not provide a valid response or were too young\nto talk at the time of the census were excluded from Table 1.\nWe can see that in a period of less than twenty years, both the demographic\nand linguistic make-up of New Zealand have significantly changed. An increase in\nlinguistic diversity coupled with the legislative recognition of te reo M¯aori in 1987\nand NZSL in 2006 have increased the public consciousness to language rights and\nlinguistic inclusion.\n2.2 Surveying Language\nThere are practical reasons in understanding the changing linguistic profile of New\nZealand and allow communities to provide linguistically appropriate services across\ndifferent populations. As of present, the most reliable source of language data for\nNew Zealand comes from the census. The census provides the official count of all\npeople and dwellings in New Zealand every five years [22]. This information is used\nto determine electoral boundaries and informs the distribution of public funding.\n\n\n4\nSidney Wong, Benjamin Adams, and Jonathan Dunn\nThe questions in the census are not static and have frequently changed over time.\nThe changes depend on the needs of New Zealand’s official statistics system. For\nexample, questions relating to iwi and ethnic affiliation have only been included\nas part of the individual form since the 1991 Census. Other variables which were\ndeemed no longer relevant such as race, ethnic origin, or nationality have since been\nremoved. The language census topic was only included as part of the 1996 Census.\nThe languages spoken variable is derived from the language census topic question\nin the individual form. More specifically, the questions asks “In which language(s)\ncould you have a conversation about a lot of everyday things?”. This question is a\nmultiple response variable which means participants can include up to six languages\nwhich are coded according to the Language Standard Classification 1999 [27]. In\na multiple response variable, each response is counted towards each applicable\nlanguage classification. Other modalities of language such as signed languages (e.g.,\nNZSL, American Sign Language) are also included as part of the question.\nThe languages spoken variable is a priority three variable which means the vari-\nable does not directly fit with the main purpose of the census, but this information\nis still important to certain populations and communities [27]. The language census\ntopic and the languages spoken variable serves the following functions:\n1. To formulate, target and monitor policies and programmes to revitalise the M¯aori\nlanguage as an official language of New Zealand.\n2. As an indicator of iwi vitality and cultural resources.\n3. To assess the need to provide multi-lingual pamphlets and other translation ser-\nvices in a variety of areas such as education, health and welfare.\n4. To evaluate and monitor existing language education programmes and services.\n5. To provide information for television and radio programmes and services.\n6. To understand the diversity and diversification of the New Zealand population\nover time, as well as language maintenance, retention and distribution.\nOne limitation of the language census topic is that it does not collect written\nlinguistic ability or fluency. It is not possible to determine linguistic competence\nacross different modalities from the language census topic alone. Only Te Kupenga,\na sample survey of 8,500 M¯aori aged 15 years and over living across New Zealand,\nincludes questions regarding written ability and fluency in te reo M¯aori [23]. This\nmeans the census may not meet the needs of heritage language revitalisation efforts\nfor other culturally and linguistically diverse communities.\nDespite the benefits of the census, the national survey is a costly endeavour. The\n2023 Census was estimated to cost NZ$250 million - almost double the cost of\nthe 2018 Census [32]. The success of a census, measured by its response rate, is\nsusceptible to social, environment, and political factors. The proposed 2011 Census\nwas postponed to 2013 due to the 2010-2011 Canterbury Earthquakes. There is also\na trend in declining response rates to surveys [8]. This downward trend was reflected\nin the results of the 2018 Census which was known for its low response rate of 83.3%\n[24]. This is significant 8.9% decline from the 2013 Census.\nIn the case of the language census topic, only 83.8% of responses came from\nthe 2018 Census, 8.2% of responses were derived from 2013 Census data, and\n\n\nDetecting Linguistic Diversity on Social Media\n5\nthe remaining 8.0% of responses were derived through statistical imputation [27].\nDespite the low response, the data quality of the languages spoken variable was rated\nhigh quality. This is particularly concerning for policy makers and researchers who\nuse this information to determine the success of language revitalisation programmes.\n2.3 Alternative Data Sources\nThe five-year (and in some cases ten-year for some countries) census cycles which\nhave traditionally met the requirements to simply count a population. This does not\nmeet the needs of data hungry public and private sector organisations who require\nmore contemporaneous demographic insights on populations. If the purpose of a\ncensus is to understand the changing demographic profile of a population, then\nperhaps there are other data sources and methods to meet this need.\nOne suggested method is the use administrative data as an alternative to a con-\nventional census survey [19]. Administrative data includes all the transactional data\nheld by central government ministries and departments. This information is currently\nmanaged by Stats NZ as part of the Integrated Data Infrastructure (IDI) [16]. For ex-\nample, information about individuals in New Zealand such as location and ethnicity\ndata are held by the Ministry of Health, education and training data are held by the\nMinistry of Education, work and income data are held by Inland Revenue to list a\nfew. This method will only be suitable for some topics and not others.\nThere are currently no administrative sources identified which is suitable to\nreplace the languages spoken variable [1]. The only alternative sources for language\ninformation from other official sources are the biennial General Social Survey (GSS),\na sample survey of 8,000 individuals in households across New Zealand aged 15\nyears and over [30], and enrolment data from the Ministry of Education [17]. This\nmeans we are unlikely to get good quality data about language equivalent to a census\nfrom administrative sources [19].\nThese data gaps coupled with technical issues surrounding sensitive demographic\nattributes such as ethnicity, the lack of consent sought from individuals, and barriers\nin access to the data in the IDI raise legislative and ethical issues as a source of\nofficial statistics. These issues have serious consequences for the government and its\ncommitment to Te Tiriti o Waitangi in upholding M¯aori Data Sovereignty (MDS)\n[9]. In essence, MDS ensures M¯aori communities have the ability to exercise power\nover usage and outputs of data produced by M¯aori and about M¯aori [31].\nBeyond administrative data, social media has been offered as an alternative data\nsource for national censuses and surveys outside of New Zealand [10]. Early work in\nthe United Kingdom investigating the feasibility of using social media data to derive\ndemographic information found that gender and language information from Twitter,\nalso known as X, is proportional to results observed in the 2011 Census [21].\nThere were an estimated 4.99 million internet users in New Zealand in January\n2023. This is equivalent to an internet penetration rate of 95.9% out of New Zealand’s\nestimated population of 5.21 million [14]. More specifically, there was an estimated\n\n\n6\nSidney Wong, Benjamin Adams, and Jonathan Dunn\nTable 2 Social media platform users in New Zealand\nPlatform\nUsers (in millions)\nFacebook\n2.95\nYoutube\n4.24\nInstagram\n2.15\nTikTok\n1.65\nLinkedIn\n2.50\nSnapchat\n1.45\nX (Twitter)\n0.79\nPinterst\n0.67\n4.24 million social media users in New Zealand. This means over 80.0% of New\nZealand’s population have subscribed to a social media platform. Based on this level\nof coverage, social media may be a feasible alternative data source. The user base\nfor different social media platforms have been included in Table 2.\n2.4 Digital Language and Place\nOf the different social media platforms, X (Twitter) is by far the greatest contributor\nof digital language data despite representing 15.2% of New Zealand. A benefit of\nX (Twitter) is the massive volume of publicly available data. X (Twitter) provides\nresearchers access to their data including tweet and user information through their\nApplication Programming Interface (API). Researchers with ‘Essential’ access can\nretrieve up to 500 thousand tweets per month while those who qualify for ‘Academic\nResearch’ access can retrieve up to 10 million tweets per month.\nResearchers can also access temporal and spatial information for each tweet\nthrough the API based on X (Twitter)’s user-enabled geotagging feature [15]. The\nsearch polygon shape is determined by the type of data request: rectangular for\nreal-time tweets and circular for historical tweets. There are no limits to the search\npolygon size, but there is no guarantee all tweets within a geographic area will be\nretrieved.\nThere are some limitations to this data. The number of geotagged tweets only\naccount for a small proportion of tweets as the geotagging feature is disabled by\ndefault. Sloan et al. [21] found that only 0.85% of tweets were geotagged in a sample\nof 113 million tweets. There are also significant demographic differences between\nthe type of users who enable geotagging based on the user’s perceived age, social\nclass, language background, and user interface language [20].\nX (Twitter)’s user base may not be a balanced sample of an underlying population.\nIn the US context, X (Twitter) users tend to skew towards younger urban users who\ncome from an ethnic minority background [18]. We must consider the types of biases\ninherent to digital language and place data when making claims about linguistic\nbehaviour on social media.\n\n\nDetecting Linguistic Diversity on Social Media\n7\nRecent work has shown progress in addressing these limitations. Dunn et al. [5]\nfound that digital language behaviour was sensitive to real-world events. The study\ntracked changes in national measures of linguistic diversity over the course of the\nCOVID-19 pandemic. They were able to account for non-local bias as nationwide\nlockdowns limited travel and migration internationally and domestically.\n3 Data and Methods\n3.1 Corpus of Global Language Use\nThe Corpus of Global Language Use (CGLU) is a web-based corpus which consists\nof the web sub-corpus retrieved through Common Crawl and the social media sub-\ncorpus retrieved through the X (Twitter) API [2]. Data collection has been on-going\nsince 2017. Our focus is the social media sub-corpus.\nThe sub-corpus is coded with both temporal and spatial information. Each tweet\nhas been coded for broad geographic region, country of origin, nearest city, date,\na corresponding fifteen-character geohash , and the content of the tweet itself. The\ngeohash is derived from the latitude and longitude information linked to geotagged\nenabled tweets. The data collection points come from the global Geonames data set\n[7]. We filtered the social media sub-corpus for tweets originating from within New\nZealand. There are one hundred data collection points across New Zealand. The\nsocial media sub-corpus contains geotagged enabled tweets within a 50-kilometre\nradius for each data collection point. The data set does not contain duplicates.\nWe linked each of the data collection points to one of the sixteen regional council\nareas. We visualised the data collection points in Figure 1. A majority of the data\ncollection points are in Te Ika a M¯aui (the North Island). This is expected as more than\na third of the population resides in Te Ika a M¯aui. Te Waipounamu (the South Island)\nis sparsely populated despite being the larger of the two islands. The data catchment\narea accounts for 97.6% of the estimated resident population as of December 2022.\nInversely, Figure 2 visualises all small urban areas (light green) situated outside the\ndata catchment area. This area amounts for a estimated population of 56,904 as of\nDecember 2022 [30]. Small urban areas and rural settlements with an estimated\npopulation greater than 1,000 people as of 2022 includes: Taumarunui (est. 4,830),\nKaik¯oura (est. 2,330), Twizel (est. 1,780), and Alexandra (est. 6,010). Overall, the\ncglu has good coverage of New Zealand despite missing data from these small urban\nsettlements\nMore detailed information on the individual data collection points are presented\nin Table 5 the Appendix. We do not have data linked to the Nelson region. With\nreference to Figure 1, tweets originating from the Nelson region are captured by data\ncollection points located in the neighbouring Tasman and Marlborough regions.\n\n\n8\nSidney Wong, Benjamin Adams, and Jonathan Dunn\nFig. 1 Settlements within CGLU catchment area.\n\n\nDetecting Linguistic Diversity on Social Media\n9\nFig. 2 Settlements outside CGLU catchment area.\n3.2 New Zealand Census of Population and Dwellings\nWe use published census data as the ground truth data set. It is a suitable source to\ndetermine the ground truth linguistic situation of New Zealand as the census is a\n\n\n10\nSidney Wong, Benjamin Adams, and Jonathan Dunn\nFig. 3 Differences between language identification models idNet (left) and pacificID (right).\nnational count of all individuals in New Zealand. Published census data is publicly\naccessible online. This data is aggregated and confidentialised. We retrieved the\nnational-level and regional-level data from NZ.Stat [26]. The data set includes data\nfrom the 2006, 2013, and 2018 census cycles. As we only have access to social media\ndata from 2017 on wards, the focus of our study is on the 2018 Census.\nWe removed additional metadata information as part of the data cleaning process.\nSigned languages such as NZSL were removed from the analytical data set. These\nwere excluded from the current analysis as they do not have a corresponding written\northography identifiable through the language identification models. Furthermore,\nunspecified languages from the statistical classification in ’Other’ and ‘None (e.g.\ntoo young to talk)’ were also removed from the analytical data set.\nWe retrieved additional population information for New Zealand from NZ.Stat\n[25] and geographic information from Datafinder [28]. A demographic summary of\nNew Zealand and the regional council areas are available in Table 6 of the Appendix.\n3.3 Language Identification\nThe first data processing step in our study is to identify the predominate language\nfor each tweet. Unlike the census language topic which is based on self-rated ability,\nthe language condition for each tweet is not implicitly available from the CGLU.\nAlthough X (Twitter) provides support for 34 written languages on sign-up, 33.0%\n\n\nDetecting Linguistic Diversity on Social Media\n11\nTable 3 Most common languages in 2018\nRank 2018 Census\nidNet\npacificLID\n1\nEnglish\nEnglish\nEnglish\n2\nte reo M¯aori\nPortuguese Portuguese\n3\nSamoan\nJapanese\nThai\n4\nNorthern Chinese\nTagalog\nJapanese\n5\nHindi\nSpanish\nSpanish\n6\nFrench\nIndonesian Tagalog\n7\nYue\nArabic\nMalaysian\n8\nSinitic not further defined French\nFrench\n9\nTagalog\nKorean\nArabic\n10\nGerman\nThai\nKorean\nof tweets were in a language that was different from the user interface in a sample\nof 113 million tweets [21].\nWe use the idNet language identification classification model to automatically\ncode the primary language of a tweet [3]. The package has a high accuracy with\na reported F1-score above 0.95 for 464 languages based on 50-character language\nsamples. The second language classifier we have used for this study is the pacificLID\npackage [4]. This language classifier was especially adapted to identify Austronesian\nlanguages (e.g., te reo M¯aori). This is particularly useful in an New Zealand context.\nWe visualised the classification differences between the language identification mod-\nels in Figure 3. Both classifiers were included as part of our analysis for comparability\nas there may be classification errors (i.e., as a result of code switching).\n3.4 Linguistic Diversity\n3.5 Language Identification\nThe last data processing step is to calculate measure of linguistic diversity for the\nvarious levels of geography (national, regional, and local). A simple implementation\nof this measure is the concentration ratio (CR) which we can use as a proxy for\nlinguistic diversity [12]. A CR is used to determine the market structure and com-\npetitiveness of the market and provides a range between 0% to 100%. Common CR\nmeasures include 4-firm (CR4) and 8-firm (CR8). We used a 10-firm CR (𝐶𝑅10).\nThis means for each geographic location, we selected the top ten most common\nlanguages in use. The CR is calculated as in Equation 1.\n𝐶𝑅𝑛= 𝐶1 + 𝐶2 + ... + 𝐶𝑛\n(1)\nWhere: Cn defines the share of the 𝑛th largest languages as a % of a population\nand n defines the number of languages included in the CR calculation.\n\n\n12\nSidney Wong, Benjamin Adams, and Jonathan Dunn\nA𝐶𝑅10 measure under 0.40 suggests low concentration (i.e., perfect competition).\nA 𝐶𝑅10 measure between 0.40 to 0.70 suggests medium concentration (i.e., an\noligopoly). A𝐶𝑅10 measure over 0.70 suggests high concentration (i.e., a monopoly).\nThis means the the lower the measure, the higher the levels of linguistic diversity.\n4 Results\nWe observed a high level of agreement between the idNet and the pacificLID models.\nThere were 76,007 mismatches between the two language identification models. The\nrate of mismatch between the two packages was equivalent to 0.76%. A closer\ninspection of mismatched tweets in English and te reo M¯aori found that a majority of\ntweets were reclassified from English to te reo M¯aori between idNet and pacificLID.\nIn the case of te reo M¯aori tweets identified with idNet, most tweets were re-\nclassified to Tongan and the remaining tweets were equally reclassified to English,\nChamorro, Japanese, Niuean, Albanian, and Wallisian. The reclassification of tweets\nin te reo M¯aori to Chamorro, Niuean, Tongan, and Wallisian was expected as the\npacificLID package was trained to be more sensitive to Austronesian languages.\nThe reclassified tweets could be corrections, or indeed classification errors as\nsome tweets which were predominately in English were reclassified as te reo M¯aori.\nOther reasons for the mismatches between the idNet and the pacificLID could be the\nresult of code-switching or other translanguaging practices that were not captured\nby a classification type language identification model.\nWe identified 403 distinct languages within the sub-corpus. This number is signif-\nicantly higher than the 196 languages listed in the Language Standard Classification\n1999. The nature of written language differs significantly from spoken language.\nThere is not always a one-to-one relationship between the two modes of language.\nIn some cases there could be a zero-to-one, one-to-many, or many-to-many relation-\nships between modes. This means language varieties represented in the census data\nmay not appear in the social media sub-corpus and vice versa.\nTable 3 compares the top ten most common languages from the latest census and\nthe social media language data. English was the most common language across the\ntwo models. This was expected. However, there were a few unexpected results. For\nexample, te reo M¯aori, Samoan, Hindi, German, or any of the Sinitic languages were\nrarely observed on X (Twitter). While census data only provides spatial granularity\nto neighbourhood level geographies, social media data provides both spatial and\ntemporal granularity. This is because census cycles occur once every five (or ten\nyears), while social media platforms can capture data contemporaneously. Figure 4\nand Figure 5 demonstrate this by visualising the monthly frequency count of tweets\nin some language conditions.\nFigure 4 includes languages with high (where y-limit is 500,000) and medium\n(where y-limit is 10,000) frequency counts while Figure 5 includes languages with\nlow (where y-limit is 10,000). The dashed lines on the figure compares the results\nfrom idNet (in grey) and pacificLID (in black). We can see a high level of consistency\n\n\nDetecting Linguistic Diversity on Social Media\n13\nFig. 4 High and Medium Frequency Languages on X (Twitter)\n\n\n14\nSidney Wong, Benjamin Adams, and Jonathan Dunn\nFig. 5 Low Frequency Languages on X (Twitter)\nbetween the two language identification models for English, Spanish, Portuguese,\nand French in Figure 4 and Korean and German in Figure 5.\nIt is clear that the level of consistency is not the result of data availability (as shown\nby the monthly frequency count on the y-axis), but the efficacy of the language\nidentification model itself. There are some severe irregularities between the two\nmodels particularly in Japanese, Tagalog, and written vernacular Chinese, Tagalog\nin Figure 4 and te reo M¯aori and Hindi in Figure 5. The discrepancies between\nBahasa Indonesia and Malay is a result of how these languages are coded in the two\nmodels. These differences require a deeper analysis beyond the scope of this chapter.\nDespite these irregularities, there is value in this information as we can conduct\ntime series analyses on different language conditions. For te reo M¯aori in particular\nas shown in Figure 5, we can observe a seasonal peak of tweets during the second\nhalf of the year. This increase in frequency coincides with Mahuru M¯aori where\nparticipants are encouraged to use te reo M¯aori in all facets of everyday life. This\nsuggests we can observe real-time changes to the linguistic profile of New Zealand\nbased on significant cultural movements.\n\n\nDetecting Linguistic Diversity on Social Media\n15\nFig. 6 Monthly frequency counts of tweets and corresponding 𝐶𝑅10 measures\n4.1 Linguistic Diversity\nNow that we have confirmed that the language identification models can suitably\nidentify languages in social media sub-corpus, we can calculate measures of linguis-\ntic diversity based on the 10-fold concentration ratio (𝐶𝑅10) measure. The initial\nmeasure of the national 𝐶𝑅10 measure for 2006 was 0.76, 0.81 in 2013, and 0.79 in\n2018. These measures from the census suggest that New Zealand is typically linguis-\ntically homogeneous. The 𝐶𝑅10 measures from the social media sub-corpus for 2018\nwas 0.79 (idNet) and 0.72 (pacificLID). We can see from this national measure, idNet\nwas more consistent with the 2018 Census. Once again, these results suggest that\neven the digital presence of New Zealand is typically linguistically homogeneous.\nThese national-level measures provide little detail on how the linguistic profile\nof New Zealand has changed over time. We are again interested in the temporal\ngranularity of the social media language data and how the frequency counts of\ntweets over time (i.e., the sampling) may have an effect on linguistic diversity. We\ncan analyse the stability of the time series visually. Figure 6 is a multiple line graph\nwith two y-axes. The primary y-axis represents the daily counts of tweets (in grey)\nand the secondary y-axis represents the linguistic diversity (in black). We have only\nincluded tweets where idNet and pacificLID matched. We inverted the secondary\ny-axis to improve interpretability.\nThe most striking result is the missing data between 2017-2018 and the spikes of\nfrequency counts. These are clearly outliers in the data collection. When we discount\nthese outliers, we can observe a stable relationship between the frequency of tweets\nand the 𝐶𝑅10 which suggests the time series is stationary. A stationary time series\nis particularly important if we were to model trends from the social media language\ndata.\n\n\n16\nSidney Wong, Benjamin Adams, and Jonathan Dunn\nTable 4 𝐶𝑅10 measures for 2018 by regional council areas\nRegion Name\nCensus idNet pacificLID\nNorthland\n0.76\n0.56\n0.52\nAuckland\n0.60\n0.79\n0.73\nWaikato\n0.76\n0.81\n0.75\nBay of Plenty\n0.75\n0.80\n0.73\nGisborne\n0.70\n0.88\n0.48\nHawkes Bay\n0.78\n0.95\n0.86\nTaranaki\n0.85\n0.49\n0.44\nManawat¯u-Wanganui\n0.80\n0.71\n0.66\nWellington\n0.72\n0.89\n0.82\nWest Coast\n0.89\n0.66\n0.60\nCanterbury\n0.80\n0.79\n0.72\nOtago\n0.82\n0.90\n0.83\nSouthland\n0.87\n0.89\n0.81\nTasman\n0.86\n0.58\n0.54\nMarlborough\n0.85\n0.88\n0.78\n4.2 Regional Insights\nOf interest to policy makers and researchers is the potential to use alternative data\nsources to provide insights at regional and local geographies. Table 4 provides the\n𝐶𝑅10 measures for the census and social media sub-corpus for 2018 year by regional\ncouncil areas. Once again, we compare the results from idNet and pacificLID.\nFirstly, the 𝐶𝑅10 measures from the census differ between regional council ar-\neas. Auckland is the most linguistically diverse, while the West Coast is the least\nlinguistically diverse. Regions in Te Ika a M¯aui (with the exception of Taranaki)\nare more linguistically diverse than regions in Te Waipounamu. As we compare\nthe 𝐶𝑅10 measures from the different data sources we see greater variability. Sur-\nprisingly, Taranaki region is the most linguistically diverse according to the social\nmedia sub-corpus which significantly contrasts its corresponding measure from the\ncensus. There is little consistency between the census measures and the social media\nlanguage measures with the exception of Canterbury, Southland, and Marlborough\nregions which have similar levels of linguistic diversity.\nAnother striking observation is that the 𝐶𝑅10 measures derived from the paci-\nficLID exhibited greater levels of linguistic diversity than census or idNet derived\nmeasures (with the exception of Auckland). The fact that we can only observe these\ndifferences at the regional level suggest a downstream effect of the 76,007 mis-\nmatches from the two language identification models. A regional breakdown of the\nnumber of tweets and proportion of tweets is in Table 5 in the Appendix.\nWe carried out a simple non-parametric test between the 𝐶𝑅10 observed in Table 4\nand demographic information derived from census and X (Twitter) as presented in\nTable 5 and Table 6 of the Appendix. We did not observe a relationship between the\ncensus derived𝐶𝑅10 measures and X (Twitter) derived𝐶𝑅10 measures at the regional\nlevel. The correlation coefficient (Spearman’s Rho) between the census and idNet\n\n\nDetecting Linguistic Diversity on Social Media\n17\nFig. 7 Monthly 𝐶𝑅10 Measures for Te Ika a M¯aui by regional council area\n\n\n18\nSidney Wong, Benjamin Adams, and Jonathan Dunn\nFig. 8 Monthly 𝐶𝑅10 Measures for Te Waipounamu by regional council area\nwas -0.27 and pacificLID was -0.08. We observed a moderate negative relationship\nbetween the census derived 𝐶𝑅10 measure with population density (-0.73**) and a\nweak positive relationship with median age (0.56*).\nWe did observe a statistically significant strong positive correlation coefficient\nbetween the 𝐶𝑅10 measures derived from idNet and pacificLID which was 0.80***.\nThis suggests a high level of consistency between the language identification mod-\nels. There was no relationship between X (Twitter) derived 𝐶𝑅10 measures and the\ndemographic information in Table 6. However, we observed a weak negative rela-\ntionship between census derived 𝐶𝑅10 with the proportion of tweets per region in\nrelation to corpus size (-0.51*).\nThe results from the Spearman’s Rho suggest there is a weak association between\ndemographic measures derived from census and social media sub-corpus at regional\ngeographies. It is possible the small sample of regional geographies is not sufficient\nto identify a consistent relationship between the two sources of data.\nFigure 7 and Figure 8 provide a monthly breakdown of the 𝐶𝑅10 by regional\ncouncil area over time on the primary y-axis (solid in black). The regions are grouped\nby islands and urban-rural in order to standardise the y-axis limit. We inverted the\n\n\nDetecting Linguistic Diversity on Social Media\n19\ny-axis to improve interpretability. We have only included the 𝐶𝑅10 measures derived\nfrom the pacificLID. The monthly mean frequency count of tweets are shown on the\nsecondary y-axis (dotted in black). Furthermore, We included 2018 Census 𝐶𝑅10\nmeasures as our baseline to see how linguistic diversity on social media compares\nwith the ground truth measures of linguistic diversity for each regional council area\n(dotted in grey).\nWe can see from Figure 7 and Figure 8 that a higher monthly mean frequency\ncount corresponds to a more stable𝐶𝑅10 measure. This is shown in the urban regional\ncouncils such as Auckland, Waikato, and Wellington regions in Te Ika a M¯aui as\nshown in Figure 7 and Canterbury and Otago regions in Te Waipounamu as shown\nin Figure 7. Furthermore, linguistic diversity from the social media sub-corpus is\nconsistently lower for all urban regions than the census, while the opposite is true\nfor rural regions (with the exception of Bay of Plenty, Gisborne, and Hawke’s Bay\nregion.\n4.3 Case Study: Wellington\nIn contrast to the other regions with significant urban areas within New Zealand\nwhere linguistic diversity remained stable over times, we observed a significant\nincrease of linguistic diversity in the Wellington region with a peak mid-2020.\nWhile other regions also experienced significant fluctuations over time, we could\nattribute this sampling method where data collection points from rural areas were\nunderrepresented in the social media sub-corpus.\nWellington is the capital region of New Zealand. The Wellington urban area\nconsists of Wellington City, Lower Hutt City, Upper Hutt City, and P¯orirua. The hin-\nterland of the capital region includes the K¯apiti Coast (i.e., ¯Otaki) and the Wairarapa\n(i.e., Masterton, Waipawa). We can see a significant level of overlap in the data\ncatchment area due to the short proximity between the cities within the Wellington\nurban area. Therefore, we would expect a higher level of internal consistency within\nthe Wellington urban area.\nAs shown in Figure 6, we observed an increase of linguistic diversity in the period\nbetween April 2020 and September 2020. A further deep dive of the individual data\ncollection points within the Wellington region revealed a consistent increase of\nlinguistic diversity between the data collection points within the Wellington urban\narea not observed in the hinterland. We validated this pattern to see if this was a result\nof data sampling. Figure 9 provides the monthly 𝐶𝑅10 measures and mean frequency\ncount of tweets for each data collection point within the Wellington region. We can\nsee the consistent increase of linguistic diversity for Lower Hutt, P¯orirua, Upper\nHutt, and Wellington. However, we can see a consistent volume of tweets sampled\nfrom each data collection point when we refer to the mean frequency count of tweets.\nWe therefore conclude that this effect is not the result of sampling, but a change in\nlinguistic behaviour within the Wellington urban area.\n\n\n20\nSidney Wong, Benjamin Adams, and Jonathan Dunn\nFig. 9 Monthly 𝐶𝑅10 measures and mean counts for data collection points in the Wellington region\nIn Figure 10, we provide a stacked bar graph of the most common languages\nidentified by idNet and pacificLID broken down by month for the Wellington region.\nWe removed English from the stacked bar graph to improve interpretability. It is\nevident that the significant increase of linguistic diversity was due to actual changes\nin linguistic behaviour. We can observe a first wave increase of tweets in Spanish\nwhich was followed by a second wave increase of tweets in Portuguese. This period\ncoincided with the beginning of the national lockdown and border closures as a\nresult of the Covid-19 pandemic. This strongly suggests the Spanish and Portuguese\ntweets produced by the users were based in New Zealand at the time. This increase\nis unexpected for the Wellington region as the majority of Spanish and Portuguese\nspeakers in Aoteaora are located in the Auckland region [26].\nThe coronavirus disease was known colloquially as ‘corona’ before it was officially\nnamed ‘Covid-19’ by the World Health Organisation. We considered the possibility\nthat the language identification models erroneously classified tweets with this specific\n\n\nDetecting Linguistic Diversity on Social Media\n21\nFig. 10 Proportion of languages for the Wellington region by language identification model\nstring as Spanish or Portuguese. When we removed these two strings, it did not have\nan impact on the 𝐶𝑅10 or proportion of languages for the Wellington region. We\nalso considered the increase of tweets in Spanish and Portuguese was the result of\nprotests and civil unrest across Latin America during this period. Intuitively, this is a\nreasonable assumption as Wellington is the capital region. The increase of tweets in\nSpanish and Portuguese during this period remains unresolved and requires further\nanalysis on the content of the tweets themselves which is beyond the scope of the\ncurrent chapter.\n\n\n22\nSidney Wong, Benjamin Adams, and Jonathan Dunn\n5 Discussion\nHow do census data and social media data compare in terms of their basic\ncharacteristics and what they might tell us about language use variation over space\nand time?\nWe acknowledge there are significant conceptual differences between the census data\nand the social media language data. For example, the purpose of the census data is\nto collect information on articulated languages (such as spoken and signed) based\non self-report, whereas social media data does not include this information. This\nis evident in the differences in the most common languages used between different\nspaces as shown in Table 3 and the differences in measures of linguistic diversity as\nshown in Table 4.\nOne unexpected difference in the social media sub-corpus is that none of the\nSinitic languages appeared in the top ten list from either models. This is consistent\nwith previous research where user’s with written vernacular Chinese as their language\nbackground or user interface language were significantly underrepresented in a\nsurvey of geotagging enabled users [20]. This indicates that the choice of social media\nplatform may lead to differences in how well certain languages are represented.\nAlthough there are differences in the basic characteristics of the two data sources,\nthis does not mean social media language is without its benefits. We can see from\nour results that linguistic behaviour on social media is sensitive to real-world events\nin the case of te reo M¯aori as shown in Figure 5 and an increase in linguistic diversity\nin the Wellington region as discussed in Section 4.3. This otherwise would not be\npossible with census data.\nFrequency counts and linguistic diversity were only two measures we compared\nbetween the two data sources; however, social media language data allows us to\nobserve other forms of linguistic behaviour. We can measure the level of code-\nswitching or translanguaging behaviour as we have access to the linguistic signal. It\nis also possible to see how different language conditions vary over space and time\nby breaking down the signal into different levels of analysis (e.g., at the word or\nsentence level).\nHow might we use social media data in place of official statistics when performing\nanalyses based on language use information?\nWith reference to the functions of language census topic as discussed in Section 2.2,\nsocial media language data can only indirectly support the needs of culturally and\nlinguistically diverse communities. For example, we could potentially use social\nmedia language data to formulate, target and monitor policies and programmes\nto revitalise te reo M¯aori or understand the diversity and diversification of New\nZealand over time; however, the insights taken from social media may not be a\nbalanced sample of New Zealand’s population.\n\n\nDetecting Linguistic Diversity on Social Media\n23\nThis is because the sample frame and purpose between the census and the social\nmedia data are not equivalent. The census provides better coverage of the entire\npopulation and spatial granularity, however, it lacks in temporal granularity. This is\nan advantage of social media data as demonstrated in Figure 6 and Figure 7 where\nwe can observe changes in linguistic diversity at a regional geographies. However,\nwe may need to up sample rural regions to ensure our sample is representative of the\npopulation.\nAnother advantage to the social media data is the direct access to linguistic\nbehaviour and how people are using language in New Zealand. Some components of\nlinguistic behaviour we can observe include linguistic content, style, sentiment, and\nstructure. These aspects of linguistic behaviour cannot be observed from a national\ncensus or survey. It will be useful to revisit the Wellington case study as discussed\nin Section 4.3 with additional methods from natural language processing such as\ntopic or sentiment analysis to determine why there was an increase in Spanish and\nPortuguese during that period.\nIn a similar vein to administrative data where participant consent is not explicitly\ngiven, there are also ethical concerns with social media data [33]. We need to\nconsider how the use of social media language data for official statistical purposes\nuphold MDS and the potential risks this may impose on M¯aori communities across\nNew Zealand [31]. We can suggest using alternative data sources alongside official\nstatistics to enrich our understanding of the changing linguistic profile and linguistic\nbehaviour of New Zealand.\n6 Conclusion\nThe results from the current study suggest that we can use online social media\nlanguage data to observe spatial and temporal changes in linguistic diversity for\nlow-level regional and local geographies. We should be cautious in how we interpret\ntrends and how they can be applied to policy and research as there are conceptual\ndifferences between ground truth official statistics and alternative data sources like\nsocial media. This does limit the conclusions we can draw from our current analysis\nas further data validation is required. Despite these limitations, the current chapter\nprovides promising results for alternative data sources to be used alongside census\ninformation. Census provides a snapshot of a location at a specific time point, while\nsocial media data provides more contemporaneous information about a place. The\ninformation available to policy makers and researchers from social media, provides a\nrich source of language data us to observe real-time changes in linguistic behaviour.\n\n\n24\nSidney Wong, Benjamin Adams, and Jonathan Dunn\nReferences\n[1] Bycroft C, Reid G, McNally J, Gleisner F (2016) Identifying M¯aori populations\nusing administrative data: A comparison with the census. Tech. rep., Statistics\nNew Zealand, URL https://www.stats.govt.nz/\n[2] Dunn J (2019) Global Syntactic Variation in Seven Languages: Toward a\nComputational Dialectology. Frontiers in Artificial Intelligence 2:15, DOI 10.\n3389/frai.2019.00015, URL https://www.frontiersin.org/article/\n10.3389/frai.2019.00015\n[3] Dunn\nJ\n(2020)\nMapping\nlanguages:\nthe\nCorpus\nof\nGlobal\nLan-\nguage\nUse.\nLanguage\nResources\nand\nEvaluation\n54(4):999–1018,\nDOI\n10.1007/s10579-020-09489-2,\nURL\nhttps://doi.org/10.1007/\ns10579-020-09489-2\n[4] Dunn\nJ,\nNijhof\nW\n(2022)\nLanguage\nIdentification\nfor\nAustrone-\nsian\nLanguages.\nIn:\n13th\nInternational\nConference\non\nLanguage\nResources\nand\nEvaluation,\nURL\nhttps://jdunn.name/2022/04/29/\nlanguage-identification-for-austronesian-languages/\n[5] Dunn J, Coupe T, Adams B (2020) Measuring Linguistic Diversity During\nCOVID-19. In: Proceedings of The Fourth Workshop on the Fourth Workshop\non Natural Language Processing and Computational Social Science, DOI\n10.18653/v1/P17, URL https://arxiv.org/abs/2104.01290\n[6] Eisenstein J, O’Connor B, Smith NA, Xing EP (2014) Diffusion of lexical\nchange in social media. PLOS ONE 9(11):e113114, DOI 10.1371/journal.\npone.0113114, URL https://journals.plos.org/plosone/article?\nid=10.1371/journal.pone.0113114, publisher: Public Library of Science\n[7] GeoNames (2018) GeoNames. URL https://www.geonames.org/\n[8] Greaves LM, Oldfield LD, Von Randow M, Sibley CG, Milne BJ (2020)\nHow low can we go? Declining survey response rates to new zealand elec-\ntoral roll mail surveys over three decades. Political Science 72(3):228–244,\nDOI 10.1080/00323187.2021.1898995, URL https://doi.org/10.1080/\n00323187.2021.1898995\n[9] Greaves LM, Lindsay Latimer C, Muriwai E, Moore C, Li E, Sporle A, Clark\nTC, Milne BJ (2023) M¯aori and the integrated data infrastructure: an assess-\nment of the data system and suggestions to realise m¯aori data aspirations [Te\nM¯aori me te Integrated Data Infrastructure: he aromatawai i te p¯unaha ra-\nraunga me ng¯a marohitanga e poipoia ai ng¯a wawata raraunga M¯aori]. Journal\nof the Royal Society of New Zealand 0(0):1–17, DOI 10.1080/03036758.2022.\n2154368, URL https://doi.org/10.1080/03036758.2022.2154368\n[10] Gromm´e F (2018) Is Facebook the future of the national census? The Conver-\nsation URL http://theconversation.com/\n[11] Henderson A (2003) Untapped talents: The employment and settlement experi-\nences of skilled Chinese in New Zealand. Unfolding history, evolving identity:\nthe Chinese in New Zealand pp 141–164\n[12] Hirschman A (1945) National Power and the Structure of Foreign Trade. Uni-\nversity of California Press\n\n\nDetecting Linguistic Diversity on Social Media\n25\n[13] Kachru BB (1982) The Other tongue: English across cultures. University of\nIllinois Press, Urbana-Champaign\n[14] Kemp S (2023) Digital 2023: New zealand. URL https://datareportal.\ncom/\n[15] Mart´ı P, Serrano-Estrada L, Nolasco-Cirugeda A (2019) Social media\ndata: Challenges, opportunities and limitations in urban studies. Com-\nputers,\nEnvironment\nand\nUrban\nSystems\n74:161–174,\nDOI\n10.1016/\nj.compenvurbsys.2018.11.001, URL https://www.sciencedirect.com/\nscience/article/pii/S0198971518302333\n[16] Milne BJ, Atkinson J, Blakely T, Day H, Douwes J, Gibb S, Nicolson M, Shack-\nleton N, Sporle A, Teng A (2019) Data Resource Profile: The New Zealand\nIntegrated Data Infrastructure (IDI). International Journal of Epidemiology\n48(3):677–677e, DOI 10.1093/ije/dyz014\n[17] Ministry\nof\nEducation\n(2023)\nStatistics.\nURL\nhttps://www.\neducationcounts.govt.nz\n[18] Mislove A, Lehmann S, Ahn YY, Onnela JP, Rosenquist J (2011) Understanding\nthe Demographics of Twitter Users. Proceedings of the International AAAI\nConference on Web and Social Media 5(1):554–557, URL https://ojs.\naaai.org/index.php/ICWSM/article/view/14168, number: 1\n[19] O’Byrne E, Bycroft C, Gibb S (2014) An initial investigation into the\npotential for administrative data to provide census long-form information:\nCensus Transformation. Tech. rep., Statistics New Zealand, URL https:\n//www.stats.govt.nz/\n[20] Sloan L, Morgan J (2015) Who tweets with their location? understanding\nthe relationship between demographic characteristics and the use of geoser-\nvices and geotagging on twitter. PLoS ONE 10(11):e0142209, DOI 10.\n1371/journal.pone.0142209, URL https://www.ncbi.nlm.nih.gov/pmc/\narticles/PMC4636345/\n[21] Sloan L, Morgan J, Housley W, Williams M, Edwards A, Burnap P, Rana\nO (2013) Knowing the Tweeters: Deriving Sociologically Relevant Demo-\ngraphics from Twitter. Sociological Research Online 18(3):74–84, DOI\n10.5153/sro.3001, URL https://doi.org/10.5153/sro.3001, publisher:\nSAGE Publications Ltd\n[22] Statistics New Zealand (2001) Introduction to the Census. Statistics New\nZealand, Wellington\n[23] Stats NZ (2018) Differences between Te Kupenga 2013 and 2018 surveys. Tech.\nrep., Stats NZ, URL https://www.stats.govt.nz/\n[24] Stats NZ (2019) 2018 Census collection response rates unacceptably low. URL\nhttps://www.stats.govt.nz/\n[25] Stats NZ (2020) Age and sex by ethnic group (grouped total responses), for\ncensus usually resident population counts, 2006, 2013, and 2018 Censuses (RC,\nTA, SA2, DHB). URL https://nzdotstat.stats.govt.nz/\n[26] Stats NZ (2020) Languages spoken (total responses) and birthplace (broad\ngeographic areas) by age group and sex, for the census usually resident\n\n\n26\nSidney Wong, Benjamin Adams, and Jonathan Dunn\npopulation count, 2006, 2013, and 2018 Censuses (RC, TA, DHB). URL\nhttps://nzdotstat.stats.govt.nz/\n[27] Stats NZ (2021) Languages spoken (information about this variable and its\nquality). URL https://datainfoplus.stats.govt.nz/\n[28] Stats NZ (2021) Regional Council 2018 Clipped (generalised). URL https:\n//datafinder.stats.govt.nz/\n[29] Stats NZ (2023) 2001 Census Language Spoken. URL https://statsnz.\ncontentdm.oclc.org/\n[30] Stats NZ (2023) General social survey (GSS). URL https://datainfoplus.\nstats.govt.nz/\n[31] Te Mana Raraunga: M¯aori Data Sovereignty Network (2018) Principles of\nM¯aori data sovereignty. URL https://www.temanararaunga.maori.nz/\n[32] Williams D (2022) Stats NZ braces for $250m census. Newsroom URL https:\n//www.newsroom.co.nz/\n[33] Williams ML, Burnap P, Sloan L (2017) Towards an Ethical Framework\nfor Publishing Twitter Data in Social Research: Taking into Account Users’\nViews, Online Context and Algorithmic Estimation. Sociology 51(6):1149–\n1168, DOI 10.1177/0038038517708140, URL https://doi.org/10.1177/\n0038038517708140\n\n\nDetecting Linguistic Diversity on Social Media\n27\nAppendix\nTable 5 Data collection points and regional council areas\nRegion\nData Collection Points\nNo. Tweets % of Corpus\nNorthland\nDargaville, Kawakawa, Kerikeri, Moerewa, Ngun-\nguru, Paihia, Taipa, Waimate North, Whang¯arei\n372,366\n3.7%\nAuckland\nAuckland, North Shore, Parakai, Waitakere, Wark-\nworth, Wellsford\n1,850,642\n18.5%\nWaikato\nCoromandel, Hamilton, Muriwai Beach, Ng¯atea,\nOtorohanga,\nPaeroa,\nPukekohe\nEast,\nRaglan,\nTairua, Taup¯o, Te Kauwhata, Thames, Tokoroa,\nT¯urangi, Waihi, Waiuku, Whangamata, Whitianga\n2,133,361\n21.3%\nBay of Plenty\nEdgecumbe,\nKatikati,\nKawerau,\nMurupara,\n¯Op¯otiki,\nRotorua,\nTauranga,\nWaihi\nBeach,\nWhakat¯ane\n383,597\n3.8%\nGisborne\nGisborne\n49,535\n0.5%\nHawkes Bay\nHastings, Napier, Wairoa\n218,106\n2.2%\nTaranaki\nEltham, H¯awera, New Plymouth, ¯Opunake, Patea,\nWaitara\n262,045\n2.6%\nManawat¯u-Wanganui Bulls, Foxton, Levin, Manakau, Palmerston North,\nWaiouru, Wanganui\n658,375\n6.6%\nWellington\nLower Hutt, Masterton, ¯Otaki, Porirua, Upper Hutt,\nWaipawa, Wellington\n1,244,145\n12.4%\nWest Coast\nGreymouth, Hokitika, Westport\n136,344\n1.4%\nCanterbury\nAmberley, Burnham, Christchurch, Darfield, Lee-\nston, Lincoln, Methven, Oxford, Pleasant Point,\nRolleston, Timaru, Woodend\n1,379,036\n13.8%\nOtago\nDunedin, ¯Oamaru, Queenstown, W¯anaka\n446,041\n4.5%\nSouthland\nBalclutha, Bluff, Gore, Invercargill, Milton, River-\nton, Te Anau, Winton\n237,929\n2.4%\nTasman\nBrightwater, M¯apua, Motueka, T¯akaka, Wakefield\n405,008\n4.0%\nMarlborough\nBlenheim, Picton\n235,719\n2.4%\n\n\n28\nSidney Wong, Benjamin Adams, and Jonathan Dunn\nTable 6 Demographic summary of regional council areas\nRegion\nPop. Density Median Age\nNorthland\n14.3\n42.6\nAuckland\n318.1\n34.7\nWaikato\n19.2\n37.4\nBay of Plenty\n25.6\n40.2\nGisborne\n5.7\n37.0\nHawkes Bay\n11.8\n40.6\nTaranaki\n16.2\n40.0\nManawat¯u-Wanganui\n10.7\n39.4\nWellington\n63.0\n37.2\nWest Coast\n5.4\n46.0\nCanterbury\n4.5\n45.5\nOtago\n1.4\n45.7\nSouthland\n13.5\n38.7\nTasman\n7.2\n38.2\nMarlborough\n3.1\n39.8\n\n\n"}
{"text": "Visual Attention Exploration in Vision-Based Mamba Models\nJunpeng Wang*\nVisa Research\nChin-Chia Michael Yeh†\nVisa Research\nUday Singh Saini‡\nVisa Research\nMahashweta Das§\nVisa Research\n𝑎\n𝑏\n𝑐\n𝑑\nFigure 1: Our visual exploration tool contains two visualization components. The Scatterplot view on the left shows the dimensionality\nreduction results. The Patch view on the right shows the patch layout and highlights the patches of interest.\nABSTRACT\nState space models (SSMs) have emerged as an efficient alterna-\ntive to transformer-based models, offering linear complexity that\nscales better than transformers. One of the latest advances in SSMs,\nMamba, introduces a selective scan mechanism that assigns trainable\nweights to input tokens, effectively mimicking the attention mech-\nanism. Mamba has also been successfully extended to the vision\ndomain by decomposing 2D images into smaller patches and arrang-\ning them as 1D sequences. However, it remains unclear how these\npatches interact with (or attend to) each other in relation to their\noriginal 2D spatial location. Additionally, the order used to arrange\nthe patches into a sequence also significantly impacts their attention\ndistribution. To better understand the attention between patches and\nexplore the attention patterns, we introduce a visual analytics tool\nspecifically designed for vision-based Mamba models. This tool\nenables a deeper understanding of how attention is distributed across\npatches in different Mamba blocks and how it evolves throughout\na Mamba model. Using the tool, we also investigate the impact of\ndifferent patch-ordering strategies on the learned attention, offering\nfurther insights into the model’s behavior.\n1\nINTRODUCTION\nState space models (SSMs) use state variables to mathematically\ndescribe the state of a dynamic system. They have a long history of\nmodeling time series problems, where the state variables are time-\ndependent. Recent advances [4,5,12] have shown that SSMs achieve\nperformance on par with state-of-the-art transformer models [3,13].\nAdditionally, their linear time complexity allows them to outperform\ntransformers in latency-critical applications.\n*e-mail: junpenwa@visa.com\n†e-mail: miyeh@visa.com\n‡e-mail: udasaini@visa.com\n§e-mail: mahdas@visa.com\nMamba [4], also known as S6, is a cutting-edge SSM that en-\nhances its predecessor S4 [5] with a selective scan mechanism. This\nmechanism enables the model to assign trainable weights to input\ntokens, allowing it to filter out less relevant information and empha-\nsize more relevant details. These trainable weights are similar to the\nattention mechanism in transformers, which determines how much\nfocus a token should place on other tokens. Since its release in late\n2023, vision-based applications of Mamba have been rapidly devel-\noped. Similar to how transformers are adapted for vision tasks [3],\nvision-based Mamba models also decompose images into smaller\npatches and arrange them into sequences as inputs.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9 10 11\n12 13 14 15\nroute 1\nroute 1 order\nroute 2\nroute 3\nroute 4\nFigure 2: The four-way cross-scan in VMamba [10].\nIn vision transformers [3], all image patches are fed into the\nmodels simultaneously, with positional encoding used to differenti-\nate patches at different positions. As a result, the order of patches\nhas a relatively minor impact. However, in vision-based Mamba\nmodels, the patches are fed into the models sequentially one af-\nter another (similar to RNNs). This means that the order of the\npatches is crucial, as each patch can only collect information from\nits preceding ones. To ensure that each patch has access to all other\npatches, both forward and backward scans are often performed. For\nexample, vision-Mamba (Vim) [17] uses routes 1 and 3 in Fig. 2 to\nallow each patch to “see” both its preceding and succeeding patches.\nVMamba [10] uses all four routes in Fig. 2 to preserve spatial locality\nbetween patches within the same row and column.\nA vision-based Mamba model consists of a hierarchy of Mamba\nblocks, each of which learns the attention between image patches.\nHowever, it remains unclear (1) if the attention pattern is fixed within\na block, (2) how attention patterns evolve across blocks, and (3) how\narXiv:2502.20764v1  [cs.LG]  28 Feb 2025\n\n\npatch-ordering strategies impact the attention pattern. This paper\nseeks to address these questions using a visual analytics approach.\nSpecifically, we extract the attention learned at each stage of the\nvision-based Mamba model and apply dimensionality reduction tech-\nniques to reveal attention patterns across stages. We also profile the\nattention patterns at each patch to disclose how attention is spatially\ndistributed relative to the patch’s position. Lastly, we introduce dif-\nferent patch-ordering strategies and compare their resulting attention\npatterns. In summary, our contributions are twofold:\n1. We design and develop a visual analytics tool to explore and\nsummarize attention patterns in vision-based Mamba models.\n2. We introduce multiple patch-ordering strategies and investigate\ntheir impact on patch attention in vision-based Mamba models.\n2\nBACKGROUND AND RELATED WORK\nOur work focuses on interpreting the attention mechanisms in vision-\nbased Mamba models. Here, we briefly review the history of SSMs\nand summarize early work on attention interpretation.\n2.1\nState Space Models (SSMs)\nState space models (SSMs) are sequence modeling techniques that\ndate back to the 1960s. They gained significant attention after Gu et\nal. [5] integrated the HiPPO matrix into them. The resulting model,\nknown as S4, demonstrated substantial improvements in efficiency\nfor modeling long sequences. However, the performance of S4\nis not comparable to that of transformers, as it cannot effectively\ndistinguish different input tokens. To overcome this, Gu and Dao [4]\nintroduced the selective scan mechanism into S4, giving rise to S6,\nalso known as Mamba. The selective scan allows Mamba to assign\ndifferent weights to input tokens, enabling the model to learn the\nrelative importance of each token. This allows Mamba to focus more\non important tokens and selectively ignore less relevant ones. The\nlearned weights are similar to the attention weights in transformers.\nMamba demonstrates performance on par with transformers across a\nwide range of applications. More importantly, its linear complexity\nmakes it a superior choice for many latency-critical applications,\nwhere the quadratic complexity of transformers would be prohibitive.\ns×𝑠×3\n×2\ns/8×𝑠/8×2ℎ\ndown   sample\ns/16×𝑠/16×4ℎ\ns/32×𝑠/32×8ℎ\ns/32×𝑠/32×8ℎ\navg_pool\nlinear\n# class\ns/8×𝑠/8×2ℎ\ns/16×𝑠/16×4ℎ\ndecompose\n𝑏𝑙𝑜𝑐𝑘\n𝑏𝑙𝑜𝑐𝑘\n𝑏𝑙𝑜𝑐𝑘\n𝑏𝑙𝑜𝑐𝑘\ndown   sample\ndown   sample\n×2\n×8\n×2\n𝑠𝑡𝑎𝑔𝑒 0\n𝑠𝑡𝑎𝑔𝑒 1\n𝑠𝑡𝑎𝑔𝑒 2\n𝑠𝑡𝑎𝑔𝑒 3\n𝑝×𝑝×ℎ\n𝑝×𝑝×ℎ\n𝑠𝑐𝑎𝑛\n𝑚𝑒𝑟𝑔𝑒\ns/4×𝑠/4×ℎ\ns/4×𝑠/4×ℎ\n𝑎\nFigure 3: The architecture of VMamba for image classification [10].\nIn the vision domain, vision-Mamba (Vim) [17] and VMamba\n[10] are two seminal works that extend Mamba from NLP to vision\ntasks. Fig. 3 illustrates the architecture of VMamba for image clas-\nsification. Given an RGB input image of size s×s, the model first\ndecomposes it into p×p smaller patches. Each patch is considered\nas a token, and all patches are arranged into a sequence of length\np×p. This sequence is then treated similarly to textual data and fed\ninto Mamba for learning. Specifically, the VMamba block in Fig. 3a\nemploys four different scan orders (Fig. 2) to process the patches:\n(1) left→right then top→down, (2) top→down then left→right, (3)\nright→left then bottom→up, and (4) bottom→up then right→left.\nEach scan order introduces a unique dependency between patches,\nand consequently, the learned attention between patches also varies.\nThe merge operation aggregates the attention learned for each patch\nacross the four different scan orders. The output of this process\nis a p2×p2 attention matrix, where each element at position (i, j)\nrepresents the attention strength between patch i and patch j. Stack-\ning multiple scan-merge blocks forms a stage, and Fig. 3 shows\nfour such stages, each containing 2, 2, 8, and 2 blocks, respectively.\nBetween stages, the latent representations are downsampled to distill\nessential features. Finally, a fully-connected layer transforms the\nlatent representation into a vector, the length of which corresponds\nto the number of possible classes. Each element in this vector deter-\nmines the probability for the corresponding class. Vim [17] follows\na similar process, but uses only a two-way scan (routes 1 and 3 in\nFig. 2). Therefore, we focus on VMamba in this work.\n2.2\nAttention Analysis and Visualization\nInterpreting and diagnosing machine learning models is an important\ntopic in the visualization literature [6,9,15], and there are multiple\nworks focusing on interpreting the attention mechanisms of these\nmodels. For example, Abnar and Zuidema [1], Park et al. [11], and\nLi et al. [7] all used heatmaps to externalize the attention strengths in\ntransformer-based NLP models. Vig [14] employed multiple parallel\ncoordinate plots to visualize the attention patterns in BERT and GPT-\n2 models. DeRose et al. [2] introduced a radial layout for visualizing\nattention in BERT, layer by layer, which also facilitates comparisons\nbetween the attentions of two models. For vision transformers, Li et\nal. [8] used dimensionality reduction and scatterplots to summarize\nattention patterns within self-attention heads and across attention\nlayers. Yeh et al. [16] employed a similar approach to visualize\nthe joint q/k embedding space, providing a global view of attention\npatterns across a transformer model.\nTo the best of our knowledge, no studies have yet focused on\nanalyzing the attention patterns in Mamba models. In particular, we\nare interested in whether regular attention patterns emerge within a\nMamba block and whether hierarchical attention patterns develop\nacross different stages of a Mamba model. To explore these ques-\ntions, we have developed a visual analytics tool specifically designed\nto analyze attention patterns in vision-based Mamba models.\n3\nPROBLEM AND METHODOLOGY\nIn this study, we aim to investigate the attention patterns in Mamba,\nfocusing on two key aspects based on insights from existing literature\nand the interests of domain experts working with Mamba:\n• Inter-Block Attention Pattern: Are the attention patterns con-\nsistent across different Mamba blocks within the same stage?\n• Intra-Block Attention Pattern: How is the attention spatially\ndistributed across image patches within a single Mamba block?\nWe provide solutions to answer these two questions in Sec. 3.1\nand Sec. 3.2, respectively. For our study, we focus on the VMamba\nmodel [10], whose architecture is illustrated in Fig. 3. The model is\ntrained for 400 epochs on the ImageNet dataset (1,281,167 training\nimages). After training, we use 1000 test images to investigate the\nattention patterns. The input images are of size 224×224×3 (i.e.,\ns=224 in Fig. 3) and the number of patches at the four stages is\n56×56, 28×28, 14×14, and 7×7, respectively.\n3.1\nInter-Block Attention Pattern\nThe attention matrices for different Mamba blocks at the same stage\nhave the same size. For example, in Fig. 3, the attention matrices\nfrom the two blocks at stage 0 are of the same size, which is 562×562\nwhen s=224. This allows us to compare the matrices and assess\nhow the attention patterns differ across blocks at the same stage.\nHowever, attention matrices from blocks across different stages have\nvarying sizes, and thus, these blocks are not directly comparable.\nWhen an image is fed through a Mamba block, it produces an\nattention matrix of size p2×p2, where the element at position (i, j)\nrepresents the attention strength from patch i to patch j. For example,\n\n\nAlgorithm 1 Generating cluster pattern across blocks of a stage.\nRequire: n,m\n▷number of images and blocks\nRequire: stage\n▷the focused stage\nRequire: image\n▷the array of images\nRequire: VMamba,DR ▷the VMamba model and DR algorithm\n1: attentions = []\n2: for i ←1 to n do\n3:\nfor j ←1 to m do\n4:\nattention = VMamba(image[i],stage, j)\n5:\nattentions.append(attention)\n6:\nend for\n7: end for\n8: points = DR(attentions)\n▷points ∈Rm×n×2\nat stage 0 in Fig. 3, the attention matrix will be of shape 562×562. If\nthe focused stage contains m blocks, we will obtain m such p2×p2\nmatrices. By feeding all n=1000 test images through the VMamba\nmodel and collecting their attention matrices from the m blocks at\nthe focused stage, we compile the attention data into a large matrix\nof shape (m×n)×(p2×p2). This process is outlined in lines 1 to 7\nof the pseudo-code in Algorithm 1.\nWe then apply dimensionality reduction (DR) techniques to map\nthe m×n points, originally in the p2×p2 dimensional space, to a 2D\nspace for visualization as a scatterplot. If the n points from different\nblocks form isolated clusters, this indicates that the attention patterns\nbetween the blocks are significantly different. The details of the\nvisualization and interactions are described later in Sec. 4.1.\n3.2\nIntra-Block Attention Pattern\nAccording to the formulation in Sec. 3.1, the attention matrix for\na single block and a single image has the shape p2×p2 (line 4 of\nAlgorithm 1). The i-th row of this matrix represents the attention\nstrength from patch i to all p2 patches. To identify patches with\nsimilar attention patterns, we can apply dimensionality reduction\n(DR) techniques to reduce the p2×p2 matrix to a p2×2 matrix,\nwhich can then be analyzed through a scatterplot visualization to\nassess the similarity between patches.\nHowever, the approach described above can be significantly influ-\nenced by the image content, making it difficult to extract common,\ncontent-agnostic patterns. To enhance the identification of common\nattention patterns, we generate the p2×p2 attention matrix for all n\nimages and aggregate the resulting n matrices into a single p2×p2\nmatrix for pattern augmentation. The details of this process are\noutlined in Algorithm 2 (lines 1 to 6).\nWe employ a scatterplot to visualize the p2 2D points. The color\nand size of each point correspond to the column and row of the\nrespective patch. By examining the clustering patterns of these\npoints, we can identify patches with similar attention patterns and\nrelate these similarities to their spatial position in the image space\n(detailed in Sec. 4.1).\nAlgorithm 2 Generating cluster pattern within a block.\nRequire: n\n▷number of images\nRequire: stage,block\n▷the focused stage and block\nRequire: image\n▷the array of images\nRequire: VMamba,DR ▷the VMamba model and DR algorithm\n1: attentions = []\n2: for i ←1 to n do\n3:\nattention = VMamba(image[i],stage,block)\n4:\nattentions.append(attention)\n5: end for\n6: avg attn = np.mean(np.array(attentions),axis = 0)\n7: points = DR(avg attn)\n▷points ∈Rp2×2\n4\nVISUAL ANALYTICS SYSTEM\nWe have developed a visual analytics tool to effectively visualize\nthe outputs of Algorithm 1 and Algorithm 2. The tool features two\ndistinct visualization views: the Scatterplot view (Fig. 1, left) and\nthe Patch view (Fig. 1, right).\n4.1\nThe Scatterplot View\nThe Scatterplot view provides two visualization modes for exploring\nthe inter-block and intra-block attention patterns.\nMode 1:\nUsers can select a stage and a block from the header\nof the Scatterplot view (Fig. 1, left). If the block ID is not specified,\nthe Scatterplot will display m×n points based on the output of\nAlgorithm 1, where m represents the number of blocks in the selected\nstage, and n=1000 is the number of images. Each point corresponds\nto a p2×p2 attention matrix for a given image from a specific block.\nFor example, in the VMamba architecture depicted in Fig. 3, there\nare 4 stages, with 2, 2, 8, and 2 blocks per stage, respectively. The\ndimensionality reduction results for the four stages are shown in\nFig. 4, where each figure represents the output of a single stage.\nThe color in each figure corresponds to the block ID, and distinct\nclusters of points are clearly visible, with different colors separating\nthe blocks. This indicates that blocks within the same stage exhibit\nnoticeably different attention patterns.\n𝑠𝑡𝑎𝑔𝑒 0\n𝑠𝑡𝑎𝑔𝑒 1\n𝑠𝑡𝑎𝑔𝑒 2\n𝑠𝑡𝑎𝑔𝑒 3\n𝑏𝑙𝑜𝑐𝑘 0\n𝑏𝑙𝑜𝑐𝑘 1\n𝑏𝑙𝑜𝑐𝑘 0\n𝑏𝑙𝑜𝑐𝑘 1\n𝑏𝑙𝑜𝑐𝑘 0\n𝑏𝑙𝑜𝑐𝑘 1\n0\n1\n2\n3\n4\n5\n6\n7\nFigure 4: Attention pattern similarity between blocks from the four\nstages of the VMamba model in Fig. 3.\nMode 2:\nWhen a block ID is specified, the Scatterplot will show\np2 points based on the output of Algorithm 2. In this case, each point\nrepresents the averaged attention across n=1000 test images for a\nspecific patch position. The color and size of each point correspond\nto the column and row of the corresponding patch, respectively. As\nillustrated by the clustering pattern in Fig. 1, patches from the same\nrow (points in Fig. 1a with the same size) or the same column (points\nin Fig. 1b with the same color) exhibit similar attention patterns.\nFigs. 1c and 1d reveal further clusters that share similar attention\npatterns to those in Figs. 1a and 1b, respectively.\nFor both modes, we provide three popular dimensionality reduc-\ntion techniques: PCA, tSNE, and UMAP. Each technique has its\nunique strengths and limitations, so offering multiple options allows\nusers to explore different perspectives and uncover subtle clustering\npatterns. When processing very high-dimensional data using tSNE\nand UMAP, we first use PCA to project them to a relatively lower\ndimension (i.e., 100D), then apply tSNE and UMAP for efficiency.\n\n\nThe Scatterplot view also supports zooming, enabling users to ex-\namine cluster details at various levels of granularity. In Mode 2,\nusers can perform lasso selection to highlight a group of points. The\ncorresponding patches for the selected points will then be visualized\nin the Patch view (described in the following section).\n4.2\nThe Patch View\nThe Patch view also offers two visualization modes, allowing users\nto explore patch-level details from the selected stage.\nMode 1:\nIn this mode, the patches from the selected stage are\ndisplayed as gray squares. For example, in Fig. 1 (right), the 28×28\npatches from stage 1 are visualized as 28×28 squares. Meanwhile,\nthe selected patches from the Scatterplot view (Fig. 1, left) are\nhighlighted as red squares in the Patch view, visually indicating\ntheir spatial location within the image. This highlighting directly\ncorresponds to the cluster patterns observed in the Scatterplot view\n(Fig. 1, left), which helps reveal the spatial relationships between\npatches that exhibit similar attention patterns. This coordinated\nvisualization provides a clear understanding of how attention patterns\nare distributed across different regions of the image.\n𝑎\n𝑏\n𝑐\nrow: 5, col:4\nrow: 6, col:4\nrow: 7, col:4\nsmall\nlarge\nFigure 5: Patches in the same column exhibit similar attention.\nMode 2:\nWhen a square/patch is clicked in the Patch view, all\nsquares/patches will be colored according to their attention strength\nto the clicked one. Attention strengths from small to large are\nmapped to colors from light-yellow to dark-red. For example, in\nFig. 5a, the clicked patch is located at row 5, column 4. This patch\nshows strong attention to: (1) itself, (2) the patches in the same\nrow to its left, and (3) the patches to its right. From the Scatterplot\nview, we noticed that patches in the same column as the selected\npatch have similar attention patterns. Based on this, we select two\nadditional patches for further exploration. As shown in Figs. 5b\nand 5c, these two patches, which are in the same column as the one\nin Fig. 5a, exhibit very similar attention patterns.\n5\nFINDINGS AND PATTERN SUMMARY\nUsing the developed visual analytics tool, we conducted a detailed\nexploration of the four stages and their respective blocks in the\nVMamba model (Fig. 3). This section summarizes the key findings\nfrom our analysis.\nFinding 1:\nBlocks within the same stage exhibit significantly\ndifferent attention patterns. This finding is clearly illustrated in\nFig. 4. To examine the differences further, we focus on the two\nblocks at stage 0 for a detailed exploration. As shown in Figs. 6-a1\n(stage 0, block 0) and b1 (stage 0, block 1), the color and size of\nthe points (representing individual patches) change progressively\nalong both the row and column directions. This indicates a gradual\ntransition of the attention pattern for patches along rows and columns.\nWhen individual patches are clicked in the Patch view, their attention\npatterns are displayed on the right. It is evident that the same patch\nfrom the two blocks exhibits very different attention patterns. In\nblock 0 (Fig. 6-a2), the selected patch strongly attends to (1) itself,\n(2) patches in the first row with a larger column ID, (3) patches in\nthe first column with a larger row ID, and (4) patches that have both\nlarger row and column IDs than the selected patch. In contrast, the\npatch in block 1 (Fig. 6-b2) pays noticeably weaker attention to itself\nand patches along similar rows and columns, but stronger attention\nto patches in other areas of the image. We checked multiple patches\nfrom these two blocks, and the observation is consistent across them.\nFinding 2:\nSome blocks exhibit complementary attention pat-\nterns. For instance, the selected patch at stage 2, block 1 (Fig. 6-e2)\nshows stronger attention to itself as well as to patches in the same\nrow and column. In contrast, the same patch at stage 2, block 3\n(Fig. 6-f2) and block 5 (Fig. 6-g2) exhibits weaker attention to the\npatch itself and to patches along the same row and column. This\ncomplementary attention behavior across blocks enables VMamba\nto selectively focus on relevant patches, contributing to its flexibility\nin attending to different regions of the input images.\nFinding 3:\nThere is a hierarchy regarding the attention pattern\nlearned from early to later stages. At early stages, patches that are\nspatially closer tend to exhibit similar attention patterns, while at\nlater stages, the attention is more influenced by the image content.\nAt stage 0 (Fig. 6, a1-b1), attention patterns show smooth and pro-\ngressive changes for patches in the same row and column, indicating\na strong spatial correlation. At stage 1 (Fig. 6, c1-d1), clear clusters\nform for patches in the same row and column—represented by points\nof the same color but varying sizes or the same size but different\ncolors. This pattern persists at stage 2 (Fig. 6, e1-h1), though clus-\ntering is less pronounced for some blocks at this stage. By stage\n3, the clustering structure becomes less obvious in the Scatterplot\nview (Fig. 6, i1-j1), suggesting that the attention patterns have be-\ncome more diverse and content-dependent. This phenomenon aligns\nwith the behavior observed in CNNs and vision transformers, where\nlower layers tend to focus on local, content-agnostic features, while\nhigher layers capture more complex, content-relevant patterns.\n6\nIMPACT OF PATCH ORDER ON ATTENTION PATTERNS\nOne of our key findings is that patches within the same row or\ncolumn often exhibit similar attention patterns. We hypothesize that\nthis is influenced by the order in which patches are arranged into\nsequences. To test this hypothesis, we introduce three alternative\npatch orders, shown in Fig. 7, and examine how the attention patterns\nchange when these new orders are applied.\nFig. 7a shows a patch order that scans patches along the diagonal.\nFig. 7b employs the Morton order (z-order curve), a well-known\nspace-filling curve that is particularly effective at preserving spatial\nlocality. Fig. 7c arranges patches in a spiral layout, where the\ninnermost patch retains the highest spatial locality. We modified the\nVMamba code to implement each of these patch orders and trained\nthe model from scratch for 400 epochs. All three scanning methods\nachieved accuracy levels similar to the original VMamba model, i.e.,\nachieving accuracy greater than or equal to 82.6% on ImageNet as\nreported in the original VMamba paper [10].\nFigs. 8-a1, a2, and a3 show the results of exploring attention\npatterns using the diagonal order. In Fig. 8-a1, all patches are\nprojected onto a continuous curve. By selecting points in a local\nregion, we observe that the corresponding patches are adjacent along\nthe diagonal, as shown in Fig. 8-a2. When clicking on any patch\nin the Patch view, we see that its attention behavior mirrors what\nwas observed in Fig. 6 but along the diagonals instead of the rows\nor columns. For example, the attention pattern of a patch at stage 1,\nblock 0 is displayed in Fig. 8-a3. This patch strongly attends to its\npreceding patches along the diagonal, which is very similar to what\nwas observed in Fig. 6-c2. However, in Fig. 6-c2, the preceding\npatches are those based on the cross-scan order in Fig. 2.\nNext, we conduct similar explorations with the VMamba model\ntrained using the Morton order. As shown in Fig. 8-b1, patches at\nstage 1, block 0 are grouped into clusters. Selecting a cluster high-\nlights the patches within a local region in Fig. 8-b2. These patches\nare actually in the same z-order curve block. The strong spatial lo-\ncality among them results in similar attention patterns. Clicking on a\n\n\n𝑠𝑡𝑎𝑔𝑒 0, 𝑏𝑙𝑜𝑐𝑘 1, 𝑈𝑀𝐴𝑃\n𝑠𝑡𝑎𝑔𝑒 2 𝑏𝑙𝑜𝑐𝑘 1, 𝑃𝐶𝐴\n𝑠𝑡𝑎𝑔𝑒 2 𝑏𝑙𝑜𝑐𝑘 5, 𝑡𝑆𝑁𝐸\n𝑠𝑡𝑎𝑔𝑒 2 𝑏𝑙𝑜𝑐𝑘 7, 𝑡𝑆𝑁𝐸\n𝑠𝑡𝑎𝑔𝑒 3 𝑏𝑙𝑜𝑐𝑘 0, 𝑃𝐶𝐴\n𝑠𝑡𝑎𝑔𝑒 3 𝑏𝑙𝑜𝑐𝑘 1, 𝑃𝐶𝐴\n𝑠𝑡𝑎𝑔𝑒 2 𝑏𝑙𝑜𝑐𝑘 3, 𝑃𝐶𝐴\n𝑠𝑡𝑎𝑔𝑒 0, 𝑏𝑙𝑜𝑐𝑘 0, 𝑈𝑀𝐴𝑃\n𝑠𝑡𝑎𝑔𝑒 1, 𝑏𝑙𝑜𝑐𝑘 1, 𝑡𝑆𝑁𝐸\n𝑠𝑡𝑎𝑔𝑒 1, 𝑏𝑙𝑜𝑐𝑘 0, 𝑡𝑆𝑁𝐸\n# 𝑝𝑎𝑡𝑐ℎ: 56 × 56\n# 𝑝𝑎𝑡𝑐ℎ: 56 × 56\n# 𝑝𝑎𝑡𝑐ℎ: 28 × 28\n# 𝑝𝑎𝑡𝑐ℎ: 28 × 28\n# 𝑝𝑎𝑡𝑐ℎ: 14 × 14\n# 𝑝𝑎𝑡𝑐ℎ: 14 × 14\n# 𝑝𝑎𝑡𝑐ℎ: 14 × 14\n# 𝑝𝑎𝑡𝑐ℎ: 14 × 14\n# 𝑝𝑎𝑡𝑐ℎ: 7 × 7\n# 𝑝𝑎𝑡𝑐ℎ: 7 × 7\n𝑜𝑛𝑒 𝑟𝑜𝑤\n𝑎1\n𝑎2\n𝑏1\n𝑏2\n𝑐1\n𝑐2\n𝑑1\n𝑑2\n𝑒1\n𝑒2\n𝑓1\n𝑓2\n𝑔1\n𝑔2\nℎ1\nℎ2\n𝑖1\n𝑖2\n𝑗1\n𝑗2\nFigure 6: The attention pattern similarity between patches from blocks of different stages. (a1-j1) Two, two, four, and two blocks from stages 0, 1,\n2, and 3 are shown, respectively. (a2-j2) Selecting a patch to inspect its attention pattern.\n\n\n𝐷𝑖𝑎𝑔𝑛𝑎𝑙\n𝑀𝑜𝑟𝑡𝑜𝑛\n𝑎\n𝑏\n𝑆𝑝𝑖𝑟𝑎𝑙\n𝑐\n𝑟𝑜𝑢𝑡𝑒 1\n𝑟𝑜𝑢𝑡𝑒 2\n𝑟𝑜𝑢𝑡𝑒 3\n𝑟𝑜𝑢𝑡𝑒 4\nFigure 7: Arranging patches in VMamba following different orders.\npatch reveals that it also strongly attends to its preceding patches, as\nshown in Fig. 8-b3. The results for the spiral patch order, shown in\nFigs. 8-c1, c2, and c3, are consistent with those observed from other\npatch orders. These findings confirm that stage 1, block 0 of the\nVMamba model exhibits a fixed attention pattern: any patch in this\nblock consistently attends strongly to its preceding patches. How-\never, the definition of “preceding patches” depends on the specific\npatch order used.\n𝐷𝑖𝑎𝑔𝑜𝑛𝑎𝑙:  𝑠𝑡𝑎𝑔𝑒 1, 𝑏𝑙𝑜𝑐𝑘 0, 𝑡𝑆𝑁𝐸\n𝑎1\n𝑎2\n𝑎3\n𝑏1\n𝑏2\n𝑏3\n𝑀𝑜𝑟𝑡𝑜𝑛:  𝑠𝑡𝑎𝑔𝑒 1, 𝑏𝑙𝑜𝑐𝑘 0, 𝑡𝑆𝑁𝐸\n𝑆𝑝𝑖𝑟𝑎𝑙:  𝑠𝑡𝑎𝑔𝑒 1, 𝑏𝑙𝑜𝑐𝑘 0, 𝑡𝑆𝑁𝐸\n𝑐1\n𝑐2\n𝑐3\nFigure 8: Exploring attention patterns when patches are arranged\nfollowing the three new orders in Fig. 7: Diagonal, Morton, and Spiral.\n7\nCONCLUSION AND FUTURE WORK\nIn this paper, we introduced a visual analytics tool to explore\nand compare attention patterns within and across VMamba blocks.\nThrough our exploration, we discovered several key insights: (1)\nVMamba blocks within the same stage exhibit distinct attention\npatterns; (2) the order of patches significantly influences the result-\ning attention patterns; and (3) patches that are close in the input\nsequence generally exhibit similar attention patterns. Given the\nsignificant impact of patch arrangement on attention patterns, we\nproposed multiple new patch orders that better preserve the patches’\nspatial locality. Using our tool, we further investigated VMamba\nmodels trained with these new orders, and found similar attention\nbehaviors to those observed in the original patch order.\nLooking ahead, we aim to extend our tool in several directions.\nFirst, we plan to explore more complex VMamba models with addi-\ntional stages and blocks per stage. We hypothesize that the general\ntrends observed in this study will hold as the architecture becomes\nmore intricate. Second, our current work focuses on identifying\ncontent-agnostic attention patterns by averaging attention over n\nimages. In the future, we intend to integrate additional views into\nthe tool to present content-relevant attention patterns, which will be\nparticularly useful for diagnosing specific images of interest.\nREFERENCES\n[1] S. Abnar and W. Zuidema. Quantifying attention flow in transformers.\nIn Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, pp. 4190–4197, July 2020.\n[2] J. F. DeRose, J. Wang, and M. Berger. Attention flows: Analyzing and\ncomparing attention mechanisms in language models. IEEE Trans. Vis.\nComput. Graphics, 27(2):1160–1170, 2020.\n[3] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In 9th International Conference\non Learning Representations (ICLR), 2021.\n[4] A. Gu and T. Dao. Mamba: Linear-time sequence modeling with\nselective state spaces. arXiv preprint arXiv:2312.00752, 2023.\n[5] A. Gu, K. Goel, and C. R´e. Efficiently modeling long sequences\nwith structured state spaces. In International Conference on Learning\nRepresentations, 2022.\n[6] F. Hohman, M. Kahng, R. Pienta, and D. H. Chau. Visual analytics\nin deep learning: An interrogative survey for the next frontiers. IEEE\nTransactions on Visualization and Computer Graphics, 25(8):2674–\n2693, 2018.\n[7] R. Li, W. Xiao, L. Wang, H. Jang, and G. Carenini. T3-vis: visual ana-\nlytic for training and fine-tuning transformers in NLP. In Proceedings\nof the 2021 Conference on Empirical Methods in Natural Language\nProcessing: System Demonstrations, pp. 220–230, 2021.\n[8] Y. Li, J. Wang, X. Dai, L. Wang, C.-C. M. Yeh, Y. Zheng, W. Zhang,\nand K.-L. Ma. How does attention work in vision transformers? a visual\nanalytics attempt. IEEE Transactions on Visualization and Computer\nGraphics, 29(6):2888–2900, 2023.\n[9] S. Liu, W. Yang, J. Wang, and J. Yuan. Visualization for Artificial\nIntelligence. Springer Cham, 2025. doi: 10.1007/978-3-031-75340-4\n[10] Y. Liu, Y. Tian, Y. Zhao, H. Yu, L. Xie, Y. Wang, Q. Ye, J. Jiao, and\nY. Liu. VMamba: Visual state space model. In The Thirty-eighth\nAnnual Conference on Neural Information Processing Systems, 2024.\n[11] C. Park, I. Na, Y. Jo, S. Shin, J. Yoo, B. C. Kwon, J. Zhao, H. Noh,\nY. Lee, and J. Choo. Sanvis: Visual analytics for understanding self-\nattention networks. In 2019 IEEE Visualization Conference (VIS), pp.\n146–150. IEEE, 2019.\n[12] J. T. Smith, A. Warrington, and S. W. Linderman. Simplified state\nspace layers for sequence modeling. arXiv preprint arXiv:2208.04933,\n2022.\n[13] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin. Attention is all you need. Ad-\nvances in Neural Information Processing Systems, 2017.\n[14] J. Vig. A multiscale visualization of attention in the transformer model.\narXiv preprint arXiv:1906.05714, 2019.\n[15] J. Wang, S. Liu, and W. Zhang. Visual analytics for machine learning:\nA data perspective survey. IEEE Transactions on Visualization and\nComputer Graphics, 30(12):7637–7656, 2024.\n[16] C. Yeh, Y. Chen, A. Wu, C. Chen, F. Vi´egas, and M. Wattenberg. At-\ntentionviz: A global view of transformer attention. IEEE Transactions\non Visualization and Computer Graphics, 30(01):262–272, 2024.\n[17] L. Zhu, B. Liao, Q. Zhang, X. Wang, W. Liu, and X. Wang. Vision\nmamba: Efficient visual representation learning with bidirectional state\nspace model. arXiv preprint arXiv:2401.09417, 2024.\n\n\n"}
{"text": "Regional climate projections using a deep-learning–based\nmodel-ranking and downscaling framework: Application to\nEuropean climate zones\nParthiban Loganathan1*, Elias Zea1, Ricardo Vinuesa1, Evelyn Otero1\n1Department of Engineering Mechanics, KTH Royal Institute of Technology,\nStockholm, Sweden.\n*Corresponding author(s). E-mail(s): parthi@kth.se;\nAbstract\nAccurate regional climate forecast calls for high-resolution downscaling of Global Climate Mod-\nels (GCMs). This work presents a deep-learning-based multi-model evaluation and downscaling\nframework ranking 32 Coupled Model Intercomparison Project Phase 6 (CMIP6) models using\na Deep Learning-TOPSIS (DL-TOPSIS) mechanism and so refines outputs using advanced deep-\nlearning models. Using nine performance criteria, five K¨oppen-Geiger climate zones—Tropical,\nArid, Temperate, Continental, and Polar—are investigated over four seasons. While TaiESM1\nand CMCC-CM2-SR5 show notable biases, ranking results show that NorESM2-LM, GISS-E2-\n1-G, and HadGEM3-GC31-LL outperform other models. Four models contribute to downscaling\nthe top-ranked GCMs to 0.1° resolution. Vision Transformer (ViT), Geospatial Spatiotemporal\nTransformer with Attention and Imbalance-Aware Network (GeoSTANet), CNN-LSTM, CNN-\nLong Short-Term Memory (ConvLSTM). Effectively capturing temperature extremes (TXx, TNn),\nGeoSTANet achieves the highest accuracy (Root Mean Square Error (RMSE) = 1.57°C, Kling-\nGupta Efficiency (KGE) = 0.89, Nash-Sutcliffe Efficiency (NSE) = 0.85, Correlation (r) = 0.92, so\nreducing RMSE by 20% over ConvLSTM. CNN-LSTM and ConvLSTM do well in Continental and\nTemperate zones; ViT finds fine-scale temperature fluctuations difficult. These results confirm that\nmulti-criteria ranking improves GCM selection for regional climate studies and transformer-based\ndownscaling exceeds conventional deep-learning methods. This framework offers a scalable method\nto enhance high-resolution climate projections, benefiting impact assessments and adaptation plans.\nKeywords Climate Downscaling, GCM Ranking, High-Resolution Climate Projections, K¨oppen-\nGeiger Climate Zones, Regional Climate Impact, Transformer-Based Downscaling.\n1 Introduction\nConsidered one of the most pressing global concerns of the twenty-first century, climate change has\npossibly major consequences on ecosystems, infrastructure, and society all around Summary for Pol-\nicymakers (2023). Understanding and forecasting climate dynamics has never been more important\nas world temperatures rise and severe storms become more frequent. Expanding our scientific knowl-\nedge of past, current, and future climatic conditions in this setting depends on numerical models\nthat faithfully depict the intricate physical processes of the Earth system. Among climate modelling\nagencies, the Coupled Model Intercomparison Project (CMIP) has become a core cooperative effort\npromoting worldwide cooperation and standardizing GCMs Eyring et al. (2016). Currently in its sixth\nphase (CMIP6), the project has driven the creation of ever more advanced GCMs simulating climate\nvariability and change with improved realism Intergovernmental Panel on Climate Change (IPCC)\n(2023). These models usually run at coarse resolutions—often between 50 and 250 km grid cells—that\n1\narXiv:2502.20132v2  [cs.LG]  28 Feb 2025\n\n\nare insufficient for resolving localized climate events, even if they provide insightful analysis at global\nand continental scales. Such coarse resolution reduces the capacity to represent important regional\naspects including localized precipitation extremes, temperature changes in complicated topography,\nand subtle land-sea contrasts Fowler et al. (2007); Haarsma et al. (2016). Effective regional climate\nimpact assessments and adaptation planning depend on downscaling techniques that can convert\ncoarse GCM data into high-resolution localized estimates, so there is a great demand for them.\nEmerging as a vital answer to the gap between the coarse spatial resolution of GCM outputs and\nthe high-resolution data needed for local decision-making are downsizing methods. Dynamic and sta-\ntistical downscaling are the two main approaches that are now in use. Dynamic downscaling nests\nhigh-resolution simulations inside the boundary conditions given by global models using Regional\nClimate Models (RCMs). This method enables the explicit simulation of local physical processes,\nincluding those affected by intricate topography and land-use features Rosenzweig et al. (2014); Cava-\nzos et al. (2024). Statistical downscaling techniques, on the other hand, depend on experimentally\ngenerated connections between local observations and major climate factors. Fine-resolution climate\ndata has been produced using extensively applied techniques like quantile mapping, weather typing,\nweather generators, and regression-based processes. Generally speaking, statistical approaches are\npreferred because of their reduced computational requirements and capacity to include site-specific\nobservational data—which can be very helpful when addressing localized climate events von Storch\n(2011). Both dynamic and statistical downscaling techniques have difficulties notwithstanding their\nstrengths, especially in areas with high climatic fluctuation where observational data may be few. The\nneed to choose the most appropriate GCMs for downscaling becomes even more evident as regional\nstudies progressively need customized climate projections, enabling sophisticated model evaluation\nand selection methods.\nChoosing the best GCM for regional climate research is a difficult and important chore since dif-\nferent models may show differing performances depending on the certain region and climate variable\nunder examination. Variability in model physics, parameterizations, initial circumstances, and the\nmodelling of important feedback processes means no single model can routinely outperform others\nacross all scenarios Riahi et al. (2017); Santer et al. (2014). Multi-criteria decision-making (MCDM)\ntechniques provide a disciplined framework for assessing several, occasionally contradictory, perfor-\nmance indicators to negotiate this complexity. The Technique for Order Preference by Similarity\nto Ideal Solution (TOPSIS), which ranks options by evaluating their proximity to an ideal solution\nwhile concurrently calculating their distance from the anti-ideal, or worst-case scenario Chen et al.\n(1981); Yoon and Hwang (1995), is one well-established MCDM method. Conversely, conventional\nTOPSIS implementations depend on either fixed or subjectively set weights for every performance\ncriterion, a method that could result in biased or inconsistent model selections Wang et al. (2009).\nDL-TOPSIS has been created as a novel approach to solve this restriction. DL-TOPSIS offers a more\nobjective and flexible ranking system by learning a neural network to determine the ideal weights\nstraight from the data. This method combines a wide range of performance criteria—including bias,\nRoot Mean Square Error (RMSE), Pearson correlation (r), Kling-Gupta Efficiency (KGE), Nash-\nSutcliffe Efficiency (NSE), and measures of distribution overlap—to guarantee that the chosen GCM\nis best suited for catching the climatic characteristics relevant to a given region Dwivedi et al. (2019);\nSchuster (2004).\nDefined as the variation between the daily maximum (tasmax) and minimum (tasmin) tempera-\ntures, the diurnal temperature range (DTR) has become more important as a climatic indicator with\nbroad effects on agricultural output, human health, and ecological systems Makowski et al. (2008);\nDai et al. (1999). A significant indicator of underlying changes in air circulation, cloud cover, and\nland–surface interactions, variations in DTR can reflect Wild et al. (2017); Alexander et al. (2006)\nclimatic dynamics. Downscaling attempts historically concentrated on mean temperature or precip-\nitation, but increasing attention is being paid to precisely capturing the DTR. Most statistical early\ndownscaling strategies use quantile mapping, weather typing, and linear regression. While these meth-\nods are computationally efficient, they frequently fail to adequately depict non-linear connections\nand intricate inter-dependencies between climatic variables Gudmundsson et al. (2012).\nDeep learning has transformed pattern identification in big datasets in recent years thanks to\nearly applications modelling correlations between coarse GCM outputs and local observations using\nmulti-layer perceptrons (MLPs) Alzubaidi et al. (2021); Zorita and von Storch (1999). Recent devel-\nopments in deep learning methods spanning from convolutional neural networks to transformer-based\narchitectures have shown a strong ability for modelling and predicting challenging physical events\nin atmospheric modelling Jardines et al. (2024); Solera-Rico et al. (2024); Yousif et al. (2023).\n2\n\n\nMLPs could not, however, efficiently use spatial information included in climate data. By allowing\nthe extraction of spatial information capturing localized meteorological events, Convolutional Neu-\nral Networks (CNNs) offered a major improvement. Recurrent Neural Networks (RNNs) and Long\nShort-Term Memory (LSTM) networks have been used concurrently to model temporal dependen-\ncies, such as seasonal cycles and sequential variability, so improving the capacity to replicate daily\nand seasonal oscillations in DTR Vandal (2018); Hochreiter and Schmidhuber (1997). To handle the\ncomplexity of downscaling DTR Goodfellow et al. (2017); Zhang et al. (2019), hybrid models com-\nbining CNNs with LSTMs, as well as methods using Generative Adversarial Networks (GANs), have\nshown great potential.\nBuilding on these advances, transformer-based deep-learning models have brought fresh capabil-\nities for climate downscaling. Representing the forefront in modelling spatiotemporal climate data,\nViTs and the specialized GeoSTANet capture long-range spatial dependencies using self-attention\nmechanisms, therefore allowing the modelling of broad atmospheric circulation patterns. Though\nViTs show great promise, they can find it challenging to generalize across areas with quite different\nclimate conditions. Conversely, GeoSTANet is especially meant to solve these difficulties by combin-\ning transformer-based attention mechanisms with an imbalance-aware training approach, therefore\nenabling it to thrive in both catching extreme occurrences and regular climate patterns. In recreating\nfine-scale characteristics, diurnal cycles, and seasonal transitions, these advanced hybrid architec-\ntures—including CNN-LSTM and ConvLSTM models—have been demonstrated to be quite superior\nto conventional statistical methods Zhu et al. (2023). Notwithstanding their remarkable performance,\nthese deep-learning techniques have several difficulties including the need for large, high-quality\ndatasets like ERA5 data Hersbach et al. (2020), rising computational demands connected with high-\nresolution spatiotemporal grids Dueben and Bauer (2018), possible over-fitting risk, and problems\nwith model interpretability Ham et al. (2019); Hunt et al. (2021). Reliable support of advanced\ndownscaling models for regional climate impact assessments depends on addressing these problems.\nThe present study defines five climate zones (Tropical, Arid, Temperate, Continental, and Polar)\nreflecting the approximate climatological variations across the region, each characterized by unique\ntemperature and precipitation patterns influencing GCM performance and the representation of\nextremes Cohen et al. (2014); Cook et al. (2016). The present work’s core argument is that the initial\nchoice of the best-performing GCMs for a specific region determines optimal downscaling perfor-\nmance Loganathan and Mahindrakar (2020a,b, 2021). First, the DL-TOPSIS methodology is used\nto objectively rank CMIP6 models based on a comprehensive set of performance metrics includ-\ning extreme temperature indicators maximum tasmax (TXx) and lowest tasmin (TNn), Probability\nDensity Function overlap (PDF Overlap), standard deviation differences (SD Diff) as well as tra-\nditional metrics like bias, RMSE, correlation, NSE, and KGE Hempel et al. (2013); Xie and Arkin\n(1997). Advanced deep-learning downscaling methods including CNN-LSTM, ConvLSTM, ViT, and\nGeoSTANet are then fed, top-ranked models. Essential for climate services and impact assessments\nin sectors including agriculture, urban planning, health, and energy, this combined approach not\nonly reduces the transmission of biases from coarse-scale models but also improves the accuracy of\nhigh-resolution forecasts Olsson et al. (2015); Urban and Fricker (2010). Moreover, by using high-\nperformance computational resources and large-scale observational data, the proposed methodology\noffers a scalable and repeatable blueprint for closing the gap between local adaptation techniques and\nglobal climate modelling Eyring et al. (2019); Rasp et al. (2018). In the end, this multidisciplinary\napproach merging advanced machine learning with operations research has the potential to produce\nmore accurate and useful climate projections for practitioners and legislators all around.\n2 Study area and Data description\nAdvanced downscaling methods find a useful proving ground in Europe’s varied climatic zones, which\nrange from Mediterranean beaches and temperate coastal areas to continental interiors and Arctic\ntundra Beck et al. (2018); Kottek et al. (2006). From the warm Mediterranean basin to the Arctic\ntundra, Europe offers one of the most climatically varied areas worldwide and is thus a perfect testbed\nfor assessing climate models and downscaling methods. Based on temperature and precipitation\npatterns, the K¨oppen-Geiger climate classification offers a disciplined approach to split the continent\ninto five main zones: Tropical, Arid, Temperate, Continental, and Polar. These categories allow one\nto evaluate model performance geographically with a customizing effect. Different seasonal changes\nin every zone affect local temperature extremes, air circulation patterns, and precipitation dynamics.\nThe North Atlantic Oscillation (NAO) significantly affects Winter, thereby influencing storm courses\n3\n\n\nand temperature variation. Spring marks a change in temperature and a changing precipitation\nzone phase. Heat waves and convective activity define Summer, especially in Temperate and Arid\nzones; Autumn marks a resumption of baroclinic activity and mid-latitude storm systems. Evaluating\nmodel performance depends on these seasonal dynamics as various GCMs might struggle with certain\nregional climatic variables. Figure 1 shows the K¨oppen-Geiger classification for Europe, stressing the\nspatial distribution of the climatic zones applied in this work.\nFig. 1: Spatial representation of K¨oppen-Geiger climate zones classification across Europe.\nThirty-two CMIP6 models were chosen to evaluate climate model accuracy using a variety of geo-\ngraphical resolutions (50–250 km) and many parameterizing approaches. Coordinated by the World\nClimate Research Program (WCRP), the CMIP6 repository offers the most developed suite of GCMs\naccessible for historical and future climate simulations. Different physical parameterization, land-\natmosphere coupling, and climate sensitivity among these models affect their capacity to replicate\nregional climatic variability. Data availability, the inclusion of tasmax and tasmin, and the adop-\ntion of a standardized r1i1p1f1 ensemble member to provide consistent initialization-guided model\nselection. Developed by ECMWF, observational benchmarks originate from ERA5 data and offer\nhigh-resolution data at 0.1° x 0.1° (∼10km). ERA5 produces continuous, high-quality climate records\nby combining satellite, in situ, and reanalysis data, unlike crude GCM outputs. Selected to provide\nconsistent assessment between CMIP6 historical simulations and observational data was the histori-\ncal period 1985–2014. Table 1 gives a summary of the thirty-two CMIP6 GCMs together with their\ngeographic resolution, original institution, and salient features.\n4\n\n\nTable 1: List of CMIP6 models used in the study.\nS. No\nModel\nInstitute (Country)\nResolution (km)\nKey Features\n1\nACCESS-CM2\nCSIRO (Australia)\n∼100\nImproved ocean-atmosphere coupling\n2\nACCESS-ESM1-5\nCSIRO (Australia)\n∼100\nEarth System Model with Advanced Carbon Cycle\n3\nBCC-CSM2-MR\nBeijing Climate Center (China)\n∼100\nMedium resolution; aerosol-cloud interactions\n4\nBCC-ESM1\nBeijing Climate Center (China)\n∼280\nCoupled Earth System Model with biogeochemistry\n5\nCAMS-CSM1-0\nChinese Academy of Meteorological Sciences\n∼100\nAtmospheric physics enhancements\n6\nCAS-ESM2-0\nChinese Academy of Sciences (China)\n∼100\nIntegrated land-vegetation-atmosphere coupling\n7\nCESM2\nNCAR (USA)\n∼110\nBiogeochemical cycles; high-resolution processes\n8\nCESM2-FV2\nNCAR (USA)\n∼50\nA high-resolution version of CESM2\n9\nCESM2-WACCM\nNCAR (USA)\n∼110\nWhole Atmosphere Coupling; upper-atmosphere focus\n10\nCIESM\nChinese Institute of Earth System Modeling\n∼100\nImproved monsoon simulations\n11\nCMCC-CM2-SR5\nCMCC (Italy)\n∼100\nHigh-resolution atmosphere-ocean coupling\n12\nCMCC-ESM2\nCMCC (Italy)\n∼100\nDynamic vegetation and Earth system processes\n13\nCNRM-CM6-1\nCNRM (France)\n∼100\nAdvanced surface energy balance representation\n14\nCNRM-ESM2-1\nCNRM (France)\n∼100\nEarth System Model with carbon-climate feedbacks\n15\nCanESM5\nCCCma (Canada)\n∼250\nEnhanced aerosol-radiation interactions\n16\nEC-Earth3\nEC-Earth Consortium (Europe)\n∼100\nFocus on European climate variability\n17\nFGOALS-f3-L\nIAP (China)\n∼100\nPrecipitation physics improvement\n18\nFGOALS-g3\nIAP (China)\n∼280\nGlobal hydrological cycle representation\n19\nGFDL-CM4\nGFDL (USA)\n∼100\nHigh-resolution ocean-atmosphere coupling\n20\nGFDL-ESM4\nGFDL (USA)\n∼100\nEarth System Model with advanced biogeochemistry\n21\nINM-CM4-8\nINM (Russia)\n∼150\nCoupled dynamical processes\n22\nINM-CM5-0\nINM (Russia)\n∼150\nImproved cloud parametrization\n23\nIPSL-CM6A-LR\nIPSL (France)\n∼250\nAdvanced aerosol-cloud interactions\n24\nKACE-1-0-G\nKMA (Korea)\n∼100\nFocus on regional climate dynamics\n25\nMIROC6\nMIROC (Japan)\n∼100\nMulti-scale atmosphere-ocean coupling\n26\nMPI-ESM1-2-HR\nMPI-M (Germany)\n∼100\nA high-resolution version of MPI Earth System Model\n27\nMPI-ESM1-2-LR\nMPI-M (Germany)\n∼250\nLow-resolution Earth System Model\n28\nMRI-ESM2-0\nMRI (Japan)\n∼100\nOcean biogeochemistry and carbon dynamics\n29\nNESM3\nNUIST (China)\n∼100\nImproved monsoon and hydrological cycles\n30\nNorESM2-LM\nNCC (Norway)\n∼250\nLow-resolution Nordic Earth System Model\n31\nNorESM2-MM\nNCC (Norway)\n∼100\nMedium-resolution Nordic Earth System Model\n32\nUKESM1-0-LL\nMet Office (UK)\n∼100\nCoupled Earth System Model with land-atmosphere focus\n5\n\n\nPerformance measures are computed at a constant 0.1° × 0.1° spatial grid for complete model\nevaluation, guaranteeing a direct comparison between ERA5 and regridded CMIP6 results. Calcu-\nlated from tasmax and tasmin, the DTR is a fundamental indication of radiative and land-surface\nprocesses. Multiple statistical tests are run to measure model biases, including Bias, RMSE, r, KGE,\nNSE, and PDF Overlap. These measurements evaluate multi-dimensional model quality by cap-\nturing mean biases, variability, distributional integrity, and extreme event representation. Seasonal\nstratification and zone-based categorization let one thoroughly examine model strengths and short-\ncomings in several climatic regimes. Following downscaling studies will apply the processed datasets\nand assessment methodology to guarantee that high-resolution climate forecasts are produced from\nthe best-performing models.\n3 Methodology\nThis study proposes a hybrid DL-TOPSIS framework to rank CMIP6 climate models based on their\nhistorical performance and applies advanced deep-learning architectures for statistical downscaling.\nThe methodological workflow consists of three primary blocks, as presented in Figure 2:\nFig. 2: Methodology workflow for model ranking and downscaling.\n• Data Block: Daily maximum and minimum temperature data (tasmax, tasmin) from ERA5\nreanalysis (observations) and 32 CMIP6 models (simulations) are pre-processed.\n• Ranking Block: The top CMIP6 models are identified using a DL-TOPSIS ranking system, which\ndynamically assigns weights to performance metrics.\n• Downscaling Block: The selected top GCM models are downscaled to high-resolution cli-\nmate projections using deep-learning architectures such as CNN-LSTM, ConvLSTM, ViT, and\nGeoSTANet.\nTo ensure a strong model assessment, both ranking and downscaling steps use an independent\nset of performance measures. The proposed framework improves regional climate projections for\nadaptation and mitigating strategies and allows a fair evaluation of climate model integrity.\n6\n\n\n3.1 Data pre-processing block:\nThe data pre-processing stage ensures that ERA5 and CMIP6 datasets are aligned for direct\ncomparison and statistical downscaling. This process involves multiple steps:\n1. Data acquisition: a) Observational data: ERA5 reanalysis dataset (0.1° × 0.1° resolution,\n1985–2014), and b) Climate model simulations: 32 CMIP6 models, historical experiments (r1i1p1f1\nensemble), daily tasmax and tasmin.\n2. Regridding and land-only masking: CMIP6 models have varying spatial resolutions (50 km to 250\nkm). To ensure smooth processing, all CMIP6 GCM outputs are scaled using bilinear interpolation\nto match the ERA5 grid (0.1° × 0.1°), and a land-only mask is applied to exclude oceanic regions\nto maintain consistency in model evaluation.\n3. Computation of the DTR: DTR is derived from tasmax and tasmin for both ERA5 and CMIP6\ndatasets:\nDTR = tasmax −tasmin\n(1)\nDTR is an important climate analysis metric, representing the day-night temperature contrast\nand serving as a regional climate variability indicator.\n4. Seasonal and climate zone classification: For climate-specific assessment, the data is categorized\naccording to: a) Seasons: 1) Winter (December, January, and February), 2) Spring (March, April,\nand May), 3) Summer (June, July, and August), 4) Autumn (September, October, and November),\nand 5) Annual; b) Climate Zones: The K¨oppen-Geiger classification is used to classify data into\nsix zones: 1) Tropical, 2) Arid, 3) Temperate, 4) Continental, 5) Polar, and 6) Entire Europe.\n5. Calculation of evaluation metrics: A comprehensive set of statistical and physical performance\nmetrics is computed to assess the fidelity of each CMIP6 model: a) Bias, b) RMSE, c) KGE, d)\nNSE, e) r, f) PDF Overlap, g) Extreme temperature indices: TXx and TNn. The pre-processing\nsteps standardize the ERA5 and CMIP6 datasets, facilitating impartial model evaluation and\nefficient statistical downscaling in later phases. The performance evaluation metrics and their\ndescriptors are shown in Table 2.\n7\n\n\nTable 2: Performance metrics for CMIP6 model evaluation.\nParameter\nFormula\nSignificance\nRMSE (Root Mean Square Error)\nv\nu\nu\nt 1\nn\nn\nX\ni=1\n(Mi −Oi)2\nMeasures overall deviation between predictions and observations; lower is\nbetter.\nBias\n1\nn\nn\nX\ni=1\n(Mi −Oi)\nQuantifies systematic error; values near zero indicate minimal bias.\nNSE (Nash-Sutcliffe Efficiency)\n1 −\nPn\ni=1(Oi −Mi)2\nPn\ni=1(Oi −¯O)2\nEvaluates predictive skill; values close to 1 indicate high performance.\nKGE (Kling-Gupta Efficiency)\n1 −\np\n(r −1)2 + (β −1)2 + (γ −1)2\nβ = µM/µO,\nγ = (σM/µM)/(σO/µO)\nCombines correlation, bias, and variability; optimal value is 1.\nr2 (Coefficient of Determination)\n\n\nPn\ni=1(Mi −¯\nM)(Oi −¯O)\nqPn\ni=1(Mi −¯\nM)2 Pn\ni=1(Oi −¯O)2\n\n\n2\nProportion of variance in observations explained by the model; higher values\nindicate better performance.\nr (Correlation Coefficient)\nPn\ni=1(Mi −¯\nM)(Oi −¯O)\nqPn\ni=1(Mi −¯\nM)2 Pn\ni=1(Oi −¯O)2\nMeasures linear association; values near 1 indicate strong positive correlation.\nPDF (Probability Density Function Overlap)\nZ\nmin\n\u0010\nPM(x), PO(x)\n\u0011\ndx\nQuantifies similarity between model and observed distributions; 1 indicates\nperfect overlap.\nTXx (Maximum Temperature)\nmax\n\u0000tasmax\n\u0001\nIndicates extreme high temperatures; critical for assessing heat events.\nTNn (Minimum Temperature)\nmin\n\u0000tasmin\n\u0001\nIndicates extreme low temperatures; essential for evaluating cold events.\n8\n\n\n3.2 Model ranking block:\nA hybrid DL-TOPSIS architecture objectively ranks CMIP6 models throughout several climate zones\nand seasons. This method combines multi-criteria decision-making (MCDM) methodologies with a\nneural network-based dynamic weighting mechanism to guarantee a strong and data-driven ranking\nof models. Through their proximity to an ideal solution and distance from an anti-ideal solution,\nthe Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) ranks models. This\nguarantees that the models with maximum performance also minimize bias and maximize correlation\nwith observable data. The TOPSIS rating comprises five steps:\n1. Normalization of performance metrics: Each performance metric Cij for model i and metric j is\nnormalized to remove scale variations:\nNij =\nCij\nqPm\ni=1 C2\nij\n(2)\nwhere, Cij is the computed value of metric j for model i, and m is the total number of CMIP6\nmodels.\n2. Weighted normalization: Each normalized metric is multiplied by its respective weight wj, which\nis learned dynamically using a neural network:\nWij = wj · Nij\n(3)\n3. Calculation of ideal and anti-ideal solutions: The ideal solution represents the best possible values\nacross all models, while the anti-ideal solution represents the worst:\nA+ = {max(Wij), for benefit metrics; min(Wij), for cost metrics},\n(4)\nA−= {min(Wij), for benefit metrics; max(Wij), for cost metrics},\n(5)\nwhere, the higher the benefit metrics (KGE, NSE, correlation, and PDF overlap) the better, and\nthe lower the cost metrics (Bias, RMSE) the better.\n4. Euclidean distance calculation: The Euclidean distance of each model from the ideal and anti-ideal\nsolutions is computed:\nD+\ni =\nv\nu\nu\nt\nn\nX\nj=1\n(Wij −A+\nj )2,\nD−\ni =\nv\nu\nu\nt\nn\nX\nj=1\n(Wij −A−\nj )2\n(6)\nwhere, D+\ni represents the distance from the ideal solution, and D−\ni represents the distance from\nthe anti-ideal solution.\n5. Computation of Closeness Coefficient: The closeness coefficient (CC) determines the final ranking\nof each model:\nCCi =\nD−\ni\nD+\ni + D−\ni\n(7)\nwhere a higher CCi value indicates a better-performing model. Models are ranked in descending\norder of CCi.\n3.2.1 Dynamic weighting with deep-learning:\nFor every performance evaluation parameter, traditional TOPSIS implementations used fixed, sub-\njective weights. Whereas, climate model evaluation requires a data-driven approach, where the\nimportance of certain criteria varies based on region and season. A deep neural network (DNN) that\ndynamically learns ideal metric weights to handle this is presented.\n3.2.2 Neural network architecture\nThe neural network is meant to decide ideal ranking metric weights. Its design comprises:\n• Input Layer: Performance metrics of each model such as Bias, RMSE, KGE, Correlation, etc.\n• Hidden Layers: Two fully connected layers with 64 and 32 neurons, ReLU activation.\n• Output Layer: Softmax activation with normalized weights output for each metric.\n9\n\n\n3.2.3 Neural network training setup\nMinimizing the reconstruction error between expected and observed ranking patterns helps the neural\nnetwork discover the ideal weighting scheme. The loss function is:\nLoss =\nn\nX\ni=1\n(Wij −ˆWij)2\n(8)\nwhere, Wij represents the ground-truth weight of metric j for model i, and ˆWij is the predicted\nweight.\n• Optimizer: Adam\n• Learning Rate: 0.001\n• Batch Size: 32\n• Epochs: 50\n• Loss Function: Mean Squared Error (MSE)\nThe model is trained using stochastic gradient descent (SGD) and then refined using an Adam\noptimizer Jardines et al. (2024). Once trained, the learned weights derived from the neural network\nreplace the TOPSIS system’s static weights. This implies that the ranking process is adaptable\nenough for regional climate conditions, in which multiple performance criteria could have different\nrelevance. The top-ranked models are selected for every season and climate zone after the computation\nof closeness coefficients (CCi) for every 32 CMIP6 model. The best-performing models then go to\nthe downscaling block, where statistical downscaling techniques rooted in deep-learning are applied.\nThis hybrid DL-TOPSIS approach ensures that high-resolution climate projections only rely on the\nmost trustworthy CMIP6 models, hence improving the accuracy of the next climate assessments.\n3.3 Statistical downscaling block\nThis section describes the deep-learning architectures used to downscale coarse-resolution CMIP6\noutputs (50–250 km) to a high-resolution grid (0.1◦×0.1◦). Four models are developed to capture\nboth spatial and temporal features: (i) CNN-LSTM, (ii) ConvLSTM, (iii) ViT, and (iv) GeoSTANet.\n1. CNN-LSTM: This model combines CNNs for spatial feature extraction and LSTM networks for\ntemporal sequence modelling. The architecture is described in what follows.\n(a) Input: A climate data cube\nX ∈Rt×h×w×c\n(9)\nwhere t is the number of time steps, h × w are the spatial dimensions, and c is the number of\nvariables.\n(b) CNN block: Spatial features are extracted through a series of convolutional layers. For the\nlth layer, the feature map is computed as:\nF (l)\nij = ReLU\n\n\np\nX\nk=−p\np\nX\nm=−p\nW (l)\nkm X(l−1)\n(i+k)(j+m) + b(l)\n\n\n(10)\nWhere, W (l)\nkm and b(l) are the convolutional weights and bias, respectively, and p defines the\nradius of the convolution kernel, so that the full kernel size is (2p+1)×(2p+1). Here, ReLU(·)\ndenotes the Rectified Linear Unit activation function (Solera-Rico et al. 2024).\n(c) Flattening: The resulting spatial feature maps are flattened into a one-dimensional vector.\n(d) LSTM block: Temporal dependencies are modelled using LSTM cells. At each time step t,\nthe following equations are computed:\nit = σ (Wixt + Uiht−1 + bi)\n(11)\nft = σ (Wfxt + Ufht−1 + bf)\n(12)\not = σ (Woxt + Uoht−1 + bo)\n(13)\nct = ft ⊙ct−1 + it ⊙tanh (Wcxt + Ucht−1 + bc)\n(14)\nht = ot ⊙tanh(ct)\n(15)\n10\n\n\nwhere W{·}, U{·}, and b{·} are learnable parameters and ⊙denotes element-wise multiplication.\n(e) Output layer: A fully connected layer maps the LSTM output to produce the high-resolution\nprojection.\n2. ConvLSTM: This model integrates two-dimensional convolution operations within the LSTM\ngates to preserve spatial structure while modelling temporal dynamics (Yousif et al. 2023). at each\ntime step t, the ConvLSTM cell computes:\nit = σ\n\u0010\nWi ∗Xt + Ui ∗ht−1 + bi\n\u0011\n(16)\nft = σ\n\u0010\nWf ∗Xt + Uf ∗ht−1 + bf\n\u0011\n(17)\not = σ\n\u0010\nWo ∗Xt + Uo ∗ht−1 + bo\n\u0011\n(18)\nct = ft ⊙ct−1 + it ⊙tanh\n\u0010\nWc ∗Xt + Uc ∗ht−1 + bc\n\u0011\n(19)\nht = ot ⊙tanh(ct)\n(20)\nwhere ∗denotes 2D convolution over the spatial dimensions. In this architecture, multiple stacked\nConvLSTM layers are employed, batch normalization is applied after each convolution to stabilize\ntraining, and an up-sampling module is integrated to achieve the target resolution.\n3. ViT: The model divides the input climate grid into patches and applies self-attention mechanisms\nto capture long-range spatial dependencies.\n(a) Patch embedding: The input grid is partitioned into N fixed-size patches. Each patch Xi is\nflattened and projected linearly:\nZ0 =\n\n\nX1E\nX2E\n...\nXNE\n\n+ Epos\n(21)\nwhere E ∈R(p×p×c)×d is the learnable projection matrix and Epos provides positional encoding.\n(b) Transformer Encoder: The sequence of patch embeddings is processed by a transformer\nencoder. The self-attention mechanism is given by:\nAttention(Q, K, V ) = softmax\n\u0012QKT\n√dk\n\u0013\nV\n(22)\nwhere Q, K, and V denote the query, key, and value matrices, and dk is the key dimension.\n(c) Regression Head: The encoder output is passed through a fully connected regression head\nto generate the high-resolution output.\n4. GeoSTANet: This model extends the ViT architecture by integrating geospatial and tempo-\nral encoding to capture the spatiotemporal variability in climate data. GeoSTANet specifically\nincorporates the geographic coordinates of each patch to increase spatial context, and it analy-\nses the generated sequence along the temporal dimension. Each image patch is associated with\ngeospatial coordinates - latitude and longitude. These coordinates are embedded into a higher-\ndimensional space using a learnable projection matrix Wgeo. Specifically, given a 2D coordinate\nvector Xlatlon ∈R2, the geospatial encoding is computed as:\nGeoEnc = Wgeo Xlatlon\n(23)\nWhere Wgeo ∈Rd×2 is a learnable matrix that projects the 2-dimensional coordinates into a d-\ndimensional embedding. This geospatial encoding is then combined (e.g., via concatenation or\naddition) with the corresponding patch embedding to provide explicit spatial context.\n(a) Temporal transformer: To model the temporal dynamics of the data, the sequence of\nenriched patch embeddings is processed by a dedicated transformer encoder that operates along\nthe temporal dimension. Let Ht denote the hidden state at time t. The temporal evolution is\ndefined as:\nHt = TransformerEncoder(Ht−1),\nfor t ≥1\n(24)\n11\n\n\nWith the initial state H0 set as the geospatial enriched embedding from the first time step.\nThis block enables the model to capture sequential dependencies over time, integrating both\nvisual and geospatial information.\n(b) Upsampling: After the temporal processing, the final feature representation is passed through\nan upsampling module—such as a transposed convolution layer—to reconstruct the output\non a high-resolution grid. This step is essential for applications like climate forecasting where\ndetailed spatial predictions are required.\n(c) Input and output: GeoSTANet accepts as input a time-ordered sequence of image patches,\neach accompanied by its geospatial coordinates. Initially, each patch is embedded using the\nstandard ViT patch embedding method. The geospatial coordinates are then projected via Wgeo\ninto a d-dimensional space and combined with the patch embeddings. The resulting sequence\nis processed by the temporal transformer block to model the temporal dependencies, and\nfinally, the features are up-sampled to produce a high-resolution output. This pipeline distin-\nguishes GeoSTANet from the previously described CNN and ConvLSTM models by explicitly\nincorporating geographic context and dedicated temporal processing.\nThis complete approach combines advanced deep-learning-based downscaling systems with a data-\ndriven DL-TOPSIS ranking algorithm. By combining multi-criteria model evaluation with robust\nspatial and temporal feature extraction, the framework produces high-resolution climate projections\nthat accurately capture both mean behaviour and extremes, thereby supporting informed regional\nclimate impact assessments.\n4 Results and discussions\nThe reported results cover the performance of model ranking via the proposed DL-TOPSIS framework\nand the statistical downscaling via the four deep-learning models discussed above. Starting with\ncoarse-resolution CMIP6 model evaluation, these results offer a whole picture of model fidelity at\nseveral levels, and then deep-learning transforms top-ranked GCM outputs into high-resolution fields.\nThis approach yields model strengths and limits in various seasons and climate zones as well as\nmeasuring gains made by the downscaling process.\n4.1 Model ranking through DL-TOPSIS\nThe DL-TOPSIS approach was used to do a complete ranking of the thirty-two CMIP6 models. These\nranking results for every climate zone (Tropical, Arid, Temperate, Continental, and Polar) spanning\nwinter, spring, summer, autumn, and the whole year are shown on the heat map in Figure 3. Better\nratings are indicated by cooler (blue) tones; poorer ratings by warmer (red) tones. Particularly in\ntemperate and continental areas, which are difficult due to great seasonal variability and complex pre-\ncipitation regimes, NorESM2-LM, HadGEM3-GC31-LL, and GISS-E2-1-G routinely ranked among\nthe best models across many zones. Among the lowest-ranked models were TaiESM1, CMCC-CM2-\nSR5, BCC-CSM2-MR, FGOALS-g3, and showing notable temperature mean representation biases\nand errors. While many models battled with cold extremes, NorESM2-LM and MPI-ESM1-2-LR\nexcelled others in showing subzero temperature ranges (TNn) in Polar zones.\n12\n\n\nFig. 3: CMIP6 model ranking heatmap across European climate zones and seasons.\n13\n\n\nTable 3, represents the top 5 CMIP6 Models for each season and over Europe, Using performance\nevaluation criteria such as Score, Bias, RMSE, KGE, NSE, and PDF Overlap the table provides\na seasonal evaluation of CMIP6 models throughout six climate zones (Tropical, Arid, Temperate,\nContinental, Polar, and Overall). Especially in the Polar region, Winter suggests NorESM2-LM\nperforms better in many zones with the best RMSE. GISS-E2-1-G leads in the Tropical zone, Spring\ndenotes UKESM1-0-LL as the top performer. Summer shows HadGEM3-GC31-LL, and MPI-ESM1-\n2-LR perform well in the Tropical zone. In all zones, MIROC-ES2L does well in Autumn showing\ngreat PDF Overlap and NSE. Over the Full Year, MIROC-ES2L turns out to be the best overall\nmodel, and NorESM2-LM performs well in the Polar and Continental zones. Although higher KGE\nand NSE show a stronger correlation with observations, bias and RMSE trends indicate better\naccuracy from lower levels. The results highlight seasonal changes in model performance. These multi-\nmodel assessments are important since they expose no one universal model that performs under all\ncircumstances. Rather, the top five can be seen as a group of good candidates, each with particular\ncharacteristics (e.g., strong skill in heat extremes or cold extremes) that can be used for different\nclimate-sensitive applications.\n14\n\n\nTable 3: Top 5 CMIP6 models by climate zone and season.\nSeason\nZone\nModel\nScore\nBias\nRMSE\nKGE\nNSE\nPDF Overlap\nWinter (DJF)\nTropical\nACCESS-CM2\n0.9501\n0.3596\n4.6423\n0.0585\n-0.8327\n0.6908\nArid\nNorESM2-LM\n0.9333\n-0.4068\n5.0143\n0.1041\n-0.8527\n0.9626\nTemperate\nCMCC-ESM2\n0.9277\n-0.0098\n5.1524\n0.0817\n-1.5013\n0.8205\nContinental\nGISS-E2-1-G\n0.9103\n0.3599\n6.6133\n0.0913\n-2.0429\n0.8450\nPolar\nNorESM2-LM\n0.9542\n-1.8734\n4.3212\n0.0307\n-1.5815\n0.4112\nOverall\nNorESM2-LM\n0.9234\n0.0008\n5.7904\n0.1376\n-1.3970\n0.8968\nSpring (MAM)\nTropical\nGISS-E2-1-G\n0.9335\n0.8253\n5.1894\n0.1516\n-0.5558\n0.8763\nArid\nMIROC-ES2L\n0.9142\n-0.6355\n5.1720\n0.1324\n-0.5849\n0.8500\nTemperate\nNorESM2-MM\n0.9305\n-0.2988\n5.4334\n0.1444\n-0.6687\n0.7891\nContinental\nUKESM1-0-LL\n0.9253\n-0.0932\n6.0145\n0.1288\n-0.9561\n0.8234\nPolar\nGFDL-CM4\n0.9042\n-1.1415\n3.9518\n0.0189\n-1.3881\n0.4376\nOverall\nUKESM1-0-LL\n0.9286\n-0.2803\n5.6790\n0.1821\n-0.7016\n0.8384\nSummer (JJA)\nTropical\nMPI-ESM1-2-LR\n0.9429\n1.1819\n4.8028\n0.2363\n-0.2472\n0.7864\nArid\nNorESM2-LM\n0.8794\n-0.1021\n4.4709\n0.2579\n-0.5831\n0.7724\nTemperate\nACCESS-CM2\n0.9189\n-0.3081\n5.2273\n0.0928\n-0.9402\n0.8255\nContinental\nHadGEM3-GC31-LL\n0.9131\n0.2221\n4.8654\n0.1478\n-0.7857\n0.8489\nPolar\nMIROC-ES2L\n0.9235\n-1.0053\n2.7864\n0.0616\n-0.6326\n0.4830\nOverall\nHadGEM3-GC31-LL\n0.9322\n0.0261\n4.9257\n0.2812\n-0.5076\n0.8623\nAutumn (SON)\nTropical\nCanESM5\n0.9324\n1.1847\n4.7562\n0.3193\n-0.2525\n0.8518\nArid\nMIROC-ES2L\n0.9096\n-0.2656\n5.0993\n0.3271\n-0.2698\n0.8453\nTemperate\nMIROC-ES2L\n0.9097\n-0.2064\n4.7253\n0.2887\n-0.3334\n0.8233\nContinental\nMIROC-ES2L\n0.9257\n-0.0333\n4.3083\n0.2709\n-0.4697\n0.9152\nPolar\nMIROC-ES2L\n0.9412\n-1.7524\n3.4698\n0.0641\n-1.2028\n0.3976\nOverall\nMIROC-ES2L\n0.9502\n-0.1184\n4.5932\n0.4399\n-0.0892\n0.8851\nFull Year\nTropical\nNorESM2-LM\n0.9192\n0.8440\n4.8845\n0.3179\n-0.2764\n0.8386\nArid\nUKESM1-0-LL\n0.9201\n-0.2241\n5.1109\n0.3529\n-0.3425\n0.8980\nTemperate\nMIROC-ES2L\n0.9105\n-0.2939\n4.9621\n0.3198\n-0.3238\n0.8455\nContinental\nNorESM2-LM\n0.9225\n0.1673\n5.5920\n0.2487\n-0.8120\n0.8843\nPolar\nNorESM2-LM\n0.9408\n-1.5537\n3.7186\n0.0498\n-1.2792\n0.4842\nOverall\nMIROC-ES2L\n0.9312\n-0.1362\n5.0937\n0.3607\n-0.3015\n0.8934\n15\n\n\nFigure 4 illustrates the overall CMIP6 GCMs Model performance by climate zones and seasons.\nThe plot shows the CMIP6 GCMs’ average performance ratings across several climate zones and\nseasons, ranging from 0.775 to 0.868. A higher score indicates a better match with the actual obser-\nvation. Temperate and tropical zones exhibit optimal conditions in the spring and winter, indicating\nthat models adequately represent seasonal fluctuations in these regions (scores ¿ 0.86). In contrast,\nthe Continental zone has the poorest Winter performance (0.775), indicating difficulties in modelling\ncold-season climate processes. The Overall category shows model endurance by maintaining consis-\ntent performance (0.84-0.85) across seasons. The Polar zone fluctuates; it peaks in the Full Year\n(0.844) but drops in the Spring (0.810), indicating difficulties in maintaining high-latitude activ-\nity. The arid zone is stable ( 0.81-0.83) with minimal seasonal impact. These findings highlight the\nimportance of model evaluations based on regional and seasonal contexts, as they show that climate\nmodels are generally more reliable in certain seasons and locations. Figure 5 displays the best-ranked\nCMIP6 GCM spatially across Europe over various K¨oppen-Geiger Climate Zones.\nFig. 4: Overall Performance of CMIP6 Models in Europe.\n4.2 Downscaling performance results\nAfter determining the best-performing models for every season and zone, four advanced deep-learning\narchitectures CNN-LSTM, ConvLSTM, ViT, and GeoSTANet downscaled the selected GCM outputs\nto a fine-resolution grid (0.1°×0.1°). Across the same temperature zones and seasons, Figure 6 offers a\nrelative visualization of different structures. Deeper greens indicate better competence in performance\nmeasures like Bias, RMSE, NSE, KGE, correlation (r), and PDF overlap—which are color-coded.\nEspecially in capturing DTR and extremes TXx and TNn, this picture emphasizes how each design\nmanages the spatiotemporal complexity of daily temperature fields. Figure 6 represents a heat map\nof performance evaluation metrics of downscaling for various zones and seasons over Europe using\nvarious deep-learning models.\nIn areas with moderate geographical variability—that is, temperate and continental zones,\nCNN-LSTM showed particularly high accuracy. While the LSTM units modelled daily-to-season tem-\nperature variations, the convolutional layers efficiently identified spatially localized characteristics.\nCNN-LSTM did, however, occasionally show over-smoothing in locations with steep gradients, such\nas coastal zones or mountainous areas, clearly shown in somewhat higher RMSE values. In areas\nwith fast temperature transitions e.g., mountainous borders between continental and polar zones -\nConvLSTM sustained spatial coherence better than CNN-LSTM, by incorporating convolution oper-\nations directly into the LSTM gating mechanism. This benefit was particularly evident in winter\nwhen daily maximum and minimum temperatures can be quite influenced by convective processes.\nConvLSTM did, however, occasionally show training stability problems that needed careful learning\nrate and batch size tweaking to prevent over-fitting.\nThe ViT model downscaled the results by treating each climate map as a set of patches and using\nmulti-head self-attention. Particularly in tropical and subtropical zones, where broad-scale circulation\ncan control temperature distributions, this method was quite good in catching large-scale atmospheric\n16\n\n\nFig. 5: Spatial distribution of top-ranked CMIP6 models in Europe.\npatterns and interconnections. ViT did, however, sometimes suffer from local topographic effects\nsince self-attention may not always prioritize fine-scale terrain characteristics unless the patch size\nand positional embeddings are precisely optimized. ViT yielded competitive KGE and correlation\nvalues in arid zones; small-scale extremes occasionally seemed unnaturally smoothed, suggesting that\npatch-based embeddings may need more fine-grasping to manage localized events.\nParticularly in arid zones and TNn in polar zones, GeoSTANet, the geospatial-spatiotemporal\ntransformer, routinely outperformed the other architectures in capturing temperature extremes.\nGeoSTANet dynamically learned which areas and time steps were most important for forecasting\ndaily maxima and minima by including explicit geographical encoding (latitude and longitude embed-\ndings) and temporal attention blocks. In demanding environments—including the transitional zones\nbetween temperate and arctic climates—this capacity produced reduced bias and RMSE values.\nHigher PDF overlap scores in some zones suggest that GeoSTANet was able to faithfully replicate\nthe distribution tails for daily temperature using synergy between attention-based mechanisms and\nan imbalance-aware training method (focusing on rare extremes). For climate impact studies, which\nusually rely on accurate forecasts of unusual events like heat waves or severe cold spells, such an\nadvantage is highly desired.\nDownscaling accuracy is significantly impacted by the interaction of the GCM choice with\ndownscaling architecture. Using highly ranked GCMs with low bias and variability minimizes the cor-\nrectional load on statistical downscalers, thus lowering residual errors. On the other hand, even the\nmost sophisticated downscaling model inherits large-scale biases when associated with poorly ranked\nGCMs including CMCC-CM2-SR5, TaiESM1, and BCC-CSM2-MR, which displayed systematic\nbiases and high RMSE across many climate zones. Particularly in the Continental and Temper-\nate zones, where temperature variability is high, our analysis shows that pairing top-performing\nGCMs—NorESM2-LM, GISS-E2-1-G, and HadGEM3-GC31-LL—with GeoSTANet routinely per-\nforms better than other combinations across seasonal and annual scales. Maintaining strong KGE and\nNSE values, this combination achieves RMSE cuts of up to 20% over the next-best alternative. These\ndata highlight the need for a two-stage strategy: a) Strong multi-metric evaluation of GCMs to choose\nthe most dependable climate forecasts. b) Using GeoSTANet, advanced deep-learning-based down-\nscaling captures spatiotemporal dependencies and sub-grid processes, guaranteeing high-resolution\nclimate forecasts.\n17\n\n\nFig. 6: Performance comparison of downscaling models across zones and seasons.\n18\n\n\n4.3 Seasonal performance results\nSeasonal assessments of the downscaled outputs show important variations in the capacity of every\narchitecture to replicate temperature extremes. Especially in continental and dry zones, convective\nand radiative processes produce significant diurnal variations throughout summer. While ConvLSTM\nperformed better, CNN-LSTM periodically under-predicted TXx, presumably because the convolu-\ntional gating preserved local convective fingerprints. Although ViT’s patch-based method usually\nperformed well in capturing more general patterns, it may have missed small-scale heat islands, which\nwould have somewhat understated the peak daily maximum. Higher correlation values and better\nPDF overlap for TXx distributions show GeoSTANet’s most skilful resolution of these localized hot\nspots. Errors in winter tended to gather around significant temperature inversions or cold extremes.\nWhile GeoSTANet once more excelled by using temporal attention to track the development of cold\nair masses, ConvLSTM controlled spatiotemporal transitions well. In particular, biases were usually\nsmaller in winter than in summer, implying that downscaling designs may find simpler large-scale\nsynoptic circumstances to record.\n4.3.1 Regional performance results\nFrom a regional standpoint, the desert zone (B) presented special difficulties because of sharp daily\nfluctuations; the polar zone (E) necessitated strong handling of negative temperature extremes.\nCNN-LSTM and ConvLSTM both demonstrated a rather good ability in arid zones to capture daily\ntemperature fluctuations; nevertheless, if the GCM inputs included systematic warm or cold biases,\nthey could overstate the magnitude of extremes. Advanced attention levels of GeoSTANet regu-\nlarly reduced these biases, suggesting that attention-based designs are appropriate for arid areas\nwith high radiative forcing. In polar areas, model evaluation proved much dependent on the abil-\nity to depict negative temperature extremes (TNn). While ConvLSTM performed better because of\nits inherent spatiotemporal gating, CNN-LSTM occasionally suffered with capturing extended cold\nspells if they were not prominent in the training process. In these high-latitude areas, GeoSTANet’s\ngeographic encoding was particularly helpful since it more closely matched observed data to temper-\nature projections. PDF overlap was utilized to assess not only mean values but also the distribution\nof temperature, therefore evaluating each architecture. GeoSTANet’s better depiction of the whole\ntemperature distribution, including both central trends and tails, clearly showed consistently higher\noverlap scores than the other models. For research on climate change, where precise tail behaviour\ncan imply the difference between an underestimated or realistically expressed risk of extreme events,\nthis advantage is essential. The performance of ViT in PDF overlap was partially reliant on patch size\nand training procedures; hence, it suggests possible improvements if patch embedding or positional\nencodings were optimized for tasks related to the climate. Although CNN-LSTM and ConvLSTM\nusually showed modest overlap scores, they occasionally revealed small changes in the distribution\ntails depending on the training data size or if the GCM inputs included persistent biases.\n4.4 Discussion and outlook\nThe results naturally validate that advanced deep-learning architectures could significantly improve\ndaily temperature data’s spatial and temporal accuracy. The success of GeoSTANet highlights how\nimportant specialized design choices, geospatial encoding and attention-based mechanisms are to\ncapture climate-related variability. Regardless, CNN-LSTM and ConvLSTM are competitive options,\nparticularly in computationally limited environments, and can create downscaled fields that exceed\nconventional statistical approaches. ViT distinguishes itself for recording notable connections and\npatterns, even though it may need more fine-tuning for localized aspects. It is crucial to underline\nthe wider implications for climate adaptation and decision-making even as one discusses these out-\ncomes. Reliable high-quality temperature fields assist in enhancing risk assessments in public health,\ninfrastructure design, and agriculture. For instance, whereas accurate modelling of TXx in desert\nand temperate zones can guide early warning systems for heat waves, a strong representation of TNn\nhelps winter hazard planning and ecosystem preservation activities. Moreover, the suggested DL-\nTOPSIS structure takes advantage of the synergy between the downscaling technique and improved\nmodel choice to give decision-makers more consistent data. Instead of depending just on single GCM\noutputs or simpler downscaling methods, this two-tiered approach targets the most reliable global\nmodels and improves them with state-of-the-art neural architectures for finer detail.\n19\n\n\nFuture studies should consider extending this approach to other variables such as precipitation,\nwind speed, or soil moisture, where the interaction of local effects and large-scale circulation may\ndiffer greatly from temperature fields. Further increasing confidence in the downscaled products\ncould be ensemble-based methods including Bayesian uncertainty quantification or combining sev-\neral deep-learning architectures. Employing several emission scenarios e.g., SSP1-2.6, SSP5-8.5 these\narchitectures would also guide changes in model biases under future warming and whether sophis-\nticated downscalers are still robust. Combining several observational or reanalysis products such as\nground-based station networks or MERRA-2 may provide a more complete training and validation\ndataset, possibly improving model performance in data-sparse areas such as mountainous or high-\nlatitude regions. Considering these, modern deep-learning models seem to be quite able to refine the\ncoarse outputs of top-ranked CMIP6 GCMs. GeoSTANet shows to be the most consistent design\nacross several climate zones and seasons especially in capturing distribution extremes TXx and TNn\nand obtaining high PDF overlap. Still, excellent choices are CNN-LSTM and ConvLSTM; ConvLSTM\nis especially good at preserving spatial coherence in regions with strong temperature gradients. ViT\nshows promise in gathering general climate characteristics but might need local-scale event-specific\ncorrections. These findings confirm the need of exactly match strong GCMs with advanced down-\nscaling models to produce high-resolution climate forecasts that regularly direct scientific research,\npolicy, and adaptation strategies.\n5 Conclusions\nCombining a data-driven GCM ranking system (DL-TOPSIS) with deep-learning-based downscal-\ning, this work presents a strong two-stage framework for high-resolution climate projections over\nEurope. Objectively evaluating 32 CMIP6 models across five K¨oppen-Geiger climate zones (Tropical,\nArid, Temperate, Continental, and Polar) and several seasons (Winter, Spring, Summer, Autumn,\nand Full Year) the ranking system dynamically assigns weights to performance metrics to lower\nbias. While CMCC-CM2-SR5, TaiESM1, and BCC-CSM2-MR show systematic biases and weak\ncorrelation with observations, so less suitable for high-resolution downscaling, the results confirm\nthat NorESm2-LM, GISS-E2-1-G, HadGEM3-GC31-LL, MPI-ESm1-2-LR, and ACCESS-CM2-SR5\nconsistently outperform other models in different climate conditions.\nFour advanced deep-learning architectures—CNN-LSTM, CNN-LSTM, ViT, GeoSTANet—were\nused in the second stage to downscale top-ranked GCM outputs to a fine-scale resolution (0.1° ×\n0.1°). With a 20% RMSE decrease, GeoSTANet stands out as the most successful downscaling model\nsince it achieves statistically significant improvements over other methods. Extreme temperature\nfluctuations are faithfully captured by its geospatial and temporal attention mechanisms, preserv-\ning high KGE (0.89), NSE (0.85), and PDF overlap scores (0.91). Whereas CNN-LSTM improves\ntemporal coherence, ConvLSTM also performs well in areas with fast spatial transitions. ViT needs\nmore tuning to improve fine-scale resolution even if it shines in catching broad climatological trends.\nThese findings underline the need to combine advanced downscaling methods with ideal model\nselection to improve regional climate projections. Through better daily temperature forecasts, this\nframework offers insightful analysis for infrastructure design, climate risk assessments, and adapta-\ntion strategies. Future research will concentrate on extending this framework using multi-variable\ndownscaling architectures to other important climate variables including precipitation extremes, wind\nfields, and soil moisture. We also wish to evaluate model confidence levels by including Bayesian uncer-\ntainty quantification. We will also discuss the generalization of this method to other geographical\nareas, including East Asia and North America. At last, ensemble-based approaches will be examined\nto increase resilience under several emission scenarios (SSP1-2.6, SSP5-8.5).\nDeclarations\nAcknowledgements The authors acknowledge the SESAR 3 Joint Undertaking and its members\nfor their support in funding this research under grant agreement No. 101114795 as part of the E-\nCONTRAIL project. We also appreciate ECMWF for providing the ERA5 reanalysis dataset and\nWCRP for facilitating CMIP6 data access. Special thanks to our colleagues and collaborators for\ntheir valuable insights.\nFunding This research was funded by the SESAR 3 Joint Undertaking under the E-CONTRAIL\nproject (Grant Agreement No. 101114795).\n20\n\n\nAuthors’ Contributions Parthiban Loganathan: Conceptualization, data collection, inves-\ntigation, writing, and visualization. Elias Zea, Ricardo Vinuesa, and Evelyn Otero: Project\nadministration, review, and editing.\nEthical Approval Not applicable.\nConsent to Participate Not applicable.\nConsent to Publish All authors consent to the publication of this manuscript.\nCompeting Interests The authors declare no competing interests.\nData Availability Statement The data used in this study is included in the manuscript.\nAdditional data or supplementary materials can be provided upon reasonable request.\nReferences\nAlexander, L.V., X. Zhang, T.C. Peterson, J. Caesar, B. Gleason, A.M.G.K. Tank, M. Haylock,\nD. Collins, B. Trewin, F. Rahimzadeh, A. Tagipour, K.R. Kumar, J. Revadekar, G. Griffiths,\nL. Vincent, D.B. Stephenson, J. Burn, E. Aguilar, M. Brunet, others, and J.L. Vazquez-Aguirre.\n2006. Global observed changes in daily climate extremes of temperature and precipitation. J.\nGeophys. Res. Atmos. 111(D5). https://doi.org/10.1029/2005jd006290 .\nAlzubaidi, L., J. Zhang, A.J. Humaidi, A. Al-Dujaili, Y. Duan, O. Al-Shamma, J. Santamar´ıa,\nM.A. Fadhel, M. Al-Amidie, and L. Farhan. 2021. Review of deep learning: Concepts, cnn archi-\ntectures, challenges, applications, future directions. J. Big Data 8(1). https://doi.org/10.1186/\ns40537-021-00444-8 .\nBeck, H.E., N.E. Zimmermann, T.R. McVicar, N. Vergopolan, A. Berg, and E.F. Wood. 2018. Present\nand future k¨oppen-geiger climate classification maps at 1-km resolution. Sci. Data 5(1). https:\n//doi.org/10.1038/sdata.2018.214 .\nCavazos, T., M.L. Bettolli, D. Campbell, R.A.S. Rodr´ıguez, M. Mycoo, P.A. Arias, J. Rivera, M.S.\nReboita, C. Gulizia, H.G. Hidalgo, E.J. Alfaro, T.S. Stephenson, A.A. S¨orensson, R. Cerezo-Mota,\nE. Castellanos, D. Ley, and R. Mahon. 2024. Challenges for climate change adaptation in latin\namerica and the caribbean region. Front. Clim. 6. https://doi.org/10.3389/fclim.2024.1392033 .\nChen, S.J., C.L. Hwang, M.J. Beckmann, and W. Krelle. 1981. Multiple attribute decision making:\nMethods and applications.\nCohen, J., J.A. Screen, J.C. Furtado, M. Barlow, D. Whittleston, D. Coumou, J. Francis, K. Dethloff,\nD. Entekhabi, J. Overland, and J. Jones. 2014. Recent arctic amplification and extreme mid-\nlatitude weather. Nat. Geosci. 7(9): 627–637. https://doi.org/10.1038/ngeo2234 .\nCook, B.I., K.J. Anchukaitis, R. Touchan, D.M. Meko, and E.R. Cook. 2016. Spatiotemporal drought\nvariability in the mediterranean over the last 900 years. J. Geophys. Res. Atmos. 121(5): 2060–\n2074. https://doi.org/10.1002/2015jd023929 .\nDai, A., K.E. Trenberth, and T.R. Karl. 1999. Effects of clouds, soil moisture, precipitation, and\nwater vapor on diurnal temperature range. J. Clim. 12(8): 2451–2473. https://doi.org/10.1175/\n1520-0442(1999)012 .\nDueben, P.D. and P. Bauer. 2018. Challenges and design choices for global weather and climate\nmodels based on machine learning. Geosci. Model Dev. 11(10): 3999–4009. https://doi.org/10.\n5194/gmd-11-3999-2018 .\nDwivedi, Y.K., L. Hughes, E. Ismagilova, G. Aarts, C. Coombs, T. Crick, Y. Duan, R. Dwivedi,\nJ. Edwards, A. Eirug, V. Galanos, P.V. Ilavarasan, M. Janssen, P. Jones, A.K. Kar, H. Kizgin,\nB. Kronemann, B. Lal, B. Lucini, others, and M.D. Williams. 2019. Artificial intelligence (ai):\nMultidisciplinary perspectives on emerging challenges, opportunities, and agenda for research,\npractice and policy. Int. J. Inf. Manag. 57: 101994. https://doi.org/10.1016/j.ijinfomgt.2019.08.002\n.\n21\n\n\nEyring, V., S. Bony, G.A. Meehl, C.A. Senior, B. Stevens, R.J. Stouffer, and K.E. Taylor. 2016.\nOverview of the coupled model intercomparison project phase 6 (cmip6) experimental design and\norganization. Geosci. Model Dev. 9(5): 1937–1958. https://doi.org/10.5194/gmd-9-1937-2016 .\nEyring, V., P.M. Cox, G.M. Flato, P.J. Gleckler, G. Abramowitz, P. Caldwell, W.D. Collins, B.K.\nGier, A.D. Hall, F.M. Hoffman, G.C. Hurtt, A. Jahn, C.D. Jones, S.A. Klein, J.P. Krasting,\nL. Kwiatkowski, R. Lorenz, E. Maloney, G.A. Meehl, others, and M.S. Williamson. 2019. Taking\nclimate model evaluation to the next level. Nat. Clim. Change 9(2): 102–110. https://doi.org/10.\n1038/s41558-018-0355-y .\nFowler, H.J., S. Blenkinsop, and C. Tebaldi. 2007. Linking climate change modelling to impacts stud-\nies: Recent advances in downscaling techniques for hydrological modelling. Int. J. Climatol. 27(12):\n1547–1578. https://doi.org/10.1002/joc.1556 .\nGoodfellow, I., J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville,\nand Y. Bengio. 2017.\nGan (generative adversarial nets).\nJ. Jpn. Soc. Fuzzy Theory Intell.\nInformat. 29(5): 177. https://doi.org/10.3156/jsoft.29.5 177 2 .\nGudmundsson, L., J.B. Bremnes, J.E. Haugen, and T. Engen-Skaugen. 2012. Technical note: Down-\nscaling rcm precipitation to the station scale using statistical transformations – a comparison of\nmethods. Hydrol. Earth Syst. Sci. 16(9): 3383–3390. https://doi.org/10.5194/hess-16-3383-2012 .\nHaarsma, R.J., M.J. Roberts, P.L. Vidale, C.A. Senior, A. Bellucci, Q. Bao, P. Chang, S. Corti, N.S.\nFuˇckar, V. Guemas, J. von Hardenberg, W. Hazeleger, C. Kodama, T. Koenigk, L.R. Leung, J. Lu,\nJ. Luo, J. Mao, M.S. Mizielinski, and J. von Storch. 2016. High resolution model intercomparison\nproject (highresmip v1.0) for cmip6. Geosci. Model Dev. 9(11): 4185–4208. https://doi.org/10.\n5194/gmd-9-4185-2016 .\nHam, Y., J. Kim, and J. Luo. 2019. Deep learning for multi-year enso forecasts. Nature 573(7775):\n568–572. https://doi.org/10.1038/s41586-019-1559-7 .\nHempel, S., K. Frieler, L. Warszawski, J. Schewe, and F. Piontek. 2013. A trend-preserving bias\ncorrection – the isi-mip approach.\nEarth Syst. Dyn. 4(2): 219–236.\nhttps://doi.org/10.5194/\nesd-4-219-2013 .\nHersbach, H., B. Bell, P. Berrisford, S. Hirahara, A. Hor´anyi, J. Mu˜noz-Sabater, J. Nicolas, C. Peubey,\nR. Radu, D. Schepers, A. Simmons, C. Soci, S. Abdalla, X. Abellan, G. Balsamo, P. Bechtold,\nG. Biavati, J. Bidlot, M. Bonavita, others, and J. Th´epaut. 2020. The era5 global reanalysis. Q.\nJ. R. Meteorol. Soc. 146(730): 1999–2049. https://doi.org/10.1002/qj.3803 .\nHochreiter, S. and J. Schmidhuber. 1997. Long short-term memory. Neural Comput. 9(8): 1735–1780.\nhttps://doi.org/10.1162/neco.1997.9.8.1735 .\nHunt, K.M.R., A.G. Turner, and R.K.H. Schiemann. 2021. How interactions between tropical depres-\nsions and western disturbances affect heavy precipitation in south asia.\nMon. Weather Rev..\nhttps://doi.org/10.1175/MWR-D-20-0373.1 .\nIntergovernmental Panel on Climate Change (IPCC). 2023, Jun. Climate Change 2022 – Impacts,\nAdaptation and Vulnerability. Cambridge, England: Cambridge University Press.\nJardines, A., H. Eivazi, E. Zea, J. Garc´ıa-Heras, J. Simarro, E. Otero, M. Soler, and R. Vinuesa.\n2024. Thunderstorm prediction during pre-tactical air-traffic-flow management using convolutional\nneural networks. Expert Syst. Appl. 241: 122466. https://doi.org/10.1016/j.eswa.2023.122466 .\nKottek, M., J. Grieser, C. Beck, B. Rudolf, and F. Rubel. 2006. World map of the k¨oppen-geiger\nclimate classification updated. Meteorol. Z. 15(3): 259–263. https://doi.org/10.1127/0941-2948/\n2006/0130 .\nLoganathan, P. and A.B. Mahindrakar. 2020a. Assessment and ranking of cmip5 gcms performance\nbased on observed statistics over cauvery river basin – peninsular india. Arab. J. Geosci. 13(22).\n22\n\n\nhttps://doi.org/10.1007/s12517-020-06217-6 .\nLoganathan, P. and A.B. Mahindrakar. 2020b. Intercomparing the robustness of machine learning\nmodels in simulation and forecasting of streamflow. J. Water Clim. Change 12(5): 1824–1837.\nhttps://doi.org/10.2166/wcc.2020.365 .\nLoganathan, P. and A.B. Mahindrakar. 2021.\nStatistical downscaling using principal component\nregression for climate change impact assessment at the cauvery river basin.\nJ. Water Clim.\nChange 12(6): 2314–2324. https://doi.org/10.2166/wcc.2021.223 .\nMakowski, K., M. Wild, and A. Ohmura. 2008. Diurnal temperature range over europe between 1950\nand 2005. Atmos. Chem. Phys. 8(21): 6483–6498. https://doi.org/10.5194/acp-8-6483-2008 .\nOlsson, T., J. Jakkila, N. Veijalainen, L. Backman, J. Kaurola, and B. Vehvil¨ainen. 2015. Impacts\nof climate change on temperature, precipitation and hydrology in finland – studies using bias\ncorrected regional climate model data. Hydrol. Earth Syst. Sci. 19(7): 3217–3238. https://doi.\norg/10.5194/hess-19-3217-2015 .\nRasp, S., M.S. Pritchard, and P. Gentine. 2018. Deep learning to represent subgrid processes in\nclimate models. Proc. Natl. Acad. Sci. USA 115(39): 9684–9689. https://doi.org/10.1073/pnas.\n1810286115 .\nRiahi, K., D.P. van Vuuren, E. Kriegler, J. Edmonds, B.C. O’Neill, S. Fujimori, N. Bauer, K. Calvin,\nR. Dellink, O. Fricko, W. Lutz, A. Popp, J.C. Cuaresma, S. Kc, M. Leimbach, L. Jiang, T. Kram,\nS. Rao, J. Emmerling, and M. Tavoni. 2017. The shared socioeconomic pathways and their energy,\nland use, and greenhouse gas emissions implications: An overview. Glob. Environ. Change 42:\n153–168. https://doi.org/10.1016/j.gloenvcha.2016.05.009 .\nRosenzweig, C., J. Elliott, D. Deryng, A.C. Ruane, C. M¨uller, A. Arneth, K.J. Boote, C. Folberth,\nM. Glotter, N. Khabarov, K. Neumann, F. Piontek, T.A.M. Pugh, E. Schmid, E. Stehfest, H. Yang,\nand J.W. Jones. 2014.\nAssessing agricultural risks of climate change in the 21st century in a\nglobal gridded crop model intercomparison. Proc. Natl. Acad. Sci. USA 111(9): 3268–3273. https:\n//doi.org/10.1073/pnas.1222463110 .\nSanter, B.D., S. Solomon, C. Bonfils, M.D. Zelinka, J.F. Painter, F. Beltran, J.C. Fyfe, G. Johan-\nnesson, C. Mears, D.A. Ridley, J. Vernier, and F.J. Wentz. 2014. Observed multivariable signals\nof late 20th and early 21st century volcanic activity.\nGeophys. Res. Lett. 42(2): 500–509.\nhttps://doi.org/10.1002/2014gl062366 .\nSchuster, C. 2004. A note on the interpretation of weighted kappa and its relations to other rater\nagreement statistics for metric scales. Educ. Psychol. Meas. 64(2): 243–253. https://doi.org/10.\n1177/0013164403260197 .\nSolera-Rico, A., C. Sanmiguel Vila, M. G´omez-L´opez, Y. Wang, A. Almashjary, S.T.M. Dawson, and\nR. Vinuesa. 2024. ß-variational autoencoders and transformers for reduced-order modelling of fluid\nflows. Nat. Commun. 15(1): 1361. https://doi.org/10.1038/s41467-024-45578-4 .\nSummary for Policymakers. 2023, Jul. Summary for Policymakers. Cambridge, England: Cambridge\nUniversity Press.\nUrban, N.M. and T.E. Fricker. 2010. A comparison of latin hypercube and grid ensemble designs\nfor the multivariate emulation of an earth system model. Comput. Geosci. 36(6): 746–755. https:\n//doi.org/10.1016/j.cageo.2009.11.004 .\nVandal, N. 2018. Statistical downscaling of global climate models with image super-resolution and\nuncertainty quantification.\nvon Storch, H. 2011.\nReview of benestad, r. and i. hanssen-bauer (2008): Empirical-statistical\ndownscaling. Meteorol. Z. 20(1): 85–88 .\n23\n\n\nWang, J., Y. Jing, C. Zhang, and J. Zhao. 2009.\nReview on multi-criteria decision analysis aid\nin sustainable energy decision-making. Renew. Sustain. Energy Rev. 13(9): 2263–2278. https:\n//doi.org/10.1016/j.rser.2009.06.021 .\nWild, M., A. Ohmura, C. Sch¨ar, G. M¨uller, D. Folini, M. Schwarz, M.Z. Hakuba, and A. Sanchez-\nLorenzo. 2017. The global energy balance archive (geba) version 2017: A database for worldwide\nmeasured surface energy fluxes. Earth Syst. Sci. Data 9(2): 601–613. https://doi.org/10.5194/\nessd-9-601-2017 .\nXie, P. and P.A. Arkin. 1997.\nGlobal precipitation: A 17-year monthly analysis based on gauge\nobservations, satellite estimates, and numerical model outputs. Bull. Am. Meteorol. Soc. 78(11):\n2539–2558. https://doi.org/10.1175/1520-0477(1997)078 .\nYoon, K. and C.L. Hwang. 1995.\nMultiple attribute decision making: An introduction, SAGE\nPublications eBooks.\nYousif, M.Z., M. Zhang, L. Yu, R. Vinuesa, and H. Lim. 2023.\nA transformer-based synthetic-\ninflow generator for spatially developing turbulent boundary layers.\nJ. Fluid Mech. 957: A6.\nhttps://doi.org/10.1017/jfm.2022.1088 .\nZhang, C., P. Patras, and H. Haddadi. 2019.\nDeep learning in mobile and wireless networking:\nA survey. IEEE Commun. Surv. Tutor. 21(3): 2224–2287. https://doi.org/10.1109/comst.2019.\n2904897 .\nZhu, H., H. Liu, Q. Zhou, and A. Cui. 2023. Towards an accurate and reliable downscaling scheme for\nhigh-spatial-resolution precipitation data. Remote Sens. 15(10): 2640. https://doi.org/10.3390/\nrs15102640 .\nZorita, E. and H. von Storch. 1999. The analog method as a simple statistical downscaling technique:\nComparison with more complicated methods. J. Clim. 12(8): 2474–2489. https://doi.org/10.1175/\n1520-0442(1999)012 .\n24\n\n\n"}
{"text": " \nDavid C. Wyld et al. (Eds): AIMLA, CCNET, NLTM – 2025 \npp. 11-22, 2025. CS & IT - CSCP 2025                                                          DOI: 10.5121/csit.2025.150302 \n \nEVALUATING THE LONG-TERM \nVIABILITY OF EYE-TRACKING FOR \nCONTINUOUS AUTHENTICATION IN \nVIRTUAL REALITY \n \nSai Ganesh Grandhi  and Saeed Samet \n \nSchool of Computer Science, University of Windsor, Windsor, Canada \n \nABSTRACT \n \nTraditional authentication methods, such as passwords and biometrics, verify a user’s \nidentity only at the start of a session, leaving systems vulnerable to session hijacking. \nContinuous authentication, however, ensures ongoing verification by monitoring user \nbehavior. This study investigates the long-term feasibility of eye-tracking as a behavioral \nbiometric for continuous authentication in virtual reality (VR) environments, using data \nfrom the GazebaseVR dataset. Our approach evaluates three architectures—Transformer \nEncoder, DenseNet, and XGBoost—on short- and long-term data to determine their \nefficacy in user identification tasks. Initial results indicate that both Transformer Encoder \nand DenseNet models achieve high accuracy rates of up to 97% in short-term settings, \neffectively capturing unique gaze patterns. However, when tested on data collected 26 \nmonths later, model accuracy declines significantly, with rates as low as 1.78% for some \ntasks. To address this, we propose periodic model updates incorporating recent data, \nrestoring accuracy to over 95%. These findings highlight the adaptability required for \ngaze-based continuous authentication systems and underscore the need for model \nretraining to manage evolving user behavior. Our study provides insights into the efficacy \nand limitations of eye-tracking as a biometric for VR authentication, paving the way for \nadaptive, secure VR user experiences. \n \nKEYWORDS \n \nContinuous authentication, Virtual reality, Eye-tracking, Biometrics, Transformers \n \n1. INTRODUCTION \n \nThe most prevalent authentication method today is static authentication, which includes \npasswords, biometrics, PINs, and more. While techniques like these are challenging to by pass \ndue to their complex identity structure, static authentication has a noteworthy drawback. Once a \nmalicious actor successfully navigates through static authentication, they gain unrestricted access \nto the system, which continues to recognize them as a legitimate user[1]. \n \nIn contrast, continuous authentication offers a dynamic alternative that uniquely combines \nongoing user identification with a seamless user experience. Traditional authentication methods \ntypically verify the identity only at the beginning of a session, assuming 1 that the user remains \nunchanged throughout the session. However, continuous authentication monitors user behavior in \nreal time, making it considerably more challenging for unauthorized individuals to maintain \naccess. Any deviations from established behavioral patterns can trigger immediate responses, \n\n\n12                                       Computer Science & Information Technology (CS & IT) \n \nsuch as reauthentication or session termination. This proactive approach significantly mitigates \nthe risk of session hijacking, in which an attacker could take control of an already authenticated \nsession[2]. \n \nThe evolution of continuous authentication has progressed from desktop environments to mobile \ndevices and now extends to virtual reality (VR) headsets. These headsets utilize behavioral \nbiometrics such as eye tracking, hand movements, and head gestures. Recent studies, including \nthe work by Lohr et al. (2022)[3] and their 2024 follow-up[4], have demonstrated the \neffectiveness of eye tracking as a behavioral biometric, achieving low Equal Error Rates (EERs) \nin user identification[3]. \n \nAlthough these advances are promising, behavioral biometric patterns can change over time. It \nhappens due to users changing behavioral patterns; as noted in various studies, behavioral \npatterns are known to evolve[5]. When behavioral patterns evolve and change, authenticating \nimpostors versus legitimate users becomes difficult, and as a result, EER scores increase. Other \nbehavioral biometrics, such as gaze and touch patterns used in mobile devices, also undergo \nchanges that require continuous updates during the registration phase[5]. \n \nIn this study, we investigate the long-term usability of eye tracking for continuous authentication \nusing the GazebaseVR dataset, which spans 26 months and encompasses three distinct rounds of \ndata collection. We developed user models based on the transformer encoder architecture \nutilizing eye-tracking information from Round 1 (the first month) and tested the model’s ability to \npredict user data from Round 3 (captured after 26 months). Initial results showed low accuracy \nscores of approximately 10 percent. However, when incorporating data from all three rounds into \nthe user model, accuracy scores surged beyond 95 percent. This finding suggests that for eye \ntracking to serve as a reliable behavioral biometric, the user model must be updated with \nbehavioral data over time. \n \nOur research aims to analyze eye tracking as a viable behavioral biometric for VR/AR headsets \nby examining its long-term usability. In this study we also explore various authentication \narchitectures—including DenseNet, Transformer Encoder, and XGBoost—to ascertain which \nalgorithms yield optimal results for user identification. Our research findings indicate that while \nall three architectures achieve commendable accuracy rates, XGBoost demonstrates lower \nperformance with accuracies ranging from 85% to 90%. In contrast, both Transformer Encoder \nand DenseNet achieve accuracies between 90% and 97%. \n \n2. LITERATURE REVIEW \n \n2.1. Datasets for Eye Tracking in Virtual Reality \n \nThe development of reliable and scalable continuous authentication systems based on eye-\ntracking requires comprehensive datasets. Several datasets have been introduced in recent years, \neach offering unique attributes relevant to eye movement biometrics. \n \n2.1.1. GazeBaseVR \n \nGazeBaseVR, developed by Lohr et al. (2023), is a large-scale longitudinal dataset capturing eye \nmovements from 407 college-aged participants using a VR-enabled eye tracker at 250 Hz[6]. \nCollected over 26 months, the dataset includes 5,020 recordings across five tasks (vergence, \nsmooth pursuit, video viewing, reading, and random saccades), allowing for diverse eye \nmovement analysis. Unique to GazeBaseVR, the dataset provides 3D positional data (X, Y, Z) for \n\n\n \nComputer Science & Information Technology (CS & IT)                                           13 \n \nboth eyes and offers rich demographic diversity, contributing significantly to the field of eye \nmovement biometrics. Compared to its predecessor, GazeBase, this dataset adds novel task types, \nlike vergence, and supports binocular tracking, making it ideal for advanced VR-specific EMB \nstudies. Such extensive data supports the development of robust, generalizable machine learning \nmodels for eye movement analysis and authentication applications. \n \n2.1.2. GazeBase \n \nThe GazeBase dataset, presented by Griffith et al. (2021)[7], is a comprehensive longitudinal \ndataset featuring 12,334 monocular eye-movement recordings from 322 collegeaged \nparticipants[7]. Collected across nine rounds over 37 months, the data includes seven eye-\ntracking tasks, such as fixation, saccades, reading, and free viewing. All recordings were captured \nusing an EyeLink 1000 eye tracker at 1,000 Hz, with calibration performed for each task to \nensure accuracy. Due to its scale and repeated measures, GazeBase is well-suited for studies in \neye movement biometrics and machine learning applications focused on eye signal analysis. \nAdditionally, classification labels and pupil area data are available for a subset, providing \nvaluable resources for supervised learning in gaze analysis[7]. \n \n2.2. Eye Tracking in Continuous Authentication \n \nEye movement biometrics (EMB) have gained significant attention as potential mechanisms for \ncontinuous authentication in VR, where eye-tracking sensors can facilitate real-time identity \nverification. Recent studies have shown that gaze-driven biometrics can yield low equal error \nrates (EERs), essential for effective authentication. \n \n2.2.1. EKYT and DenseNet Implementation \n \nThe Eye Know You Too (EKYT)[3], based on DenseNet architecture, is optimized for eye \nmovement-based biometrics. The EKYT network employs eight convolutional layers with \ndensely connected layers to enhance feature extraction, followed by a global average pooling \nlayer and a fully connected layer, which generates a 128-dimensional embedding for each user. \nThis architecture has demonstrated robust performance for gaze-based continuous authentication, \nand its DenseNet foundation supports the efficient reuse of features across layers, addressing \nchallenges related to feature extraction in eye-tracking data[3]. \n \n2.2.2. Gaze Base VR and DenseNet Implementation \n \nThe GazeBaseVR dataset collects 5020 binocular eye movement recordings from 407 college-\naged participants over three rounds, enabling EMB research in VR environments[6]. Raju et al.[4] \nhave implemented the EKYT architecture in the GazeBaseVR data set. This study contrasts the \nbiometric performance of VR-collected data with a high-end 1,000 Hz eye tracker, showing that \nwhile VR data are noisier, it remains viable for authentication, achieving an equal error rate \n(EER) of 1.67% in short-term scenarios. These findings underline the potential of VR-based \nEMB, suggesting that VR eye-tracking data, despite challenges, may offer a convenient, accurate \nbiometric solution. \n \n2.3. Challenges in Eye Tracking for Continuous Authentication \n \nWhile gaze-based biometrics hold promise, challenges remain, particularly in terms of \ncalibration, signal quality, and user behavior variation over time. For instance, the visual axis, \nrequiring user-specific calibration, can be challenging for continuous authentication. Raju et al. \n(2024) noted that spatial accuracy directly affects authentication performance, with higher error \n\n\n14                                       Computer Science & Information Technology (CS & IT) \n \nrates observed when calibration is not consistently maintained. Further, the dynamic nature of \nuser behavior suggests a need for adaptive models that update with user data to maintain high \nauthentication accuracy over time. \n \n3. METHODOLOGY \n \n3.1. Dataset \n \nThe GazebaseVR dataset stands as the most comprehensive publicly accessible dataset focused \non eye-tracking data acquired from virtual reality (VR) and augmented reality (AR) headsets. \nThis dataset encompasses eye-tracking data collected from both eyes of participants while \nimmersed in VR, a setup essential for analyzing human gaze behaviors in virtual environments. \nThe study began with 465 individuals, but 58 were later excluded due to various considerations. \nThe data collection spanned three recording rounds over a 26-month period, with each round \nincorporating two separate recording sessions approximately 30 minutes apart. The eye-tracking \ndata (ET) were recorded using SensoMotoric Instruments’ (SMI’s) VR device, which samples \ndata from both eyes at a nominal rate of 250 Hz. Such a high sampling frequency enables precise \ncapture of eye movements, making the dataset ideal for analyzing fine-grained eye movement \npatterns. \n \n3.1.1. Dataset Tasks \n \nTo capture a comprehensive set of eye movement patterns, researchers instructed participants to \nperform five distinct tasks. These tasks were specifically designed to induce various eye \nmovements such as vergence, smooth pursuit, saccades, and fixations, providing a rich basis for \neye movement analysis. \n \nTable1:Overview of Eye Movement Tasks [6] \n \nTask \nFeatures \nDescription \nVergence task(VRG) \nConvergence and divergence A black dot appears on a large \nSquare plane and alternates between \ndifferent depths. \nSmooth \n(PUR) \npursuit task Saccades, fixations \nA \nsmall \nblack \nsphere \nmoves \nSmoothly between the left and right edges \nof the viewing region. \nVideo \n(VID) \nviewing task Multiple features \nA video is displayed on a large, \nRectangular plane. \nReading task(TEX) \nMultiple features \nAnexcerptofapproximately820 \ncharacters \nfrom \nNational \nGeo- \ngraphic is displayed. \nRandoms accade task \n(RAN) \nSaccades, fixations \nA small black sphere jump storan- \nDom screen positions. \n \n3.1.2. Dataset features \n \nThe ET API provided by SMI produces 3-dimensional unit vectors representing the gaze \ndirection of each eye and timestamps with nanosecond precision. There are 250 timestamp \nrecords for each second (250Hz), which provides a rich analysis of eye movements [6]. The \nfollowing features are collected for each user. We only utilize features n, clx, cly, clz, crx, cry, crz \n\n\n \nComputer Science & Information Technology (CS & IT)                                           15 \n \nand an additional feature created by us called user. The first 7 features provide patterns of eye \nmovements such as fixations, saccades, blink and more of users and these are used to train the \nuser model which is described in further sections. The last column user is used for multi-\nclassification, it basically represents which users information and we classify a user. \n \n3.2. Pre-Processing of Eye-Tracking Data \n \nThe pre-processing of raw eye-tracking data is essential to ensure consistency and quality for \nsubsequent analysis and model training. This involves selecting relevant features, normalizing \ndata temporally and spatially, and structuring it into segments suitable for model input. \n \nFigure1: Overview of the Methodology \n \n3.2.1. Feature Selection \n \nKey features are selected from the raw data to focus on essential gaze patterns. These include the \ntimestamp (’n’) and six spatial coordinates: ’clx’, ’cly’, ’clz’ for the left eye, and ’crx’, ’cry’, \n’crz’ for the right eye. For temporal normalization, timestamps are converted from milliseconds \nto seconds, ensuring consistency across devices and enabling standardized time-based pattern \nanalysis[4]. \n \n3.2.2. Normalization \n \nThe normalization process for eye-tracking data in this study involves both temporal and spatial \naspects, ensuring consistency and optimal input for model training. Temporal normalization \nconverts timestamps from milliseconds to seconds: \n \n \nSpatial normalization adjusts each coordinate to a range of [-1, 1] using Min-Max normalization: \n \n \n \nwhere x is the original value and xmin and xmax are the minimum and maximum values across \nthe dataset. The range is centered around zero, which can be beneficial for algorithms like \nTransformers, which those using activation functions. [8]. \n \n3.2.3. Windowing and Data Structuring \n \nTo prepare the continuous eye-tracking data for model input, we employ a segmentation strategy \nadapted from the DenseNet architecture [3]. The data stream is divided into fixed-size windows, \neach containing 1250 samples. This window size corresponds to a 5-second interval when \n\n\n16                                       Computer Science & Information Technology (CS & IT) \n \nsampled at 250 Hz, striking a balance between capturing meaningful temporal patterns and \nmaintaining a manageable input size for machine learning algorithms. The data within each \nwindow is then restructured into a 2D array format. In this arrangement, rows represent the \nspatial coordinates of eye movements, while columns correspond to discrete time points. This \norganization results in a 3D array structure (windows × coordinates × time points), which is \nparticularly well-suited for sequential data processing in transformer models [9]. \n \n3.3. Transformer Encoder Architecture \n \nThe proposed architecture, known as the Transformer Model, is specifically designed for \nprocessing eye movement biometrics (EMB). This model performs a mapping f : R(C×T) → RN , \nwhere C represents the number of input channels, T denotes the length of the input sequence and \nN corresponds to the number of output classes. The architecture is inspired by the original \nTransformer model [9] and has been adapted to efficiently handle the time-series data inherent in \neye tracking applications. \n \nThe Transformer Encoder architecture has not previously been applied to continuous \nauthentication using eye tracking as a behavioral biometric. This study seeks to benchmark its \nperformance against state-of-the-art models like EKYT and DenseNet [3]. Our findings reveal \nthat the Transformer Encoder achieved strong results and, in some instances, surpassed the \nDenseNet architecture. \n \nThe Transformer Model employs a dimensionality of dmodel = 64, utilizes 4 attention heads, and \ncomprises 2 transformer encoder layers. To mitigate over fitting, dropout is systematically \napplied throughout the architecture. This design ensures that the model remains compact and \nefficient, making it suitable for deployment in resource-constrained environments such as virtual \nreality (VR) and augmented reality (AR) devices. \n \nThe choice of dmodel = 64 was made to maintain a compact model size while preserving \nsufficient representational capacity. This dimensionality allows for efficient processing of the \neye-tracking time series data, which consists of 7 features (timestamp and 3D coordinates for \neach eye) sampled at 250 Hz. The model uses 4 attention heads and 2 encoder layers. This \nconfiguration was chosen to capture complex temporal dependencies in eye movement patterns \nwhile keeping the model lightweight. \n \n3.3.1. Training Parameters \n \nThe model is trained for 50 epochs. This number was chosen to provide sufficient iterations for \nthe model to learn patterns in the eye-tracking data while balancing computational resources. An \ninitial learning rate of 0.001 is used with the Adam optimizer. This relatively low learning rate \nwas selected to ensure stable training, particularly important for transformer models which can be \nsensitive to learning rate. A batch size of 32 is employed, striking a balance between \ncomputational efficiency and providing sufficient stochastic gradient estimates. These parameter \nchoices reflect a balance between model complexity and computational efficiency, tailored to the \nspecific requirements of continuous authentication in VR environments using eye-tracking data. \n \n3.4. Model Training for Eye-Tracking Data \n \nThe model training process involves distinct steps for each of the three models: XGBoost, \nTransformer Encoder, and DenseNet. Each model is tailored to leverage eye-tracking data for \nuser identification, utilizing varying architectures and methodologies to capture unique gaze \npatterns. The primary motivation for training XGBoost is its lower computational cost compared \n\n\n \nComputer Science & Information Technology (CS & IT)                                           17 \n \nto neural network-based approaches. This study aims to evaluate whether computationally \nefficient machine learning algorithms can deliver comparable performance to neural network \nmodels. \n \n3.4.1. Training XG Boost Model \n \nThe XGBoost model, widely recognized for its gradient boosting capabilities, is used here with a \nflattened 2D input derived from the original 3D eye-tracking data. The data is first reshaped to a \n2D format, allowing XGBoost to process it as a feature matrix where each row represents a user \nsample. XGBoost is highly suitable for tabular data due to its ability to optimize complex feature \ninteractions through gradient boosting [10]. For 7 model evaluation, the dataset is split in an \n80:20 ratio, with the training set comprising 80% of the data and the test set comprising 20 \n \nThe training objective is set to ”multi:softmax” for multi-class classification, with a learning rate \n(η) of 0.3 and maximum depth (max depth) of 6. A DMatrix is created for each dataset split, \nproviding a structured way for XGBoost to handle labeled data. After training, predictions are \nmade on the test set \n \nThis approach capitalizes on XGBoost’s strength in handling non-linear data interactions, \ndelivering high accuracy on gaze-based classification tasks. \n \n3.4.2. Training Transformer Encoder Model \n \nThe Transformer Encoder model is designed to leverage the sequential nature of eyetracking data, \nusing attention mechanisms to capture temporal dependencies and spatial relationships in the gaze \npatterns. In this setup, the pre-processed eye-tracking data is first converted to PyTorch tensors, \nwith user labels encoded for classification. The data is split into training and testing sets, with \nbatches handled by PyTorch’s ‘DataLoader‘ to optimize memory and computation. \n \nThe model architecture consists of an embedding layer that maps the input to a dmodel \ndimensional space, followed by a positional encoding layer to account for sequence order. The \nencoder structure includes multi-head self-attention and feed forward layers, following the \nformula: \n \n \n \nwhere Q, K, and V represent query, key, and value matrices, respectively, and dk is the \ndimensionality of the keys. The model is trained for 50 epochs with Cross Entropy Loss as the \ncriterion, and parameter updates are managed by the Adam optimizer with a learning rate of \n0.001 [9]. After each epoch, average loss is recorded to track model convergence. This model \neffectively learns sequential dependencies in eye movements, a crucial feature for reliable user \nidentification. \n \n3.4.3.  Training DenseNet Model \n \nThe DenseNet model, adapted from a convolutional neural network (CNN) structure, is tailored \nfor eye-tracking data through dense connections that encourage feature reuse and efficient \ngradient flow[11]. The model architecture begins with an initial convolutional layer, followed by \nmultiple dense layers, where each layer receives input from all preceding layers within the dense \nblock. This connectivity enhances learning efficiency and mitigates the vanishing gradient \n\n\n18                                       Computer Science & Information Technology (CS & IT) \n \nproblem, a common issue in deep CNNs. \n \nThe DenseNet model processes each input sample in a 1D convolutional format, with dense \nblocks of increasing dilation rates, allowing it to capture spatial dependencies across varying \nscales. The loss function used is Cross Entropy Loss, and the Adam optimizer updates parameters \nto minimize classification error. Similar to the Transformer model, DenseNet is trained for 50 \nepochs, with loss values logged for each epoch. \n \nDenseNet’s structure is advantageous for gaze data, as its dense connections capture both fine-\ngrained spatial details and broader contextual information, contributing to high performance in \ngaze-based user identification tasks. \n \n4. RESULTS AND DISCUSSION \n \n4.1. Short-Term Model Results \n \nThe initial phase of the experiment focused on short-term model performance, where XGBoost, \nTransformer Encoder, and DenseNet models were trained on Round 1 eyetracking data of 407 \nusers and evaluated on the same round, using an 80:20 train-test split. Table 2 shows the \naccuracies achieved for different eye movement tasks, demonstrating that both transformer and \nDenseNet models performed exceptionally well in classifying users based on their gaze patterns. \n \nIn this short-term scenario, the models achieved accuracies between 80.25% and 97.77% across \nvarious tasks, with few exceptions going below 80%, indicating that Transformer Encoder and \nDenseNet effectively capture unique gaze characteristics over a short timeframe. However, \nXGBoost is limited in performance, with 79.31% accuracy combined with all tasks. Although the \naccuracy of XGBoost does not meet with neural network architecture, it still provides exemplary \naccuracy. On the other hand, high accuracies with neural networks suggest that gaze patterns \ncontain distinct features that can differentiate users with a high degree of reliability when the data \ncollection and testing occur within a relatively close period. For tasks like Vergence (VRG) and \nSmooth Pursuit (PUR), which involve precise eye movements, the accuracy was exceptionally \nhigh, reflecting the stability of these gaze patterns over a short term. \n \nThis finding highlights the feasibility of using gaze-based biometrics for short-term \nauthentication in VR settings, where users’ gaze patterns remain stable and predictable. The high \nshort-term accuracy also underscores the potential of Transformer-based architectures to handle \nsequential eye-tracking data effectively. The window size for all tasks is 5 seconds, and data from \nall 407 users is used in training and testing. \n \nTable2:Model Accuracy Comparison-Short-term Training \n \nTask \nDenseNet \nTransformer \nXG Boost \nTrain Round \nTest Round \nAll \n97.09% \n97.20% \n79.31% \nRound1 \nRound1 \nPUR \n96.61% \n96.80% \n84.16% \nRound1 \nRound1 \nRAN \n95.52% \n95.58% \n80.25% \nRound1 \nRound1 \nTEX \n90.22% \n91.00% \n57.48% \nRound1 \nRound1 \nVID \n87.22% \n90.50% \n57.66% \nRound1 \nRound1 \nVRG \n96.47% \n97.77% \n87.39% \nRound1 \nRound1 \n \n \n\n\n \nComputer Science & Information Technology (CS & IT)                                           19 \n \n4.2. Long-Term Model Results \n \nTo evaluate the long-term stability of gaze-based biometrics, we assessed the model performance \nby training on the data of Round 1 and testing the data of Round 3 collected 26 months later. As \nshown in Table 3, this resulted in a significant drop in accuracy, with scores ranging from 1.78% \nto 10.28% depending on the task and model. This dramatic decrease in performance suggests that \ngaze patterns are not static and may evolve over 9 time, potentially influenced by factors such as \nchanges in user behavior, eye health, or VR interaction habits. This marked decrease suggests that \ngaze patterns undergo considerable changes over time[?], presenting significant challenges for \nmaintaining reliable user differentiation in long-term scenarios. \n \nDenseNet demonstrated the highest overall accuracy at 7.79% when combining all tasks, while \nthe Transformer Encoder showed the poorest performance at 3.01%. XGBoost exhibited mixed \nresults, outperforming other models in some tasks like TEX (12.11%) and VRG (11.46%), but \nunderperforming in others such as PUR (4.89%) and VID (1.78%). \n \nThe task-specific variations in accuracy suggest that certain gaze behaviors may be more stable \nover time, while others are highly variable. For instance, tasks related to text reading (TEX) and \nvergence (VRG) showed relatively higher accuracies, indicating potentially more consistent gaze \npatterns for these activities. In contrast, the video-watching task (VID) yielded the lowest \naccuracies across all models, highlighting the complexity of long-term gaze-based user \nidentification, particularly for dynamic visual stimuli[12]. \n \nThese findings underscore the need for more robust models and feature extraction techniques that \ncan adapt to temporal changes in gaze patterns. Future research should focus on developing \nmethods that can maintain higher accuracy levels over extended periods, possibly by \nincorporating adaptive learning mechanisms or by identifying more stable, long-term gaze \ncharacteristics[12]. \n \nThe low long-term accuracy indicates that models trained on older data fail to generalize well \nwhen tested on data collected after a long interval. For example, the Random Saccade (RAN) and \nVideo Viewing (VID) tasks, which depend heavily on dynamic gaze shifts, experienced \nsubstantial performance degradation. The results highlight a critical limitation in the long-term \nuse of gaze patterns for continuous authentication. Behavioral biometrics, like gaze data, are \ninherently dynamic[13], and this variability over time implies that models trained on gaze data \nmust be periodically updated. This need for frequent retraining or model adjustment aligns with \nfindings in related studies on continuous authentication, where user behavior tends to evolve, \nleading to potential identification challenges. \n \n \nTable3:Model Accuracy Comparison-Long-term Testing \n \nTask \nDenseNet \nTransformer \nXG Boost \nTrain Round \nTest Round \nAll \n7.79% \n3.01% \n4.85% \nRound1 \nRound3 \nPUR \n10.28% \n9.94% \n4.89% \nRound1 \nRound3 \nRAN \n5.98% \n7.67% \n6.15% \nRound1 \nRound3 \nTEX \n8.40% \n3.71% \n12.11% \nRound1 \nRound3 \nVID \n4.00% \n2.45% \n1.78% \nRound1 \nRound3 \nVRG \n8.06% \n7.57% \n11.46% \nRound1 \nRound3 \n \n \n \n\n\n20                                       Computer Science & Information Technology (CS & IT) \n \n4.3. Revised Long-term Model Results with Updated Data \n \nTo address the observed decline in long-term accuracy, a revised model was trained on a \ncombined dataset of Round 1 and Round 3 data. In this setup, the model was tested 10 on \npreviously unused Round 3 data to assess whether incorporating recent data would enhance \nperformance. As seen in Table 4, this approach resulted in significant improvement, with \naccuracies reaching up to 98.71%, closely matching the short-term results. \n \nThese improved results suggest that by continuously updating the training dataset with recent \ndata, the model can better adapt to evolving gaze patterns. This approach, which mirrors periodic \nretraining, can help maintain high authentication accuracy even as user behavior changes over \ntime. For continuous authentication systems to remain reliable, incorporating recent data into \ntraining may be essential, particularly for biometrics subject to temporal variability, like gaze. \n \nThis finding underlines the importance of adaptive modeling in the context of continuous \nauthentication. Behavioral biometrics, unlike static identifiers, require flexible models that can \naccommodate gradual changes in user behavior. Consequently, for eyetracking authentication \nsystems to be feasible in the long term, regular updates with recent behavioral data are likely \nrequired. Future research could explore optimal retraining intervals and data selection strategies \nto achieve a balance between computational cost and authentication accuracy. \n \nTable4:Model Accuracy Comparison-Long-term Training with Updated Data \n \nTask \nDenseNet \nTransformer \nXG Boost \nTrain Round \nTest Round \nAll \n98.71% \n96.52% \n93.25% \nRound1+3 \nRound3 \nPUR \n98.50% \n97.46% \n88.50% \nRound1+3 \nRound3 \nRAN \n97.14% \n98.32% \n83.66% \nRound1+3 \nRound3 \nTEX \n98.75% \n98.22% \n55.13% \nRound1+3 \nRound3 \nVID \n86.22% \n91.78% \n55.78% \nRound1+3 \nRound3 \nVRG \n97.17% \n94.48% \n87.55% \nRound1+3 \nRound3 \n \n4.4. Ethical Implications of Eye-Tracking for Continuous Authentication \n \nWhile eye-tracking offers promising advancements in continuous authentication for virtual reality \nenvironments, it also raises several ethical concerns that must be carefully addressed: \n \n4.4.1. Privacy Concerns \n \nEye movement patterns can potentially reveal sensitive information about a user’s mental or \nphysical state. For instance, certain gaze patterns might indicate cognitive load, emotional states, \nor even medical conditions such as attention deficit disorders or early signs of neurological \ndiseases[14]. It is crucial to ensure that this data is used solely for authentication purposes and not \nfor unauthorized analysis or profiling. \n4.4.2. Data Security \n \nThe secure storage and transmission of eye-tracking data is paramount. Given the sensitive nature \nof biometric information, robust encryption and data protection measures must be implemented to \nprevent unauthorized access or data breaches[15]. Developers of VR systems must adhere to strict \ndata protection standards and regularly audit their security protocols. \n \n \n \n\n\n \nComputer Science & Information Technology (CS & IT)                                           21 \n \n4.4.3. User Consent and Control \n \nClear and comprehensive user agreements are essential when implementing eye-tracking \nauthentication. Users must be fully informed about what data is collected, how it is used, and who \nhas access to it. Additionally, users should have the option to opt out of continuous authentication \nor choose alternative methods, ensuring their autonomy in deciding how their biometric data is \nused[15]. \n \nBy addressing these ethical considerations, we can work towards developing eye tracking \nauthentication systems that not only enhance security but also respect user privacy, promote \ninclusivity, and maintain high ethical standards in the rapidly evolving field of virtual reality \ntechnology. \n \n5. CONCLUSION \n \nThis study investigated the use of eye-tracking data as a behavioral biometric for continuous user \nauthentication in virtual reality (VR) environments, with a focus on both short-term and long-\nterm usability. Using the GazebaseVR dataset, we evaluated the performance of Transformer \nEncoder and DenseNet models for user identification, achieving promising results in short-term \nexperiments with accuracy levels reaching over 97%. These results indicate that gaze patterns in \nthe short term can serve as a reliable biometric, with both the Transformer and DenseNet \narchitectures proving effective in classifying users based on unique eye movement characteristics. \n \nHowever, when testing model performance over an extended period, significant accuracy \ndegradation was observed, with accuracy dropping to as low as 1.78% for certain tasks after 26 \nmonths. This decline highlights a key limitation of behavioral biometrics such as eye tracking: \ngaze patterns are subject to temporal changes, likely influenced by behavioral shifts, health \nfactors, or user adaptation to VR environments. The findings underscore that, while effective in \nthe short term, static models fail to generalize well over time, making continuous model updates \nessential for sustaining high accuracy in real-world applications. \n \nTo address this, we explored an adaptive model training approach by incorporating recent data \ninto the training set. This method restored accuracy to near short-term levels, with performance \nimprovements exceeding 98%. Such results suggest that periodic model retraining with recent \ndata is crucial to maintaining the viability of gaze-based continuous authentication systems. \nAdaptive modeling, where data from subsequent sessions are used to update the user model, can \npotentially offset the temporal variability in gaze patterns, providing a practical solution for long-\nterm user identification in VR. \n \nIn conclusion, this study demonstrates the feasibility of using eye-tracking as a behavioral \nbiometric for continuous authentication in VR settings, while emphasizing the need for adaptive \nmodel retraining to account for behavioral drift over time. Future work could focus on \ndetermining optimal retraining intervals and exploring additional features such as head or hand \nmovements to enhance model robustness. As VR applications grow in importance, developing \nreliable, adaptive biometric systems for continuous authentication will be essential for enhancing \nuser security in immersive environments. \n \n \n \n \n \n\n\n22                                       Computer Science & Information Technology (CS & IT) \n \nACKNOWLEDGEMENTS \n \nWe extend our thanks to the School of Computer Science at the University of Windsor for \nproviding the resources necessary for conducting this research. Special appreciation goes to Dr. \nOleg V. Komogortsev and his team for developing and sharing the GazeBaseVR dataset, which \nplayed a crucial role in our analysis. Additionally, we are grateful to our colleagues for their \nconstructive feedback and insightful discussions that enriched our work. \n \nREFERENCES \n \n[1] \nKeystrike. Continuous authentication vs two-factor authentication, 2022. \n[2] \nBeyond Identity. Continuous authentication: A dynamic approach to user verification, 2022. \n[3] \nDillon Lohr and Oleg V. Komogortsev. Eye know you too: Toward viable end-to-end eye movement \nbiometrics for user authentication. IEEE Transactions on Information Forensics and Security, \n17:3151–3164, 2022. \n[4] \nMehedi Hasan Raju, Dillon J Lohr, and Oleg V Komogortsev. Evaluating eye movement biometrics \nin virtual reality: A comparative analysis of vr headset and high-end eye-tracker collected dataset, \n2024. \n[5] \nMingming Hu, DingWang, Chen Li, Yang Xu, and Bibo Tu. Behavioral biometrics based continuous \nauthentication using a lightweight latent representation masked one-class autoencoder. IEEE \nTransactions on Dependable and Secure Computing, pages 1–16, 2024. \n[6] \nDavid Lohr, Shahbaz Aziz, Linda Friedman, et al. Gazebasevr, a large-scale, longitudinal, binocular \neye-tracking dataset collected in virtual reality. Scientific Data, 10:177, 2023. \n[7] \nHannah Griffith, David Lohr, Evgeniy Abdulin, et al. Gazebase, a large-scale, multistimulus, \nlongitudinal eye movement dataset. Scientific Data, 8:184, 2021. \n[8] \nSimon Eberz, Kasper B. Rasmussen, Vincent Lenders, and Ivan Martinovic. Looks like eve: Exposing \ninsider threats using eye movement biometrics. ACM Trans. Priv. Secur., 19(1), June 2016. \n[9] \nAshish Vaswani, John Shardlow, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \nŁukasz Kaiser, Simon Kattner, and Niki Parmar. Attention isall you need. Advances in Neural \nInformation Processing Systems, 2017. \n[10] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the \n22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’16, \npage 785–794. ACM, August 2016. \n[11] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected \nconvolutional networks, 2018. \n[12] Efe Bozkir, Benedikt B¨uhler, Hanna Deininger, Peter Goldberg, Peter Gerjets, Ulrich Trautwein, and \nEnkelejda Kasneci. Gaze-based detection of mind wandering during audio-guided panorama viewing, \n2024. \n[13] Benedikt B¨uhler, Efe Bozkir, Hanna Deininger, Peter Goldberg, Peter Gerjets, Ulrich Trautwein, and \nEnkelejda Kasneci. Gaze-based detection of mind wandering during audio-guided panorama viewing. \nScientific Reports, 14(1):2724, 2024. \n[14] Maria K Eckstein, Bel´en Guerra-Carrillo, Alison T Miller Singley, and Silvia A Bunge. Beyond eye \ngaze: What else can eyetracking reveal about cognition and cognitive development? Developmental \ncognitive neuroscience, 25:69–91, 2017. \n[15] Daniel J Liebling and S¨oren Preibusch. Privacy considerations for a pervasive eye tracking world. \nProceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous \nComputing: Adjunct Publication, pages 1169–1177, 2014. \n \n \n \n \n© 2025 By AIRCC Publishing Corporation. This article is published under the Creative Commons \nAttribution (CC BY) license. \n\n\n"}
{"text": "Generative Uncertainty in Diffusion Models\nMetod Jazbec*1\nEliot Wong-Toi2\nGuoxuan Xia3\nDan Zhang4\nEric Nalisnick5\nStephan Mandt2\n1UvA-Bosch Delta Lab, University of Amsterdam\n2University of California, Irvine\n3Imperial College, London\n4Bosch Center for AI\n5Johns Hopkins University\nAbstract\nDiffusion models have recently driven significant\nbreakthroughs in generative modeling. While state-\nof-the-art models produce high-quality samples\non average, individual samples can still be low\nquality. Detecting such samples without human\ninspection remains a challenging task. To address\nthis, we propose a Bayesian framework for esti-\nmating generative uncertainty of synthetic sam-\nples. We outline how to make Bayesian inference\npractical for large, modern generative models and\nintroduce a new semantic likelihood (evaluated in\nthe latent space of a feature extractor) to address\nthe challenges posed by high-dimensional sample\nspaces. Through our experiments, we demonstrate\nthat the proposed generative uncertainty effectively\nidentifies poor-quality samples and significantly\noutperforms existing uncertainty-based methods.\nNotably, our Bayesian framework can be applied\npost-hoc to any pretrained diffusion or flow match-\ning model (via the Laplace approximation), and\nwe propose simple yet effective techniques to min-\nimize its computational overhead during sampling.\n1\nINTRODUCTION\nDiffusion (and flow-matching) models [Sohl-Dickstein\net al., 2015, Song et al., 2020a,b, Lipman et al., 2022] have\nrecently pushed the boundaries of generative modeling due\nto their strong theoretical underpinnings and scalability.\nAcross various domains, they have enabled the generation\nof increasingly realistic samples [Rombach et al., 2022,\nEsser et al., 2024, Li et al., 2024]. Despite the impressive\nprogress, state-of-the-art models can still generate low\nquality images that contain artefacts and fail to align\nwith the provided conditioning information. This poses a\n*Corresponding author: <m.jazbec@uva.nl>\nchallenge for deploying diffusion models, as it can lead to\na poor user experience by requiring multiple generations\nto manually find an artefact-free sample.\nBayesian inference has long been applied to detect\npoor-quality predictions in predictive models [MacKay,\n1992b, Gal et al., 2016, Wilson, 2020, Arbel et al., 2023].\nBy capturing the uncertainty of the model parameters due\nto limited training data, each prediction can be assigned\na predictive uncertainty, which, when high, serves as a\nwarning that the prediction may be unreliable. Despite its\nwidespread use for principled uncertainty quantification\nin predictive models, Bayesian methodology has been far\nless commonly applied to detecting poor generations in\ngenerative modeling. This raises a key question: How can\nBayesian principles help us detect poor generations?\nIn this work, we propose a Bayesian framework for esti-\nmating generative uncertainty in modern generative models,\nsuch as diffusion. To scale Bayesian inference for large\ndiffusion models, we employ the (last-layer) Laplace ap-\nproximation [MacKay, 1992a, Ritter et al., 2018, Daxberger\net al., 2021a]. Additionally, to address the challenge posed\nby the high-dimensional sample spaces of data such as nat-\nural images, we introduce a semantic likelihood, where we\nleverage pretrained image encoders (such as CLIP [Radford\net al., 2021]) to compute variability in a latent, semantic\nspace instead. Through our experiments, we demonstrate\nthat generative uncertainty is an effective tool for detecting\nlow-quality samples and propose simple strategies to\nminimize the sampling overhead introduced by Bayesian in-\nference. In particular, we make the following contributions:\n1. We formalize the notion of generative uncertainty and\npropose a method to estimate it for modern generative\nmodels (Section 3). Analogous to how predictive\nuncertainty helps identify unreliable predictions in\npredictive models, generative uncertainty can be used\nto detect low-quality generations in generative models.\n2. We show that our generative uncertainty strongly\noutperforms previous uncertainty-based approaches\narXiv:2502.20946v1  [cs.LG]  28 Feb 2025\n\n\nfor filtering out poor samples [Kou et al., 2024, De Vita\nand Belagiannis, 2025]. Additionally, we achieve\ncompetitive performance with non-uncertainty-based\nmethods, such as realism score [Kynkäänniemi et al.,\n2019] and rarity score [Han et al., 2023] (Section 4.1),\nwhile also highlighting the complementary benefits\nof uncertainty (Appendix A.5).\n3. We propose effective strategies to reduce the sampling\noverhead of Bayesian uncertainty (Section 4.2) and\ndemonstrate the applicability of our framework beyond\ndiffusion models by applying it to a (latent) flow\nmatching model (Section A.7).\n2\nBACKGROUND\n2.1\nGENERATIVE MODELING\nSampling in Generative Models\nModern deep gen-\nerative models like variational autoencoders (VAEs)\n[Kingma, 2013], generative adversarial networks (GANs)\n[Goodfellow et al., 2014], and diffusion models differ in\ntheir exact probabilistic frameworks and training schemes,\nyet share a common sampling recipe: start with random\nnoise and transform it into a new data sample [Tomczak,\n2022]. Specifically, let x ∈X denote a data sample and\nz ∈Z an initial noise. A new sample is generated by:\nz ∼p(z) ,\nˆx = gθ(z) ,\nwhere p(z) is an initial noise (prior) distribution, typically a\nstandard Gaussian N(0, I), and gθ : Z →X is a generator\nfunction with model parameters θ ∈RP .\nDiffusion Models\nThe primary focus of this work is on\ndiffusion models [Sohl-Dickstein et al., 2015]. These mod-\nels operate by progressively corrupting data into Gaussian\nnoise and learning to reverse this process. For a data sample\nx0 ∼q(x), the forward (noising) process is defined as\nxt = √¯αtx0 +\n√\n1 −¯αtϵ, ϵ ∼N(0, I)\nwhere ¯αt = Qt\ns=1(1 −βs) and {βs}T\ns=1 is a noise schedule\nchosen such that xT ∼N(0, I) (approximately). In the\nbackward process, a denoising network fθ is learned via\na simplified regression objective (among various possible\nparameterizations, see Song et al. [2020b] or Karras et al.\n[2022]):\nL(θ; D) = Et,x0,ϵ\nh\f\f\f\ffθ(√¯αtx0+\n√\n1−¯αtϵ, t) −ϵ\n\f\f\f\f2\n2\ni\n(1)\nwhere D = {xn}N\nn=1 denotes a training dataset of images.\nAfter training, diffusion models generate new samples via\na generator function, gˆθ, which consists of sequentially\napplying the learned denoiser, fˆθ, and following specific\ntransition rules from samplers such as DDPM [Ho et al.,\n2020] or DDIM [Song et al., 2020a].\n2.2\nBAYESIAN DEEP LEARNING\nBayesian neural networks (BNNs) go beyond point\npredictions and allow for principled uncertainty quantifi-\ncation [Buntine and Weigend, 1991, Neal, 1995, Kendall\nand Gal, 2017, Jospin et al., 2022]. Let hψ : X →Y\ndenote a predictive model with parameters ψ ∈RO and\nD = {(xn, yn)}N\ni=1 denote training data. Instead of finding\na single fixed set of parameters, ˆψ = arg max L(ψ; D),\nthat maximizes an objective function L, BNNs specify a\nprior p(ψ) over model parameters and define a likelihood\np(y|hψ(x)), which together yield a posterior distribution\nvia Bayes rule: p(ψ|D)\n∝\np(ψ) QN\nn=1 p(yn|hψ(xn)).\nUnder this Bayesian view, a predictive model for a new\ntest point x∗is then obtained via the posterior predictive\ndistribution [Murphy, 2022]:\np(y|x∗, D) = Ep(ψ|D)\n\u0002\np(y|hψ(x∗))\n\u0003\n.\nFor large models, finding the exact posterior distribution is\ncomputationally intractable, hence an approximate posterior\nq(ψ|D) is used instead. Popular approaches for approximate\ninference include deep ensembles [Lakshminarayanan\net al., 2017], variational inference [Blundell et al., 2015,\nZhang et al., 2018], SWAG [Mandt et al., 2017, Maddox\net al., 2019], and Laplace approximation [Daxberger et al.,\n2021a]. Moreover, to alleviate computational overhead, it is\ncommon to give a ‘Bayesian treatment’ only to a subset of\nparameters [Kristiadi et al., 2020, Daxberger et al., 2021b,\nSharma et al., 2023]. Finally, the intractable expectation\nintegral in the posterior predictive is approximated via\nMonte-Carlo (MC) sampling:\np(y|x∗, D)≈1\nM\nM\nX\nm=1\np(y|hψm(x∗)), ψm ∼q(ψ|D),\n(2)\nwith M denoting the number of MC samples. By measuring\nthe variability of the posterior predictive distribution, e.g.,\nits entropy, one can obtain an estimate of the model’s pre-\ndictive uncertainty for a given test point u(x∗). The utility\nof such uncertainties has been demonstrated on a wide\nrange of tasks such as out-of-distribution (OOD) detection\n[Daxberger et al., 2021a], active learning [Gal et al., 2017],\nand detection of influential samples [Nickl et al., 2024].\n3\nGENERATIVE UNCERTAINTY\nVIA BAYESIAN INFERENCE\nWhile Bayesian neural networks (BNNs) have traditionally\nbeen applied to predictive models to estimate predictive un-\ncertainty, in this section we demonstrate how to apply them\nto diffusion to estimate generative uncertainty (see Figure 1\nand Algorithm 1 for an overview of our method). Later in\nSection 4, we show that generative uncertainty can be used\nto detect poor-quality samples. Our focus is on generative\n\n\n…\n…\nθi ∼q(θ|D)\ngθ1\ngθm\ngθM\ncϕ\ncϕ\ncϕ\nLow u(z1)\n…\n…\nz1 ∼p(z)\n…\n…\ncϕ\ncϕ\ncϕ\n…\n…\nHigh u(z2)\ngθ1\ngθm\ngθM\nz2 ∼p(z)\nu(z) := H(p(x|z, D)\nFigure 1: Illustration of how we compute generative uncertainty for each random noise z. For a generative model gθ, we\nsample M sets of model parameters from our posterior distribution q(θ|D) and generate M images. Then, we evaluate\nthe semantic likelihood for each by computing feature embeddings with a pretrained encoder cϕ (e.g., CLIP) and take the\nuncertainty (e.g., entropy) over these embeddings. Random noises z with low uncertainty (left) tend to lead to consistent,\nhigh quality generations while random noises with high uncertainty (right) lead to poor, discordant generations.\nmodels for natural images, where x ∈RH×W ×C. For ease\nof exposition, we consider unconditional generation in\nthis section, though our methodology can also be applied\ndirectly to conditional models (see Section 4.1).\n3.1\nGENERATIVE UNCERTAINTY\nAs in traditional Bayesian predictive models (cf. Sec-\ntion 2.2), the central principle for obtaining a Bayesian\nnotion of uncertainty in diffusion models is the posterior\npredictive distribution:\np(x|z, D) = Ep(θ|D)\n\u0002\np(x|gθ(z))\n\u0003\n.\n(3)\nHere, we use z (with a slight abuse of notation) to denote\nthe entire randomness involved in the diffusion sampling\nprocess.1 Generative uncertainty is then defined as the\nvariability of the posterior predictive:\nu(z) := V(p(x|z, D))\n(4)\nwhere V(·) denotes the variability measure, such as entropy.\nWe propose a tractable estimator of the posterior predictive\nlater in Eq. 8.\nIn the same way that the predictive uncertainty u(x∗),\nof a predictive model provides insight into the quality\nof its prediction for a new test point x∗, the generative\nuncertainty u(z) of a generative model gθ should offer\ninformation about the quality of the generation gθ(z) for\na ‘new’ random noise sample z. We demonstrate this\n1In DDIM [Song et al., 2020a] and ODE sampling [Song et al.,\n2020b], randomness is only present at the start of the sampling\nprocess (akin to VAEs and GANs). In contrast, in DDPM [Ho\net al., 2020] and SDE sampling [Song et al., 2020b], randomness\nis introduced at every step throughout the sampling process.\nrelationship experimentally in Section 4. Next, we discuss\nhow to make Bayesian inference on (large) diffusion models\ncomputationally tractable.\n3.2\nLAST-LAYER LAPLACE APPROXIMATION\nState-of-the-art diffusion models are extremely large\n(100M to 1B+ parameters) and can take weeks to train.\nConsequently, the computational overhead of performing\nBayesian inference on such large models is of signif-\nicant concern. To address this, we adopt the Laplace\napproximation [MacKay, 1992a, Shun and McCullagh,\n1995] to approximate the posterior q(θ|D). The Laplace\napproximation is among the most computationally efficient\napproximate inference methods while still offering com-\npetitive performance [Daxberger et al., 2021a]. Moreover, a\nparticularly appealing feature of the Laplace approximation\nis that it can be applied post-hoc to any diffusion model.\nWe leverage this property in Section 4, where we apply it\nto a variety of popular diffusion and flow-matching models.\nThe Laplace approximation of the posterior is given by:\nq(θ|D) = N(θ|ˆθ, Σ), Σ =\n\u0000∇2\nθL(θ; D)\n\f\fˆθ\n\u0001−1,\n(5)\nwhere ˆθ represents the parameters of a pre-trained diffusion\nmodel, and Σ is the inverse Hessian of the diffusion training\nloss from Eq. 1. To reduce the computational cost further,\nwe apply a ‘Bayesian’ treatment only to the last layer of\nthe denoising network fθ.\nWe note that the use of last-layer Laplace approximation for\ndiffusion models has been previously proposed in BayesDiff\n[Kou et al., 2024]. While our implementation of the Laplace\napproximation closely follows theirs, there are significant\ndifferences in how we utilize the approximate posterior,\n\n\nq(θ|D). Specifically, in our approach, we use it within\nthe traditional Bayesian framework (Eq. 3) to sample new\ndiffusion model parameters, leaving the diffusion sampling\nprocess, gθ, unchanged. In contrast, BayesDiff resamples\nnew weights from q(θ|D) at every diffusion sampling\nstep t, which necessitates substantial modifications to\nthe diffusion sampling process through their variance\npropagation approach. We later demonstrate in Section 4\nthat modifications such as variance propagation are\nunnecessary for obtaining Bayesian generative uncertainty\nand staying closer to the traditional Bayesian setting leads\nto the best empirical performance.\n3.3\nSEMANTIC LIKELIHOOD\nWe next discuss the choice of likelihood for estimating gen-\nerative uncertainty in diffusion models. Since the denoising\nproblem in diffusion is modeled as a (multi-output) regres-\nsion problem, the most straightforward approach is to place\na simple Gaussian distribution over the generated sample:\np(x|gθ(z)) = N(x | gθ(z), σ2I),\n(6)\nwhere σ2 represents the observation noise.\nHowever, as we will demonstrate in Section 4, this\nlikelihood leads to non-informative estimates of gener-\native uncertainty (Eq. 4). The primary issue is that the\nsample space of natural images is high-dimensional (i.e.,\n|X|\n=\nHWC). Consequently, placing the likelihood\ndirectly in the sample space causes the variability of the\nposterior predictive distribution to be based on pixel-level\ndifferences. This is problematic because it is well-known\nthat two images can appear nearly identical to the human eye\nwhile exhibiting a large L2-norm difference in pixel space\nX (see, for example, the literature on adversarial examples\n[Szegedy, 2013]). To get around this, we propose to map the\ngenerated samples to a ‘semantic’ latent space, S, via a pre-\ntrained feature extractor, cϕ : X →S (e.g., an inception-net\n[Szegedy et al., 2016] or a CLIP encoder [Radford et al.,\n2021]). The resulting semantic likelihood has the form\np(x|gθ(z); ϕ) = N(e(x) | cϕ\n\u0000gθ(z)\n\u0001\n, σ2I)\n(7)\nwhere e(x) ∈S is the (random) vector of semantic features.\nBy combining the (last-layer) Laplace approximate poste-\nrior and the semantic likelihood, we can now approximate\nthe posterior predictive (Eq. 3) as\np(x|z, D) ≈N\n\u0000e(x)\n\f\f ¯e, Diag\n\u0000 1\nM\nM\nX\nm=1\ne2\nm −¯e2\u0001\n+ σ2I\n\u0001\n,\n¯e = 1\nM\nM\nX\nm=1\nem,\nem = cϕ\n\u0000gθm(z)\n\u0001\n, θm ∼q(θ|D),\n(8)\nwhere M denotes the number of Monte Carlo samples.\nAdditionally, we approximate the posterior predictive with a\nAlgorithm 1: Diffusion Sampling with Generative Unc.\nInput\n:random noise z, pretrained diffusion model\ngˆθ, Laplace posterior q(θ|D) (Eq. 5), number\nof MC samples M, semantic feature extractor\ncϕ, semantic likelihood noise σ\nOutput :generated sample ˆx0, generative uncertainty\nestimate u(z)\n1 Generate a sample ˆx0 = gˆθ(z)\n2 Get semantic features e0 = cϕ(ˆx0)\n3 for m = 1 →M do\n4\nθm ∼q(θ|D)\n5\nˆxm = gθm(z)\n6\nem = cϕ(ˆxm)\n7 end\n8 Compute p(x|z, D) using {em}M\nm=0 (Eq. 7)\n9 Compute the entropy u(z) = H(p(x|z, D))\n10 return ˆx0, u(z)\nsingle Gaussian via moment matching here, a common prac-\ntice in Bayesian neural networks for regression problems\n[Lakshminarayanan et al., 2017, Antorán et al., 2020].\nUnlike in the posterior predictive for predictive models\n(Eq. 2), where it is used to obtain both the prediction and the\nassociated uncertainty, the generative posterior predictive\n(Eq. 8) is used solely to estimate the generative uncertainty\nu(z). The actual samples ˆx are still generated using the pre-\ntrained diffusion model gˆθ (see Algorithm 1). As a variabil-\nity measure V(·) in our generative uncertainty framework,\nwe propose to use entropy (denoted with H(·) in Algo-\nrithm 1) due to its simplicity and widespread use in quantify-\ning predictive uncertainty. However, we note that alternative\nmeasures of variability, such as pairwise-distance estimators\n(PAiDEs) [Berry and Meger, 2023], can also be employed.\n3.4\nEPISTEMIC UNCERTAINTY\nUncertainty is commonly decomposed into two components:\naleatoric and epistemic [Hüllermeier and Waegeman, 2021,\nSmith et al., 2024]. Aleatoric uncertainty represents the\nirreducible uncertainty inherent in the data-generating\nprocess, while epistemic uncertainty arises from observing\nonly a limited amount of training data. In our framework, we\nfix the observation noise in the semantic likelihood (Eq. 7)\nto a small constant value (e.g., σ = 0.001). As a result, the\ngenerative uncertainty we capture is primarily epistemic,\nreflecting uncertainty about the model parameters θ due\nto limited training data via q(θ|D). Since the parameters\nϕ of the semantic feature extractor cϕ are kept fixed in the\nsemantic likelihood, the resulting generative uncertainty\nu(z) continues to reflect the epistemic uncertainty of the\ndiffusion model parameters θ. Extending our framework\nto capture the aleatoric uncertainty of a generative process\npresents an interesting avenue for future research.\n\n\n4\nEXPERIMENTS\nIn our experiments, we demonstrate that generative uncer-\ntainty is an effective method for detecting poor samples\nin diffusion models (Section 4.1). We also discuss the\nsampling overhead introduced by our Bayesian approach\nand show that it can be effectively minimized (Section 4.2).\nFinally, we extend our Bayesian framework beyond diffu-\nsion by applying it to detect low-quality samples in a (latent)\nflow matching model (Appendix A.7). Our code is available\nat https://github.com/metodj/DIFF-UQ.\n4.1\nDETECTING LOW-QUALITY GENERATIONS\nTo evaluate whether our newly introduced generative uncer-\ntainty can be used to detect low-quality generations, we fol-\nlow the experimental setup from prior work on uncertainty-\nbased filtering [Kou et al., 2024, De Vita and Belagiannis,\n2025]. Specifically, we generate 12K samples using a given\ndiffusion model and compute the uncertainty estimate for\neach sample. We then select the 10K samples with the lowest\nuncertainty. If uncertainty reliably reflects the visual quality\nof generated samples, filtering based on it should yield\ngreater improvements in population-level metrics (such as\nFID) compared to selecting a random subset of 10K images.\nImplementation Details\nTo ensure a fair comparison\nwith BayesDiff [Kou et al., 2024], we adopt their proposed\nimplementation of the last-layer Laplace approximation.\nSpecifically, we use an Empirical Fisher approximation\nof the Hessian with a diagonal factorization [Daxberger\net al., 2021a]. When computing the posterior predictive\ndistribution (Eq. 8), we use M = 5 Monte Carlo samples.\nFor the semantic feature extractor cϕ, we leverage a\npretrained CLIP encoder [Radford et al., 2021]. Additional\nimplementation details are provided in Appendix B.\nBaselines We first compare our proposed generative\nuncertainty to existing uncertainty-based approaches for\ndetecting low-quality samples: BayesDiff and the aleatoric\nuncertainty (AU) approach proposed by De Vita and Bela-\ngiannis [2025]. BayesDiff estimates epistemic uncertainty in\ndiffusion models using a last-layer Laplace approximation\nand tracks this uncertainty throughout the entire sampling\nprocess. In contrast, in AU aleatoric uncertainty is computed\nby measuring the sensitivity of intermediate diffusion\nscores to random perturbations. Unlike our approach, both\nmethods estimate uncertainty directly in pixel space.\nImportantly, we also compare our method against non-\nuncertainty-based sample-level metrics, such as the realism\nscore [Kynkäänniemi et al., 2019] and the rarity score [Han\net al., 2023]. These metrics work by measuring the distance\nof a generated sample from the data manifold (derived\nfrom a reference dataset) in a semantic space spanned by\nthe inception-net features [Szegedy et al., 2016]. Notably,\nprior work [Kou et al., 2024, De Vita and Belagiannis,\n2025] has not considered such comparisons, which we\nbelieve are essential for assessing the practical utility of\nuncertainty-based filtering.\nEvaluation Metrics\nIn addition to the widely used\nFréchet Inception Distance (FID) [Heusel et al., 2017] for\nevaluating the quality of a filtered set of images, we also\nreport precision and recall metrics [Sajjadi et al., 2018,\nKynkäänniemi et al., 2019]. To compute these quantities\nwe fit two manifolds in feature space: one for the generated\nimages and another for the reference (training) images.\nPrecision is the proportion of generated images that lie in\nthe reference image manifold while recall is the proportion\nof reference images that lie in the generated image manifold.\nPrecision measures the quality (or fidelity) of generated\nsamples, whereas recall quantifies their diversity (or\ncoverage over the reference distribution).\nResults\nWe present our main results on the ImageNet\ndataset in Table 1. We first observe that existing uncertainty-\nbased approaches (BayesDiff and AU) result in little to no\nimprovement in metrics that assess sample quality (FID and\nprecision). In contrast, our generative uncertainty method\nleads to significant improvements in terms of both FID\nand precision. For example, on the UViT model [Bao et al.,\n2023], a subset of images selected based on our uncertainty\nmeasure achieves an FID of 7.89, significantly outper-\nforming both the Random baseline (9.45) and existing\nuncertainty-based methods (BayesDiff 9.16, AU 9.20).\nNext, in order to qualitatively demonstrate the effectiveness\nof our approach, we show 25 samples with the highest\nand lowest generative uncertainty (out of the original\n12K samples) according to our method in Figure 2.\nHigh-uncertainty samples exhibit numerous artefacts, and\nin most cases, it is difficult to determine what exactly they\ndepict. Combined with the quantitative results in Table 1,\nthis supports our hypothesis that (Bayesian) generative\nuncertainty is an effective metric for identifying low-quality\nsamples. Conversely, the lowest-uncertainty samples are of\nhigh quality, with most appearing as ‘canonical’ examples\nof their respective (conditioning) class.\nFor comparison, in Figure 5 we also depict the 25 ‘worst’\nand ‘best’ samples according to the uncertainty estimate\nfrom BayesDiff. It is evident that their uncertainty is less\ninformative for sample quality than ours. Moreover, their\nuncertainty measure appears to be very sensitive to the\nbackground pixels. Most images with the highest uncer-\ntainty have a ‘cluttered’ background, whereas most images\nwith the lowest uncertainty have a ‘clear’ background.\nWe attribute this issue to the fact that in BayesDiff the\nuncertainty is computed directly in the pixel space, unlike\nin our approach where we use the semantic likelihood\n(Section 3.3) to move away from the (high-dimensional)\n\n\nTable 1: Image generation results for 10K filtered samples (out of 12K). Our generative uncertainty outperforms previously\nproposed uncertainty-based approaches in terms of image quality (AU [De Vita and Belagiannis, 2025], BayesDiff [Kou\net al., 2024]), as indicated by higher FID and precision scores, and is competitive with non-uncertainty methods (Realism\n[Kynkäänniemi et al., 2019], Rarity [Han et al., 2023]). We report mean values along with standard deviations over 3 runs\nwith different random seeds.\nADM (DDIM), ImageNet 128×128\nUViT (DPM), ImageNet 256×256\nFID (↓)\nPrecision (↑)\nRecall (↑)\nFID (↓)\nPrecision (↑)\nRecall (↑)\nRandom\n11.31 ± 0.07\n58.90 ± 0.36\n70.68 ± 0.38\n9.46 ± 0.12\n60.94 ± 0.24\n73.82 ± 0.33\nBayesDiff\n11.20 ± 0.05\n58.80 ± 0.05\n70.62 ± 0.32\n9.16 ± 0.17\n61.77 ± 0.19\n73.72 ± 0.38\nAU\n11.39 ± 0.05\n58.82 ± 0.42\n70.70 ± 0.38\n9.20 ± 0.12\n61.80 ± 0.33\n73.46 ± 0.24\nOurs\n10.14 ± 0.08\n61.26 ± 0.26\n69.60 ± 0.49\n7.89 ± 0.12\n64.14 ± 0.17\n71.92 ± 0.35\nRealism\n9.76 ± 0.04\n67.95 ± 0.19\n66.32 ± 0.40\n8.24 ± 0.09\n70.29 ± 0.15\n69.12 ± 0.32\nRarity\n10.09 ± 0.02\n64.99 ± 0.16\n67.73 ± 0.47\n8.37 ± 0.11\n67.21 ± 0.10\n67.76 ± 0.48\nFigure 2: Images with highest (left) and lowest (right) generative uncertainty amongst 12K generations using a UViT\ndiffusion model [Bao et al., 2023]. Generative uncertainty correlates with visual quality: high-uncertainty samples exhibit\nnumerous artefacts, whereas low-uncertainty samples resemble canonical images of their respective conditioning class.\nFigure 3: Images with the highest (bottom) and the lowest (top) generative uncertainty among 128 generations using a UViT\ndiffusion model for 2 classes: black swan (left) and Tibetan terrier (right).\n\n\nsample space. To further verify the importance of the\nsemantic likelihood, in Figure 7 we perform an ablation\nwhere we compute our generative uncertainty directly in\nthe pixel-space. It is clear that without semantic likelihood,\nour uncertainty becomes overly sensitive to the background\npixels in the same way as in BayesDiff.\nReturning to Table 1, we observe that filtering based on\nour generative uncertainty results in some loss of sample\ndiversity, as evidenced by lower recall scores (e.g., 73.82 for\nRandom vs. 71.92 for our method on the UViT model). We\nattribute this to the fact that, in our main experiment, 12K\nimages are generated such that all 1000 ImageNet classes\nare represented.2 Since certain classes produce images\nwith higher uncertainty (see Appendix A.6 for a detailed\nanalysis), filtering based on uncertainty inevitably alters the\nclass distribution among the selected samples. Moreover,\nthe trade-off between improving sample quality (precision)\nand reducing diversity (recall) has been observed before,\nsee for example the literature on classifier-free guidance\n[Ho and Salimans, 2022].\nLastly, we compare our proposed method with non-\nuncertainty-based approaches—a comparison missing in\nprior literature [Kou et al., 2024, De Vita and Belagiannis,\n2025]. For realism, we retain the 10K images with the\nhighest scores, whereas for rarity, we keep those with\nthe lowest scores. As shown in Table 1, our generative\nuncertainty is the only uncertainty-based method that\napproaches realism and rarity in terms of FID (e.g., 7.89\nfor ours vs. 8.24 for realism and 8.37 for rarity on UViT).\nHowever, a large gap remains in precision (e.g., 64.14 for\nours vs. 70.29 for realism and 67.21 for rarity on UViT).\nNotably, realism and rarity sacrifice the most sample\ndiversity, as indicated by their lowest recall scores (e.g.,\n69.12 for realism and 67.76 for rarity on UViT).\nFurthermore, Table 2 shows that our score can be effectively\ncombined with realism or rarity scores. Specifically, com-\nbining our score with realism yields an FID of 7.60 on UViT,\ncompared to 8.26 when combining realism and rarity. We at-\ntribute higher benefits from ensembling our score to the fact\nthat, while realism and rarity exhibit a strong negative Spear-\nman correlation (-0.85), our uncertainty measure is less cor-\nrelated with them (-0.27 with realism, 0.38 with rarity), as\nshown in Figure 10. Taken together, these results indicate\nthat our uncertainty score captures (somewhat) different de-\nsirable properties of images compared to realism and rarity.\n4.2\nIMPROVING SAMPLING EFFICIENCY\nWe next examine the sampling costs associated with\nBayesian inference in diffusion sampling. As shown in\n2Following Kou et al. [2024], we use class-conditional dif-\nfusion models but randomly sample a class for each of the 12K\ngenerated samples.\nFigure 4: FID results for 10K ImageNet-filtered images us-\ning our generative uncertainty on ADM model [Dhariwal\nand Nichol, 2021]. We vary the number of Monte Carlo sam-\nples M and diffusion sampling steps T (see Algorithm 1).\nBy default, we use M=5 with T=50 ( ), incurring an addi-\ntional 250 NFEs for uncertainty estimation. Encouragingly,\nsetting M=1 and T=25 ( ) still achieves competitive perfor-\nmance while reducing the sampling overhead by 10x. Lower\nleft is best: better FID and greater computational efficiency.\nAlgorithm 1, obtaining an uncertainty estimate u(z) for\na generated sample ˆx0 = gθ(z) requires generating M\nadditional samples, resulting in MT additional network\nfunction evaluations (NFEs). For the results presented in\nTable 1, we use M = 5 and the default number of sampling\nsteps T = 50 ( ), leading to an additional 250 NFEs for\nuncertainty estimation—on top of the 50 NFEs required\nto generate the original sample. Since this overhead may\nbe prohibitively expensive in certain deployment scenarios,\nwe next explore strategies to reduce the sampling cost\nassociated with our generative uncertainty.\nThe most straightforward approach is to reduce the number\nof Monte Carlo samples M. Encouragingly, reducing M\nto as few as 1 still achieves highly competitive performance\n(see Figure 4). Further efficiency gains can be achieved\nby reducing the number of sampling steps T, leveraging\nthe flexibility of diffusion models to adjust T on the fly.\nImportantly, we lower T only for the additional M samples\nused for uncertainty assessment while keeping the default\nT for the original sample ˆx0 to ensure that the generation\nquality is not compromised. Taken together, reducing M\nand T significantly improves the efficiency of our generative\nuncertainty. Using the ADM model [Dhariwal and Nichol,\n2021], our generative uncertainty method with M = 1 and\nT = 25 ( ) achieves an FID of 10.36, which still strongly\noutperforms both the Random (11.31) and BayesDiff\n(11.20) baselines while requiring only 25 additional NFEs.\n5\nRELATED WORK\nUncertainty quantification in diffusion models has\nrecently gained significant attention. Most related to our\n\n\nwork are BayesDiff [Kou et al., 2024], which uses a Laplace\napproximation to track epistemic uncertainty throughout\nthe sampling process, and De Vita and Belagiannis [2025],\nwhich captures aleatoric uncertainty via the sensitivity\nof diffusion score estimates. Our work extends both by\nintroducing an uncertainty framework that is more general\n(applicable beyond diffusion), simpler (requiring no sam-\npling modifications), and more effective (see Section 4.1).\nAlso related is DECU [Berry et al., 2024], which employs\nan efficient variant of deep ensembles [Lakshminarayanan\net al., 2017] to capture the epistemic uncertainty of condi-\ntional diffusion models. However, DECU does not consider\nusing uncertainty to detect poor-quality generations, as its\nframework provides uncertainty estimates at the level of the\nconditioning variable, whereas ours estimates uncertainty\nat the level of initial random noise. Similarly, in Chan\net al. [2024] the use of hyper-ensembles is proposed to\ncapture epistemic uncertainty in diffusion models for\ninverse problems such as super-resolution, but, as in DECU,\ntheir approach does not provide uncertainty estimates\nin unconditional settings or in conditional settings with\nlow-dimensional conditioning (such as class-conditional\ngeneration). Moreover, both DECU [Berry et al., 2024]\nand Chan et al. [2024] require modifying and retraining\ndiffusion model components, whereas our approach oper-\nates post-hoc with any pretrained diffusion model via the\nLaplace approximation [Daxberger et al., 2021a]. A recent\napproach, PUNC [Franchi et al., 2024], focuses only on\ntext-to-image models. The uncertainty of image generation\nwith respect to text conditioning is measured through the\nalignment between a caption generated from a generated\nimage and the original prompt used to generate said image.\nAdditionally, a large body of work explores conformal\nprediction for uncertainty quantification in diffusion models\n[Angelopoulos et al., 2022, Sankaranarayanan et al., 2022,\nTeneggi et al., 2023, Belhasin et al., 2023]. However, these\napproaches are primarily designed for inverse problems\n(e.g., deblurring), and cannot be directly applied to detect\nlow-quality samples in unconditional generation.\nBayesian inference in generative models has been\nexplored previously outside the domain of diffusion\nmodels. Prominent examples include Saatci and Wilson\n[2017] where a Bayesian version of a GAN is proposed,\nshowing improvements for semi-supervised learning,\nand Daxberger and Hernández-Lobato [2019], where a\nBayesian VAE [Tran et al., 2023] is shown to provide\nmore informative likelihood estimates for the unsupervised\nout-of-distribution detection compared to the non-Bayesian\ncounterparts [Nalisnick et al., 2018]. Since diffusion models\ncan be interpreted as neural ODEs [Song et al., 2020b],\nanother relevant work is Ott et al. [2023], which employs a\nLaplace approximation to quantify uncertainty when solving\nneural ODEs [Chen et al., 2018]. However, Ott et al. [2023]\nfocuses solely on low-dimensional regression problems.\nNon-uncertainty based approaches for filtering out\npoor generations include the realism [Kynkäänniemi et al.,\n2019], rarity [Han et al., 2023], and anomaly scores [Hwang\net al., 2024]. Our work is the first to establish a connection\nbetween these scores and uncertainty-based methods,\nwhich we hope will inspire the development of even better\nsample-level metrics in the future. Additionally, a large\nbody of work focuses on specially designed sample-quality\nscoring models [Gu et al., 2020, Zhao et al., 2024] or,\nalternatively, on leveraging large pretrained vision-language\nmodels (VLMs) [Zhang et al., 2024] for scoring generated\nimages. However, these approaches require either access to\nsample-quality labels or rely on (expensive) external VLMs.\nIn contrast, our uncertainty-based method requires neither,\nmaking it a more accessible and scalable alternative.\n6\nLIMITATIONS\nWhile we have demonstrated in Section 4 that semantic like-\nlihood is essential for addressing the over-sensitivity of prior\nwork to background pixels [Kou et al., 2024], our reliance\non a pretrained image encoder like CLIP [Radford et al.,\n2021] limits the applicability of our diffusion uncertainty\nframework to natural images. Removing the dependence on\nsuch encoders would unlock the application our Bayesian\nframework to other modalities where diffusion models are\nused, such as molecules [Hoogeboom et al., 2022, Cornet\net al., 2024] or text [Gong et al., 2022, Yi et al., 2024]. Ex-\nploring whether insights from the literature on uncovering\nsemantic features in diffusion models [Kwon et al., 2022,\nLuo et al., 2024, Namekata et al., 2024] could help achieve\nthis represents a promising direction for future work.\nMoreover, the large size of modern diffusion models neces-\nsitates the use of cheap and scalable Bayesian approximate\ninference techniques, such as the (diagonal) last-layer\nLaplace approximation employed in our work (following\n[Kou et al., 2024]). A more comprehensive comparison of\navailable approximate inference methods could be valuable,\nas improving the quality of the posterior approximation\nmay further enhance the detection of low-quality samples\nbased on Bayesian generative uncertainty.\n7\nCONCLUSION\nWe introduced generative uncertainty and demonstrated how\nto estimate it in modern generative models such as diffusion.\nOur experiments showed the effectiveness of generative\nuncertainty in filtering out low-quality samples. For future\nwork, it would be interesting to explore broader applications\nof Bayesian principles in generative modeling beyond\ndetecting poor-quality generations. Promising directions\ninclude guiding synthetic data generation, detecting memo-\nrized samples, and optimizing diffusion hyperparameters via\nthe marginal likelihood using the Laplace approximation.\n\n\nAcknowledgements\nThis project was generously supported by the Bosch Center\nfor Artificial Intelligence. Eric Nalisnick did not utilize\nresources from Johns Hopkins University for this project.\nStephan Mandt acknowledges funding from the National\nScience Foundation (NSF) through an NSF CAREER\nAward IIS-2047418, IIS-2007719, the NSF LEAP Center,\nthe IARPA WRIVA program, and the Hasso Plattner\nResearch Center at UCI.\nReferences\nMichael S Albergo, Nicholas M Boffi, and Eric Vanden-\nEijnden.\nStochastic interpolants: A unifying frame-\nwork for flows and diffusions.\narXiv preprint\narXiv:2303.08797, 2023.\nAnastasios N Angelopoulos, Amit Pal Kohli, Stephen\nBates, Michael Jordan, Jitendra Malik, Thayer Alshaabi,\nSrigokul Upadhyayula, and Yaniv Romano.\nImage-\nto-image regression with distribution-free uncertainty\nquantification and applications in imaging. In Interna-\ntional Conference on Machine Learning, pages 717–730.\nPMLR, 2022.\nJavier Antorán, James Allingham, and José Miguel\nHernández-Lobato. Depth uncertainty in neural networks.\nAdvances in neural information processing systems, 33:\n10620–10634, 2020.\nJulyan Arbel, Konstantinos Pitas, Mariia Vladimirova, and\nVincent Fortuin. A primer on bayesian neural networks:\nreview and debates. arXiv preprint arXiv:2309.16314,\n2023.\nFan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan\nLi, Hang Su, and Jun Zhu. All are worth words: A vit\nbackbone for diffusion models. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern\nrecognition, pages 22669–22679, 2023.\nOmer Belhasin, Yaniv Romano, Daniel Freedman, Ehud\nRivlin, and Michael Elad. Principal uncertainty quan-\ntification with spatial correlation for image restoration\nproblems. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 2023.\nLucas Berry and David Meger.\nEscaping the sample\ntrap: Fast and accurate epistemic uncertainty estimation\nwith pairwise-distance estimators.\narXiv preprint\narXiv:2308.13498, 2023.\nLucas Berry, Axel Brando, and David Meger. Shedding\nlight on large generative networks: Estimating epistemic\nuncertainty in diffusion models. In The 40th Conference\non Uncertainty in Artificial Intelligence, 2024.\nCharles Blundell, Julien Cornebise, Koray Kavukcuoglu,\nand Daan Wierstra. Weight uncertainty in neural network.\nIn International conference on machine learning, pages\n1613–1622. PMLR, 2015.\nWray L. Buntine and Andreas S. Weigend.\nBayesian\nback-propagation. Complex Syst., 5, 1991. URL https:\n//api.semanticscholar.org/CorpusID:\n14814125.\nMatthew Albert Chan, Maria J Molina, and Christopher Met-\nzler. Estimating epistemic and aleatoric uncertainty with\na single model. In The Thirty-eighth Annual Conference\non Neural Information Processing Systems, 2024.\nRicky TQ Chen, Yulia Rubanova, Jesse Bettencourt,\nand David K Duvenaud. Neural ordinary differential\nequations. Advances in neural information processing\nsystems, 31, 2018.\nFrançois Cornet, Grigory Bartosh, Mikkel N Schmidt, and\nChristian A Naesseth. Equivariant neural diffusion for\nmolecule generation.\nIn 38th Conference on Neural\nInformation Processing Systems, 2024.\nQuan Dao, Hao Phung, Binh Nguyen, and Anh Tran.\nFlow matching in latent space.\narXiv preprint\narXiv:2307.08698, 2023.\nErik Daxberger and José Miguel Hernández-Lobato.\nBayesian\nvariational\nautoencoders\nfor\nunsuper-\nvised out-of-distribution detection.\narXiv preprint\narXiv:1912.05651, 2019.\nErik Daxberger, Agustinus Kristiadi, Alexander Immer,\nRuna Eschenhagen, Matthias Bauer, and Philipp Hen-\nnig. Laplace redux-effortless bayesian deep learning.\nAdvances in Neural Information Processing Systems, 34:\n20089–20103, 2021a.\nErik Daxberger, Eric Nalisnick, James U Allingham, Javier\nAntorán, and José Miguel Hernández-Lobato. Bayesian\ndeep learning via subnetwork inference. In International\nConference on Machine Learning, pages 2510–2521.\nPMLR, 2021b.\nMichele De Vita and Vasileios Belagiannis.\nDiffusion\nmodel guided sampling with pixel-wise aleatoric\nuncertainty estimation. IEEE/CVF Winter Conference\non Applications of Computer Vision (WACV), 2025.\nPrafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis.\nAdvances in neural\ninformation processing systems, 34:8780–8794, 2021.\nPatrick Esser, Sumith Kulal, Andreas Blattmann, Rahim\nEntezari, Jonas Müller, Harry Saini, Yam Levi, Dominik\nLorenz, Axel Sauer, Frederic Boesel, Dustin Podell,\nTim Dockhorn, Zion English, and Robin Rombach.\n\n\nScaling rectified flow transformers for high-resolution\nimage synthesis.\nIn Forty-first International Con-\nference on Machine Learning, 2024.\nURL https:\n//openreview.net/forum?id=FPnUhsQJ5B.\nGianni\nFranchi,\nDat\nNguyen\nTrong,\nNacim\nBelkhir, Guoxuan Xia, and Andrea Pilzer.\nTo-\nwards\nunderstanding\nand\nquantifying\nuncer-\ntainty for text-to-image generation, 2024.\nURL\nhttps://arxiv.org/abs/2412.03178.\nYarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep\nbayesian active learning with image data. In International\nconference on machine learning, pages 1183–1192.\nPMLR, 2017.\nYarin Gal et al. Uncertainty in deep learning. 2016.\nShansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu,\nand LingPeng Kong. Diffuseq: Sequence to sequence\ntext generation with diffusion models. arXiv preprint\narXiv:2210.08933, 2022.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial\nnets. Advances in neural information processing systems,\n27, 2014.\nShuyang Gu, Jianmin Bao, Dong Chen, and Fang Wen.\nGiqa: Generated image quality assessment. In Computer\nVision–ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23–28, 2020, Proceedings, Part XI 16, pages\n369–385. Springer, 2020.\nJiyeon Han, Hwanil Choi, Yunjey Choi, Junho Kim,\nJung-Woo Ha, and Jaesik Choi. Rarity score : A new\nmetric to evaluate the uncommonness of synthesized\nimages. In The Eleventh International Conference on\nLearning Representations (ICLR), 2023.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained\nby a two time-scale update rule converge to a local nash\nequilibrium. Advances in neural information processing\nsystems, 30, 2017.\nJonathan Ho and Tim Salimans. Classifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022.\nJonathan Ho, Ajay Jain, and Pieter Abbeel.\nDenoising\ndiffusion probabilistic models.\nAdvances in neural\ninformation processing systems, 33:6840–6851, 2020.\nEmiel Hoogeboom, Vıctor Garcia Satorras, Clément\nVignac, and Max Welling.\nEquivariant diffusion for\nmolecule generation in 3d. In International conference\non machine learning, pages 8867–8887. PMLR, 2022.\nEyke Hüllermeier and Willem Waegeman.\nAleatoric\nand epistemic uncertainty in machine learning: An\nintroduction to concepts and methods. Machine learning,\n110(3):457–506, 2021.\nJaehui Hwang, Junghyuk Lee, and Jong-Seok Lee. Anomaly\nscore: Evaluating generative models and individual gen-\nerated images based on complexity and vulnerability. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 8754–8763, 2024.\nAlexander Immer, Matthias Bauer, Vincent Fortuin, Gunnar\nRätsch, and Khan Mohammad Emtiyaz.\nScalable\nmarginal likelihood estimation for model selection in\ndeep learning. In International Conference on Machine\nLearning, pages 4563–4573. PMLR, 2021.\nLaurent Valentin Jospin, Hamid Laga, Farid Boussaid,\nWray Buntine, and Mohammed Bennamoun. Hands-on\nbayesian neural networks—a tutorial for deep learning\nusers. IEEE Computational Intelligence Magazine, 17\n(2):29–48, 2022.\nTero Karras, Miika Aittala, Timo Aila, and Samuli\nLaine. Elucidating the design space of diffusion-based\ngenerative models.\nAdvances in neural information\nprocessing systems, 35:26565–26577, 2022.\nAlex Kendall and Yarin Gal. What uncertainties do we need\nin bayesian deep learning for computer vision? Advances\nin neural information processing systems, 30, 2017.\nDiederik P Kingma.\nAuto-encoding variational bayes.\narXiv preprint arXiv:1312.6114, 2013.\nSiqi Kou, Lei Gan, Dequan Wang, Chongxuan Li, and Zhijie\nDeng.\nBayesdiff: Estimating pixel-wise uncertainty\nin diffusion via bayesian inference.\nIn The Twelfth\nInternational Conference on Learning Representations\n(ICLR), 2024.\nAgustinus Kristiadi, Matthias Hein, and Philipp Hennig.\nBeing bayesian, even just a bit, fixes overconfidence in\nrelu networks. In International conference on machine\nlearning, pages 5436–5446. PMLR, 2020.\nMingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusion\nmodels already have a semantic latent space.\narXiv\npreprint arXiv:2210.10960, 2022.\nTuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko\nLehtinen, and Timo Aila. Improved precision and recall\nmetric for assessing generative models.\nAdvances in\nneural information processing systems, 32, 2019.\nBalaji Lakshminarayanan, Alexander Pritzel, and Charles\nBlundell.\nSimple and scalable predictive uncertainty\nestimation using deep ensembles. Advances in neural\ninformation processing systems, 30, 2017.\n\n\nZehui Li, Yuhao Ni, Guoxuan Xia, William Beardall,\nAkashaditya Das, Guy-Bart Stan, and Yiren Zhao.\nAbsorb & escape: Overcoming single model limitations\nin generating heterogeneous genomic sequences.\nIn\nThe Thirty-eighth Annual Conference on Neural In-\nformation Processing Systems, 2024.\nURL https:\n//openreview.net/forum?id=XHTl2k1LYk.\nYaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maxim-\nilian Nickel, and Matt Le. Flow matching for generative\nmodeling. arXiv preprint arXiv:2210.02747, 2022.\nXingchao Liu, Chengyue Gong, and Qiang Liu.\nFlow\nstraight and fast: Learning to generate and transfer data\nwith rectified flow. arXiv preprint arXiv:2209.03003,\n2022.\nCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan\nLi, and Jun Zhu.\nDpm-solver: A fast ode solver for\ndiffusion probabilistic model sampling in around 10 steps.\nAdvances in Neural Information Processing Systems, 35:\n5775–5787, 2022.\nGrace Luo, Lisa Dunlap, Dong Huk Park, Aleksander\nHolynski, and Trevor Darrell. Diffusion hyperfeatures:\nSearching through time and space for semantic corre-\nspondence. Advances in Neural Information Processing\nSystems, 36, 2024.\nDavid JC MacKay.\nBayesian interpolation.\nNeural\ncomputation, 4(3):415–447, 1992a.\nDavid JC MacKay. Information-based objective functions\nfor active data selection.\nNeural computation, 4(4):\n590–604, 1992b.\nWesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P\nVetrov, and Andrew Gordon Wilson. A simple baseline\nfor bayesian uncertainty in deep learning. Advances in\nneural information processing systems, 32, 2019.\nStephan Mandt, Matthew D Hoffman, David M Blei, et al.\nStochastic gradient descent as approximate bayesian\ninference. Journal of Machine Learning Research, 18\n(134):1–35, 2017.\nKevin P Murphy.\nProbabilistic machine learning: an\nintroduction. MIT press, 2022.\nEric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan\nGorur, and Balaji Lakshminarayanan. Do deep generative\nmodels know what they don’t know?\narXiv preprint\narXiv:1810.09136, 2018.\nKoichi Namekata, Amirmojtaba Sabour, Sanja Fidler, and\nSeung Wook Kim.\nEmerdiff: Emerging pixel-level\nsemantic knowledge in diffusion models. arXiv preprint\narXiv:2401.11739, 2024.\nRadford M Neal. Bayesian learning for neural networks,\nvolume 118. Springer Science & Business Media, 1995.\nPeter Nickl, Lu Xu, Dharmesh Tailor, Thomas Möllenhoff,\nand Mohammad Emtiyaz E Khan.\nThe memory-\nperturbation equation: Understanding model’s sensitivity\nto data.\nAdvances in Neural Information Processing\nSystems, 36, 2024.\nKatharina Ott, Michael Tiemann, and Philipp Hennig.\nUncertainty and structure in neural ordinary differential\nequations. arXiv preprint arXiv:2305.13290, 2023.\nWilliam Peebles and Saining Xie. Scalable diffusion models\nwith transformers.\nIn Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages\n4195–4205, 2023.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al.\nLearning transferable visual models from natural\nlanguage supervision. In International conference on\nmachine learning, pages 8748–8763. PMLR, 2021.\nHippolyt Ritter, Aleksandar Botev, and David Barber. A\nscalable laplace approximation for neural networks. In\n6th international conference on learning representations,\nICLR 2018-conference track proceedings, volume 6. Inter-\nnational Conference on Representation Learning, 2018.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Björn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pages 10684–10695, June\n2022.\nYunus Saatci and Andrew G Wilson.\nBayesian gan.\nAdvances in neural information processing systems, 30,\n2017.\nMehdi SM Sajjadi, Olivier Bachem, Mario Lucic, Olivier\nBousquet, and Sylvain Gelly.\nAssessing generative\nmodels via precision and recall.\nAdvances in neural\ninformation processing systems, 31, 2018.\nSwami\nSankaranarayanan,\nAnastasios\nAngelopoulos,\nStephen Bates, Yaniv Romano, and Phillip Isola.\nSemantic uncertainty intervals for disentangled latent\nspaces. In NeurIPS, 2022.\nMrinank Sharma, Sebastian Farquhar, Eric Nalisnick, and\nTom Rainforth. Do bayesian neural networks need to\nbe fully stochastic?\nIn International Conference on\nArtificial Intelligence and Statistics, pages 7694–7722.\nPMLR, 2023.\n\n\nZhenming Shun and Peter McCullagh. Laplace approxima-\ntion of high dimensional integrals. Journal of the Royal\nStatistical Society Series B: Statistical Methodology, 57\n(4):749–760, 1995.\nFreddie Bickford Smith, Jannik Kossen, Eleanor Trollope,\nMark van der Wilk, Adam Foster, and Tom Rainforth.\nRethinking aleatoric and epistemic uncertainty. arXiv\npreprint arXiv:2412.20892, 2024.\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli. Deep unsupervised learning using\nnonequilibrium thermodynamics.\nIn International\nconference on machine learning, pages 2256–2265.\nPMLR, 2015.\nJiaming Song, Chenlin Meng, and Stefano Ermon. De-\nnoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020a.\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma,\nAbhishek Kumar, Stefano Ermon, and Ben Poole. Score-\nbased generative modeling through stochastic differential\nequations. arXiv preprint arXiv:2011.13456, 2020b.\nC Szegedy. Intriguing properties of neural networks. arXiv\npreprint arXiv:1312.6199, 2013.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\nShlens, and Zbigniew Wojna. Rethinking the inception\narchitecture for computer vision.\nIn Proceedings of\nthe IEEE conference on computer vision and pattern\nrecognition, pages 2818–2826, 2016.\nJacopo Teneggi, Matthew Tivnan, Web Stayman, and\nJeremias Sulam. How to trust your diffusion model: A\nconvex optimization approach to conformal risk control.\nIn International Conference on Machine Learning, pages\n33940–33960. PMLR, 2023.\nLucas Theis.\nWhat makes an image realistic?\narXiv\npreprint arXiv:2403.04493, 2024.\nLucas Theis, Aäron van den Oord, and Matthias Bethge. A\nnote on the evaluation of generative models, 2016. URL\nhttps://arxiv.org/abs/1511.01844.\nJakub M. Tomczak. Deep Generative Modeling. Springer,\nGermany, February 2022. ISBN 978-3-030-93157-5. doi:\n10.1007/978-3-030-93158-2.\nBa-Hien Tran, Babak Shahbaba, Stephan Mandt, and Mau-\nrizio Filippone. Fully bayesian autoencoders with latent\nsparse gaussian processes. In International Conference\non Machine Learning, pages 34409–34430. PMLR, 2023.\nJoost Van Amersfoort, Lewis Smith, Yee Whye Teh, and\nYarin Gal. Uncertainty estimation using a single deep\ndeterministic neural network. In International conference\non machine learning, pages 9690–9700. PMLR, 2020.\nAndrew Gordon Wilson.\nThe case for bayesian deep\nlearning. arXiv preprint arXiv:2001.10995, 2020.\nQiuhua Yi, Xiangfan Chen, Chenwei Zhang, Zehai Zhou,\nLinan Zhu, and Xiangjie Kong. Diffusion models in text\ngeneration: a survey. PeerJ Computer Science, 10:e1905,\n2024.\nCheng Zhang, Judith Bütepage, Hedvig Kjellström, and\nStephan Mandt.\nAdvances in variational inference.\nIEEE transactions on pattern analysis and machine\nintelligence, 41(8):2008–2026, 2018.\nZicheng Zhang, Haoning Wu, Chunyi Li, Yingjie Zhou, Wei\nSun, Xiongkuo Min, Zijian Chen, Xiaohong Liu, Weisi\nLin, and Guangtao Zhai. A-bench: Are lmms masters\nat evaluating ai-generated images?\narXiv preprint\narXiv:2406.03070, 2024.\nGanning Zhao, Vasileios Magoulianitis, Suya You, and\nC-C Jay Kuo. A lightweight generalizable evaluation\nand enhancement framework for generative models and\ngenerated samples. In Proceedings of the IEEE/CVF\nWinter Conference on Applications of Computer Vision,\npages 450–459, 2024.\n\n\nAPPENDIX\nThe supplementary material is organized as follows:\n• In Appendix A.1, we qualitatively compare our method with BayesDiff [Kou et al., 2024].\n• In Appendix A.2, we perform ablations on our semantic likelihood (Section 3.3).\n• In Appendix A.3, we demonstrate how to use our generative uncertainty for pixel-wise uncertainty.\n• In Appendix A.4, we show that diffusion’s own likelihood is not useful for filtering out poor samples.\n• In Appendix A.5, we further compare our generative uncertainty to realism [Kynkäänniemi et al., 2019] and rarity\n[Han et al., 2023] scores.\n• In Appendix A.6, we investigate the drop in sample diversity by looking at the average generative uncertainty per\nconditioning class.\n• In Appendix A.7, we apply our generative uncertainty to detect low-quality samples in a latent flow matching model\n[Dao et al., 2023].\n• In Appendix B, we provide implementation and experimental details.\nA\nADDITIONAL RESULTS\nA.1\nQUALITATIVE COMPARISON WITH BAYESDIFF\nTo further highlight the differences between our generative uncertainty and BayesDiff [Kou et al., 2024], we present samples\nwith the highest and lowest uncertainty according to BayesDiff in Figure 5. These samples are drawn from the same set of 12K\nImageNet ‘unconditional’ images generated using the UViT model [Bao et al., 2023] as in Figure 2. Notably, BayesDiff’s\nuncertainty score appears highly sensitive to background pixels—images with high uncertainty tend to have cluttered\nbackgrounds, while those with low uncertainty typically feature clear backgrounds. Furthermore, as reflected in BayesDiff’s\npoor performance in terms of FID and precision (see Table 1), some low-uncertainty examples exhibit noticeable artefacts,\nwhereas certain high-uncertainty samples are of rather high-quality. For example, the image of a dog in the bottom-right\ncorner of the high-uncertainty grid in Figure 5 looks quite good despite being assigned (very) high uncertainty.\nSimilarly, in Figure 6, we show low- and high-uncertainty samples according to BayesDiff for the same set of 128 images\nper class as in Figure 3. Once again, we observe that BayesDiff’s uncertainty metric is less informative regarding a sample’s\nvisual quality compared to our generative uncertainty.\nA.2\nABLATION ON SEMANTIC LIKELIHOOD\nTo highlight the importance of using a semantic likelihood (Section 3.3) when leveraging uncertainty to detect low-quality\ngenerations, we conduct an ablation study in which we replace it with a standard Gaussian likelihood applied directly in\npixel space (Eq. 6). Figure 7 presents the highest and lowest uncertainty images according to this ‘pixel-space’ generative\nuncertainty. Notably, pixel-space uncertainty is overly sensitive to background pixels, mirroring the issue observed in\nBayesDiff (see Appendix A.1). This highlights the necessity of using semantic likelihood to obtain uncertainty estimates\nthat are truly informative about the visual quality of generated samples.\nA.3\nPIXEL-WISE UNCERTAINTY\nWhile not the primary focus of our work, we demonstrate how our generative uncertainty framework (Algorithm 1) can be\nadapted to obtain pixel-wise uncertainty estimates. This is achieved by replacing our proposed semantic likelihood (Eq. 7)\nwith a standard ‘pixel-space’ likelihood (Eq. 6). Figure 8 illustrates pixel-wise uncertainty estimates for 5 generated samples.\nAlthough pixel-wise uncertainty received significant attention in past work [Kou et al., 2024, Chan et al., 2024, De Vita and\nBelagiannis, 2025], there is currently no principled method for evaluating its quality. Most existing approaches rely on quali-\ntative inspection, visualizing pixel-wise uncertainty for a few generated samples (as we do in Figure 8). This further motivates\nour focus on sample-wise uncertainty estimates, where more rigorous evaluation frameworks—such as improvements in FID\nand precision on a set of filtered images (see Table 1)—enable more meaningful comparisons between different approaches.\n\n\nFigure 5: Images with the highest (left) and the lowest (right) BayesDiff uncertainty among 12K generations using a UViT\ndiffusion model [Bao et al., 2023]. BayesDiff uncertainty correlates poorly with visual quality and is overly sensitive to the\nbackground pixels. Same set of 12K generated images is used as in Figure 2 to ensure a fair comparison.\nFigure 6: Images with the highest (bottom) and the lowest (top) BayesDiff uncertainty among 128 generations using a UViT\ndiffusion model for 2 classes: black swan (left) and Tibetan terrier (right). Same set of 128 generated images\nper class is used as in Figure 3 to ensure a fair comparison.\nA.4\nCOMPARISON WITH LIKELIHOOD\nWe compare our generative uncertainty filtering criterion with a likelihood selection approach on the 12K images generated\nby ADM trained on ImageNet 128x128. In the same way as in our other comparisons, we retain the 10K generated images\nwith highest likelihood. We utilize the implementation in Dhariwal and Nichol [2021] to compute the bits-per-dimension\nof each sample (one-to-one with likelihood). The 25 samples with lowest and highest likelihood are shown in Figure 9.\nVisually, the likelihood objective heavily prefers simple images with clean backgrounds and not necessarily image quality.\nNote that this is consistent with other works that have reported likelihood to be an inconsistent identifier of image quality\n[Theis et al., 2016, Theis, 2024]. Quantitative results for image quality were consistent with our qualitative observations. The\nFID, precision, and recall for the best 10K images according to bits-per-dimension were 11.86 ± 0.0026, 58.23 ± 0.02160,\nand 70.45 ± 0.0237 over three runs. By point estimate, all three metrics are worse or indistinguishable from the Random\nbaseline (11.31 ± 0.07, 58.90 ± 0.36, 70.68 ± 0.38). Results for all image selection methods can be found in Table 1.\n\n\nFigure 7: Images with the highest (left) and the lowest (right) ‘pixel-space’ generative uncertainty among 12K generations\nusing a UViT diffusion model. Pixel-space uncertainty correlates poorly with visual quality and is overly sensitive to the\nbackground pixels. Same set of 12K generated images is used as in Figure 2 to ensure a fair comparison.\nFigure 8: Pixel-wise uncertainty based on our generative uncertainty for 5 generated samples using UViT diffusion.\nA.5\nCOMPARISON WITH REALISM & RARITY\nTo better understand the relationship between our generative uncertainty and non-uncertainty-based approaches such as\nrealism [Kynkäänniemi et al., 2019] and rarity [Han et al., 2023] scores, we compute the Spearman correlation coefficient\nbetween different sample-level metrics on a set of 12K generated images from the experiment in Section 4.1. As shown\nin Figure 10, realism and rarity scores exhibit a strong correlation (< −0.8). This is unsurprising, as both scores are derived\nfrom the distance of a generated sample to a data manifold obtained using a reference dataset (e.g., a subset of training\ndata or a separate validation dataset).3\nIn contrast, our generative uncertainty exhibits a weaker correlation (< 0.4) with both realism and rarity scores. We attribute\nthis to the fact that our uncertainty primarily reflects the limited training data used in training diffusion models (i.e., epistemic\nuncertainty, see Section 3.4), rather than the distance to a reference dataset, as is the case for realism and rarity scores.\nNext, we investigate whether combining different scores can improve the detection of low-quality generations. When\n3Such distance-based approaches are also commonly used to estimate prediction’s quality in predictive models; see, for example,\nVan Amersfoort et al. [2020].\n\n\nFigure 9: The 25 ‘worst’ (left) and ‘best’ (right) samples generated by ADM trained on ImageNet 128x128 selected by\nlowest and highest likelihood among 12K generations.\ncombining two scores, we first rank the 12K images based on each score individually, then compute the combined ranking\nby summing the two rankings and re-ranking accordingly. The results, shown in Table 2, indicate that combining realism and\nrarity leads to minor or no improvements in FID (9.81 compared to 9.76 for realism alone on ADM. However, combining our\ngenerative uncertainty with either realism or rarity achieves the best FID performance (9.54 on ADM). These results suggest\nthat ensembling scores that capture different aspects of generated sample quality is a promising direction for future research.\nFigure 10: Spearman correlation coefficient between different sample quality metrics for 12K ImageNet images generated\nusing ADM (left) and UViT (right).\nA.6\nCLASS-AVERAGED GENERATIVE UNCERTAINTY\nTo better understand the drop in sample diversity (recall) when using our generative uncertainty to filter low-quality\nsamples in Table 1, we analyze the distribution of average entropy per conditioning class. Specifically, for each of the\n12K generated images, we randomly sample a conditioning class to mimic unconditional generation. As a result, all 1,000\nImageNet classes are represented among the 12K generated samples. Next, we compute our generative uncertainty for each\nsample and then average the uncertainties within each class. A plot of class-averaged uncertainties is shown in Figure 11.\nSince class-averaged uncertainties exhibit considerable variance, the class distribution in the 10K filtered samples deviates\nsomewhat from that of the original 12K images, thereby explaining the reduction in diversity (recall).\n\n\nTable 2: Image generation results for 10K filtered samples (out of 12K) based on combined metrics. Combining our\ngenerative uncertainty outperforms combining realism and recall in terms of FID. We report mean values along with standard\ndeviation over 3 runs with different random seeds.\nADM (DDIM), ImageNet 128×128\nUViT (DPM), ImageNet 256×256\nFID (↓)\nPrecision (↑)\nRecall (↑)\nFID (↓)\nPrecision (↑)\nRecall (↑)\nRealism + Rarity\n9.81 ± 0.06\n67.06 ± 0.29\n66.73 ± 0.37\n8.26 ± 0.07\n69.01 ± 0.33\n69.86 ± 0.36\nOurs+ Realism\n9.54 ± 0.04\n66.41 ± 0.15\n67.04 ± 0.47\n7.60 ± 0.10\n68.33 ± 0.09\n69.75 ± 0.42\nOurs + Rarity\n9.56 ± 0.06\n65.44 ± 0.26\n67.36 ± 0.54\n7.56 ± 0.12\n67.48 ± 0.18\n70.18 ± 0.40\nFigure 11: A histogram of class-averaged generative uncer-\ntainties for 12K generated samples using UViT.\nWhile our primary focus in this work is on providing\nper-sample uncertainty estimates u(z), we can also ob-\ntain uncertainty estimates for the conditioning variable\nu(y) (e.g., a class label), by averaging over all samples\ncorresponding to a particular y ∈Y as done in Figure 11.\nThese estimates resemble the epistemic uncertainty scores\nproposed in DECU [Berry et al., 2024] and could be used\nto identify conditioning variables for which generated\nsamples are likely to be of poor quality. We leave fur-\nther exploration of generative uncertainty at the level of\nconditioning variables for future work.\nA.7\nFLOW MATCHING\nTo demonstrate that our generative uncertainty framework\n(Section 3) extends beyond diffusion models, we apply\nit here to the recently popularized flow matching approach [Lipman et al., 2022, Liu et al., 2022, Albergo et al., 2023].\nSpecifically, we consider a latent flow matching formulation [Dao et al., 2023] with a DiT backbone [Peebles and Xie, 2023].\nFor sampling, we employ a fifth-order Runge-Kutta ODE solver (dopri5). In Figure 12, we illustrate the samples with the\nhighest and lowest generative uncertainty among 12K generated samples. On a filtered set of 10K images, our generative\nuncertainty framework achieves an FID of 10.48 and a precision of 64.71, significantly outperforming a random baseline,\nwhich yields an FID of 11.80 and a precision of 61.04.\nB\nIMPLEMENTATION DETAILS\nAll our experiments can be conducted on a single A100 GPU, including the fitting of the Laplace posterior (Section 3.2).\nCode for reproducing our experiments is publicly available at https://github.com/metodj/DIFF-UQ.\nAll Params.\nLL Params.\nLL Name\nADM\n∼421 × 106\n∼14 × 103\nout.2\nUViT\n∼500 × 106\n∼18 × 103\ndecoder_pred\nDiT\n∼131 × 106\n∼1.2 × 106\nfinal_layer\nTable 3: Details of our last-layer (LL) Laplace approximation.\nThe first column presents the total number of model parame-\nters, while the second and third columns indicate the number\nof parameters in the last layer and its name, respectively\nLaplace Approximation\nWhen fitting a last-layer\nLaplace approximation (Section 3.2), we closely follow\nthe implementation from BayesDiff [Kou et al., 2024].\nSpecifically, we use the empirical Fisher approximation\nwith a diagonal factorization for Hessian computation.\nThe prior precision parameter and observation noise are\nfixed at γ = 1 and σ = 1, respectively. For Hessian\ncomputation, we utilize 1% of the training data for Ima-\ngeNet 128×128 and 2% for ImageNet 256×256. Further\ndetails about the last layer of each diffusion model are\nprovided in Table 3, where we observe that fewer than\n1% of the parameters receive a ‘Bayesian treatment’. We\nutilize laplace4 library in our implementation.\n4https://github.com/aleximmer/Laplace\n\n\nFigure 12: Images with the highest (left) and the lowest (right) generative uncertainty (Eq. 8) among 12K generations\nusing a latent flow matching model [Dao et al., 2023]. Generative uncertainty correlates with visual quality, as high-\nuncertainty samples exhibit numerous artefacts, whereas low-uncertainty samples resemble canonical images of their\nrespective conditioning class.\nAs discussed in Section 6, improving the quality of the Laplace approximation—such as incorporating both first and last\nlayers instead of only the last layer [Daxberger et al., 2021b, Sharma et al., 2023] or optimizing Laplace hyperparameters\n(e.g., prior precision and observation noise) [Immer et al., 2021]—could further enhance the quality of generative uncertainty\nand represents a promising direction for future work.\nSampling with Generative Uncertainty\nFor our main experiment in Section 4.1, we generate 12K images using the\npretrained ADM model [Dhariwal and Nichol, 2021] for ImageNet 128×128 and the UViT model [Bao et al., 2023] for\nImageNet 256×256. Following BayesDiff [Kou et al., 2024], we use a DDIM sampler [Song et al., 2020a] for the ADM\nmodel and a DPM-2 sampler [Lu et al., 2022] for the UViT model, both with T = 50 sampling steps.\nTo compute generative uncertainty (Algorithm 1), we first sample M = 5 sets of weights from the posterior q(θ|D). Then,\nfor each of the initial 12K random seeds, we generate M additional samples. The same set of model weights {θm}M\nm=1 is\nused for all 12K samples for efficiency reasons. For semantic likelihood (Eq. 7), we use a pretrained CLIP encoder [Radford\net al., 2021] and set the semantic noise to σ2 = 0.001 .\nBaselines\nFor all baselines, we use the original implementation provided by the respective papers, except for [De Vita\nand Belagiannis, 2025], which we reimplemented ourselves since we were unable to get their code to run. Moreover, we\nuse the default settings (e.g., hyperparameters) recommended by the authors for all baselines. For realism [Kynkäänniemi\net al., 2019] and rarity [Han et al., 2023] we use InceptionNet [Szegedy et al., 2016] as a feature extractor and a subset of\n50K ImageNet training images as the reference dataset. For samples where the rarity score is undefined (i.e., those that lie\noutside the estimated data manifold), we set it to inf.\n\n\n"}
{"text": "Are LLMs Ready for Practical\nAdoption for Assertion Generation?\nVaishnavi Pulavarthi, Deeksha Nandal\nElectrical and Computer Engg. Dept.\nUniversity of Illinois Chicago\nChicago, USA\n{vpulav2, dnanda6}@uic.edu\nSoham Dan\nMicrosoft\nsohamdan@microsoft.com\nDebjit Pal\nElectrical and Computer Engg. Dept.\nUniversity of Illinois Chicago\nChicago, USA\ndpal2@uic.edu\nAbstract—Assertions have been the de facto collateral for\nsimulation-based and formal verification of hardware designs for\nover a decade. The quality of hardware verification, i.e., detection\nand diagnosis of corner-case design bugs, is critically dependent on\nthe quality of the assertions. With the onset of generative AI such\nas Transformers and Large-Language Models (LLMs), there has\nbeen a renewed interest in developing novel, effective, and scalable\ntechniques of generating functional and security assertions from\ndesign source code. While there have been recent works that use\ncommercial-of-the-shelf (COTS) LLMs for assertion generation,\nthere is no comprehensive study in quantifying the effectiveness\nof LLMs in generating syntactically and semantically correct\nassertions. In this paper, we first discuss AssertionBench from\nour prior work, a comprehensive set of designs and assertions to\nquantify the goodness of a broad spectrum of COTS LLMs for\nthe task of assertion generations from hardware design source\ncode. Our key insight was that COTS LLMs are not yet ready for\nprime-time adoption for assertion generation as they generate a\nconsiderable fraction of syntactically and semantically incorrect\nassertions. Motivated by the insight, we propose AssertionLLM,\na first of its kind LLM model, specifically fine-tuned for as-\nsertion generation. Our initial experimental results show that\nAssertionLLM considerably improves the semantic and syntactic\ncorrectness of the generated assertions over COTS LLMs.\nIndex Terms—component, formatting, style, styling, insert\nI. INTRODUCTION\nSystem-on-Chip (SoC) designs are crucial for many safety-\ncritical computing applications, including vehicular systems,\nmilitary, and industrial automation. SoCs often use sensitive and\nusers’ private data to perform numerous complex computations.\nIt is crucial for our national and personal well-being to ensure\nthat the SoCs are functionally correct, safe, and secure.\nAssertions are mathematical encoding of desired design\nproperties that should hold True for a design. Assertions are\nwidely used for hardware design validation throughout its life\ncycle, e.g., pre-silicon formal verification and simulation-based\nverification, emulation, and often synthesized in a fabricated\nchip for post-silicon validation and in-field debug and diagno-\nsis. In the past decade, assertion-based Verification (ABV) [1]\nhas emerged as the de facto standard to verify the security and\nfunctional correctness of hardware designs. However, crafting a\nsuccinct yet expressive set of assertions that capture subtle and\nimportant hardware design behaviors is a tedious and time-\nconsuming task, requiring a considerable amount of human\ningenuity. Too many assertions (i) can negatively affect verifica-\ntion performance with a prolonged verification closure and (ii)\nmay require a substantial amount of on-chip resources, whereas\ntoo few assertions may result in insufficient design coverage\ncausing corner case design bugs to escape to production and\nmass manufacturing. The ever increasing hardware design com-\nplexity and rapidly broadening target applications (e.g., deep\nlearning, AI) have only worsened the problem. Consequently,\ndeveloping an automated and scalable technique is crucial to\nrapidly generate a succinct set of hardware design properties\ntargeting design functionality and security.\nA considerable amount of research has leveraged two dif-\nferent paradigms – lightweight static analysis of design source\ncode and formal verification\n[2]–[4], and data-driven statis-\ntical analysis, e.g., data mining [5]–[10]. While static analy-\nsis can generalize and capture corner-case design behaviors,\nit suffers from prohibitive computational complexity limiting\nits scalability to industrial-scale designs. Alternatively, data-\ndriven dynamic analysis can scale to large designs with a\nconsiderable amount of trace data due to its computational\nefficiency, however, it often generates spurious design prop-\nerties due to the lack of design insights and domain context.\nMore recently, researchers have proposed assertion generation\ntechniques that combine static analysis and data-driven dynamic\nanalysis [11]–[13] and developed algorithms to induce ranking\non such automatically generated assertions based on the subtle\ndesign behavior they capture [14]. However, all such techniques\ngenerate a large number of assertions, many of which are re-\ndundant and do not capture model-level or system-level design\nbehaviors, and fail to scale to large industry-scale designs due\nto the algorithmic complexity of the underlying static analysis.\nDespite intense research across academia and industry over the\nlast decade, there is a widening gap between assertion solutions\nand the industry’s actual requirements in terms of assertion\nquality for complex hardware designs.\nWith recent advances in deep-learning (DL) and generative\nAI models, especially Large-Language Models (LLMs), e.g.,\nGPT-3.5, GPT-4o, CodeLLaMa 2, and LLaMa3-70B, there is\na renewed interest to harness the power of LLMs to tame the\never-widening gap. Most recent assertion generation approaches\n(c.f., Section VIII) treat a LLM model as a black box and use\nprompt engineering to iteratively refine the set of generated\nassertions. However, there is no in-depth study nor a dataset to\narXiv:2502.20633v1  [cs.LG]  28 Feb 2025\n\n\n1\nmodule arb2(clk, rst, req1, req2, gnt1, gnt2);\n2\ninput clk, rst, req1, re2;\n3\noutput gnt1, gnt2;\n4\nreg gnt_, gnt1, gnt2;\n5\nalways @(posedge clk or posedge rst)\n6\nif(rst)\n7\ngnt_ <= 0;\n8\nelse\n9\ngnt_ <= gnt1;\n10\nalways @(*)\n11\nif (gnt_)\n12\nbegin\n13\ngnt1 = req1 & req2;\n14\ngnt2 = req2;\n15\nend\n16\nelse\n17\nbegin\n18\ngnt1 = req1;\n19\ngnt2 = req2 & ˜req1;\n20\nend\n21\nendmodule\nFig. 1: A Verilog code for a 2-port Arbiter [14].\nPre-condition\nCovered\nUnreachable\nTrue\nValid\nVacuous\nPost-\nCondition\nFalse\nCEX\nFig. 2: Assertion status based on pre-condition and post-\ncondition evaluation. CEX: Counter example.\nevaluate the fit of different state-of-the-art (SOTA) LLM models\nfor generating a succinct and correct set of assertions without\na considerable amount of designer-developed prompts.\nIn this work, first, we discuss our prior work AssertionBench\n[15], the first comprehensive benchmark consisting of 100\ncurated hardware designs of varying complexity and their\nformally verified assertions to quantify the efficacy of SOTA\nand upcoming LLMs for assertion generation. Our primary\nfocus is to quantify the quality of the generated assertions\nfrom SOTA LLMs learned on a set of labeled designs and their\nformally verified assertions. Our key insight is that almost all\nSOTA LLMs generate a considerable fraction of syntactically\nand semantically incorrect assertions. Leveraging this insight,\nwe develop AssertionLLM, a fine-tuned LLM model that\ncan automatically generate substantially higher fraction (up\nto 25%) of syntactically and semantically correct assertions\nfrom design source codes without any iterative inputs from\nthe verification engineer. We further outline several research\nchallenges and opportunities that are worth pursuing to truly\nexploit the potential of generative AI for assertion generation.\nII. BACKGROUND\nA. Assertions: Syntax, Semantics, and Validity\nWe consider a hardware design D in Verilog1 as a com-\nposition of a set of concurrent processes Pi, e.g., (always\nand assign blocks). Let V be the set of design signals,\nI ⊂V be the set of input signals, O ⊂V be the set\n1We consider Verilog as the demonstration vehicle for this work, however,\nour work can naturally be extended to other hardware design languages, e.g.,\nVHDL, SystemC, and other hardware description languages.\nof output signals, and R ⊂V be the set of registers. Fig-\nure 1 shows a Verilog design D of a 2-port Arbiter, con-\nsisting of two concurrent processes P1 (line 6) and P2 (line\n11), and V = {clk, rst, req1, req2, gnt1, gnt2, gnt }, I =\n{clk, rst, req1, req2}, O = {gnt1, gnt2}, and R = {gnt }.\nAn assertion is a temporal formula in LTL [16] of the\nformat P = G(A →C) where the antecedent A is of the\nform A = Vm\ni=0 X i(Ai) and consequent C is of the form\nC = X n(Cn), where n ≥m. Each Ai (Ci) is a proposition\nand is a (var, val) pair where var ∈V and val ∈{0, 1}. X is\ncalled the next-cycle operator and X i(i ≥0) is equal to a delay\nof i clock cycles XX . . . X\n|\n{z\n}\ni times\n. Although SystemVerilog Assertion\n(SVA) [17] defines a rich set of grammar for assertions, we\nconsider a restricted subset (sequential assertion) as captured\nby P. We say an assertion P is True (Valid) if D |= P (read\nas D models P), otherwise, the assertion is False, i.e., D ̸|= P\nand there exists a Boolean value assignment to a subset of\ndesign signals known as CEX (counter-example) that shows a\nrefutation of the assertion P on D. The implication operator\n→in P are of two types, overlapped and non-overlapped. The\noverlapped implication operator (→) implies if there is a match\non the antecedent A, then the consequent C is evaluated in\nthe same clock cycle whereas the non-overlapped implication\noperator (⇒) implies if there is a match on the antecedent A,\nthen the consequent C is evaluated in the next clock cycle.\nIn Figure 2, we show the assertion evaluation status. Note\nA →C can be re-written as ¬A ∨C. Consequently, if pre-\ncondtion A is unreachable (or False), then the assertion P is\nvacuously True (i.e., ¬False ∨C). If pre-condition A is True\nand post-condition C is True as well, the assertion is reported\nto be Valid (i.e., D |= P), otherwise, if the post-condition C is\nFalse, then a counter example CEX is generated.\nFor the Arbiter of Figure 1, consider assertions P1\n:\nG((req1 == 1 ∧req2 == 0) →(gnt1 == 1)) and\nP2 : G((req2 == 0 ∧gnt\n== 1) ∧X(req1 == 1) →\nX(gnt1 == 1)). The assertion P1 evaluates True if req1\nis 1’b1 and req2 is 1’b0 at the current clock cycle, then\ngnt1 is 1’b1 in the current clock cycle. The assertion P2\nevaluates True if req2 is 1’b0 and gnt is 1’b1 in the current\ncycle, req1 is 1’b1 in the next cycle, then gnt1 is 1’b1\nin the cycle after (i.e., in the 2nd cycle). Note that P2 can\nbe re-written using the non-overlapped implication operator,\nP2 : G((req2 == 0 ∧gnt\n== 1) ∧X(req1 == 1) ⇒\n(gnt1 == 1)) where the ⇒subsumes the X operator in the\nconsequent. On discharging a proof for P1 and P2 using a\nformal property verification (FPV) engine2, we find P1 is a\nvalid assertion whereas P2 generates a CEX.\nB. Large-Language Models\nLarge-Language Models (LLMs) are an instance of genera-\ntive AI built on top of encoder-decoder transformer architec-\ntures [19]. LLMs can be classified primarily in three classes,\n(i) encoder-only LLMs [20], (ii) decoder-only LLMs [21],\nand (iii) encoder-decoder LLMs [22]. Encoder-only LLMs\n2We use Cadence JasperGold [18], however, any other FPV tool will work.\n\n\nDesign\nLine No\n0\n250\n500\n750\n1000\n1250\nfht_1d_x8.v\nmtx_trps_8x8_dpsra\nbitNegator.v\ninputReg.v\ntcReset.v\nkey_expander.v\ncavlc_read_total_coe\nPSGBusArb.v\nPSGOutputSummer.v\ncrc_control_unit.v\nqadd.v\nnode.v\nclean_rst.v\nge_1000baseX_rx.v\neth_l3_checksum.v\neth_clockgen.v\nflow_ctrl.v\nreg_int_sim.v\ncounter.v\nrxStateMachine.v\ncan_crc.v\ncan_register_asyn_s\neth_fifo.v\nphasecomparator.v\nfifo_mem.v\nFig. 3: Design details in the test set in terms of the number\nof lines of code (excluding comments and blank lines).\nAssertion \nGeneration via \ntrained LLM Model\nAssertion Syntax \nCorrector\nLLM Model ICL\n(1-shot / 5-shot)\n𝑨𝟏→𝑪𝟏\nTrain Examples\nTest \nDesigns\n𝑨′𝟏→𝑪′𝟏\n𝑨𝒎′ →𝑪′𝒎\n𝑨𝒏→𝑪𝒏\nFormal Verification\nEngine (JG)\nPASS\nFAIL\n1\n2\n3\n4\nFig. 4: Framework to evaluate LLMs for assertion genera-\ntion [15]. JG: JasperGold Formal Property Verification Engine.\nemploy a bi-directional transformer during pre-training for each\ntoken to attend every other token, decoder-only LLMs employ\nunidirectional language modeling for each token to attend its\npredecessor tokens, and where tokens can only participate in\nprevious tokens, and encoder-decoder LLMs employ denoising\nsequence-to-sequence pre-training objectives. The decoder-only\nLLM performs excellently in auto-regressive tasks such as\ncode completion and generation. Since assertion generation is\na special kind of code generation, in this work, we employ\ndecoder-only LLMs, e.g., GPT-3.5, GPT-4o, etc.\nThere are two distinct paradigms for LLM usage for different\ntasks – (i) in-context learning (ICL), where a foundational\nLLM (e.g., GPT-4o) is seeded with a few examples of the\ndesired task followed by deployment and (ii) fine tuning where\na foundational LLM is trained with a small amount of high-\nquality downstream task-specific data to construct a task-\nspecific LLM. In this work, we use ICL to evaluate the fitness\nof COTS LLMs for assertion generation and use finetuning to\ndevelop specialized LLMs for assertion generation tasks.\nIII. ASSERTIONBENCH: BENCHMARK TO QUANTIFY\nGOODNESS OF LLMS FOR ASSERTION GENERATION\nAssertionBench3 is a comprehensive suite of Verilog design\nand associated formally verified assertions to evaluate the\ngoodness of the COTS LLMs for assertion generation [15].\nAssertionBench consists of designs from OpenCores [23].\nFigure 3 and Table I show representative details of the designs.\nOur benchmark consists of five ICL examples for 1-shot and\n5-shot learning, where each example is a tuple consisting of\na Verilog design and its formally verified assertions, generated\nfrom GOLDMINE [11] and HARM [13], and verified using\nCadence JasperGold [18]. The training set comprises funda-\nmental designs such as Arbiter, Half Adder, Full Adder, T-flip-\nflop, and Full Subtractor. Among these designs, Arbiter and\nT flip-flops are sequential, while the others are combinational.\nOur training set assertions contain both overlapped and non-\noverlapped implication operators. The test design set contains\n3https://github.com/achieve-lab/assertion data for LLM.\n1 You are an expert in SystemVerilog Assertions.\n2 Your task is to generate the list of assertions to\nthe given verilog design. An example is shown\nbelow. Generate only the list of assertions for\nthe test program with no additional text.\n3 Program 1: module arb2(clk, rst, req1, req2, gnt1,\ngnt2); input clk, rst; ...\n4 Assertions 1: (state == 1 & req2 == 1) |-> (gnt1 ==\n0);...\n5 Test Program:\n6 module fifo_mem #(parameter DEPTH=8, DATA_WIDTH=8,\nPTR_WIDTH=3) ( input wclk, w_en, rclk, r_en,\ninput [PTR_WIDTH:0] b_wptr,\n...\n7 Test Assertions:\nFig. 5: An example of the prompt for 1-shot learning [15].\nThe example consists of a tuple, a Verilog design (Program\n1) and a set of formally verified assertions for the design\n(Assertions 1). The Test Program is the Verilog de-\nsign for which we generate assertions using the trained LLM.\n100 Verilog designs (split in combinational and sequential\ndesigns) from OpenCores [23] that are more complex than\nthose in the training set, to evaluate LLMs’ 1-shot and 5-\nshot learning. The set cover a wide variety of hardware in-\ncluding communication controllers, random number generators\n(RNG) for security hardware, Floating Point Unit (FPU), state\nmachines, and flow control hardware. The test designs code\nsize varies from 10 lines to 1150 lines (excluding blanks and\ncomments) as measured by cloc [24].\nIV. EXPERIMENTAL SETUP\nEvaluation Protocol: Figure 4 shows our evaluation frame-\nwork. To evaluate the effectiveness of the different LLMs, our\nk-shot ICL consists of 1-shot and 5-shot in-context examples\n(ICE) ( 1 in Figure 4). Each ICE is a tuple ⟨D, A⟩, where D\nis a Verilog source code and A is a set of formally verified\nassertions containing a minimum of two (2) and a maximum\nof 10 assertions with an average of 4.8 assertions per source\ncode. We use a prompt as shown in Figure 5 consisting of four\nparts – (i) an English language description of the task in hand,\n(ii) an example Verilog design with newlines and comments\nremoved, (iii) an example assertion in SVA format, and (iv)\na test Verilog design with new lines and comments removed.\nFollowed by training, we provide each trained model with 100\ntest Verilog programs to infer assertions ( 2 in Figure 4). In our\nexperiments, we have found all of the LLM models generate\nsyntactically erroneous assertions, i.e., each LLM fails to learn\nthe SVA syntax from the training examples. Consequently, we\nuse a syntax corrector ( 3 in Figure 4) using GPT-3.5 and feed\nthe output of the syntax corrector to Cadence JasperGold FPV\nengine to evaluate the quality of the generated assertions. Note\nany other FPV engine compatible with SVA will work as well.\nICL Compute Platform: We use UIUC (University of Illinois\nUrbana-Champaign) NCSA’s (National Center for Supercom-\nputing Applications) Delta Cluster [25] to run our experiments.\nWe use GPU nodes containing 1-way, 4-way, and 8-way\nNVIDIA A40 (with 48GB GDDR6) and A100 (with 40GB\nSXM ) GPUs to perform k-shot learning.\n\n\nTABLE I: Details of a few representative designs in the test set of AssertionBench benchmark.\nVerilog Design\n# of Lines\nDesign Type\nDesign Functionality\nca prng\n1144\nSequential\nA compact Pattern Generator\ncavlc read total coeffs\n1090\nSequential\nVideo Encoder for generic audio visual.\ncavlc read total zeros\n637\nCombinational\nVideo Encoder for generic audio visual.\nge 1000baseX rx\n544\nSequential\nVerilog implementation of Physical Coding\nSublayer (PCS) type.\nMAC tx Ctrl\n504\nSequential\nAn Ethernet MAC controller.\nPre-trained Models and EDA Tools: We use pre-trained\nLLMs from the HuggingFace [26] for evaluation and Cadence\nJasperGold version 2022.06p002 to formally verify the asser-\ntions generated from the test Verilog designs. We use two\nSOTA tools GOLDMINE [11], [14] and HARM [13] to generate\nassertions for Verilog designs in the ICE. Below, we summarize\nthe COTS LLMs that we evaluate using AssertionBench.\n1) GPT-3.5 is a commercial LLM built using the GPT ar-\nchitecture [27]. It is part of OpenAI’s GPT (Generative Pre-\ntrained Transformer) series of models designed to understand\nand generate text based on the input it receives.\n2) GPT-4o (‘o’ for “omni”) is the newest model of OpenAI’s\nGPT, which accepts any combination of input audio, image,\nvideo, and text and responds with an output consisting of\nimage, audio, video, and text [21]. With larger training data,\nincreased model size, and faster response than other GPT\nmodels, GPT-4o is a unified model for text, vision, and audio.\n3) CodeLLaMa 2 is a collection of generative text models\ndeveloped by Meta [28] with parameters ranging from 7B to\n70B. The model accepts only text as input and output. It is an\nauto-regressive language model. The context window length for\nCodeLLaMa 2 is 4096. The large 70B model uses Grouped-\nQuery Attention for improved inference scalability.\n4) LLaMa3-70B is available in two parameter sizes – 8B and\n70B. The context window length for LLaMa3-70B is 8192\nand is pre-trained with 15 Trillion tokens of publicly available\ndata [29]. LLaMa3-70B excels at translation, contextual under-\nstanding, and dialogue generation. It has enhanced capabilities\nsuch as code generation, reasoning, and following instructions.\nICL Hyperparameters: For all LLMs under consideration, the\nhyperparameters have been set to their default values. Specifi-\ncally, the maximum output tokens is set to 1024, employing a\ngreedy decoding strategy and maintaining a temperature of 1.0\n(most creative), top p of 0.95. The random seed is set to 50.\nMetrics: We evaluate the generated assertions from the test\ndesigns using following metrics for each k-shot ICL per LLM.\n1) Pass quantifies the fraction of generated assertions that\nFPV engine attests as valid for the design. This includes the\nVacuous and the Pass cases from Figure 2.\n2) Fail quantifies the fraction of generated assertions that FPV\nengine attests as wrong for the design and generates a coun-\nterexample trace. This includes the Fail case from Figure 2.\n3) Error: It quantifies the fraction of generated assertions for\nwhich the FPV engine identifies one or more syntactic errors\nin the assertions even after syntax correction by the GPT-3.5.\nV. OBSERVATIONS AND INSIGHTS FROM ASSERTIONBENCH\nWe depict our observations and insights [15] in Figure 6\nand Figure 7 and discuss them below.\nObservation 1: Most LLMs generate valid assertions with\nan increasing number of ICL examples. For the assertion\ngeneration task, all LLMs progressively generate more valid\nassertions when the number of ICL examples is increased as\nseen in Figure 6. GPT-3.5, GPT-4o, and CodeLLaMa 2 show\non average an improvement of 2×, 1.2×, and 1.12× for valid\nassertion generation, respectively, when moved from 1-shot\nlearning to 5-shot learning. However, the LLaMa3-70B model\nloses accuracy from 31% to 24% on the same dataset. Our\nin-depth analysis shows in many cases, LLaMa3-70B either\nfails to generate assertions or generates syntactically wrong\nassertions (which even a syntax corrector fails to correct) or\ntries to generate codes in a new programming language (such\nas Java). This experiment shows that there is a considerable\nscope for improving the LLaMa3-70B model for this task,\nlikely via fine-tuning the pre-trained LLaMa3-70B model.\nObservation 2: An enhanced LLM does not necessarily\nensure a better semantic or syntactic understanding. In Fig-\nure 6, we do not see a correlation between the sophistication\n(in terms of the number of model parameters) of the LLMs and\ntheir ability to generate good assertions. For GPT-3.5 (c.f., Fig-\nure 6a), with an increase in the number of ICL examples, the\nLLM was able to produce more syntactically correct assertions,\nhowever, after such corrections, the majority of assertions (on\naverage up to 24%) generated a CEX when verified with\nJasperGold. For GPT-4o, the results were more consistent in\nterms of syntactically (in)correct assertions for both 1-shot\nand 5-shot learning (c.f., Figure 6b). For CodeLLaMa 2 and\nLLaMa3-70B, with an increase in the number of ICL examples,\nthe number of failed assertions decreased (on average up\nto 12% for CodeLLaMa 2 and LLaMa3-70B, c.f., Figure 6c\nand Figure 6d), however, both models generated more syn-\ntactically wrong assertions (on average up to 19% more for\nLLaMa3-70B). This observation is perplexing as one would\nexpect with more number of parameters, LLaMa3-70B would\nbe able to learn better to predict syntactically correct assertions.\nOur in-depth analysis shows that with a 1-shot, the variations\nin types of assertions in examples were limited. Consequently,\nLLaMa3-70B learned the assertion syntax. However, in 5-\nshot learning, we have more variations in the which made\nLLaMa3-70B’s learning task difficult. This experiment sug-\ngests that increasing the ICL examples alone will not neces-\n\n\nAccuracy\n0.0\n0.2\n0.4\n0.6\n1-shot\n5-shot\nPass\nCEX\nError\n(a)\nAccuracy\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n1-shot\n5-shot\nPass\nCEX\nError\n(b)\nAccuracy\n0.0\n0.2\n0.4\n0.6\n0.8\n1-shot\n5-shot\nPass\nCEX\nError\n(c)\nAccuracy\n0.0\n0.2\n0.4\n0.6\n1-shot\n5-shot\nPass\nCEX\nError\n(d)\nFig. 6: Comparison of accuracy of generated assertions. (a) Assertion accuracy comparison for GPT-3.5. (b) Assertion\naccuracy comparison for GPT-4o. (c) Assertion accuracy comparison for CodeLLaMa 2. (d) Assertion accuracy comparison for\nLLaMa3-70B. (a) k = 1-shot assertion accuracy. (b) k = 5-shot assertion accuracy. CEX: Counter Example trace.\nAccuracy\n0.0\n0.2\n0.4\n0.6\n0.8\nPass\nCEX\nError\nGPT3.5\nGPT 4o\nCodeLLaMa 2\nLLaMa 3\n(a) 1-shot.\nAccuracy\n0.0\n0.2\n0.4\n0.6\nPass\nCEX\nError\nGPT3.5\nGPT 4o\nCodeLLaMa 2\nLLaMa 3\n(b) 5-shot.\nFig. 7: Comparison of accuracy of generated assertions in\nterms of passing, failing, (generating a counter example),\nand syntactically wrong assertions between different LLMs\nper k-shot learning where k = 1 and k = 5.\nsarily improve LLM’s consistency in generating syntactically\nand semantically correct assertions.\nObservation 3: GPT-4o is relatively more consistent for\nassertion generation task. In Figure 7, we compare different\nLLMs in terms of generating valid assertions for 1-shot and\n5-shot learning. Our experiments show that GPT-4o is consis-\ntently superior in generating valid assertions for both 1-shot\nand 5-shot learning and generates, on average, up to 15.6%\nmore valid assertions as compared to other LLMs. This trend\nremains valid with respect to assertions generating CEX and\nsyntactically wrong assertions, i.e., GPT-4o produced less CEX\ngenerating assertions and syntactically incorrect assertions as\ncompared to other LLMs. This experiment shows that GPT-4o\nperforms relatively better than other LLMs.\nObservation 4: All LLMs need considerable improvement\nfor assertion generation task: In-depth analysis of Figure 6\nand Figure 7 show that none of the LLMs can generate valid\nassertions with an average of no more than 44% accuracy\nwhereas up to 63% generated assertions produces CEX and\non average up to 33% of generated assertions are syntactically\nwrong. Clearly, for LLMs to be of practical usage for any real-\nistic industrial-scale design, considerable improvement needs to\nbe made. Specifically, the LLMs need to capture the semantic\nmeaning of the hardware description languages, e.g., Verilog,\nfor generating higher fraction of valid assertions automatically\nwithout iterative human prompting. Our prior work [11], [14]\nshows that such critical insights are not directly available from\nthe raw design source code and need auxiliary artifacts, such\nas Control-Data Flow Graph (CDFG), Variable Dependency\nGraph (VDG), Cone of Influence (COI), etc. Future research\nin applying LLMs for assertion generation should consider\nsuch auxiliary artifacts to develop assertion-specific LLMs.\nEvaluation of the four COTS LLMs using AssertionBench\nshows that no LLM consistently outperforms other LLMs.\n𝑨𝟏→𝑪𝟏\nk Train Examples\n𝑨𝒏→𝑪𝒏\nAssertion \nGeneration via \ntrained LLM Model\nFine-tuned LLM \nModel ICL\n(1-shot / 5-shot)\nTest \nDesigns\n𝑨′𝟏→𝑪′𝟏\n𝑨𝒎′ →𝑪′𝒎\nFormal Verification\nEngine (JG)\nPASS\nFAIL\nLLM Model\nFine-tuning\n𝑨𝒏→𝑪𝒏\n𝑩𝟐→𝑫𝒏\n𝑬𝒏→𝑭𝒏\n𝑮𝒏→𝑯𝒏\nn Train Examples\n1\n2\n3\n4\nFig. 8: Our evaluation framework for AssertionLLM.\nOur analysis emphasizes that there is a considerable scope to\nenhance LLMs for assertion generation. There are two different\nways – (i) enhance ICL with diverse ICL examples or (ii) de-\nvelop an LLM specifically for assertion generation. Using this\ninsight, we develop AssertionLLM as detailed in Section VI.\nVI. ASSERTIONLLM: LLM FOR ASSERTION GENERATION\nAssertionLLM is an LLM specifically developed for the task\nof assertion generation. This is inline with the recent findings\nin other domains, e.g., financial analysis, where a task-specific\nLLM has excelled for downstream tasks as compared to a\nfoundational LLM. We use two different LLM foundational\nmodels – (i) CodeLLaMa 2 and (ii) LLaMa3-70B and finetune\neach of them using a large amount of data where each data point\nconsists of a Verilog design and its formally verified assertions.\nFigure 8 shows the end-to-end flow to finetune LLM\nfor assertion generation and how we use the finetuned LLM\nto evaluate its goodness for assertion generation. Compared\nto Figure 4, we have removed the syntax corrector block\n( 3 in Figure 4), and instead of using foundational LLM ( 1\nin Figure 4), we are using the fined-tuned LLM model for ICL\n( 2 and 3 in Figure 8).\nWe use the same compute platform and and hyperparameters\nas detailed in Section IV for finetuning. Additionally, we use\n20 epochs to finetune each foundational LLM. We split the\nAssertionBench and use 75% of data for training and the\nremaining 25% for testing the goodness of the tuned LLM.\nVII. EXPERIMENTAL RESULTS ON ASSERTIONLLM\nFigure 9 shows our experimental results on assertion gen-\neration task using finetuned CodeLLaMa 2 and LLaMa3-70B.\nWe compare these results to that of results in Figure 6.\nObservation 5: Finetuning LLMs considerably improves\nfraction of correct assertions: We observe that the finetuned\n\n\nCodeLLaMa 2 increased proven assertions by 29% and 38%\nfor 1-shot and 5-shot ICL, decreased assertions generating\nCEX by 48% and 33% for 1-shot and 5-shot ICL, respectively\n(c.f., Figure 6c and Figure 9a). With respect to LLaMa3-70B,\nfine tuning has increased proven assertions by 24% for 5-shot\nlearning (c.f., Figure 6d and Figure 9b). However, for 1-shot\nICL, the fraction of proven assertions has decreased by 4.7%\nand has increased the fraction of assertions generating CEX by\n5.4% and 12% for 1-shot and 5-shot ICL, respectively. Further\nanalysis shows that as the foundational CodeLLaMa 2 was\ntrained on large corpora of codes, it learned assertion syntax\nand semantics better during finetuning. In contrast, foundational\nLLaMa3-70B training on general text corpora struggles to\nlearn assertion syntax and semantics during finetuning. This\nexperiment shows that it is crucial to select appropriate\nfoundational LLM and dataset for an effective fine-tuned\nLLM for assertion generation.\nObservation 6: Fine-tuning LLMs does not necessarily guar-\nantee syntactic error-free assertions: Fine-tuning LLMs does\nnot necessarily nullify the fraction of syntactically erroneous\nassertions. Figure 9 shows that both finetuned CodeLLaMa 2\nand LLaMa3-70B generate a considerable fraction (upto 38%)\nof syntactically erroneous assertions. In order to reduce the\nfraction of erroneous assertions, we speculate that we will\nrequire a more comprehensive dataset with a sufficient\nnumber of examples for the diverse syntax of assertions.\nVIII. RELATED WORK\nAutomatic generation of assertions in hardware has been\nan active area of research for the past decade. IODINE is\none of the earliest works for hardware assertion generation by\nanalyzing dynamic program behavior with respect to a set of\nstandard property templates [5]. Prior works have used static\nanalysis [6], [7], dynamic simulation execution data [8]–[10],\nand data-driven statistical analysis guided by the lightweight\nstatic analysis of design source code [11], [30] for assertion\ngeneration. Following GOLDMINE, researchers have developed\na wide variety of assertion generation techniques targeting\nhardware design functionality [12]–[14], [31]–[33] and hard-\nware design security [34], [35], and to evaluate the quality of\nnumerous assertions that automatic methods generate to aid\nthe downstream verification tasks [14], [33], [36]. However, all\nthese works suffer from following shortcomings – they (i)\ndo not scale for industrial-scale designs, (ii) require a massive\namount of trace data to generate assertions, (iii) generate\nnumerous redundant design properties without any explanation\non their usability for downstream verification tasks, (iv) fail to\ngeneralize the properties beyond what is seen in the trace, and\n(v) encompass an extremely small subset of SVA, limiting the\nexpressibility and richness of the generated assertions.\nRecently,\nmassive\nsuccess\nof LLMs,\ne.g.,\nGPT\n[21],\nLLaMa [37], Gemini [38], in diverse scientific, engineering,\nand medical applications have led researchers to investigate\napplication of LLMs for hardware property generation [39]–\n[44]. However, almost all recent works on property generation\nusing LLMs suffer from the following shortcomings – they (i)\nAccuracy\n0.0\n0.2\n0.4\n0.6\n1-shot\n5-shot\nPass\nCEX\nError\n(a)\nAccuracy\n0.0\n0.2\n0.4\n0.6\n1-shot\n5-shot\nPass\nCEX\nError\n(b)\nFig. 9: Comparison of accuracy of generated assertions. (a)\nAssertion accuracy comparison for finetuned CodeLLaMa 2.\n(b) Assertion accuracy comparison for finetuned LLaMa3-70B.\nCEX: Counter Example trace.\nrequire considerable human efforts and deep understanding of\nthe target hardware designs to devise hand-crafted prompts to\ngenerate and refine hardware properties, (ii) do not generalize\nthe assertions, and (iii) do not consider execution traces,\nrisking potentially missing subtle incorrect design behaviours or\nsecurity vulnerabilities that are not obvious in the design source\ncode. In fact, there is a lack of a systematic study comparing the\neffectiveness of different commercial and open-source LLMs\nin generating valid assertions from hardware design source\ncode. AssertionBench aims to fill in the gap and provides novel\ninsights for future research on LLMs for assertion generation.\nIX. LIMITATIONS\n• Dataset: In this study, we focus on Verilog designs, given its\npredominance in hardware design language. Moving forward,\nit will be intriguing to develop benchmarks for assertions in\nother HDLs, e.g., VHDL, SystemC, to expand the scope of our\nanalysis to broader design paradigms.\n• Modeling: In this paper, we assessed the assertion generation\ncapabilities of k-shot and finetuned SOTA LLMs. There is\na considerable scope for improvement in terms of assertion\nquality and correctness. Future work should focus on modeling\nto capture design coverage of generated assertions and quantify\ntheir goodness in terms of captured design behavior.\n• Evaluation: In future work, it will be valuable to conduct\na more detailed evaluation of model errors to better understand\nthe specific limitations of each LLM for assertion generation.\nX. CONCLUSION AND FUTURE WORK\nThis work introduces AssertionBench to evaluate the current\nand future commercial and open-source LLMs for the assertion\ngeneration task and AssertionLLM to fully automate assertion\ngeneration using generative AI without the designer’s itera-\ntive intervention. Although there is no LLM that consistently\noutperforms other LLMs, we notice several promising trends\nand research directions such as (i) to quantify the goodness\nof assertion in terms of captured design behavior, (ii) to\nquantify the design coverage of the assertions, (iii) to model and\ncapture likely design security vulnerabilities as assertions, and\n(iv) going beyond temporal/sequential assertions to generate\nassertions encompassing richer set of SVA, to enhance the\npractical applicability of LLMs for assertion generation task for\nindustrial-scale designs. Pursuing these directions will further\naccelerate SoC and hardware design verification.\n\n\nREFERENCES\n[1] Hasini Witharana, Yangdi Lyu, Subodha Charles, and Prabhat Mishra. A\nSurvey on Assertion-based Hardware Verification. ACM Comput. Surv.\n(CS), 2022.\n[2] Saddek Bensalem, Yassine Lakhnech, and Hassen Saidi.\nPowerful\nTechniques for The Automatic Generation of Invariants. Int’l Conf. on\nComputer-Aided Verification (CAV), 1996.\n[3] A. Tiwari, H. Rueß, H. Sa¨ıdi, and N. Shankar. A Technique for Invariant\nGeneration. Int’l Conf. on Tools and Algorithms for the Construction and\nAnalysis of Systems (TACAS), 2001.\n[4] Corina S. P˘as˘areanu and Willem Visser. Verification of Java Programs\nUsing Symbolic Execution and Invariant Generation. Int’l SPIN Workshop\non Model Checking of Software (SPIN), 2004.\n[5] Sudheendra Hangal, Sridhar Narayanan, Naveen Chandra, and Sandeep\nChakravorty. IODINE: A Tool to Automatically Infer Dynamic Invariants\nfor Hardware Designs. Design Automation Conf. (DAC), 2005.\n[6] G. Pinter and I. Majzik. Automatic Generation of Executable Assertions\nfor Runtime Checking Temporal Requirements.\nIEEE Int’l Symp. on\nHigh-Assurance Systems Engineering (HASE), 2005.\n[7] A. Hekmatpour and A. Salehi.\nBlock-based Schema-driven Assertion\nGeneration for Functional Verification. Asian Test Symp. (ATS), 2005.\n[8] Andrew DeOrio, Adam B. Bauserman, Valeria Bertacco, and Beth C.\nIsaksen.\nInferno: Streamlining Verification With Inferred Semantics.\nIEEE Trans. on Computer-Aided Design of Integrated Circuits and\nSystems (TCAD), 2009.\n[9] Po-Hsien Chang and Li.-C Wang. Automatic Assertion Extraction via\nSequential Data Mining of Simulation Traces. Asia and South Pacific\nDesign Automation Conf. (ASP-DAC), 2010.\n[10] Chih-Neng Chung, Chia-Wei Chang, Kai-Hui Chang, and Sy-Yen Kuo.\nApplying Verification Intention for Design Customization via Property\nMining Under Constrained Testbenches. Int’l Conf. on Computer Design:\nVLSI in Computers and Processors, (ICCD), 2011.\n[11] GoldMine. GOLDMINE: An Automatic Assertion Generation Tool. http:\n//goldmine.csl.illinois.edu/, 2024. Accessed: March 3, 2025.\n[12] Mohammad Reza Heidari Iman, Gert Jervan, and Tara Ghasempouri.\nARTmine: Automatic Association Rule Mining with Temporal Behavior\nfor Hardware Verification.\nDesign, Automation, and Test in Europe\n(DATE), 2024.\n[13] Samuele Germiniani and Graziano Pravadelli.\nHARM: A Hint-Based\nAssertion Miner. IEEE Trans. on Computer-Aided Design of Integrated\nCircuits and Systems (TCAD), 2022.\n[14] Debjit Pal, Spencer Offenberger, and Shobha Vasudevan.\nAssertion\nRanking Using RTL Source Code Analysis. IEEE Trans. on Computer-\nAided Design of Integrated Circuits and Systems (TCAD), 2020.\n[15] Vaishnavi Pulavarthi, Deeksha Nandal, Soham Dan, and Debjit Pal.\nAssertionBench: A Benchmark to Evaluate Large-Language Models for\nAssertion Generation for Hardware Design. Conf. of the Nations of the\nAmericas Chapter of the Assoc. for Computational Linguistics (NAACL\nFindings), 2025.\n[16] Amir Pnueli.\nThe Temporal Logic of Programs.\nAnnual Symp. on\nFoundations of Computer Science (SFCS), 1977.\n[17] SystemVerilog. 1800-2017 - IEEE Standard for SystemVerilog–Unified\nHardware Design, Specification, and Verification Language.\nhttps://\nieeexplore.ieee.org/document/8299595, 2024. Accessed: March 3, 2025.\n[18] Cadence.\nJasperGold Apps.\nhttps://www.cadence.com/en US/home/\ntools/system-design-and-verification/formal-and-static-verification/\njasper-gold-verification-platform.html, 2024. Accessed: March 3, 2025.\n[19] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention\nis All You Need.\nInt’l Conference on Neural Information Processing\nSystems (NeurIPS), 2017.\n[20] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng,\nMing Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming\nZhou. CodeBERT: A Pre-Trained Model for Programming and Natural\nLanguages. Conf. on Empirical Methods in Natural Language Processing\n(EMNLP Findings), 2020.\n[21] OpenAI, Josh Achiam, and et al. GPT-4 Technical Report. arXiv, 2024.\n[22] Antonio\nMastropaolo,\nSimone\nScalabrino,\nNathan\nCooper,\nDavid\nNader Palacio, Denys Poshyvanyk, Rocco Oliveto, and Gabriele Bavota.\nStudying the Usage of Text-To-Text Transfer Transformer to Support\nCode-Related Tasks. Int’l Conf. on Software Engineering (ICSE), 2021.\n[23] OpenCores. https://opencores.org/, 2024. Accessed: March 3, 2025.\n[24] CLOC. https://github.com/AlDanial/cloc. Accessed: March 3, 2025.\n[25] NCSA.\nNCSA\nDelta.\nhttps://www.ncsa.illinois.edu/research/\nproject-highlights/delta/, 2024. Accessed: March 3, 2025.\n[26] HuggingFace.\nModel Repository.\nhttps://huggingface.co/, 2024.\nAc-\ncessed: March 3, 2025.\n[27] Junjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao, Shichun Liu,\nYuhan Cui, Zeyang Zhou, Chao Gong, Yang Shen, Jie Zhou, Siming\nChen, Tao Gui, Qi Zhang, and Xuanjing Huang.\nA Comprehensive\nCapability Analysis of GPT-3 and GPT-3.5 Series Models. arXiv, 2023.\n[28] Baptiste Rozi`ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai\nGat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre,\nTal Remez, J´er´emy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna\nBitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan\nXiong, Alexandre D´efossez, Jade Copet, Faisal Azhar, Hugo Touvron,\nLouis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve.\nCode Llama: Open Foundation Models for Code. 2024.\n[29] Meta.\nIntroducing Meta Llama 3: The most capable openly available\nLLM to date. https://ai.meta.com/blog/meta-llama-3/, 2024. Accessed:\nMarch 3, 2025.\n[30] Samuel Hertz, David Sheridan, and Shobha Vasudevan. Mining Hardware\nAssertions With Guidance From Static Analysis.\nIEEE Trans. on\nComputer-Aided Design of Integrated Circuits and Systems (TCAD),\n2013.\n[31] Samuel Hertz, Debjit Pal, Spencer Offenberger, and Shobha Vasudevan.\nA Figure of Merit for Assertions in Verification. Asia and South Pacific\nDesign Automation Conf. (ASP-DAC), 2019.\n[32] Samuele Germiniani and Graziano Pravadelli.\nExploiting Clustering\nand Decision-Tree Algorithms to Mine LTL Assertions Containing Non-\nboolean Expressions. IFIP/IEEE Int’l Conf. on Very Large Scale Inte-\ngration (VLSI-SoC), 2022.\n[33] Mohammad Reza Heidari Iman, Jaan Raik, Gert Jervan, and Tara\nGhasempouri. IMMizer: An Innovative Cost-Effective Method for Mini-\nmizing Assertion Sets. Euromicro Conference on Digital System Design\n(DSD), 2022.\n[34] Calvin Deutschbein, Andres Meza, Francesco Restuccia, Ryan Kastner,\nand Cynthia Sturton.\nIsadora: Automated Information Flow Property\nGeneration for Hardware Designs. Workshop on Attacks and Solutions in\nHardware Security (ASHES), 2021.\n[35] Hasini Witharana, Aruna Jayasena, Andrew Whigham, and Prabhat\nMishra. Automated Generation of Security Assertions for RTL Models.\nACM Journal on Emerging Technologies in Computing Systems (JETC),\n2023.\n[36] Avinash Ayalasomayajula, Nusrat Farzana, Debjit Pal, and Farimah\nFarahmandi.\nPrioritizing Information Flow Violations: Generation of\nRanked Security Assertions for Hardware Designs.\nIEEE Int’l Symp.\non Hardware Oriented Security and Trust (HOST), 2024.\n[37] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-\nAnne Lachaux, Timoth´ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric\nHambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. LLaMA: Open and Efficient Foundation\nLanguage Models. arXiv, 2023.\n[38] Google Gemini.\nhttps://gemini.google.com/app.\nAccessed: March 3,\n2025.\n[39] Mingjie Liu, Nathaniel Pinckney, Brucek Khailany, and Haoxing Ren.\nVerilogEval: Evaluating Large Language Models for Verilog Code Gen-\neration. Int’l Conf. on Computer-Aided Design (ICCAD), 2023.\n[40] Marcelo Orenes-Vera, Margaret Martonosi, and David Wentzlaff. Using\nLLMs to Facilitate Formal Verification of RTL. arXiv, 2023.\n[41] Rahul Kande, Hammond Pearce, Benjamin Tan, Brendan Dolan-Gavitt,\nShailja Thakur, Ramesh Karri, and Jeyavijayan Rajendran. LLM-assisted\nGeneration of Hardware Assertions. arXiv, 2023.\n[42] Wenji Fang, Mengming Li, Min Li, Zhiyuan Yan, Shang Liu, Hongce\nZhang, and Zhiyao Xie. AssertLLM: Generating and Evaluating Hard-\nware Verification Assertions from Design Specifications via Multi-LLMs.\n2024 IEEE LLM Aided Design Workshop (LAD), 2024.\n[43] Bhabesh Mali, Karthik Maddala, Sweeya Reddy, Vatsal Gupta, Chandan\nKarfa, and Ramesh Karri.\nChIRAAG: ChatGPT Informed Rapid and\nAutomated Assertion Generation. IEEE Computer Society Annual Symp.\non VLSI (ISVLSI), 2024.\n[44] Kexin Pei, David Bieber, Kensen Shi, Charles Sutton, and Pengcheng\nYin. Can Large Language Models Reason About Program Invariants?\nInt’l Conf. on Machine Learning (ICML), 2023.\n\n\n"}
{"text": "Quantum information elements in Quantum Gravity states\nand processes\nDaniele Oriti ∗\nDepto. de F´ısica Te´orica and IPARCOS, Facultad de Ciencias F´ısicas\nUniversidad Complutense de Madrid, Spain, EU\nMarch 3, 2025\nAbstract\nWe summarize basic features of quantum gravity states and processes, common to a number of\nrelated quantum gravity formalisms, and sharing a purely combinatorial and algebraic language,\nand a discrete geometric interpretation. We emphasize how, in this context, entanglement is\na seed of topological and geometric properties, and how a pre-geometric, discrete notion of\nquantum causality can be implemented, as well as some recent results (based on random tensor\nnetwork techniques) on the conditions for information transmission and holographic behaviour\nin quantum gravity states. Together, these features indicate that quantum information concepts\nand tools play a key role in defining the fundamental structure of quantum spacetime.\nIntroduction\nThe main goal of this contribution is to show how quantum gravity states and processes as\nidentified by a number of quantum gravity formalisms can be recast in the language of quantum\ninformation and how entanglement or quantum correlations can then be seen, in the same\nformalisms, as essential in the very structure of quantum spacetime. It is not a review even of the\nfew results we will summarize briefly, let alone of the substantial research done on entanglement\nand quantum information features in quantum gravity formalisms. For the latter, we refer to\n[1, 2], to remain limited to the results obtained in the quantum gravity context closer to our\nfocus.\nThe perspective we find convenient to adopt, in order to appreciate the role of quantum\ninformation-theoretic structures in these quantum gravity formalisms, is that of emergent space-\ntime, i.e. of quantum gravity as a theory of ‘spacetime constituents’with spacetime itself, geom-\netry and fields as emergent entities [3, 4, 5, 6, 7]. This perspective is motivated by several results\nin semiclassical physics, for example black hole thermodynamics and the information paradox,\ngravitational singularities, that all point in various ways to a breakdown of key notions on which\nstandard continuum, geometric physics is based, and, more indirectly, the results of analogue\ngravity in condensed matter systems, showing how effective field theory on curved backgrounds\ncan emerge rather generically from non-gravitational systems. It is also motivated by results in\nmodern quantum gravity approaches, including the ones we focus on in this contribution, with\n∗doriti@ucm.es\n1\narXiv:2502.21234v1  [gr-qc]  28 Feb 2025\n\n\ntheir combinatorial, algebraic and, indeed, (quantum) information-theoretic structures replacing\ngeometric notions and spacetime-based quantum fields. Indeed, to give some examples, canonical\nLoop Quantum gravity replaces smooth metric geometries with piecewise-degenerate quantum\ntwisted geometries encoded in combinatorial/algebraic data, lattice quantum gravity works with\npiecewise-flat quantum (often non-metric) geometries, string theory dualities also suggest that\nthe fundamental degrees of freedom of M-theory are not spacetime-based, and AdS/CFT gives\na concrete example of emergent gravity as well as a very partial ‘emergent space’, reconstructed\nfrom a lower-dimensional and non-gravitational CFT. This perspective also implies a shift away\nfrom the more traditional perspective that sees quantum gravity as the result of straightfor-\nwardly quantizing General Relativity or some other classical gravitational and spacetime-based\ntheory (whether perturbatively or non-perturbatively)1. From an emergent spacetime perspec-\ntive, a breakdown of spacetime notions, including locality, should be expected when moving to a\nmore fundamental description. One the key task is then to identify the hidden, possibly discrete\nmicrostructure replacing continuum spacetime fields in such more fundamental description of\nthe universe, with such fields, including the metric, being then understood as collective enti-\nties and gravity and the rest of continuum spatiotemporal physics as an approximate effective\ndescription of collective dynamics; controlling such collective dynamics is the second key task.\nIn other words, the universe itself is seen as a (background independent) quantum many-body\nsystem.\nTied to the notion of spacetime emergence is the further hypothesis that spacetime geomet-\nric and, possibly, topological structures can in fact emerge from the entanglement among more\nfundamental quantum constituents [8, 9], thus via a conjectured ‘entanglement/geometry cor-\nrespondence’. Support for this conjecture has been obtained mostly in a semi-classical context\nand in the AdS/CFT context, thus in presence of well-defined spacetime and geometric notions,\nstarting with the Ryu-Takanayagi entropy formula and related results [10]. However, they are\nsuggestive of something more fundamental, that calls for a concrete realization of this idea in\nfull quantum gravity, thus also in absence of spacetime and fields as we know them. This call\nhas been heard and partially answered, we would claim, in the quantum gravity formalisms on\nwhich the rest of this contribution will focus. Quantum correlations and, more generally, quan-\ntum information-theoretic notions acquire, in fact, a central role. These formalisms are canonical\nloop quantum gravity, spin foam models, group field theories and lattice quantum gravity in first\norder (tetrad-connection) variables. Despite several technical differences between them, they all\nshare many basic features. We will discuss such shared features, and only occasionally point out\nspecific differences; unless noted otherwise, we will only consider models of 4-dimensional quan-\ntum gravity and spacetime and a Lorentzian (as opposed to Euclidean/Riemannian) setting.\nWe will first discuss the nature of quantum gravity states in these formalisms, and emphasize\nthe role that entanglement among their constituents plays, their re-interpretation as quantum\ncircuits and their use to define holographic maps and quantum information channels. Then, we\nwill discuss the corresponding quantum gravity processes, indicating their possible formulation\nas quantum causal histories and, again, as quantum circuits, as well as the present limitations\nto such reformulation.\n1Interestingly, this more traditional perspective is also, historically and in part of the present community, the one\nadopted to interpret some of the same quantum gravity formalisms that we suggest can be fruitfully understood from\nan emergent spacetime perspective.\n2\n\n\n1\nQuantum gravity states as entanglement networks\nand quantum circuits\nIn the quantum gravity formalisms we are concerned with here, generic quantum gravity states\ncan be represented as (superpositions of) entanglement networks of quantum geometric con-\nstituents; more precisely, they are expressed by assigning algebraic data to a combinatorial\ngraph, where the algebraic data are taken from the (representation) theory of (Lie) groups, no-\ntably the Lorentz group or the rotation subgroup thereof. In turn, they can be seen as composed\nof elementary quantum systems, with associated one-body Hilbert space, located on nodes of\nthe graph, with the graph itself encoding a pattern of entanglement across the (sub-)systems\nliving on the nodes.\nFigure 1: The general structure of quantum gravity states as many-body systems., with the underlying\ngraph including both open links and links connecting pairs of nodes.\nThe graphs are usually taken to be dual to 3-dimensional simplicial complexes, in quantum\ngravity formalisms with a direct discrete geometric interpretation. In the following we restrict to\nsuch case. Different formalisms, as well as different models within a given formalism, will differ\nfor the class of graphs being considered and for the choice of one-body Hilbert space associated\nto the nodes of the graphs, as well as for the quantum dynamics assumed for such quantum\ngravity states. Leaving dynamical considerations aside, in this section, we now give more details\non a specific (set of) proposal(s) for the kinematical structures, and illustrate their quantum\ninformation-theoretic aspect.\n1.1\nEntanglement patterns of atoms of space - quantum space\nas a quantum circuit\nThe one-body Hilbert space can be taken to be space of states for a quantum tetrahedron,\nwhich can be constructed from SU(2)(2) group data and expressed in terms of its irreducible\nrepresentations:\nHv =\nM\n⃗jv\n\u0010\n⊗4\ni=1V ji\nv O\nI⃗jv\u0011\n(1.1)\nwhere one has a vector space V ji\nv for the representation label ji (a ‘spin’valued in the half-\nintegers) for each of the four triangles of the tetrahedron, with canonical basis |ji, ni⟩, then\ntensored together, and I⃗jv = InvG[V j1\nv ⊗· · · ⊗V j4\nv] is the space of intertwiners, i.e. tensors\n3\n\n\ninvariant under the diagonal action of the group G = SU(2)(2), built from the same four rep-\nresentation spaces.\nThis Hilbert space can be obtained from the direct quantization of the\nclassical phase space of geometries of a single tetrahedron, parametrized by Lie algebra elements\ncorresponding to normal vectors associated to its four triangles and conjugate group elements\ncorresponding to elementary parallel transports of an SU(2)(2) connection along paths dual to\nthe same triangles. It can depicted dually as a single vertex with four semi-links outgoing from\nit (each dual to one of the four triangles of the tetrahedron).\nFigure 2:\nA spin network vertex, with specific choice of labels, dual to a 3-simplex (tetrahedron).\nThe discrete geometric interpretation is confirmed by the action of geometric operators encod-\ning the tetrahedral geometries. For example, elements of the canonical basis in V ji\nv diagonalize\nthe area of the corresponding triangle, while intertwiners encode information about the volume\nof the whole tetrahedron with given triangle areas. For more details about this quantum geom-\netry, see [? ? ]. Thus, generic states in the (kinematical) Hilbert space of the quantum gravity\nformalisms we consider here can be understood as quantum many-body states built out of this\nsingle-body Hilbert space.\nIn particular, an interesting class of quantum states are those that admit a natural interpre-\ntation as corresponding to quantum tetrahedra glued to one another across shared faces to form\nextended simplicial complexes dual to 4-valent graphs. These are (maximally) entangled states of\nquantum tetrahedra [11, 12], and can be obtained by imposing, on a state corresponding to a set\nof N disconnected quantum tetrahedra |ψ⟩∈HN = NN\nn=1 Hn\nv, the projector Pγ = Q\nAixy=1 P xy\ni ,\nwhere Ai\nxy is the adjacency matrix of the graph γ ((x, y) label the pairs of vertices in the graph,\nand the additional index i runs trough the possible multiple links connecting the same two\nvertices), and the gluing projector P xy\ni\n: Hx\ni ⊗Hy\ni →Inv (Hx\ni ⊗Hy\ni ) imposes maximal entangle-\nment along the corresponding degrees of freedom of the two semi-links one intends to connect,\nby tracing over the corresponding SU(2) labels, and thus imposing (diagonal) SU(2) invariance.\nThe resulting state for the graph γ will be a (linear combination of) spin network(s) living\nin the Hilbert space\nHγ =\nM\n{j}\n\nO\n{v}\nI\n⃗|⊑O\n{e}∈∂γ\nV je\n\n\n(1.2)\nwhere we considered the general case of a graph including some open (semi-)links {e}, with all\ngraph links labelled by an irrep of SU(2) and graph vertices labelled by an intertwiner between\nthe associated irreps.\nThese spin network states constitute the kinematical Hilbert spaces of several quantum grav-\nity formalisms: canonical loop quantum Gravity, spin foam models, simplicial quantum gravity\nand tensorial group field theories (for quantum geometric models within this broader frame-\nwork). The difference between these formalisms lies in how the single-graph (or before that, the\nsingle-vertex) Hilbert spaces are embedded into the full Hilbert space that includes all possible\n4\n\n\nFigure 3: The gluing of two spin network vertices (equivalently, two quantum tetrahedra) obtained by\nthe imposition of maximal entanglement across the shared semi-link degrees of freedom (equivalently, the\nquantum data associated to a triangle on their boundary).\ngraphs and all possible numbers of vertices (as necessary to include the infinite number of de-\ngrees of freedom one would a priori expect in a full quantum theory of gravity. For example, in\ngroup field theory, this can be given by a Fock space of quantum tetrahedra. Generically, in all\nsuch formalisms generic states are thus superpositions of (open) spin network states, including\na superposition of graph structures.\nTo appreciate further the role of entanglement in these quantum gravity states, it is interest-\ning to point out a minimal version of entanglement/geometry correspondence in their structure\n[12]. We have already seen how entanglement is directly encoding graph connectivity (simpli-\ncial adjacency relations) and thus the only topological information, in fact, that is encoded in\nsuch quantum gravity states, absent any embedding of the graphs inside continuum manifolds.\nMoreover, a local measure of entanglement between simplices glued across a shared face is given\nby the dimension of the Hilbert space of shared states, i.e. the Hilbert space associated to the\nirrep j associated to the dual link: D = 2j + 1; in fact, this is also how the quantum area\nof the same triangular face scales, upon quantization of the classical area function (”entangle-\nment/area correspondence”). Further, one can ask what is the entanglement between the four\ntriangles/links associated to the same tetrahedron/vertex and the simplest measure is again the\ndimension of the corresponding Hilbert space of states, which scales like the intertwiner label;\nin turn, this scales like the quantum volume of the tetrahedron, obtained again by quantizing\nthe corresponding volume function (”entanglement/volume” correspondence).\nBefore we summarize a few recent results exploiting this entanglement structure, it is also\nworth emphasizing that such quantum gravity states can be understood in two additional ways\nthat make their quantum information theoretic nature manifest.\nFirst, they can be understood as generalised tensor networks, of the kind that have been\ncentral in much recent literature in quantum many-body physics, entanglement renormalization,\nnumerical simulations of many-body systems, lattice gauge theory, neural networks, quantum\ncomputing, AdS/CFT correspondence and more [13, 14, 15, 16, 17, 18]. More precisely, they are\ngeneralised Projected Entangled Pair States (PEPS), the generalization corresponding to the\nfact that the link bond dimension, normally held fixed and equal in all links of the network and\nher3e corresponding to the dimension of the assigned irreps of SU(2), is dynamical and assigned\nindependently in each link, and to the fact that a generic state is actually a superposition of\ntensor networks with given combinatorics and bond dimensions, with the superposition affecting\nalso the combinatorial structure (one has a superposition of different graphs).\nSecond, they can be reformulated as defining quantum circuits [19] (see also [20]). Consider a\nspin network state associated to an oriented graph with a number of open links, and consider the\ncorresponding spin network wavefunction, depending on group elements (holonomies) associated\n5\n\n\nwith the bulk links, in the corresponding irreps of SU(2). This wavefunction can be seen as a\nboundary-to-boundary map, from the Hilbert space corresponding to the tensor product of\nrepresentation spaces for the incoming boundary links to the one corresponding to the tensor\nproduct of the representation spaces for the outgoing boundary links:\nψ({ge}e∈γ) :\nO\ne∈∂γ,t(e)∈γ\nVje →\nO\ne∈∂γ,s(e)∈γ\nVje .\n(1.3)\nThis map defines a quantum circuit for the degrees of freedom living in the boundary Hilbert\nspaces, with holonomies playing the role of unitary one-spin gates and intertwiners being instead\nmulti-spin gates.\nFigure 4: The quantum circuit corresponding to a spin network wavefunction; holonomies are one-spin\nunitary gates, while intertwiners are multi-spin gates.\nThis reformulation is intriguing as it is potentially useful for further applications of quantum\ninformation ideas in fundamental quantum gravity.\n1.2\nHolographic maps and quantum channels from quantum\ngravity states\nThe correspondence between spin network states and generalised tensor networks has been ex-\nploited in a number of works, starting from [21, 16], and then in [22, 23, 24] and more recently\nin [25, 26, 27]. Here we give a brief summary of the last set of results. For other related results\nwith a similar formal setting and goals, although a slightly different perspective, see also [28].\nThe starting point is to consider quantum spin network states associated to a generically\nopen graph (which is held fixed in the following), of the general form:\n|φγ⟩=\nM\n{j}\nX\n{n}\nX\n{ι}\nφ{j}\n{n},{ι} Pγ\nO\nv\n|{jv}, {nv}, ιv⟩\n(1.4)\nwhere we have highlighted its construction from a product basis built from the one-body Hilbert\nspaces, but kept generic the assignment of irreps j for each link and intertwiner labels ι for each\nvertex, as well as the vector indices in each representation space.\nGiven such states, one can identify two subsets of data: the spin labels and magnetic numbers\nassociated to the boundary links of the graph, which we may call boundary dofs, and the spin\nlabels and intertwiner labels associated with the internal links and with the vertices of the graph,\nrespectively, which we call bulk dofs.\nIn terms of this partition, we are interested in defining two types of maps for a given quantum\nstate: bulk-boundary maps, and boundary-boundary maps, where the name indicates their\ndomain and target. In the first case, we are interested in particular in the condition that make\nsuch bulk-boundary maps holographic; in the second, we aim also to identify the conditions that\nwould characterize such boundary-boundary maps as good quantum channels of information.\nIn order to apply more straightforwardly tensor network techniques, we restrict to a subclass\nof quantum states, whose modes factorise per vertex, i.e. φ{j}\n{n}{ι} = Q\nv(fv)jv\nnv,ιv.\n6\n\n\nFigure 5: A schematic representation of a quantum state associated to an open (4-valent) graph.\nConsider the first issue. To simplify the analysis, we fix the bulk spins to specific values\ncollectively labelled J. This leads to a decoupling of boundary and remaining bulk dofs. Thus\nthe relevant Hilbert space factorizes as: HJ = N\nv I⃗jv N\ne∈∂γ V je. Now we can define a map\nMφγ, depending on the chosen quantum state, between the two bulk and boundary sub-spaces,\nmapping a generic bulk state |ζ⟩= P {ι}ζ{ι}|{ι} to the boundary state |φ∂γ(ζ)⟩= ⟨ζ|φγ⟩, which\nis clearly fully specified by the chosen quantum state for bulk+ boundary.\nAs a proxy condition for holographic behaviour, we take the isometry of this map, i.e. the\ncondition: M†\nφγMφγ = I, where I is the identity in the bulk subspace.\nOne can then show that the isometry condition is satisfied if and only if the reduced bulk\ndensity matrix ρbulk = Tr∂γ\nh\n|φγ⟩⟨φγ|\nQ\nv Djv\ni\nis maximally mixed, i.e. it has maximal entropy. In turn\nits entropy can be estimated, in terms of Renyi entropies, via standard randomization techniques\napplied to tensor networks [29, 30], which allow to translate the problem of maximizing the\nentropy of the reduced density matrix of the quantum system into that of minimizing the free\nenergy of a dual Ising model.\nIn our case, the same randomization method shows that, in\nthe regime in which spins are large (naively, a semiclassical regime), the boundary-bulk map\ndefined by our quantum gravity state is, roughly speaking, the more isometric (holographic)\nthe more inhomogeneous is the assigment of spin labels. The precise mathematical conditions\ncan be found in [25]. For related work, although relying on different methods, see [19]. This\nis interesting, because it may indicate an avenue for a microscopic, quantum gravity realization\n(and explanation?) of holography in a non-spatiotemporal and information-theoretic context.\nConsider now the second issue, i.e. transmisison of information and the entanglement be-\ntween two portions of the boundary, for the same quantum state and within the same restrictions\n(fixed bulk spins, fixed graph, factorized state). We partition the boundary dofs into two com-\nplementary sets A and ¯A, and look at the reduced density ρA = Tr ¯\nA[ρ] for the region A, where\nρ is the density matrix for the full quantum state φγ.\nWe are interested in the entanglement entropy between the two subregions of the boundary.\nAs a proxy for it, we can compute the 2nd Renyi entropy of the reduced density matrix, using\nagain the same random tensor network techniques, and thus the same dual ising model. The\ncalculation can be performed for both the homogeneous, same-spin case (all bulk spins assumed\nequal), and the inhomogeneous one.\nFrom the calculation, in the case of vanishing bulk (intertwiner) entropy, one obtains an\nexact Ryu-Takayanagi-like formula\nS(ρA)2 ≃K(j, γ)minΣA|ΣA|\n(1.5)\nwhere K is a factor depending on the details of the bulk spin assignment and |ΣA| is the\nsize (i.e. the number of crossing links) of the minimal surface in the bulk separating the two\nboundary regions.\n7\n\n\nFigure 6: Partitioning the boundary links into two subsets\nFigure 7: RT-like behaviour for the entanglement entropy between boundary subregions\nWhen the bulk (intertwiner) entropy is not negligible, one finds that the RT formula acquires\na correction term measuring the bulk contribution\nS(ρA)2 ≃K(j, γ)minΣA|ΣA| + Sbulk\n(1.6)\nwhich can also be computed. Interestingly, when the bulk entropy increases, a smaller and\nsmaller portion of the RT surface enters the bulk regions, and, under the same increase, when\nthe boundary region A tends to occupy the whole boundary, the RT surface tends to coincide\nwith the boundary of this high-entropy bulk region.\nThis is closely reminiscent of a black\nhole horizon, whose surface coincides with the RT surface, in the continuum geometry picture,\nand which encloses a maximal entropy bulk region.\nThis is intriguing because it suggest a\npossible realization of holographic behaviour and of effective black hole geometries in a non-\nspatiotemporal and information-theoretic context.\nIn trying to perform the same type of analysis for quantum gravity states involving a sum\nover spins, thus a sum over (discrete) quantum geometries, one has to face the issue that the cor-\nresponding Hilbert space does not present any obvious factorization between bulk and boundary\ndofs or between subsets of boundary ones (but it possesses a direct sum structure with respect\nto the possible spin assignments to the whole graph). The very notion of entanglement between\n8\n\n\nFigure 8: Black hole-like behaviour for the entanglement entropy between boundary subregions, when bulk\nentropy grows and the boundary region is extended\nany subset of dofs becomes ambiguous. If one tries to bypass this ambiguity by embedding the\nHilbert space into a larger, factorized one and work with a more clearly defined notion of sub-\nspaces and entanglement there, the ambiguity presents itself again in the choice of embedding.\nOne way to proceed is to work at the level of algebras of observables (acting on the given Hilbert\nspace of quantum gravity states) and to identify a notion of subsystem in terms of subalgebras,\nrather than subspaces. Moreover, holographic behaviour is then best characterized in terms of\n‘transmisison of information’, rather than entanglement scaling. This strategy has been applied\nfor quantum gravity states in [27], and outlined in its general formal aspects in [31].\nHere we just outline the main steps in the analysis.\nGiven the full algebra, one can identify algebraic subsystems, i.e. two subsets of observables\nseparately ‘testing’the two subsets of quantum dofs one is interested in mapping, to be considered\nas ‘input’and ‘output’(it could be bulk and boundary or two portions of the boundary). These\nare not subalgebras, in general.\nOne can then define a map between input-output operator\nspaces via the Choi-Jamiolkowski isomorphism.\nThe isometry condition on such map, our proxy for holography or complete transmission of\ninformation, can be shown to follow from a certain set of necessary condition, corresponding to\nthe requirement that the operator map defines a quantum channel.\nThese conditions can be identified using the same random tensor network techniques, in the\nregime of large spins (which minimizes key quantum fluctuations), and translated again also in\nconditions on the entropy of the underlying quantum gravity state.\nOne obtains again a Ruy-Takanayagi-like formula for the Renyi entropy of the quantum state\ndepending on a set of minimal surfaces, one for each spin sector, thus one for each superposed\nquantum geometry, and with a definition of ‘area spectrum’for the minimal surfaces that can be\nrelated to but differs from the one used in canonical loop quantum gravity or simplicial quantum\ngeometry.\nIn general, the necessary conditions for isometry amount to have negligible correlations be-\ntween boundary data and intertwiner bulk data, and on specific peaking properties of the quan-\ntum state around a subset of spin sectors. These same conditions may actually become also\nsufficient ones, upon additional constraints on the quantum states.\nTo summarize, for both boundary-bulk and boundary-boundary maps, holographic behaviour\nappears to require the bulk Hilbert space to be comparatively small with respect to the boundary\none (as measured by the dimensionality of their respective subspaces), and the total boundary\narea (scaling with the size of spin spaces assigned to its links) to be approximately constant\nacross different subsectors of spin assignments.\n9\n\n\nAgain, the importance of these results is not so much in the details of their conclusions, but in\nthe very fact that (quantum) geometric properties and quantum information theoretic properties\nare deeply intertwined and can be studied also in a non-spatiotemporal, purely combinatorial and\nalgebraic context, hopefully shedding light on the emergence of holographic (and gravitational)\nbehaviour at macroscopic scales as well as on its fundamental origin.\n2\nQuantum gravity processes as quantum causal his-\ntories (or not)\nWe now turn to the quantum dynamics of the quantum gravity structures we considered in the\nprevious section, and that we characterized in quantum information-theoretic terms. Our main\npoint is that a similar quantum information-theoretic characterization can be provided also for\nthe quantum gravity processes they are subject to, and that information theoretic tools can be\napplied to the analysis of their (quantum) causal properties. As in the previous section, we\nconsider a subset of quantum gravity formalisms, sharing many of their constitutive structures,\nand focus on their shared elements, rather than their differences.\n2.1\nQuantum causal processes of atoms of space - quantum\nspacetime as a quantum circuit\nA general scheme for quantum processes respecting minimal causality conditions, and to which\nwe can try to fit or adapt fundamental dynamical processes for our ‘atoms pf quantum space’,\nis represented by the formalism of quantum causal histories. The version we refer to here is the\none in [32, 33], with its initial development in a quantum gravity context to be found in [34, 35].2\nPossible dynamical processes are given by a set of ‘events’together with an order relation\nbetween pairs of them; these are also the constitutive elements of a directed graph.\nIn 4d\nquantum gravity models based on (quantum) simplicial geometry, fundamental events may be\ntaken to correspond to 4-simplices, while order relations between pairs of them correspond to\ntheir shared 3-simplices. The directed graph would then correspond to the dual 1-skeleton of\nthe oriented simplicial 4-complex. Note that this realization implies a restriction to 5-valent\ndirected graphs. For Lorentzian models, the order relations can be given a causal interpretation.\nAn important special case is represented by partially ordered sets (posets), which are directed\ngraphs which are also irreflexive, i.e. do not contain closed causal loops. Posets are also called, in\nthe quantum gravity literature, causal sets and are the basic entities in the causal set approach\nto qauntum gravity [41].\nClearly, this structure can be decomposed into elementary ‘evolution steps’, corresponding to\nthe possible orientation assignments of the 5-valent nodes: 5 links outgoing, 1 link incoming/4\nlinks outgoing, 2 links incoming/3 links outgoing, and their inverses.\nThe quantum process corresponding to each directed graph is obtained by an assignment of\nHilbert spaces to the links (and tensor products of Hilbert spaces for unordered sets of links) and\nelementary ‘evolution’operators to the nodes; in addition one can include also ‘gluing operators’to\nthe links, enforcing a prescription for the ‘transmission of information’ from one node/event to\nanother 3.\n2Of course, the abstract characterization of quantum dynamics and quantum causality, as well as its generalization\nto a context in which geometry and thus causal relations are themselves dynamical and subject to superposition, is\na hot topic in quantum foundations, with many recent developments. See [36, 37, 38, 39, 40] for a small sample.\nObviously, we are not going to review any of that.\n3In fact, a more complete and consistent definition of a quantum process in this language, of of a quantum causal\nhistory in particular, is given in terms of an assignment of algebras of operators and completely positive maps [33].\nWe give here a simplified earlier construction, which is sufficiently indicative of the general points we want to make.\n10\n\n\nFigure 9: An example of a 5-valent directed graph\nFigure 10: An example of an elementary quantum process\nA specification of a quantum dynamics and thus a specific quantum gravity model corre-\nsponds to the assignment of a quantum (probability) amplitude to each quantum process (to-\ngether with any restriction on the class of allowed processes, be it on the combinatorial structure\nor the associated operators and Hilbert spaces). Such amplitude is defined by the chosen node\noperator and gluing operator, in turn characterized by the corresponding kernels\nVn : ⊗\ne∈∂nHe\nv −→C\nPe : He\nv ⊗He∗\nv −→C\nwhere we have left implicit the dualization of the Hilbert spaces required to reflect the different\norientations of the links associated to the nodes.\nNotice that the specification of a gluing\noperator requires a ‘doubling’of the Hilbert space associated to the links of the directed graph,\ndistinguishing one copy of it associated to one of the connected nodes from the (dualized) one\nassociated to the other connected node4. Given these building blocks, the quantum amplitude\nassociated to the given process Γ can be defined as:\n4Notice that this doubling is necessary, if one wants to allow for a composition of processes or, from the point of\nview of their dual cellular complexes, the composition of different cellular complexes along shared boundaries.\n11\n\n\nA(Γ) = Tr\ne∈Γ\n Y\ne∈Γ\nPe\nY\nn∈Γ\nVn\n!\n(2.1)\nwhere with Tr we have indicated the trace operation over the (doubled) Hilbert spaces\nassociated to the links of the process. Different quantum gravity formalisms (notably canonical\nloop quantum gravity, spin foam models, lattice gravity path integrals and group field theories)\nshare this general structure of their dynamical quantum amplitudes, in a covariant language.\nFor example, for a more detailed presentation in the case of spin foam models (adapted also to\nthe group field theory language) see [42], in addition to the existing introductions to all these\nquantum gravity formalisms.\nIn fact, in quantum gravity formalisms that use this kind of discrete structures, the complete\nquantum dynamics should be completed with the definition of a continuum limit/approximation,\nsince reliance on any given cellular complex represents at best a truncation of the full set of quan-\ntum degrees of freedom of the fundamental theory. Also in accordance with the superposition\nprinciple and the interpretation of the above discrete structures as possible quantum processes,\nthis limit is encoded within a sum over all complexes/processes within the allowed class. The\nfull quantum dynamics is therefore encoded in a partition function (or transition amplitude, if\nthe complexes have boundaries) of the general form:\nZ =\nX\nΓ\nw(Γ)A(Γ)\n(2.2)\nwhere again the specification of the additional weights w(Γ) concurs to the definition of the\nspecific model being considered.\nTwo points are worth emphasizing about this construction. First, also these quantum pro-\ncesses, like the quantum states discussed in the previous section, can be reformulated in the\nlanguage of tensor networks, and this has been applied to the study of renormalization of quan-\ntum gravity dynamics from a lattice gauge theory perspective [43]. Second, at this level, thus\nbefore any extraction of coarse-grained or otherwise effective continuum dynamics, the connec-\ntion of these dynamical quantum amplitudes to that of gravitational theories can only be with\nlattice gravity, i.e. some discretized version of gravitational physics. This connection depends\nstrongly on the specific quantum gravity formalism being considered. In spin foam models, thus\nalso in group field theories, the connection can be shown (modulo remaining technical open is-\nsues) in the form of an exact equivalence with lattice gravity path integrals for 1st order gravity\ntheories5 [42], or in a semi-classical, asymptotic regime with 2nd order gravity [47] (in terms of\narea variables [48]).\nWe are interested in identifying the requirements on such quantum processes that make them\nbona fide quantum causal histories, that is, that make each quantum process encore a properly\ncausal unitary evolution. We do not discuss here to what extent this is something we should\nexpect or require, from fundamental quantum gravity processes, besides a few short remarks.\nThe required properties have been identified in [32, 33]. Consider the evolution operator\nEαβ : Hα →Hβ, obtained by composing all the elementary operators V connecting two complete\na-causal subsets α and β of links, that is subsets of links that are all causally unrelated within\neach subset and one the complete causal future (past) of the other, where the Hilbert spaces\nHα,β are the tensor product of the Hilbert spaces associated to the constituting links. For the\nquantum process to define a quantum causal history, the operators Eαβ should be: a) reflexive:\nEαα = Iα, b) antisymmetric: EαβEβα = Iα ⇔Eαβ = Eβα = Iα; c) transitive: EαβEβγ = Eαγ;\nd) unitary: P\nβ EαβE†\nαβ = P\nβ Eαβ ¯Eβα = Iα.\n5Usually, with gravity formulated as a constrained topological theory [44]. But see [45, 46] for models constructed\nfollowing alternative strategies.\n12\n\n\nOne can then check or impose that the relevant evolution operators of quantum gravity\nmodels of interest satisfy these requirements. However, one could also question their necessity.\nIndeed, we have stated that the full quantum gravity dynamics should rather be given by a sum\nover all such quantum processes. Therefore, one could argue that the operators satisfying proper\ncausality conditions are those obtained by performing this sum, that is:\nEαβ =\nX\nc\nλcEc\nαβ : Hα −→Hβ\n(2.3)\nbetween the same complete a-causal subsets. In other words, even accepting that the fundamen-\ntal quantum gravity dynamics should be expressed in the language of quantum causal histories,\none could argue that it is the full transition amplitudes that should satisfy causality constraints,\nand not the possible individual quantum processes.\nIt is easy to see [49] that these two perspectives are not equivalent, and in fact, they are\nnot even consistent with one another.\nMore precisely, while imposing ‘micro-reflexivity’(i.e.\nreflexivity of individual processes) implies full reflexivity (i.e. reflexivity of the evolution obtained\nupon summing over micro-processes) and the same is true for antisymmetry, the other two\nrequirements are much more problematic. First, one could make micro-transitivity compatible\nwith transitivity of the full evolution, by defining the latter more generally as P\nβ EαβEβγ = Eαγ,\nwhich is just the standard composition of quantum (transition) probability amplitudes; but\nmicro-transitivity itself appears to be too strong a requirement from the discrete quantum gravity\nperspective; indeed, taking into account also the dual lattice formulation of quantum processes,\nmicro-transitivity is equivalent to a condition of partial triangulation invariance of the quantum\ndynamics, i.e. a requirement that the quantum amplitudes associated to individual lattices are\npartially independent of the chosen lattice. To the extent in which gravity is not a topological\nfield theory with no local propagating degrees of freedom, this is a suspicious condition since it\nmay imply a partial triviality of the quantum dynamics. Second, one can actually verify that\nunitarity of the full evolution implies that the micro-evolution must not be unitary. So one has\nto make a choice.\nIt remains a valid goal to have the full quantum gravity dynamics, obtained via a sum over\nelementary quantum processes, defining a (‘coarse-grained’) quantum causal history. This would\nbe interesting, from the perspective we are exploring in this contribution, because it would mean\nthat quantum information and quantum computation can be found at the very heart of quantum\ngravity also at the dynamical level. The same interest would remain if instead one insists on the\nelementary processes be themselves quantum causal histories.\nIndeed, it is a general result [50] that a quantum causal history admits a unitary evolution\nbetween its acausal surfaces if and only if it can be represented as a quantum computational\nnetwork, i.e. a quantum circuit.\nThe idea of spacetime as a quantum circuit would then find a concrete realization, if quantum\ngravity evolution can be formulated as in terms of quantum causal histories, at the elementary\nor coarse-grained level, in the above language or in the more refined one of observable algebras\nand CP maps [33]. See also [51, 52] and the cited literature on process matrix formalism and\nindefinite causal structures for related directions.\nLet us now look in more detail at a couple of features of quantum gravity processes that have\nto be realized in order for them to define quantum causal histories, thus quantum circuits, and\nsee which obstacles one has to face to do so.\n2.2\nCausal hiccups and causal indifference in QG processes\nAs discussed, a proper representation in terms of quantum circuits requires: a) the absence or\n“irrelevance”of causal loops; b) suitable conditions ensuring unitarity of the evolution operators.\n13\n\n\nConcerning closed causal loops, there are three possible strategies that can be followed in\nconstructing quantum gravity models: a) define a quantum dynamics (amplitudes) that elimi-\nnates causal loops altogether; b) define a quantum dynamics (amplitudes) that suppresses causal\nloops, by assigning them subdominant contributions, in the relevant regimes; for example, one\ncould consider admitting causal loops in the theory, provided they do not spoil expected semi-\nclassical or continuum physics; c) define a quantum dynamics (amplitudes) that only allows\n“harmless”. Let us stress that the directed graphs underlying all current spin foam models and\nlattice gravity path integrals (or group field theory perturbative amplitudes) contain closed loops\nof order relations, i.e. causal loops. Thus the issue is of concrete relevance for such quantum\ngravity formalisms. While the first two options may be technically very challenging but are\nconceptually straightforward, when exactly a causal loop is physically harmless requires a more\ncareful analysis. The issue has been studied for example in [50], in the quantum gravity context,\nfollowing earlier work by Deutsch [53] in a general quantum mechanical context. The upshot\nis that causal loops are either entirely disruptive or entirely harmless, to paraphrase.\nMore\nprecisely, they either prevent the standard formulation of quantum mechanics to be applicable\nor, when they do not, they lead to no observable changes, since they simply end up contributing\nan extra subspace to the ordinary causality-respecting system. Moreover, by applying suitable\n(Deutsch) criteria, the causally well-behaved region of the process decouples entirely from the\ncausal loop, if the quantum dynamics remains linear, and if it does not stay linear, then the\ncausal loop does not carry independent degrees of freedom. For more details we refer again to\n[50].\nLet us now discuss unitarity of quantum evolution and, before that, the very dependence of\nthe quantum gravity transition amplitudes from the order of their arguments, thus a notion of\npast/future relating the quantum states it depends on, which is in many ways a prerequisite for\nit. The issue is: given two (‘initial’and ‘final’) quantum states, which kind of quantum amplitude\ndo we define, via the gravitational path integral? Let us first recall a few facts about quantum\ngravity path integrals, which can be verified at the formal level in great generality [54, 55], and\nhave to be then realized concretely in more rigorous manner by quantum gravity approaches,\nincluding the ones based on discrete structures that we have focused on here. Different quantum\ngravity amplitudes can be defined starting from the same proper-time truncation of the full path\nintegral (in canonical form), obtained via appropriate gauge-fixing of the general expression:\nK[h2\nij, h1\nij; N(τ2 −τ1)] =\nZ\nDhij(x, τ)Dπij(x, τ)eiS(hij,πij);N\n(2.4)\nwhere the amplitude depends on the fixed metric data on the two (past/future) boundaries.\nThis expression could be required to corresponds to (the matrix elements of) a unitary evolution\noperator in proper time. However, this is not the physical (transition) amplitude for quantum\ngravity, since the lack of integration over proper time (lapse) means that we have not yet imposed\nany conditions encoded in the Hamiltonian constraint of the (canonical) theory, thus no full\nquantum gravitational dynamics (the Einstein’s equations are indeed encoded in the Hamiltonian\nconstraint), beside enforcing at best some semiclassical restriction (due to the appearance of\nthe gravitational action in the expression). One can then define a ‘causal’transition amplitude\n(the analogue for quantum gravity of what would be, for a relativistic particle, the Feynman\npropagator, by integrating the lapse (proper time) over the full positive range:\nK[h2\nij, h1\nij] =\nZ N(x)=+∞\nN(x)=0\nD[N(x)(τ2 −τ1)] K[h2\nij, h1\nij; N(τ2 −τ1)] .\n(2.5)\nThis is the canonical counterpart of the straightforward Lagrangian gravitational path inte-\ngral K[h2\nij, h1\nij] =\nR\nDgµνeiS(gµν). it is invariant under Lagrangian (covariant) diffeomorphisms\nand indeed switches to its own complex conjugate under switch of spacetime orientation; in\n14\n\n\nother words, it does register an ordering between its two arguments, the boundary quantum\nstates. It does not, however, give a solution to the canonical Hamiltonian constraint, i.e. it is\nnot invariant under canonical symmetries (the canonical Dirac algebra, counterpart of covariant\ndiffeomorphisms, which are a subset of the canonical ones). A solution of the canonical Hamilto-\nnian constraint is instead obtained by integrating the lapse (proper time) over the full (positive\nand negative) real values:\nC[h2\nij, h1\nij] =\nZ N(x)=+∞\nN(x)=−∞\nD[N(x)(τ2 −τ1)] K[h2\nij, h1\nij; N(τ2 −τ1)] .\n(2.6)\nThis indeed defines (formally) a physical scalar product between canonical quantum gravity\nstates, solving all the constraints of the theory (or, equivalently, the matrix elements of the\nprojector operator onto such solutions). Its Lagrangian counterpart would look like C[h2\nij, h1\nij] =\nR\nDgµν\n\u0002\neiS(gµν) + e−iS(gµν)\u0003\n=\nR\nDgµν cos(S(gµν)) . This quantity does not register the space-\ntime orientation and it is symmetric under its switch, not encoding any ordering among it\narguments, the ‘initial’and ‘final’quantum states.\nSpin foam models (equivalently, the perturbative transition amplitudes of group field theo-\nries) aim to be discretized and thus mathematically better defined realization of the gravitational\npath integral. Which of the above quantities do they actually realize?\nAll the most studied spin foam models are discrete counterparts of the path integral for\ngravity formulated as a constrained topological BF theory. It turns out that, like their con-\ntinuum counterpart (and the path integral for topological BF theory itself) they are invariant\nunder switch of spacetime (lattice) orientation, more precisely the inversion of the orientation\nof their constitutive simplicial structures; this invariance is in fact realized locally at the level\nof each node or 4-simplex contribution V to the total amplitude A as well as at the level of\nlower-dimensional simplices (eg triangles) in the simplicial complex. Recall that the 1-skeleton\nof this simplicial complex corresponds to the directed graph whose order relations have a ten-\ntative causal interpretation (in Lorentzian models). The orientation independence of the spin\nfoam amplitudes thus implies than none of the most studied spin foam models defines a proper\nquantum causal history (a quantum circuit) and a unitary quantum gravity dynamics. This\nwould remain true even if one was able to remove causal loops from the underlying directed\ngraph.\nIt is possible to construct ‘properly causal’spin foam models, but a suitable restriction of their\namplitudes so that they register faithfully the orientation of the underlying complex. This has\nbeen done first, for the Barrett-Crane model, in [49], and more recently by similar procedures for\nthe EPRL model in [56, 57]. These restricted models are therefore candidates for the realization\nof the ‘causal propagator’for quantum gravity, and for a formulation in terms of quantum causal\nhistories and quantum circuits. However, these causality-inspired constructions are all rather ad\nhoc and we still lack a systematic construction procedure of spin foam models from first principle\n(rather than by restricting by hand a-causal models) taking into account causality restrictions,\nas well as a more complete analysis of the properties of the present ‘ad-hoc’ones.\nConclusions\nWe have argued that both semi-classical considerations and quantum gravity formalisms suggest,\nin different ways, that spacetime and gravity may be emergent, collective, not fundamental\nnotions, and that the universe may be a (peculiar, background independent) quantum many-\nbody system of pre-geometric quantum entities, some yet to be unraveled ‘atoms of space’.\nIn particular, there are intriguing indications that topology and geometry may emerge from\nthe entanglement among such fundamental quantum entities.\nMore generally, an intriguing\n15\n\n\npossibility is that quantum spacetime physics may be formulated, in its most fundamental level,\nentirely in the language of quantum information.\nA variety of quantum gravity formalisms share the same combinatorial and algebraic quantum\nstructures as quantum states: quantized simplicial structures and spin networks.\nWe have\noutlined the ways in which such quantum states can be described in quantum information-\ntheoretic terms. More precisely, we have summarized how these quantum states: a) can be seen as\ngeneralised tensor networks and realize a precise discrete entanglement/geometry (and topology)\ncorrespondence; b) can be framed as information channels (or quantum circuits); c) can be\nused to define bulk/boundary and boundary-to-boundary maps, for which one can then identify\nconditions for holographic behaviour. This could indicate an avenue toward understanding the\nmicroscopic origin of holographic behaviour in quantum gravity.\nWe have then discussed how, in the same quantum gravity formalisms, dynamical quantum\nprocesses can be recast as quantum causal histories, provided some key properties are imple-\nmented in their amplitudes, and then again as quantum circuits; we have also pointed out some\nof the challenges faced to implement the required properties Again, the main point is that quan-\ntum information tools and language may be the appropriate ones to formulate also the quantum\ndynamics of the microscopic constituents of the universe, when geometry and fields fail.\nBefore we go on to comment on some more conceptual aspects of the these conclusions, we\npoint out that the (tensorial) group field theory framework, on top of providing a completion of\nlattice gravity path integrals and spin foam models (thus sharing the same quantum amplitudes)\nand a 2nd quantized framework for spin network states (thus a convenient Fock space structure\nfor their Hilbert space) [58, 59], provides also a number of almost standard field theoretic tools\nto study them and in particular to extract effective continuum gravitational physics from them\n[60]. This means that, even in this more abstract, non-spatiotemporal, pre-geometric context,\none can apply quantum field theoretic techniques to the quantum information structures we\npresented above, to analyse their formal properties and to unravel their physical meaning.\nSome philosophical considerations: is the universe a quantum computer? - To con-\nclude let us offer some thoughts on different ways in which we could interpret the relevance\nof quantum information language and tools for encoding the fundamental microstructure of\nspacetime and the universe.\nA straightforward attitude is to give to this fact an ontological basis: the universe is a quan-\ntum computer. In this case, the fact that quantum information is the appropriate language to\nunderstanding is no surprise: it is just the language representing how it fundamentally operates,\nit is the language that constitutes its basic laws. From the epistemological point of view, this\nattitude follows from and it is grounded on a straightforward scientific realism: the world is out\nthere and entirely independent, in its properties, of our epistemic activities, which achieve (at\nbest) a faithful (albeit partial) representation of the way the world is. It is also tied to a realist\nand ontologically committed view on laws of nature: they are what governs the physical world,\ni.e. the rules by which it functions and evolves.\nChallenges against all the above are numerous and the philosophical debate about each of\nthe above points is old and intricate and interesting. Here we want to make two brief comments\nabout this ‘the universe is a quantum computer’view, implicitly based on the attitude we just\nsummarized.\nA first immediate one is that all the realist views on laws face challenges at different levels\nfrom quantum gravity, and in particular from the very possibility of space, time and geometry\nbeing emergent notions. The quantum information structures we discussed in this contributions\ncan be seen as encoding these challenges, but at the same time offering tentative ways to meet\n(if not solve) them. This is discussed in [61].\nA second one is that one can adopt (and try to develop further) a less ontologically committed\nview on the object of our scientific theories, i.e. the physical world, including laws of nature, and\n16\n\n\nthus a weaker version of (scientific) realism. One can adopt a more epistemic view on physical\nlaws based on a more substantial role given to epistemic agents, taken to be irreducible and not\nnegligible (outside convenient idealizations). This epistemic, agent-based view would tie well\nwith a more participatory form of realism, in which what is real is, roughly speaking, only the\nresult and content of the interaction between the world and the epistemic agents, none of which\nis independently real outside of such interaction. This weaker form of realism would be the\nonly kind of ontological commitment allowed by the epistemic premises. This view may have\nimplications for (and be tested with) the interpretation of quantum mechanics, as well as the\nconstruction and interpretation of theories of quantum spacetime and geometry.\nMore generally, it would lead to a view in which the universe is (to a large extent) what we\nthink it is (or what we model it as), in the sense that it is our epistemic constructions that make\nreality, rather than simply represent it. Obviously this is just a vague statement, to be made\nmore precise and articulated, but it is maybe interesting to see how a similar view changes how\nwe may interpret the role of (quantum) information theoretic structures in fundamental quantum\ngravity. In a fundamental quantum gravity context, we argued, we have no spacetime notions\nto rely on; we have to think the world (and model it) without spacetime. We are then left with\ncombinatorics and algebra as mathematical language, and with information processing, rather\nthan definite, ontologically grounded objects (whose ontological characterization would normally\nassume space and time), as the only ‘dynamical’content. This reflects the more basic, more\nirreducible structures in our thinking, which at the same time correspond (given the above view\nof what it is to be ‘real’) to the more basic structures ‘in the world’. (Quantum) Computers are\nabstract models of (quantum) information processing, and of our own thinking. To the extent in\nwhich the universe is (largely) what we think it is (in the sense outlined above), and we think like\n(quantum) computers, it is not so surprising, perhaps, that the quantum (non-spatiotemporal)\nuniverse is naturally modeled as a quantum computer.\nThese rather vague considerations are offered, here, only as potentially useful indications of\nphilosophical avenues to explore and develop further, on the basis of the scientific developments\nin quantum gravity, that we have summarized in this contribution.\nAckowledgements\nWe acknowledge support through the Grant PR28/23 ATR2023-145735 (funded by MCIN /AEI\n/10.13039/501100011033). We also thank the organizers and the participants of the “QG and\ncomputation”workshop in Sydney for a very interesting event and for many stimulating discus-\nsions, as well as the editors of this volume for their patience.\n17\n\n\nReferences\n[1] E. Bianchi and E.R. Livine, Loop Quantum Gravity and Quantum Information, (2023),\nDOI [2302.05922].\n[2] E. Colafranceschi, Emergent spacetime properties from entanglement, Ph.D. thesis,\nNottingham U., 2022.\n[3] J. Butterfield and C.J. Isham, Space-time and the philosophical challenge of quantum\ngravity, in Physics meets philosophy at the Planck scale: Contemporary theories in\nquantum gravity, C. Callender and N. Huggett, eds., pp. 33–89 (1999), DOI\n[gr-qc/9903072].\n[4] N. Seiberg, Emergent spacetime, in 23rd Solvay Conference in Physics: The Quantum\nStructure of Space and Time, pp. 163–178, 1, 2006, DOI [hep-th/0601234].\n[5] T. Padmanabhan, Emergent Gravity Paradigm: Recent Progress, Mod. Phys. Lett. A 30\n(2015) 1540007 [1410.6285].\n[6] D. Oriti, Levels of spacetime emergence in quantum gravity, 1807.04875.\n[7] S. Carlip, Challenges for Emergent Gravity, Stud. Hist. Phil. Sci. B 46 (2014) 200\n[1207.2504].\n[8] M. Van Raamsdonk, Lectures on gravity and entanglement., in Theoretical Advanced Study\nInstitute in Elementary Particle Physics: New Frontiers in Fields and Strings,\npp. 297–351, 2017, DOI [1609.00026].\n[9] B. Swingle, Spacetime from Entanglement, Ann. Rev. Condensed Matter Phys. 9 (2018)\n345.\n[10] M. Rangamani and T. Takayanagi, Holographic Entanglement Entropy, vol. 931, Springer\n(2017), 10.1007/978-3-319-52573-0, [1609.01287].\n[11] B. Bayta¸s, E. Bianchi and N. Yokomizo, Gluing polyhedra with entanglement in loop\nquantum gravity, Phys. Rev. D 98 (2018) 026001 [1805.05856].\n[12] E. Colafranceschi and D. Oriti, Quantum gravity states, entanglement graphs and\nsecond-quantized tensor networks, JHEP 07 (2021) 052 [2012.12622].\n[13] R. Orus, A Practical Introduction to Tensor Networks: Matrix Product States and\nProjected Entangled Pair States, Annals Phys. 349 (2014) 117 [1306.2164].\n[14] F. Verstraete, V. Murg and J.I. Cirac, Matrix product states, projected entangled pair\nstates, and variational renormalization group methods for quantum spin systems, Adv.\nPhys. 57 (2008) 143.\n[15] J.I. Cirac, D. Perez-Garcia, N. Schuch and F. Verstraete, Matrix product states and\nprojected entangled pair states: Concepts, symmetries, theorems, Rev. Mod. Phys. 93\n(2021) 045003 [2011.12127].\n[16] S. Singh, N.A. McMahon and G.K. Brennen, Holographic spin networks from tensor\nnetwork states, Phys. Rev. D 97 (2018) 026013 [1702.00392].\n[17] L. Tagliacozzo, A. Celi and M. Lewenstein, Tensor Networks for Lattice Gauge Theories\nwith continuous groups, Phys. Rev. X 4 (2014) 041024 [1405.4811].\n18\n\n\n[18] G. Evenbly and G. Vidal, Tensor Network Renormalization, Phys. Rev. Lett. 115 (2015)\n180405.\n[19] Q. Chen and E.R. Livine, Loop quantum gravity’s boundary maps, Class. Quant. Grav. 38\n(2021) 155019 [2103.08409].\n[20] G. Czelusta and J. Mielczarek, Quantum simulations of a qubit of space, Phys. Rev. D\n103 (2021) 046001 [2003.13124].\n[21] S. Singh, R.N.C. Pfeifer and G. Vidal, Tensor network decompositions in the presence of a\nglobal symmetry, Phys. Rev. A 82 (2010) 050301 [0907.2994].\n[22] M. Han and S. Huang, Discrete gravity on random tensor network and holographic R´enyi\nentropy, JHEP 11 (2017) 148 [1705.01964].\n[23] G. Chirco, D. Oriti and M. Zhang, Group field theory and tensor networks: towards a\nRyu–Takayanagi formula in full quantum gravity, Class. Quant. Grav. 35 (2018) 115011\n[1701.01383].\n[24] G. Chirco, A. Goeßmann, D. Oriti and M. Zhang, Group field theory and holographic\ntensor networks: dynamical corrections to the Ryu–Takayanagi formula, Class. Quant.\nGrav. 37 (2020) 095011 [1903.07344].\n[25] E. Colafranceschi, G. Chirco and D. Oriti, Holographic maps from quantum gravity states\nas tensor networks, Phys. Rev. D 105 (2022) 066005 [2105.06454].\n[26] G. Chirco, E. Colafranceschi and D. Oriti, Bulk area law for boundary entanglement in\nspin network states: Entropy corrections and horizon-like regions from volume\ncorrelations, Phys. Rev. D 105 (2022) 046018 [2110.15166].\n[27] E. Colafranceschi, S. Langenscheidt and D. Oriti, Holographic properties of superposed\nquantum geometries, Phys. Rev. D 110 (2024) 046024 [2207.07625].\n[28] C. Akers and A.Y. Wei, Background independent tensor networks, SciPost Phys. 17\n(2024) 090 [2402.05910].\n[29] P. Hayden, S. Nezami, X.-L. Qi, N. Thomas, M. Walter and Z. Yang, Holographic duality\nfrom random tensor networks, JHEP 11 (2016) 009 [1601.01694].\n[30] X.-L. Qi, Z. Yang and Y.-Z. You, Holographic coherent states from random tensor\nnetworks, JHEP 08 (2017) 060 [1703.06533].\n[31] S. Langenscheidt, E. Colafranceschi and D. Oriti, Channel-State duality with centers,\n2404.16004.\n[32] F. Markopoulou, Quantum causal histories, Class. Quant. Grav. 17 (2000) 2059\n[hep-th/9904009].\n[33] E. Hawkins, F. Markopoulou and H. Sahlmann, Evolution in quantum causal histories,\nClass. Quant. Grav. 20 (2003) 3839 [hep-th/0302111].\n[34] F. Markopoulou and L. Smolin, Causal evolution of spin networks, Nucl. Phys. B 508\n(1997) 409 [gr-qc/9702025].\n[35] F. Markopoulou and L. Smolin, Quantum geometry with intrinsic local causality, Phys.\nRev. D 58 (1998) 084032 [gr-qc/9712067].\n19\n\n\n[36] O. Oreshkov, F. Costa and C. Brukner, Quantum correlations with no causal order,\nNature Commun. 3 (2012) 1092 [1105.4464].\n[37] K. Goswami, C. Giarmatzi, M. Kewming, F. Costa, C. Branciard, J. Romero et al.,\nIndefinite Causal Order in a Quantum Switch, Phys. Rev. Lett. 121 (2018) 090503\n[1803.04302].\n[38] V. Baumann, M. Krumm, P.A. Gu´erin and v. Brukner, Noncausal Page-Wootters circuits,\nPhys. Rev. Res. 4 (2022) 013180 [2105.02304].\n[39] L. Mrini and L. Hardy, Indefinite causal structure and causal inequalities with\ntime-symmetry, 2024.\n[40] L. Hardy, Probability theories with dynamic causal structure: A new framework for\nquantum gravity, 2005.\n[41] F. Dowker and S. Surya, The Causal Set Approach to the Problem of Quantum Gravity,\n(2024), DOI.\n[42] M. Finocchiaro and D. Oriti, Spin foam models and the Duflo map, Class. Quant. Grav.\n37 (2020) 015010 [1812.03550].\n[43] B. Dittrich, S. Mizera and S. Steinhaus, Decorated tensor network renormalization for\nlattice gauge theories and spin foam models, New J. Phys. 18 (2016) 053009 [1409.2407].\n[44] E. Bianchi and F. Hellmann, The Construction of Spin Foam Vertex Amplitudes, SIGMA\n9 (2013) 008 [1207.4596].\n[45] A. Baratin, C. Flori and T. Thiemann, The Holst Spin Foam Model via Cubulations, New\nJ. Phys. 14 (2012) 103054 [0812.4055].\n[46] R. Dekhil, M. Laudonio and D. Oriti, A state sum for four-dimensional Lorentzian\nquantum geometry in terms of edge vectors, 2501.10115.\n[47] S.K. Asante, B. Dittrich and H.M. Haggard, Effective Spin Foam Models for\nFour-Dimensional Quantum Gravity, Phys. Rev. Lett. 125 (2020) 231301 [2004.07013].\n[48] B. Dittrich and J. Padua-Arg¨uelles, Twisted geometries are area-metric geometries, Phys.\nRev. D 109 (2024) 026002 [2302.11586].\n[49] E.R. Livine and D. Oriti, Implementing causality in the spin foam quantum geometry,\nNucl. Phys. B 663 (2003) 231 [gr-qc/0210064].\n[50] E.R. Livine and D.R. Terno, Quantum causal histories in the light of quantum\ninformation, Phys. Rev. D 75 (2007) 084001 [gr-qc/0611135].\n[51] P. Arrighi and S. Martiel, Quantum Causal Graph Dynamics, Phys. Rev. D 96 (2017)\n024026 [1607.06700].\n[52] P. Arrighi, A. Durbec and M. Wilson, Quantum networks theory, Quantum 8 (2024) 1508\n[2110.10587].\n[53] D. Deutsch, Quantum mechanics near closed timelike lines, Phys. Rev. D 44 (1991) 3197.\n[54] C. Teitelboim, Quantum Mechanics of the Gravitational Field, Phys. Rev. D 25 (1982)\n3159.\n20\n\n\n[55] J.J. Halliwell and J.B. Hartle, Wave functions constructed from an invariant sum over\nhistories satisfy constraints, Phys. Rev. D 43 (1991) 1170.\n[56] J. Engle and A. Zipfel, Lorentzian proper vertex amplitude: Classical analysis and\nquantum derivation, Phys. Rev. D 94 (2016) 064024 [1502.04640].\n[57] E. Bianchi and P. Martin-Dussaud, Causal Structure in Spin Foams, Universe 10 (2024)\n181 [2109.00986].\n[58] D. Oriti, The microscopic dynamics of quantum space as a group field theory, in\nFoundations of Space and Time: Reflections on Quantum Gravity, pp. 257–320, 10, 2011\n[1110.5606].\n[59] D. Oriti, Group Field Theory and Loop Quantum Gravity, 8, 2014 [1408.7112].\n[60] D. Oriti, Tensorial Group Field Theory condensate cosmology as an example of spacetime\nemergence in quantum gravity, 12, 2021 [2112.02585].\n[61] V. Lam and D. Oriti, The quantum gravity seeds for laws of nature, Eur. J. Phil. Sci. 14\n(2024) 63 [2404.12248].\n21\n\n\n"}
